<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/dva-c02-quiz-app/_next/static/css/31d717072bdb6d85.css" data-precedence="next"/><link rel="stylesheet" href="/dva-c02-quiz-app/_next/static/css/93628e14bbee05a5.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/dva-c02-quiz-app/_next/static/chunks/webpack-b20bf3550849aea8.js"/><script src="/dva-c02-quiz-app/_next/static/chunks/4bd1b696-65f2dd90969a3b23.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/684-499f1a03f1824ea2.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/main-app-01726a8d9af88025.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/app/layout-7d8a5f63f536cdcf.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/261-2d9b76ccba401937.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/app/quiz/%5Bid%5D/page-d7cb26530e4508b3.js" async=""></script><title>v0 App</title><meta name="description" content="Created with v0"/><meta name="generator" content="v0.app"/><script src="/dva-c02-quiz-app/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans __variable_1f39b6 __variable_c20681"><div class="min-h-screen bg-gradient-to-br from-background via-background to-primary/5 flex items-center justify-center"><div class="text-center"><div class="w-16 h-16 border-4 border-primary border-t-transparent rounded-full animate-spin mx-auto mb-4"></div><p class="text-muted-foreground">Loading quiz...</p></div></div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><script src="/dva-c02-quiz-app/_next/static/chunks/webpack-b20bf3550849aea8.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7555,[],\"\"]\n3:I[1295,[],\"\"]\n4:I[9742,[\"177\",\"static/chunks/app/layout-7d8a5f63f536cdcf.js\"],\"Analytics\"]\n6:I[9665,[],\"OutletBoundary\"]\n9:I[9665,[],\"ViewportBoundary\"]\nb:I[9665,[],\"MetadataBoundary\"]\nd:I[6614,[],\"\"]\n:HL[\"/dva-c02-quiz-app/_next/static/css/31d717072bdb6d85.css\",\"style\"]\n:HL[\"/dva-c02-quiz-app/_next/static/css/93628e14bbee05a5.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"yCqRTNLEPBdkYbRRbuCZd\",\"p\":\"/dva-c02-quiz-app\",\"c\":[\"\",\"quiz\",\"aws-developer-3\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"quiz\",{\"children\":[[\"id\",\"aws-developer-3\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/dva-c02-quiz-app/_next/static/css/31d717072bdb6d85.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/dva-c02-quiz-app/_next/static/css/93628e14bbee05a5.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"font-sans __variable_1f39b6 __variable_c20681\",\"children\":[[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"$L4\",null,{}]]}]}]]}],{\"children\":[\"quiz\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"id\",\"aws-developer-3\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",\"$undefined\",null,[\"$\",\"$L6\",null,{\"children\":[\"$L7\",\"$L8\",null]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"JI9L9eQmyh395SfvxKaiz\",{\"children\":[[\"$\",\"$L9\",null,{\"children\":\"$La\"}],null]}],[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$d\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"e:I[1184,[\"261\",\"static/chunks/261-2d9b76ccba401937.js\",\"200\",\"static/chunks/app/quiz/%5Bid%5D/page-d7cb26530e4508b3.js\"],\"default\"]\nf:T535,A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service.Amazon ECS supports the following task placement strategies:binpack place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.random place tasks randomly.spread place tasks evenly based on the specified value. Accepted values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availabilityzone. Service tasks are spread based on the tasks from that service. Standalone tasks are spread based on the tasks from the same task group.To minimize the number of instances in use, the binpack placement strategy is the best choice for this scenario.CORRECT: \"binpack\" is the correct answer.INCORRECT: \"random\" is incorrect as random places tasks randomly so this will not minimize the number of instances in use.INCORRECT: \"spread\" is incorrect as this places tasks evenly.INCORRECT: \"Canary\" is incorrect as this is a traffic shifting strategy associated with Elastic BeanstalkReferences: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskplacementstrategies.html10:T70c,AWS Secrets Manager encrypts secrets at rest using encryption keys that you own and store in AWS Key Management Service (KMS). When you retrieve a secret, Secrets Manager decrypts the secret and transmits it securely over TLS to your local environment.With AWS Secrets Manager, you can rotate secrets on a schedule or on demand by using the Secrets Manager console, AWS SDK, or AWS CLI.For example, to rotate a database password, you provide the database type, rotation frequency, and master database credentials when storing the password in Secrets Manager. Secrets "])</script><script>self.__next_f.push([1,"Manager natively supports rotating credentials for databases hosted on Amazon RDS and Amazon DocumentDB and clusters hosted on Amazon Redshift.CORRECT: \"Store a secret in AWS Secrets Manager and enable automatic rotation every 30 days\" is the correct answer.INCORRECT: \"Store a SecureString in Systems Manager Parameter Store and enable automatic rotation every 30 days\" is incorrect as SSM Parameter Store does not support automatic key rotation.INCORRECT: \"Store the connection string as an encrypted environment variable in Lambda and create a separate function that rotates the connection string every 30 days\" is incorrect as this is not the simplest solution. In this scenario using AWS Secrets Manager would be easier to implement as it provides native features for rotating the secret.INCORRECT: \"Store the connection string in an encrypted Amazon S3 bucket and use a scheduled CloudWatch Event to update the connection string every 30 days\" is incorrect. There is no native capability of CloudWatch to update connection strings so you would need some other service such as a Lambda function to execute and rotate the connection string which is missing from this answer.References: https://aws.amazon.com/secretsmanager/features/11:T555,The application currently resides in a single subnet and that is within a single availability zone. To increase fault tolerance the application instances should be split across subnets that are in different availability zones. This will protect against any faults that occur within a single AZ.To do this, a subnet in another AZ can be added to both the ASG and the ALB. The ASG will automatically launch instances in the new subnet and try and balance the number of instances between these subnets. The ALB will distribute connections across both subnets/AZs.CORRECT: \"Add a subnet in another AZ to the ASG and add the same subnet to the ALB\" is the correct answer.INCORRECT: \"Add a subnet in another region to the ASG and add the same subnet to the ALB\" is incorrect as it is not possible to add subne"])</script><script>self.__next_f.push([1,"ts in different regions to an ASG or ALB.INCORRECT: \"Add an Elastic IP to each instance and use Amazon Route 53 Alias records to distribute incoming connections\" is incorrect as this will not increase fault tolerance. The instances would still be in a single subnet/AZ.INCORRECT: \"Add a subnet in another VPC to the ASG and add the same subnet to the ALB\" is incorrect as you cannot add a subnet in another VPC to an ASG or ALB.References:https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscalingbenefits.html https://aws.amazon.com/elasticloadbalancing/12:T8ec,"])</script><script>self.__next_f.push([1,"Amazon Kinesis Data Streams (KDS) is a massively scalable and durable realtime data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and locationtracking events.Serverside encryption is a feature in Amazon Kinesis Data Streams that automatically encrypts data before it's at rest by using an AWS KMS customer master key (CMK) you specify. Data is encrypted before it's written to the Kinesis stream storage layer and decrypted after it’s retrieved from storage. As a result, your data is encrypted at rest within the Kinesis Data Streams service. This allows you to meet strict regulatory requirements and enhance the security of your data.With serverside encryption, your Kinesis stream producers and consumers don't need to manage master keys or cryptographic operations. Your data is automatically encrypted as it enters and leaves the Kinesis Data Streams service, so your data at rest is encrypted. AWS KMS provides all the master keys that are used by the serverside encryption feature. AWS KMS makes it easy to use a CMK for Kinesis that is managed by AWS, a userspecified AWS KMS CMK, or a master key imported into the AWS KMS service.Therefore, in this scenario the Developer can enable serverside encryption on Kinesis Data Streams with an AWS KMS CMKCORRECT: \"Enable serverside encryption on Kinesis Data Streams with an AWS KMS CMK\" is the correct answer.INCORRECT: \"Add a certificate and enable SSL/TLS connections to Kinesis Data Streams\" is incorrect as SSL/TLS is already used with Kinesis (you don’t need to add a certificate) and this only provides encryption intransit, not encryption at rest.INCORRECT: \"Use the Amazon Kinesis Consumer Library (KCL) to encrypt the data\" is incorrect. The KCL provides design patterns and code for Amazon Kinesis Data Streams consumer applications. The KCL is not used for adding encryption to the data in a stream.INCORRECT: \"Encrypt the data once it is at rest with an AWS Lambda function\" is incorrect as this is unnecessary when Kinesis natively supports serverside encryption.References: https://docs.aws.amazon.com/streams/latest/dev/whatissse.html"])</script><script>self.__next_f.push([1,"13:T6c7,VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. After you've created a flow log, you can retrieve and view its data in the chosen destination.Flow logs can help you with a number of tasks, such as:• Diagnosing overly restrictive security group rules• Monitoring the traffic that is reaching your instance• Determining the direction of the traffic to and from the network interfacesAs you can see in the image below, you can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored.Therefore, the Developer should create a flow log in the VPC and publish data to Amazon S3. The Developer could also choose CloudWatch Logs as a destination for publishing the data, but this is not presented as an option.CORRECT: \"Create a flow log in the VPC and publish data to Amazon S3\" is the correct answer.INCORRECT: \"Capture the information directly into Amazon CloudWatch Logs\" is incorrect as you cannot capture this information directly into CloudWatch Logs. You would need to capture with a flow log and then publish to CloudWatch Logs.INCORRECT: \"Capture the information using a Network ACL\" is incorrect as you cannot capture data using a Network ACL as it is a subnetlevel firewall.INCORRECT: \"Create a flow log in the VPC and publish data to Amazon CloudTrail\" is incorrect as you cannot publish data from a flow log to CloudTrail. Amazon CloudTrail captures information about API calls.References: https://docs.aws.amazon.com/vpc/latest/userguide/flowlogs.html14:T4ff,The primary differences between AWS SAM templates and AWS CloudFormation templates are the following:• Transform declaration. The declaration Transform: AWS::Serverless20161031 is required for AWS SAM templates.This declaration identifies an AWS CloudFormation template as an AWS SAM template."])</script><script>self.__next_f.push([1,"• Globals section. The Globals section is unique to AWS SAM. It defines properties that are common to all your serverless functions and APIs. All the AWS::Serverless::Function, AWS::Serverless::Api,and AWS::Serverless::SimpleTable resources inherit the properties that are defined in the Globals section.• Resources section. In AWS SAM templates the Resources section can contain a combination of AWS CloudFormation resources and AWS SAM resources. Of these three sections, only the Transform section and Resources sections are required; the Globals section is optional.CORRECT: \"Transform\" is the correct answer.INCORRECT: \"Globals\" is incorrect as this is not a required section.INCORRECT: \"Conditions\" is incorrect as this is an optional section.INCORRECT: \"Properties\" is incorrect as this is not a section in a template, it is used within a resource.References: https://docs.aws.amazon.com/serverlessapplicationmodel/latest/developerguide/samspecificationtemplateanatomy.html15:Tb00,"])</script><script>self.__next_f.push([1,"When you initialize a new service client without supplying any arguments, the AWS SDK for Java attempts to find AWS credentials by using the default credential provider chain implemented by the DefaultAWSCredentialsProviderChain class. The default credential provider chain looks for credentials in this order:1. Environment variables–AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. The AWS SDK for Java uses the EnvironmentVariableCredentialsProvider class to load these credentials.2. Java system properties–aws.accessKeyId and aws.secretKey. The AWS SDK for Java uses the SystemPropertiesCredentialsProvider to load these credentials.3. The default credential profiles file– typically located at ~/.aws/credentials (location can vary per platform) and shared by many of the AWS SDKs and by the AWS CLI. The AWS SDK for Java uses the ProfileCredentialsProvider toload these credentials.4. Amazon ECS container credentials– loaded from the Amazon ECS if the environment variable AWS_CONTAINER_CREDENTIALS_RELATIVE_URI is set. The AWS SDK for Java uses the ContainerCredentialsProvider to load these credentials. You can specify the IP address for this value.5. Instance profile credentials– used on EC2 instances and delivered through the Amazon EC2 metadata service. The AWS SDK for Java uses the InstanceProfileCredentialsProvider to load these credentials. You can specify the IPaddress for this value.Therefore, the AWS SDK for Java will find the credentials stored in environment variables before it checks for instance provide credentials and will allow access to the extra S3 buckets.NOTE: The Default Credential Provider Chain is very similar for other SDKs and the CLI as well. Check the references below for an article showing the steps for the AWS CLI.CORRECT: \"The AWS credential provider looks for instance profile credentials last\" is the correct answer.INCORRECT: \"An IAM inline policy is being used on the IAM role\" is incorrect. If an inline policy was also applied to the role with a less restrictive policy it wouldn’t matter, as the most restrictive policy would be applied.INCORRECT: \"An IAM managed policy is being used on the IAM role\" is incorrect. Though the managed policies are less restrictive by default (readonly or full access), this is not the most likely cause of the situation as we were told the policy is more restrictive and we know the environments variables have access keys in them which will be used before the policy is checked.INCORRECT: \"The AWS CLI is corrupt and needs to be reinstalled\" is incorrect. There is a plausible explanation for this situation so no reason to suspect a software bug is to blame.References:https://docs.aws.amazon.com/sdkforjava/v1/developerguide/credentials.html https://docs.aws.amazon.com/cli/latest/userguide/clichapconfigure.html"])</script><script>self.__next_f.push([1,"16:T7c5,The first time you invoke your function, AWS Lambda creates an instance of the function and runs its handler method to process the event. When the function returns a response, it stays active and waits to process additional events. If you invoke the function again while the first event is being processed, Lambda initializes another instance, and the function processes the two events concurrently. Your functions' concurrency is the number of instances that serve requests at a given time. For an initial burst of traffic, your functions' cumulative concurrency in a Region can reach an initial level of between 500 and 3000, which varies per Region. Burst Concurrency Limits: \u0026bull; 3000 - US West (Oregon), US East (N. Virginia), Europe (Ireland). \u0026bull; 1000 - Asia Pacific (Tokyo), Europe (Frankfurt). \u0026bull; 500 - Other Regions. After the initial burst, your functions' concurrency can scale by an additional 500 instances each minute. This continues until there are enough instances to serve all requests, or until a concurrency limit is reached. The default account limit is up to 1000 executions per second, per region (can be increased). It is therefore most likely that the concurrency execution limit for the account was exceeded. CORRECT: \"The concurrency execution limit for the account has been exceeded\" is the correct answer. INCORRECT: \"Amazon S3 could not handle the sudden burst in traffic\" is incorrect as S3 can easily achieve thousands of transactions per second and automatically scales to high request rates. INCORRECT: \"Lambda cannot process multiple files simultaneously\" is incorrect as Lambda can run multiple executions concurrently as explained above. INCORRECT: \"The event source mapping has not been configured\" is incorrect as the solution was working well until that large number of files were uploaded. If the event source mapping was not configured it would not have worked at all. References: https://docs.aws.amazon.com/lambda/latest/dg/scaling.html17:T7cf,You can use Amazon Kinesis Data Streams to "])</script><script>self.__next_f.push([1,"collect and process large streams of data records in real time. You can create data processing applications, known as Kinesis Data Streams applications. A typical Kinesis Data Streams application reads data from a data stream as data records.These applications can use the Kinesis Client Library, and they can run on Amazon EC2 instances. You can send the processed records to dashboards, use them to generate alerts, dynamically change pricing and advertising strategies, or send data to a variety of other AWS services.This scenario is an ideal use case for Kinesis Data Streams as large volumes of real time streaming data are being ingested.Therefore, the best approach is to use Amazon Kinesis Data Streams with Kinesis Client Library to ingest and deliver messagesCORRECT: \"Use Amazon Kinesis Data Streams with Kinesis Client Library to ingest and deliver messages\" is the correct answer.INCORRECT: \"Send the messages to an Amazon SQS queue, then process the messages by using a fleet of Amazon EC2 instances\" is incorrect as this is not an ideal use case for SQS because SQS is used for decoupling application components, not for ingesting streaming data. It would require more cost (lots of instances to process data) and introduce latency. Also, the message size limitations could be an issue.INCORRECT: \"Use the Amazon S3 API to write messages to an S3 bucket, then process the messages by using Amazon RedShift\" is incorrect as RedShift does not process messages from S3. RedShift is a data warehouse which is used for analytics.INCORRECT: \"Use AWS Data Pipeline to automate the movement and transformation of data\" is incorrect as the question is not asking for transformation of data. The scenario calls for a solution for ingesting and processing the real time streaming data for analytics and feeding some data into a system that generates an operational dashboard.References: https://docs.aws.amazon.com/streams/latest/dev/introduction.html18:T6e4,After the CloudWatch Logs agent begins publishing log data to Amazon CloudWatch, you"])</script><script>self.__next_f.push([1," can begin searching and filtering the log data by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs.CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on. You can use any type of CloudWatch statistic, including percentile statistics, when viewing these metrics or setting alarms.Filters do not retroactively filter data. Filters only publish the metric data points for events that happen after the filter was created. Filtered results return the first 50 lines, which will not be displayed if the timestamp on the filtered results is earlier than the metric creation time.Therefore, the filtered results are not being returned as CloudWatch Logs only publishes metric data for events that happen after the filter is created.CORRECT: \"CloudWatch Logs only publishes metric data for events that happen after the filter is created\" is the correct answer.INCORRECT: \"A setup of the Amazon CloudWatch interface VPC endpoint is required for filtering the CloudWatch Logs in the VPC\" is incorrect as a VPC endpoint is not required.INCORRECT: \"The log group for CloudWatch Logs should be first streamed to Amazon Elasticsearch Service before filtering returns the results\" is incorrect as you do not need to stream the results to Elasticsearch.INCORRECT: \"Metric data points to logs groups can be filtered only after they are exported to an Amazon S3 bucket\" is incorrect as it is not necessary to export the logs to an S3 bucket.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html19:T6bb,Amazon DynamoDB is a keyvalue and document database that delivers singledigit millisecond performance at any scale. Optimistic locking is a strategy to ensure that the clientside item that you are updating (or deleting) is the same as the item in Amazon DynamoDB. If you use this strategy, your database writes are protected from being overwritten by the write"])</script><script>self.__next_f.push([1,"s of others, and vice versa.With optimistic locking, each item has an attribute that acts as a version number. If you retrieve an item from a table, the application records the version number of that item. You can update the item, but only if the version number on the server side has not changed. If there is a version mismatch, it means that someone else has modified the item before you did.In the diagram below, the application on the left updates an item and increments the version number. Then, the application on the right attempts to update the item but only if the version number is 1.The update attempt fails, because the application has a stale version of the item. Optimistic locking prevents you from accidentally overwriting changes that were made by others. It also prevents others from accidentally overwriting your changes.CORRECT: \"Amazon DynamoDB\" is the correct answer.INCORRECT: \"Amazon RDS\" is incorrect as RDS is not a key/value database, nor does it support optimistic locking.INCORRECT: \"Amazon RedShift\" is incorrect as RedShift is not a key/value database, nor does it support optimistic locking.INCORRECT: \"Amazon S3\" is incorrect as though it does store objects as key/value pairs it does not support optimistic locking.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html1a:T4a3,A namespace is a container for CloudWatch metrics. Metrics in different namespaces are isolated from each other, so that metrics from different applications are not mistakenly aggregated into the same statistics.Therefore, the Developer should create a custom namespace with a unique metric name for each application. This namespace will then allow the metrics for each individual application to be shown in a single view through CloudWatch.CORRECT: \"Create a custom namespace with a unique metric name for each application\" is the correct answer.INCORRECT: \"Create a custom dimension with a unique metric name for each application\" is incorrect as a dimension further clarifie"])</script><script>self.__next_f.push([1,"s what a metric is and what data it stores.INCORRECT: \"Create a custom event with a unique metric name for each application\" is incorrect as an event is not used to organize metrics for display.INCORRECT: \"Create a custom alarm with a unique metric name for each application\" is incorrect as alarms are used to trigger actions when a threshold is reached, this is not relevant to organizing metrics for display.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html1b:T8ad,"])</script><script>self.__next_f.push([1,"A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign into your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Google, Facebook, Amazon, or Apple, and through SAML identity providers. Whether your users sign in directly or through a third party, all members of the user pool have a directory profile that you can access through a Software Development Kit (SDK).As an alternative to using IAM roles and policies or Lambda authorizers (formerly known as custom authorizers), you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway.To use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user in to the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn't authorized to make the call because the client did not have credentials that could be authorized.CORRECT: \"Create a COGNITO_USER_POOLS authorizer in API Gateway\" is the correct answer.INCORRECT: \"Create a COGNITO_IDENTITY_POOLS authorizer in API Gateway\" is incorrect as you should use a Cognito user pool for creating an authorizer in API Gateway.INCORRECT: \"Create a Lambda authorizer in API Gateway\" is incorrect as this is a mobile application and so the best solution is to use Cognito which is designed for this purpose.INCORRECT: \"Create an IAM authorizer in API Gateway\" is incorrect as there’s no such thing as an IAM authorizer. You can use IAM roles and policies but then you would need your users to have accounts in IAM. For a mobile application your users are better located in a Cognito user pool.References:https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayintegratewithcognito.html https://docs.aws.amazon.com/cognito/latest/developerguide/cognitouseridentitypools.html"])</script><script>self.__next_f.push([1,"1c:T5c9,Amazon ElastiCache provides fully managed implementations of two popular inmemory data stores – Redis and Memcached. ElastiCache is a web service that makes it easy to deploy and run Memcached or Redis protocol compliant server nodes in the cloud.The inmemory caching provided by ElastiCache can be used to significantly improve latency and throughput for many readheavy application workloads or computeintensive workloads. It is common to use ElastiCache as a cache in front of databases such as Amazon RDS.The two implementations, Memcached, and Redis, each offer different capabilities and limitations. As you can see from the table below, only Redis supports read replicas and autofailover:The Redis implementation must be used if high availability is required, as is necessary for this scenario. Therefore the correct answer is to use Amazon ElastiCache Redis.CORRECT: \"Implement Amazon ElastiCache Redis\" is the correct answer.INCORRECT: \"Implement Amazon ElastiCache Memcached\" is incorrect as Memcached does not offer read replicas or autofailover and therefore cannot provide high availability.INCORRECT: \"Migrate the database to Amazon RedShift\" is incorrect as RedShift is a data warehouse for use in online analytics processing (OLAP) use cases. It is not suitable to be used as a caching layer.INCORRECT: \"Implement Amazon DynamoDB DAX\" is incorrect as DAX is used in front of DynamoDB, not Amazon RDS.References: https://aws.amazon.com/elasticache/redisvsmemcached/1d:T63a,Amazon SNS works closely with Amazon Simple Queue Service (Amazon SQS). Both services provide different benefits for developers. Amazon SNS allows applications to send timecritical messages to multiple subscribers through a “push” mechanism, eliminating the need to periodically check or “poll” for updates.When you subscribe an Amazon SQS queue to an Amazon SNS topic, you can publish a message to the topic and Amazon SNS sends an Amazon SQS message to the subscribed queue. The Amazon SQS message contains the subject and message that were "])</script><script>self.__next_f.push([1,"published to the topic along with metadata about the message in a JSON document.CORRECT: \"Publish the messages to an Amazon SNS topic and subscribe each SQS queue to the topic\" is the correct answer.INCORRECT: \"Publish the messages to an Amazon SQS queue and configure an AWS Lambda function to duplicate the message into multiple queues\" is incorrect as this seems like an inefficient solution. By using SNS we can eliminate the initial queue and Lambda function.INCORRECT: \"Create an Amazon SWF workflow that receives the messages and pushes them to multiple SQS queues\" is incorrect as this is not a workable solution. Amazon SWF is not suitable for pushing messages to SQS queues.INCORRECT: Create and AWS Step Functions state machine that uses multiple Lambda functions to process and push the messages into multiple SQS queues\"\" is incorrect as this is an inefficient solution and there is not mention on how the functions will be invoked with the message dataReferences: https://docs.aws.amazon.com/sns/latest/dg/snssqsassubscriber.html1e:T642,In AWS CodePipeline, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.If the action is approved, the pipeline execution resumes. If the action is rejected—or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping—the result is the same as an action failing, and the pipeline execution does not continue.In this scenario, the manual approval stage would be placed in the pipeline before the deployment stage that deploys the application update into production:Therefore, the best answer is to use an approval action in a stage before deployment to productionCORRECT: \"Use an approval action in a stage before deployment\" is the correct answer.INCORRECT: \"Use an Amazon SNS notification from the deployment stage\" is incorrect as this would send a notification w"])</script><script>self.__next_f.push([1,"hen the actual deployment is already occurring.INCORRECT: \"Disable the stage transition to allow manual approval\" is incorrect as this requires manual intervention as could be easily missed and allow the deployment to continue.INCORRECT: \"Disable a stage just prior the deployment stage\" is incorrect as disabling the stage prior would prevent that stage from running, which may be necessary (could be the build / test stage). It is better to use an approval action in a stage in the pipeline before the deployment occursReferences: https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html1f:T74f,When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn't automatically delete the message. Because Amazon SQS is a distributed system, there's no guarantee that the consumer actually receives the message (for example, due to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must delete the message from the queue after receiving and processing it. Immediately after a message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds.The maximum is 12 hours.Therefore, the best thing the Developer can do in this situation is to increase the VisibilityTimeout API action on the queueCORRECT: \"Increase the VisibilityTimeout API action on the queue\" is the correct answer.INCORRECT: \"Increase the DelaySeconds API action on the queue\" is incorrect as this controls the length of time, in seconds, for which the delivery of all messages in the queue is delayed.INCORRECT: \"Increase the ReceiveMessageWaitTimeSeconds API action on the queue\" is incorrect as this is the length of time, in seconds, for which a ReceiveMessage action waits for a message to arrive. T"])</script><script>self.__next_f.push([1,"his is used to configure long polling.INCORRECT: \"Create a RedrivePolicy for the queue\" is incorrect as this is a string that includes the parameters for the dead letter queue functionality of the source queue as a JSON object.References:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsvisibilitytimeout.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html20:T5f7,A segment document conveys information about a segment to XRay. A segment document can be up to 64 kB and contain a whole segment with subsegments, a fragment of a segment that indicates that a request is in progress, or a single subsegment that is sent separately. You can send segment documents directly to XRay by using the PutTraceSegments API.Example minimally complete segment:{\"name\" : \"example.com\",\"id\" : \"70de5b6f19ff9a0a\",\"start_time\" : 1.478293361271E9,\"trace_id\" : \"1581cf771a006649127e371903a2de979\",\"end_time\" : 1.478293361449E9}A subset of segment fields are indexed by XRay for use with filter expressions. For example, if you set the user field on a segment to a unique identifier, you can search for segments associated with specific users in the XRay console or by using the GetTraceSummaries API.CORRECT: \"By using the GetTraceSummaries API with a filter expression\" is the correct answer.INCORRECT: \"By using the GetTraceGraph API with a filter expression\" is incorrect as this API action retrieves a service graph for one or more specific trace IDs.INCORRECT: \"Use a filter expression to search for the user field in the segment metadata\" is incorrect as the user field is not part of the segment metadata and metadata is not is not indexed for search.INCORRECT: \"Use a filter expression to search for the user field in the segment annotations\" is incorrect as the user field is not part of the segment annotations.References: https://docs.aws.amazon.com/xray/latest/devguide/xrayapisegmentdocuments.html21:T76c,The template shown is an AWS SAM template for deploying a serverless "])</script><script>self.__next_f.push([1,"application. This can be identified by the template header: Transform: 'AWS::Serverless20161031'The Developer will need to package and then deploy the template. To do this the source code must be available in the same directory or referenced using the “codeuri” parameter. Then, the Developer can use the “aws cloudformation package” or “sam package” commands to prepare the local artifacts (local paths) that your AWS CloudFormation template references.The command uploads local artifacts, such as source code for an AWS Lambda function or a Swagger file for an AWS API Gateway REST API, to an S3 bucket. The command returns a copy of your template, replacing references to local artifacts with the S3 location where the command uploaded the artifacts.Once that is complete the template can be deployed using the “aws cloudformation deploy” or “sam deploy” commands.Therefore, the developer has two options to prepare and then deploy this package:1. Run aws cloudformation package and then aws cloudformation deploy2. Run sam package and then sam deployCORRECT: \"Run aws cloudformation package and then aws cloudformation deploy\" is a correct answer.INCORRECT: \"Run sam package and then sam deploy\" is also a correct answer.INCORRECT: \"Run aws cloudformation compile and then aws cloudformation deploy\" is incorrect as the “compile” command should be replaced with the “package” command.INCORRECT: \"Run sam build and then sam package\" is incorrect as the Developer needs to run the “package” command first and then the “deploy” command to actually deploy the function.INCORRECT: \"Run aws serverless package and then aws serverless deploy\" is incorrect as there is no AWS CLI command named “serverless”.References: https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html22:T675,As this is a stateful application the session data needs to be stored somewhere. Amazon DynamoDB is designed to be used for storing session data and it highly scalable. To add elasticity to the architecture an"])</script><script>self.__next_f.push([1," Amazon Elastic Load Balancer (ELB) and Amazon EC2 Auto Scaling group (ASG) can be used.With this architecture the web service can scale elastically using the ASG and the ELB will distribute traffic to all new instances that the ASG launches. This is a good example of utilizing some of the key benefits of refactoring applications into the AWS cloud.CORRECT: \"Use an Elastic Load Balancer and Auto Scaling Group\" is a correct answer.CORRECT: \"Store the session state in an Amazon DynamoDB table\" is also a correct answer.INCORRECT: \"Use Amazon CloudFormation and the Serverless Application Model\" is incorrect. AWS SAM is used in CloudFormation templates for expressing serverless applications using a simplified syntax. This application is not a serverless application.INCORRECT: \"Use Amazon CloudFront with a Web Application Firewall\" is incorrect neither protection from web exploits nor improved performance for content delivery are requirements in this scenario.INCORRECT: \"Store the session state in an Amazon RDS database\" is incorrect as RDS is not suitable for storing session state data. DynamoDB is a better fit for this use case.References: https://docs.aws.amazon.com/awssdkphp/v2/guide/featuredynamodbsessionhandler.html https://digitalcloud.training/certificationtraining/awsdeveloperassociate/awscompute/amazonec2autoscaling/ https://digitalcloud.training/certificationtraining/awsdeveloperassociate/awsdatabase/amazondynamodb/23:T557,AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. It is highly scalable, available, and durable.You can store values as plaintext (unencrypted data) or ciphertext (encrypted data). You can then reference values by using the unique name that you specified when you created the parameter.There are no additional charges for using SSM Parameter Store. However, there are limit of 10,000 parameters per accountCORRECT: \"AWS"])</script><script>self.__next_f.push([1," Systems Manager Parameter Store\" is the correct answer.INCORRECT: \"AWS IAM with the Security Token Service (STS)\" is incorrect as the application is using credentials to connect, it is not using IAM.INCORRECT: \"AWS Secrets Manager\" is incorrect as it is not the lowest cost solution as it is a chargeable service. Secrets Manager performs native key rotation; however, this isn’t required in this scenario as the application is handling credential rotation.INCORRECT: \"AWS Key Management Service (KMS)\" is incorrect as this service is involved with encryption keys, it is not used for storing credentials. You can however encrypt you credentials in SSM using KMS.References: https://docs.aws.amazon.com/systemsmanager/latest/userguide/systemsmanagerparameterstore.html24:T5b0,In Amazon Route 53 when you create an A record you must supply an IP address for the resource to connect to. For a public hosted zone this must be a public IP address.There are three types of IP address that can be assigned to an Amazon EC2 instance:• Public – public address that is assigned automatically to instances in public subnets and reassigned if instance is stopped/started.• Private – private address assigned automatically to all instances.• Elastic IP – public address that is static.To ensure ongoing connectivity the Developer needs to use an Elastic IP address for the EC2 instance and DNS A record as this is the only type of static, public IP address you can assign to an Amazon EC2 instance.CORRECT: \"Elastic IP address\" is the correct answer.INCORRECT: \"Public IP address\" is incorrect as though this is a public IP address, it is not static and will change every time the EC2 instance restarts. Therefore, connectivity would be lost until you update the Route 53 A record.INCORRECT: \"Dynamic IP address\" is incorrect as a dynamic IP address is an IP address that will change over time. For this scenario a static, public address is required.INCORRECT: \"Private IP address\" is incorrect as a public IP address is required for the public "])</script><script>self.__next_f.push([1,"DNS A record.References:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/usinginstanceaddressing.html https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/ResourceRecordTypes.html25:T9cd,"])</script><script>self.__next_f.push([1,"With provisioned capacity mode, you specify the number of data reads and writes per second that you require for your application.Read capacity unit (RCU):• Each API call to read data from your table is a read request.• Read requests can be strongly consistent, eventually consistent, or transactional.• For items up to 4 KB in size, one RCU can perform one strongly consistent read request per second.• Items larger than 4 KB require additional RCUs.• For items up to 4 KB in size, one RCU can perform two eventually consistent read requests per second.• Transactional read requests require two RCUs to perform one read per second for items up to 4 KB.• For example, a strongly consistent read of an 8 KB item would require two RCUs, an eventually consistent read of an 8 KB item would require one RCU, and a transactional read of an 8 KB item would require four RCUs.Write capacity unit (WCU):• Each API call to write data to your table is a write request.• For items up to 1 KB in size, one WCU can perform one standard write request per second.• Items larger than 1 KB require additional WCUs.• Transactional write requests require two WCUs to perform one write per second for items up to 1 KB.• For example, a standard write request of a 1 KB item would require one WCU, a standard write request of a 3 KB item would require three WCUs, and a transactional write request of a 3 KB item would require six WCUs.To determine the number of RCUs required to handle 3 strongly consistent reads per/second with an average item size of 7KB, perform the following steps:1. Determine the average item size by rounding up the next multiple of 4KB (7KB rounds up to 8KB).2. Determine the RCU per item by dividing the item size by 4KB (8KB/4KB = 2).3. Multiply the value from step 2 with the number of reads required per second (2x3 = 6).To determine the number of WCUs required to handle 1 standard write per/second, simply multiply the average item size by the number of writes required (7x1=7).CORRECT: \"6 RCU and 7 WCU\" is the correct answer.INCORRECT: \"3 RCU and 7 WCU\" is incorrect. This would be the correct answer for eventual consistent reads and standard writes.INCORRECT: \"6 RCU and 14 WCU\" is incorrect. This would be the correct answer for strongly consistent reads and transactional writes.INCORRECT: \"12 RCU and 14 WCU\" is incorrect. This would be the correct answer for transactional reads and transactional writesReferences: https://aws.amazon.com/dynamodb/pricing/provisioned/"])</script><script>self.__next_f.push([1,"26:T4c9,To add a role to an Amazon EC2 instance using the AWS CLI you must first create an instance profile. Then you need to add the role to the instance profile and finally assign the instance profile to the Amazon EC2 instance.The following example commands would achieve this outcome:1. aws iam createinstanceprofile instanceprofilename EXAMPLEPROFILENAME2. aws iam addroletoinstanceprofile instanceprofilename EXAMPLEPROFILENAME rolenameEXAMPLEROLENAME3. aws ec2 associateiaminstanceprofile iaminstanceprofile Name=EXAMPLEPROFILENAME instanceid i012345678910abcdeCORRECT: \"Run the aws iam createinstanceprofile command\" is a correct answer.CORRECT: \"Run the aws iam addroletoinstanceprofile command\" is a correct answer.CORRECT: \"Run the aws ec2 associateinstanceprofile command\" is a correct answer.INCORRECT: \"Run the CreateInstanceProfile API\" is incorrect as this is an API action, not an AWS CLI command.INCORRECT: \"Run the AddRoleToInstanceProfile API\" is incorrect as this is an API action, not an AWS CLI command.INCORRECT: \"Run the AssignInstanceProfile API\" is incorrect as this is an API action, not an AWS CLI command.References: https://aws.amazon.com/premiumsupport/knowledgecenter/attachreplaceec2instanceprofile/27:T9c9,"])</script><script>self.__next_f.push([1,"For web distributions, you can configure CloudFront to require that viewers use HTTPS to request your objects, so that connections are encrypted when CloudFront communicates with viewers. You also can configure CloudFront to use HTTPS to get objects from your origin, so that connections are encrypted when CloudFront communicates with your origin. If you configure CloudFront to require HTTPS both to communicate with viewers and to communicate with your origin, here's what happens when CloudFront receives a request for an object:1. A viewer submits an HTTPS request to CloudFront. There's some SSL/TLS negotiation here between the viewer and CloudFront. In the end, the viewer submits the request in an encrypted format.2. If the object is in the CloudFront edge cache, CloudFront encrypts the response and returns it to the viewer, and the viewer decrypts it.3. If the object is not in the CloudFront cache, CloudFront performs SSL/TLS negotiation with your origin and, when the negotiation is complete, forwards the request to your origin in an encrypted format.4. Your origin decrypts the request, encrypts the requested object, and returns the object to CloudFront.5. CloudFront decrypts the response, reencrypts it, and forwards the object to the viewer. CloudFront also saves the object in the edge cache so that the object is available the next time it's requested.6. The viewer decrypts the response.To enable SSL between the origin and the distribution the Developer can configure the Origin Protocol Policy. Depending on the domain name used (CloudFront default or custom), the steps are different. To enable SSL between the enduser and CloudFront the Viewer Protocol Policy should be configured.CORRECT: \"Configure the Origin Protocol Policy\" is a correct answer.CORRECT: \"Configure the Viewer Protocol Policy\" is also a correct answer.INCORRECT: \"Create an Origin Access Identity (OAI)\" is incorrect as this is a special user used for securing objects in Amazon S3 origins.INCORRECT: \"Add a certificate to the Auto Scaling Group\" is incorrect as you do not add certificates to an ASG. The certificate should be located on the ALB listener in this scenario.INCORRECT: \"Create an encrypted distribution\" is incorrect as there’s no such thing as an encrypted distributionReferences:https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/usinghttpsviewerstocloudfront.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/usinghttpscloudfronttocustomorigin.html"])</script><script>self.__next_f.push([1,"28:T6d6,AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments.All at once:• Deploys the new version to all instances simultaneously.Rolling:• Update a few instances at a time (bucket), and then move onto the next bucket once the first bucket is healthy (downtime for 1 bucket at a time).Rolling with additional batch:• Like Rolling but launches new instances in a batch ensuring that there is full availability.Immutable:• Launches new instances in a new ASG and deploys the version update to these instances before swapping traffic to these instances once healthy.• Zero downtime.Blue / Green deployment:• Zero downtime and release facility.• Create a new “stage” environment and deploy updates there.The immutable and blue/green options both provide zero downtime as they will deploy the new version to a new version of the application. These are also the only two options that will ONLY deploy the updates to new EC2 instances.CORRECT: \"Immutable\" is the correct answer.CORRECT: \"Blue/green\" is the correct answer.INCORRECT: \"Allatonce\" is incorrect as this will deploy the updates to existing instances.INCORRECT: \"Rolling\" is incorrect as this will deploy the updates to existing instances.INCORRECT: \"Rolling with additional batch\" is incorrect as this will launch new instances but will also update the existing instances as well (which is not allowed according to the requirements).References:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.rollingversiondeploy.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html29:T5b9,Amazon Cognito lets you save end user data in datasets containing keyvalue pairs. This data is associated with an Amazon Cognito identity, so that it can be accessed across logins and devices. To sync this data between the Amazon Cognito service and an end user’s devices, invoke "])</script><script>self.__next_f.push([1,"the synchronize method. Each dataset can have a maximum size of 1 MB. You can associate up to 20 datasets with an identity.The Amazon Cognito Sync client creates a local cache for the identity data. Your app talks to this local cache when it reads and writes keys. This guarantees that all of your changes made on the device are immediately available on the device, even when you are offline. When the synchronize method is called, changes from the service are pulled to the device, and any local changes are pushed to the service. At this point the changes are available to other devices to synchronize.CORRECT: \"Amazon Cognito\" is the correct answer.INCORRECT: \"AWS Lambda\" is incorrect. AWS Lambda provides serverless functions that run your code, it is not used for mobile client data synchronization.INCORRECT: \"Amazon API Gateway\" is incorrect as API Gateway provides APIs for traffic coming into AWS. It is not used for mobile client data synchronization.INCORRECT: \"Amazon DynamoDB\" is incorrect as DynamoDB is a NoSQL database. It is not used for mobile client data synchronization.References: https://docs.aws.amazon.com/cognito/latest/developerguide/synchronizingdata.html2a:T521,AWS WAF is a web application firewall service that helps protect your web apps from common exploits that could affect app availability, compromise security, or consume excessive resources.AWS WAF helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection and crosssite scripting.AWS WAF gives you control over which traffic to allow or block to your web applications by defining customizable web security rules.CORRECT: \"AWS Web Application Firewall (WAF)\" is the correct answer.INCORRECT: \"AWS CloudFront\" is incorrect. CloudFront does provide DDoS attack protection (through AWS Shield), however it is primarily a content delivery network (CDN) so you woul"])</script><script>self.__next_f.push([1,"dn’t put it infront of a web application unless you wanted it to cache your content. i.e. its primary use case would not be protection from Internet threats.INCORRECT: \"Amazon Cognito\" is incorrect as this is a service for providing signup and signin capabilities to mobile applications.INCORRECT: \"AWS CloudHSM\" is incorrect as this is a service that is used for storing cryptographic keys using a hardware device.References: https://aws.amazon.com/waf/2b:T5f7,The GenerateDataKey API is used with the AWS KMS services and generates a unique symmetric data key. This operation returns a plaintext copy of the data key and a copy that is encrypted under a customer master key (CMK) that you specify. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data.For this scenario we can use GenerateDataKey to obtain an encryption key from KMS that we can then use within the function code to encrypt the file. This ensures that the file is encrypted BEFORE it is uploaded to Amazon S3.CORRECT: \"Use the GenerateDataKey API, then use the data key to encrypt the file using the Lambda code\" is the correct answer.INCORRECT: \"Enable serverside encryption on the S3 bucket and create a policy to enforce encryption\" is incorrect. This would not encrypt data before it is uploaded as S3 would only encrypt the data as it is written to storage.INCORRECT: \"Use the S3 managed key and call the GenerateDataKey API to encrypt the file\" is incorrect as you do not use an encryption key to call KMS. You call KMS with the GenerateDataKey API to obtain an encryption key. Also, the S3 managed key can only be used within the S3 service.INCORRECT: \"Use the default KMS key for S3 and encrypt the file using the Lambda code\" is incorrect. You cannot use the default KMS key for S3 within the Lambda code as it can only be used within the S3 service.References: https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html2c:Tadf,"])</script><script>self.__next_f.push([1,"DynamoDB stores data as groups of attributes, known as items. Items are similar to rows or records in other database systems. DynamoDB stores and retrieves each item based on the primary key value, which must be unique.Items are distributed across 10GB storage units, called partitions (physical storage internal to DynamoDB). Each table has one or more partitions, as shown in the following illustration.DynamoDB uses the partition key’s value as an input to an internal hash function. The output from the hash function determines the partition in which the item is stored. Each item’s location is determined by the hash value of its partition key.All items with the same partition key are stored together, and for composite partition keys, are ordered by the sort key value.DynamoDB splits partitions by sort key if the collection size grows bigger than 10 GB.DynamoDB evenly distributes provisioned throughput—read capacity units (RCUs) and write capacity units (WCUs)—among partitions and automatically supports your access patterns using the throughput you have provisioned. However, if your access pattern exceeds 3000 RCU or 1000 WCU for a single partition key value, your requests might be throttled with a ProvisionedThroughputExceededException error.To avoid request throttling, design your DynamoDB table with the right partition key to meet your access requirements and provide even distribution of data. Recommendations for doing this include the following:• Use high cardinality attributes (e.g. email_id, employee_no, customer_id etc.)• Use composite attributes• Cache popular items• Add random numbers or digits from a predetermined range for writeheavy use casesIn this case there is a hot partition due to the order date being used as the partition key and this is causing writes to be throttled. Therefore, the best solution to ensure the writes are more evenly distributed in this scenario is to add a random number suffix to the partition key values.CORRECT: \"Add a random number suffix to the partition key values\" is the correct answer.INCORRECT: \"Increase the read and write capacity units for the table\" is incorrect as this will not solve the hot partition issue and we know that the consumed throughput is lower than provisioned throughput.INCORRECT: \"Add a global secondary index to the table\" is incorrect as a GSI is used for querying data more efficiently, it will not solve the problem of write performance due to a hot partition.INCORRECT: \"Use an Amazon SQS queue to buffer the incoming writes\" is incorrect as this is not the lowest cost option. You would need to have producers and consumers of the queue as well as paying for the queue itself.References: https://aws.amazon.com/blogs/database/choosingtherightdynamodbpartitionkey/"])</script><script>self.__next_f.push([1,"2d:T410,Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.These services both enable asynchronous message passing in the form of a message bus (SQS) and notifications (SNS).CORRECT: \"Amazon SQS\" is the correct answer.CORRECT: \"Amazon SNS\" is also a correct answer.INCORRECT: \"Amazon Kinesis\" is incorrect. Kinesis is used for streaming data, it is used for realtime analytics, mobile data capture and IoT and similar use cases.INCORRECT: \"Amazon ECS\" is incorrect. ECS is a service providing Docker containers on Amazon EC2.INCORRECT: \"AWS Lambda\" is incorrect. AWS Lambda is a compute service that runs functions in response to triggers.References:https://aws.amazon.com/sqs/ https://aws.amazon.com/sns/2e:T446,AWS CodeCommit is a version control service hosted by Amazon Web Services that you can use to privately store and manage assets (such as documents, source code, and binary files) in the cloud.CodeCommit is optimized for team software development. It manages batches of changes across multiple files, which can occur in parallel with changes made by other developers.CORRECT: \"AWS CodeCommit\" is the correct answer.INCORRECT: \"AWS CodeBuild\" is incorrect as it is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy.INCORRECT: \"AWS CodePipeline\" is incorrect as it is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.INCORRECT: \"Amazon S3\" is incorrect. Amazon S3 versioning supports the recovery of past versions of files, but it's not focused on collaborative file tracking features that software d"])</script><script>self.__next_f.push([1,"evelopment teams need.References: https://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html2f:T611,Crossorigin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich clientside web applications with Amazon S3 and selectively allow crossorigin access to your Amazon S3 resources.To configure your bucket to allow crossorigin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support for each origin, and other operationspecific information.In this case, you would apply the CORS configuration to the dctlabsimages bucket so that it will allow GET requests from the dctlabs.com origin.CORRECT: \"Cross Origin Resource Sharing is not enabled on the dctlabsimages bucket\" is the correct answer.INCORRECT: \"Cross Origin Resource Sharing is not enabled on the dctlabs.com bucket\" is incorrect as in this case the images that are being blocked are located in the dctlabsimages bucket. You need to apply the CORS configuration to the dctlabsimages bucket so it allows requests from the dctlabs.com origin.INCORRECT: \"The dctlabsimages bucket is not in the same region as the dctlabs.com bucket\" is incorrect as it doesn’t matter what regions the buckets are in.INCORRECT: \"Amazon S3 Transfer Acceleration should be enabled on the dctlabs.com bucket\" is incorrect as this feature of Amazon S3 is used to speed uploads to S3.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html30:T596,CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, onpremises instances, serverless Lambda functions, or Amazon ECS services.CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function."])</script><script>self.__next_f.push([1," You do not need to make changes to your existing code before you can use CodeDeploy.The image below shows the flow of a typical CodeDeploy inplace deployment.The above deployment could also be directed at onpremises servers. Therefore, the best answer is to use AWS CodeDeploy to deploy the software package to both EC2 instances and onpremises virtual servers.CORRECT: \"AWS CodeDeploy\" is the correct answer.INCORRECT: \"AWS CodePipeline\" is incorrect. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. You can use CodeDeploy in a CodePipeline pipeline however it is actually CodeDeploy that deploys the software packages.INCORRECT: \"AWS CloudBuild\" is incorrect as this is a build tool, not a deployment tool.INCORRECT: \"AWS Elastic Beanstalk\" is incorrect as you cannot deploy software packages to onpremise virtual servers using Elastic BeanstalkReferences: https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html31:T82f,"])</script><script>self.__next_f.push([1,"A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API.A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity.When a client makes a request to one of your API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output.There are two types of Lambda authorizers:• A tokenbased Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token.• A request parameterbased Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of headers, query string parameters, stageVariables, and $context variables.• For WebSocket APIs, only request parameterbased authorizers are supported.In this scenario, the authentication is using headers in the request and therefore the request parameterbased Lambda authorizer should be used.CORRECT: \"Implement an AWS Lambda authorizer that references the DynamoDB authentication table\" is the correct answer.INCORRECT: \"Create a model that requires the credentials, then grant API Gateway access to the authentication table\" is incorrect as a model defines the structure of the incoming payload using the JSON Schema.INCORRECT: \"Modify the integration requests to require the credentials, then grant API Gateway access to the authentication table\" is incorrect as API Gateway will not authorize directly using the table information, an authorizer should be used.INCORRECT: \"Implement an Amazon Cognito authorizer that references the DynamoDB authentication table\" is incorrect as a Lambda authorizer should be used in this example as the authentication data is being passed in request headers.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayuselambdaauthorizer.html"])</script><script>self.__next_f.push([1,"32:T57a,Amazon Cognito Identity Pools can support unauthenticated identities by providing a unique identifier and AWS credentials for users who do not authenticate with an identity provider. If your application allows users who do not log in, you can enable access for unauthenticated identities.This is the most efficient and secure way to allow unauthenticated access as the process to set it up is simple and the IAM role can be configured with permissions allowing only the access permitted for unauthenticated users.CORRECT: \"Use Amazon Cognito to associate unauthenticated users with an IAM role that has limited access to resources\" is the correct answer.INCORRECT: \"Use an IAM SAML 2.0 identity provider to establish trust\" is incorrect as we need to allow unauthenticated users access to the AWS resources, not those who have been authenticated elsewhere (i.e. Active Directory).INCORRECT: \"Use Amazon Cognito Federated Identities and setup authentication using a Cognito User Pool\" is incorrect as we need to setup unauthenticated access, not authenticated access through a user pool.INCORRECT: \"Embed access keys in the application that have limited access to resources\" is incorrect. We should try and avoid embedding access keys in application code, it is better to use the builtin features of Amazon Cognito.References: https://docs.aws.amazon.com/cognito/latest/developerguide/identitypools.html33:T771,Cryptographic best practices discourage extensive reuse of encryption keys. To create new cryptographic material for your AWS Key Management Service (AWS KMS) customer master keys (CMKs), you can create new CMKs, and then change your applications or aliases to use the new CMKs. Or, you can enable automatic key rotation for an existing customer managed CMK.When you enable automatic key rotation for a customer managed CMK, AWS KMS generates new cryptographic material for the CMK every year. AWS KMS also saves the CMK's older cryptographic material in perpetuity so it can be used to decrypt data that it encrypted. AWS KMS doe"])</script><script>self.__next_f.push([1,"s not delete any rotated key material until you delete the CMK.Key rotation changes only the CMK's backing key, which is the cryptographic material that is used in encryption operations. The CMK is the same logical resource, regardless of whether or how many times its backing key changes. The properties of the CMK do not change, as shown in the following image.Therefore, the easiest way to meet this requirement is to use AWS KMS with automatic key rotation.CORRECT: \"Use AWS KMS with automatic key rotation\" is the correct answer.INCORRECT: \"Encrypt the data before sending it to Amazon S3\" is incorrect as that requires managing your own encryption infrastructure which is not the easiest way to achieve the requirements.INCORRECT: \"Import a custom key into AWS KMS with annual rotation enabled\" is incorrect as when you import key material into AWS KMS you are still responsible for the key material while allowing KMS to use a copy of it. Therefore, this is not the easiest solution as you must manage the key materials.INCORRECT: \"Export a key from AWS KMS to encrypt the data\" is incorrect as when you export a data encryption key you are then responsible for using it and managing it.References: https://docs.aws.amazon.com/kms/latest/developerguide/rotatekeys.html34:T8fd,"])</script><script>self.__next_f.push([1,"Amazon EC2 Auto Scaling helps you maintain application availability and allows you to automatically add or remove EC2 instances according to conditions you define. You can use the fleet management features of EC2 Auto Scaling to maintain the health and availability of your fleet.You can also use the dynamic and predictive scaling features of EC2 Auto Scaling to add or remove EC2 instances. Dynamic scaling responds to changing demand and predictive scaling automatically schedules the right number of EC2 instances based on predicted demand. Dynamic scaling and predictive scaling can be used together to scale faster.A launch configuration is an instance configuration template that an Auto Scaling group uses to launch EC2 instances. When you create a launch configuration, you specify information for the instances. Include the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping. If you've launched an EC2 instance before, you specified the same information in order to launch the instance.You can specify your launch configuration with multiple Auto Scaling groups. However, you can only specify one launch configuration for an Auto Scaling group at a time, and you can't modify a launch configuration after you've created it. To change the launch configuration for an Auto Scaling group, you must create a launch configuration and then update your Auto Scaling group with it.Therefore, the Developer should create a launch configuration and use Amazon EC2 Auto Scaling.CORRECT: \"Create a launch configuration and use Amazon EC2 Auto Scaling\" is the correct answer.INCORRECT: \"Create a launch configuration and use Amazon CodeDeploy\" is incorrect as CodeDeploy is not used for auto scaling of Amazon EC2 instances.INCORRECT: \"Create a task definition and use an Amazon ECS cluster\" is incorrect as the migrated application will be running on Amazon EC2 instances, not containers.INCORRECT: \"Create a task definition and use an AWS Fargate cluster\" is incorrect as the migrated application will be running on Amazon EC2 instances, not containers.References:https://docs.aws.amazon.com/autoscaling/ec2/userguide/whatisamazonec2autoscaling.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html"])</script><script>self.__next_f.push([1,"35:T6d2,You can add AWS Elastic Beanstalk configuration files (.ebextensions) to your web application's source code to configure your environment and customize the AWS resources that it contains.Configuration files are YAML or JSONformatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle.For example, you could include a configuration file for setting the load balancer type into:.ebextensions/loadbalancer.configThis example makes a simple configuration change. It modifies a configuration option to set the type of your environment's load balancer to Network Load Balancer:Requirements• Location – Place all of your configuration files in a single folder, named .ebextensions, in the root of your source bundle. Folders starting with a dot can be hidden by file browsers, so make sure that the folder is added when youcreate your source bundle.• Naming – Configuration files must have the .config file extension.• Formatting – Configuration files must conform to YAML or JSON specifications.• Uniqueness – Use each key only once in each configuration file.Therefore, the Developer should place the file in the .ebextensions folder in the application source bundle.CORRECT: \"In the .ebextensions folder\" is the correct answer.INCORRECT: \"In the root of the source code\" is incorrect. You need to place .config files in the .ebextensions folder.INCORRECT: \"In the bin folder\" is incorrect. You need to place .config files in the .ebextensions folder.INCORRECT: \"In the loadbalancer.config.root\" is incorrect. You need to place .config files in the .ebextensions folder.References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html36:T4bd,The DynamoDB Session Handler is a custom session handler for PHP that allows developers to use Amazon DynamoDB as a session store. Using DynamoDB for session storage alleviates issues that occur with session handling in a distributed web application by moving sessions off of the local "])</script><script>self.__next_f.push([1,"file system and into a shared location. DynamoDB is fast, scalable, easy to setup, and handles replication of your data automatically.CORRECT: \"Use Amazon DynamoDB to perform scalable session handling\" is the correct answer.INCORRECT: \"Use sticky sessions with an Elastic Load Balancer target group\" is incorrect as this involves maintaining session state data on the EC2 instances which means that data is lost if an instance fails.INCORRECT: \"Use Amazon SQS to save session data\" is incorrect as SQS is not used for session data, it is used for application component decoupling.INCORRECT: \"Use Elastic Load Balancer connection draining to stop sending requests to failing instances\" is incorrect as this does not solve the problem of ensuring the session data is available, the data will be on the failing instance and will be lost.References: https://docs.aws.amazon.com/awssdkphp/v2/guide/featuredynamodbsessionhandler.html37:T5bf,AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services.This event history simplifies security analysis, resource change tracking, and troubleshooting. In addition, you can use CloudTrail to detect unusual activity in your AWS accounts. These capabilities help simplify operational analysis and troubleshooting.CloudWatch vs CloudTrail:As this scenario requests that a history of API calls are retained (auditing), AWS CloudTrail is the correct solution to use.CORRECT: \"Use Amazon CloudTrail to keep a history of API calls\" is the correct answer.INCORRECT: \"Use Amazon CloudWatch logs to keep a history of API calls\" is incorrect as this does not keep a record of API activity. CloudWatch records metrics related to performance.INCORR"])</script><script>self.__next_f.push([1,"ECT: \"Use AWS XRay to trace the API calls and keep a record\" is incorrect as XRay does not trace API calls for auditing.INCORRECT: \"Use an AWS Lambda to function to continually monitor API calls and log them to an Amazon S3 bucket\" is incorrect as this is totally unnecessary when CloudTrail can do this for you.References: https://aws.amazon.com/cloudtrail/38:T516,The policy allows the user to use all Amazon SQS actions, but only with queues whose names are prefixed with the literal string “stagingqueue”. This policy is useful to provide a queue creator the ability to use Amazon SQS actions. Any user who has permissions to create a queue must also have permissions to use other Amazon SQS actions in order to do anything with the created queues.CORRECT: \"The user will be able to use all Amazon SQS actions, but only for queues with names begin with the string “stagingqueue“\" is the correct answer.INCORRECT: \"The user will be able to create a queue named “stagingqueue“\" is incorrect as this policy provides the permissions to perform SQS actions on an existing queue.INCORRECT: \"The user will be able to apply a resourcebased policy to the Amazon SQS queue named “stagingqueue”\" is incorrect as this is a single operation and the permissions policy allows all SQS actions.INCORRECT: \"The user will be granted crossaccount access from account number “513246782345” to queue “stagingqueue” is incorrect as this is not a policy for granting crossaccount access. The account number and queue relate to the same account.References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsoverviewofmanagingaccess.html39:T71f,The process of consuming messages from a queue depends on whether you use short or long polling. By default, Amazon SQS uses short polling, querying only a subset of its servers (based on a weighted random distribution) to determine whether any messages are available for a response. You can use long polling to reduce your costs while allowing your consumers to receive m"])</script><script>self.__next_f.push([1,"essages as soon as they arrive in the queue.When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response).Therefore, the best way to optimize resource usage and reduce the number of empty responses (and cost) is to configure long polling by setting the Imaging queue ReceiveMessageWaitTimeSeconds attribute to 20 seconds.CORRECT: \"Set the Imaging queue ReceiveMessageWaitTimeSeconds attribute to 20 seconds\" is the correct answer.INCORRECT: \"Set the imaging queue visibility Timeout attribute to 20 seconds\" is incorrect. This attribute configures message visibility which will not reduce empty responses.INCORRECT: \"Set the imaging queue MessageRetentionPeriod attribute to 20 seconds\" is incorrect. This attribute sets the length of time, in seconds, for which Amazon SQS retains a message.INCORRECT: \"Set the DelaySeconds parameter of a message to 20 seconds\" is incorrect. This attribute sets the length of time, in seconds, for which the delivery of all messages in the queue is delayed.References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsshortandlongpolling.html3a:T654,The AWS Serverless Application Model (SAM) is an opensource framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax, enabling you to build serverless applications faster.To get started with building SAMbased applications, use the AWS SAM CLI. SAM CLI provides a Lambdalike execution environment that lets you "])</script><script>self.__next_f.push([1,"locally build, test, and debug applications defined by SAM templates. You can also use the SAM CLI to deploy your applications to AWS.With the SAM CLI you can package and deploy your source code using two simple commands:• sam package• sam deployAlternatively, you can use:• aws cloudformation package• aws cloudformation deployThe SAM CLI is therefore the easiest way to deploy serverless applications on AWS.CORRECT: \"Use the Serverless Application Model\" is the correct answer.INCORRECT: \"Use the Serverless Application Repository \" is incorrect as this is a managed repository for serverless applications.INCORRECT: \"Use AWS CloudFormation\" is incorrect as this would not be the simplest way to package and deploy this infrastructure. Without using SAM, you would need to build out a much more complex AWS CloudFormation template yourself.INCORRECT: \"Use AWS Elastic Beanstalk\" is incorrect as Elastic Beanstalk cannot be used to deploy Lambda, API Gateway or DynamoDB.References: https://aws.amazon.com/serverless/sam/3b:Tb6a,"])</script><script>self.__next_f.push([1,"CodeCommit is a secure, highly scalable, managed source control service that hosts private Git repositories. CodeCommit eliminates the need for you to manage your own source control system or worry about scaling its infrastructure. You can use CodeCommit to store anything from code to binaries. It supports the standard functionality of Git, so it works seamlessly with your existing Gitbased tools.With CodeCommit, you can:• Benefit from a fully managed service hosted by AWS. CodeCommit provides high service availability and durability and eliminates the administrative overhead of managing your own hardware and software. There is no hardware to provision and scale and no server software to install, configure, and update.• Store your code securely. CodeCommit repositories are encrypted at rest as well as in transit.• Work collaboratively on code. CodeCommit repositories support pull requests, where users can review and comment on each other's code changes before merging them to branches; notifications that automatically send emails to users about pull requests and comments; and more.• Easily scale your version control projects. CodeCommit repositories can scale up to meet your development needs.The service can handle repositories with large numbers of files or branches, large file sizes, and lengthy revision histories.• Store anything, anytime. CodeCommit has no limit on the size of your repositories or on the file types you can store.• Integrate with other AWS and thirdparty services. CodeCommit keeps your repositories close to your other production resources in the AWS Cloud, which helps increase the speed and frequency of your development lifecycle.It is integrated with IAM and can be used with other AWS services and in parallel with other repositories. Easily migrate files from other remote repositories. You can migrate to CodeCommit from any Gitbased repository.• Use the Git tools you already know. CodeCommit supports Git commands as well as its own AWS CLI commands and APIs.Therefore, the development team should select AWS CodeCommit as the repository they use for storing code related to the new project.CORRECT: \"AWS CodeCommit\" is the correct answer.INCORRECT: \"AWS CodeBuild\" is incorrect. AWS CodeBuild is a fully managed continuous integration (CI) service that compiles source code, runs tests, and produces software packages that are ready to deploy.INCORRECT: \"AWS CodeDeploy\" is incorrect. CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, onpremises instances, serverless Lambda functions, or Amazon ECS services.INCORRECT: \"AWS CodePipeline\" is incorrect. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.References: https://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html"])</script><script>self.__next_f.push([1,"3c:T85c,"])</script><script>self.__next_f.push([1,"You choose an API integration type according to the types of integration endpoint you work with and how you want data to pass to and from the integration endpoint. For a Lambda function, you can have the Lambda proxy integration, or the Lambda custom integration.For an HTTP endpoint, you can have the HTTP proxy integration or the HTTP custom integration. For an AWS service action, you have the AWS integration of the nonproxy type only. API Gateway also supports the mock integration, where API Gateway serves as an integration endpoint to respond to a method request.As this is a Docker deployment running on Elastic Beanstalk the HTTP integration types are applicable. There are two options: HTTP: This type of integration lets an API expose HTTP endpoints in the backend. With the HTTP integration, also known as the HTTP custom integration, you must configure both the integration request and integration response. You must set up necessary data mappings from the method request to the integration request, and from the integration response to the method response.HTTP_PROXY: The HTTP proxy integration allows a client to access the backend HTTP endpoints with a streamlined integration setup on single API method. You do not set the integration request or the integration response. API Gateway passes the incoming request from the client to the HTTP endpoint and passes the outgoing response from the HTTP endpoint to the client. As we can see from the above explanation, the most suitable integration type for this deployment is going to be the HTTP_PROXY.CORRECT: \"HTTP_PROXY\" is the correct answer.INCORRECT: \"HTTP\" is incorrect as this is a custom integration that would be used if you need to customize the data mappings.INCORRECT: \"AWS\" is incorrect as this type of integration lets an API expose AWS service actions.INCORRECT: \"AWS_PROXY\" is incorrect as this type of integration lets an API method be integrated with the Lambda function invocation action with a flexible, versatile, and streamlined integration setup.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayapiintegrationtypes.html"])</script><script>self.__next_f.push([1,"3d:T4a5,AWS Fargate is a serverless compute engine for Docker containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.As you can see in the image above, AWS Fargate is serverless, however, the EC2 launch type requires the management of EC2 instances. Therefore, the most suitable service for the Developer’s requirements is AWS Fargate.CORRECT: \"AWS Fargate\" is the correct answer.INCORRECT: \"Amazon ECS\" is incorrect as this is not a serverless service, it requires the launching and management of EC2 instances.INCORRECT: \"AWS Elastic Beanstalk\" is incorrect as this is also not a serverless service. It uses EC2 instances that are partially managed for you but must be scaled.INCORRECT: \"AWS Lambda\" is incorrect as though this is a serverless service, it is not used for running Docker containers.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html3e:T8c5,"])</script><script>self.__next_f.push([1,"With provisioned capacity mode, you specify the number of data reads and writes per second that you require for your application.Read capacity unit (RCU):• Each API call to read data from your table is a read request.• Read requests can be strongly consistent, eventually consistent, or transactional.• For items up to 4 KB in size, one RCU can perform one strongly consistent read request per second.• Items larger than 4 KB require additional RCUs.• For items up to 4 KB in size, one RCU can perform two eventually consistent read requests per second.• Transactional read requests require two RCUs to perform one read per second for items up to 4 KB.• For example, a strongly consistent read of an 8 KB item would require two RCUs, an eventually consistent read of an 8 KB item would require one RCU, and a transactional read of an 8 KB item would require four RCUs.Write capacity unit (WCU):• Each API call to write data to your table is a write request.• For items up to 1 KB in size, one WCU can perform one standard write request per second.• Items larger than 1 KB require additional WCUs.• Transactional write requests require two WCUs to perform one write per second for items up to 1 KB.• For example, a standard write request of a 1 KB item would require one WCU, a standard write request of a 3 KB item would require three WCUs, and a transactional write request of a 3 KB item would require six WCUs.To determine the number of RCUs required to handle 200 eventually consistent reads per/second with an average item size of 12KB, perform the following steps:1. Determine the average item size by rounding up the next multiple of 4KB (12KB rounds up to 12KB).2. Determine the RCU per item by dividing the item size by 8KB (12KB/8KB = 1.5).3. Multiply the value from step 2 with the number of reads required per second (1.5x200 = 300).CORRECT: \"300 Read Capacity Units\" is the correct answer.INCORRECT: \"600 Read Capacity Units\" is incorrect. This would be the value for strongly consistent reads.INCORRECT: \"1200 Read Capacity Units\" is incorrect. This would be the value for transactional reads.INCORRECT: \"150 Read Capacity Units\" is incorrect.References: https://aws.amazon.com/dynamodb/pricing/provisioned/"])</script><script>self.__next_f.push([1,"3f:T523,By increasing the instance size and number of shards in the Kinesis stream, the developer can allow the instances to handle more record processors, which are running in parallel within the instance. It also allows the stream to properly accommodate the rate of data being sent in. The data capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the capacities of its shards.Therefore, the best answer is to increase both the EC2 instance size and add shards to the stream.CORRECT: \"Increase the EC2 instance size and add shards to the stream\" is the correct answer.INCORRECT: \"Increase the number of EC2 instances to match the number of shards\" is incorrect as you can have an individual instance running multiple KCL workers.INCORRECT: \"Increase the EC2 instance size\" is incorrect as the Developer would also need to add shards to the stream to increase the capacity of the stream.INCORRECT: \"Increase the number of open shards\" is incorrect as this does not include increasing the instance size or quantity which is required as they are running at capacity.https://docs.aws.amazon.com/streams/latest/dev/kinesisrecordprocessorscaling.partial.html https://docs.aws.amazon.com/streams/latest/dev/developingconsumerswithkcl.html40:T568,A serverless application is a combination of Lambda functions, event sources, and other resources that work together to perform tasks. Note that a serverless application is more than just a Lambda function—it can include additional resources such as APIs, databases, and event source mappings.AWS SAM templates are an extension of AWS CloudFormation templates, with some additional components that make them easier to work with. To create a Lambda function using an AWS SAM template the Developer can use theAWS::Serverless::Function resource type.The AWS::Serverless::Function resource type can be used to Create a Lambda function, IAM execution role, and event source mappings that trigger the function.CORRECT: \""])</script><script>self.__next_f.push([1,"AWS::Serverless:Function\" is the correct answer.INCORRECT: \"AWS::Serverless::Application\" is incorrect as this embeds a serverless application from the AWS Serverless Application Repository or from an Amazon S3 bucket as a nested application.INCORRECT: \"AWS::Serverless:LayerVersion\" is incorrect as this creates a Lambda LayerVersion that contains library or runtime code needed by a Lambda Function.INCORRECT: \"AWS::Serverless:API\" is incorrect as this creates a collection of Amazon API Gateway resources and methods that can be invoked through HTTPS endpoints.References: https://docs.aws.amazon.com/serverlessapplicationmodel/latest/developerguide/whatissam.html41:T7e3,You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN.Each alias has a unique ARN. An alias can only point to a function version, not to another alias. You can update an alias to point to a new version of the function. You can also use traffic shifting to direct a percentage of traffic to a specific version as showing in the image below: This is the recommended way to direct traffic to multiple function versions and shift traffic when testing code updated.Therefore, the best answer is to create an Alias, assign the current and new versions and use traffic shifting to assign a percentage of traffic to the new version.CORRECT: \"Create an Alias, assign the current and new versions and use traffic shifting to assign a percentage of traffic to the new version\" is the correct answer.INCORRECT: \"Create an Amazon Route 53 weighted routing policy pointing to the current and new versions, assign a lower weight to the new version\" is incorrect. AWS Lambda endpoints are not DNS names that you can route to with Route 53. The best way to route traffic to multiple versions is using an alias.INCORRECT: \"Use an immutable update with a new ASG to deploy the new version in parallel, following testing cutover to the new version\" is "])</script><script>self.__next_f.push([1,"incorrect as immutable updates are associated with Amazon Elastic Beanstalk and this service does not deploy updates to AWS Lambda.INCORRECT: \"Use an Amazon Elastic Load Balancer to direct a percentage of traffic to each target group containing the Lambda function versions\" is incorrect as this introduces an unnecessary layer (complexity and cost) to the architecture. The best choice is to use an alias instead.References:https://docs.amazonaws.cn/en_us/lambda/latest/dg/configurationaliases.html https://docs.aws.amazon.com/lambda/latest/dg/aliasesintro.html https://docs.aws.amazon.com/lambda/latest/dg/lambdatrafficshiftingusingaliases.html42:T85f,"])</script><script>self.__next_f.push([1,"You can invoke Lambda functions directly with the Lambda console, the Lambda API, the AWS SDK, the AWS CLI, and AWS toolkits.You can also configure other AWS services to invoke your function, or you can configure Lambda to read from a stream or queue and invoke your function.When you invoke a function, you can choose to invoke it synchronously or asynchronously.• Synchronous invocation:o You wait for the function to process the event and return a response.o To invoke a function synchronously with the AWS CLI, use the invoke command.o The Invocationtype can be used to specify a value of “RequestResponse”. This instructs AWS to execute your Lambda function and wait for the function to complete.• Asynchronous invocation:o When you invoke a function asynchronously, you don’t wait for a response from the function code.o For asynchronous invocation, Lambda handles retries and can send invocation records to a destination.o To invoke a function asynchronously, set the invocation type parameter to Event.The fastest way to process all the files is to use asynchronous invocation and process the files in parallel. To do this you should specify the invocation type of EventCORRECT: \"Invoke the Lambda function asynchronously with the invocation type Event and process the files in parallel\" is the correct answer.INCORRECT: \"Invoke the Lambda function synchronously with the invocation type Event and process the files in parallel\" is incorrect as the invocation type for a synchronous invocation should be RequestResponse.INCORRECT: \"Invoke the Lambda function synchronously with the invocation type RequestResponse and process the files sequentially\" is incorrect as this is not the fastest way of processing the files as Lambda will wait for completion of once file before moving on to the next one.INCORRECT: \"Invoke the Lambda function asynchronously with the invocation type RequestResponse and process the files sequentially\" is incorrect as the invocation type RequestResponse is used for synchronous invocations.References: https://aws.amazon.com/blogs/architecture/understandingthedifferentwaystoinvokelambdafunctions/"])</script><script>self.__next_f.push([1,"43:T79f,You can publish your own metrics to CloudWatch using the AWS CLI or an API. You can view statistical graphs of your published metrics with the AWS Management Console.CloudWatch stores data about a metric as a series of data points. Each data point has an associated time stamp. You can even publish an aggregated set of data points called a statistic set. Each metric is one of the following:• Standard resolution, with data having a oneminute granularity• High resolution, with data at a granularity of one second Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a highresolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of60 seconds. Highresolution metrics can give you more immediate insight into your application's subminute activity. Keep in mind that every PutMetricData call for a custom metric is charged, so calling PutMetricData more often on a highresolution metric can lead to higher charges.Therefore, the best action to take is to Create custom metrics and configure them as high resolution. This will ensure that granularity can be down to 1 second.CORRECT: \"Create custom metrics and configure them as high resolution\" is the correct answer.INCORRECT: \"Do nothing, CloudWatch uses standard resolution metrics by default\" is incorrect as standard resolution has a granularity of oneminute.INCORRECT: \"Create custom metrics and configure them as standard resolution\" is incorrect as standard resolution has a granularity of oneminute.INCORRECT: \"Create custom metrics and enable detailed monitoring\" is incorrect as detailed monitoring has a granularity of oneminute.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html44:T652,AWS CodeBuild is a fully managed build service in the cloud. CodeBuild compiles y"])</script><script>self.__next_f.push([1,"our source code, runs unit tests, and produces artifacts that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. It provides prepackaged build environments for popular programming languages and build tools such as Apache Maven, Gradle, and more.CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, onpremises instances, serverless Lambda functions, or Amazon ECS services.CORRECT: \"AWS CodeBuild for running tests against source code\" is a correct answer.CORRECT: \"AWS CodeDeploy for installing compiled code on their AWS resources\" is also a correct answer.INCORRECT: \"AWS CodePipeline for running tests against source code\" is incorrect. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. This service works with the other Developer Tools to create a pipeline.INCORRECT: \"AWS CodeCommit for installing compiled code on their AWS resources\" is incorrect as AWS CodeCommit is a fullymanaged source control service that hosts secure Gitbased repositories.INCORRECT: \"AWS Cloud9 for running tests against source code\" is incorrect as AWS Cloud9 is a cloudbased integrated development environment (IDE) that lets you write, run, and debug your code with just a browser.References:https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html45:T422,Exponential backoff can improve an application's reliability by using progressively longer waits between retries. When using the AWS SDK, this logic is builtin. However, in this case the application is incompatible with the AWS SDK so it is necessary to manually implement exponential backoff.CORRECT: \"Add exponential backoff to the application logic\" is the correct answer.INCORRECT: \"Use Amazon SQS as an API message bus\" is incorrect as SQS requires instances or functions to pick up and process the"])</script><script>self.__next_f.push([1," messages and put them in the DynamoDB table. This is unnecessary cost and complexity and will not improve performance.INCORRECT: \"Pass API calls through Amazon API Gateway\" is incorrect as this is not a suitable method of throttling the application. Exponential backoff logic in the application is a better solution.INCORRECT: \"Send the items to DynamoDB through Amazon Kinesis Data Firehose\" is incorrect as DynamoDB is not a destination for Kinesis Data Firehose.References: https://aws.amazon.com/premiumsupport/knowledgecenter/dynamodbtablethrottled/46:T76d,AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments.Each deployment policy has advantages and disadvantages and it’s important to select the best policy to use for each situation.The following tables summarizes the different deployment policies:The “immutable” policy will create a new ASG with a whole new set of instances and deploy the updates there.Immutable:• Launches new instances in a new ASG and deploys the version update to these instances before swapping traffic to these instances once healthy.• Zero downtime.• New code is deployed to new instances using an ASG.• High cost as double the number of instances running during updates.• Longest deployment.• Quick rollback in case of failures.• Great for production environments.For this scenario a quick rollback must be prioritized over all other considerations. Therefore, the best choice is “immutable”.This deployment policy is the most expensive and longest (duration) option. However, you can roll back quickly and safely as the original instances are all available and unmodified.CORRECT: \"Immutable\" is the correct answer.INCORRECT: \"Rolling\" is incorrect as this policy requires manual redeployment if there are any issues caused by the update.INCORRECT: \"Rolling with additional batch\" is incorrect as this policy requires manual redeploym"])</script><script>self.__next_f.push([1,"ent if there are any issues caused by the update.INCORRECT: \"All at once\" is incorrect as this takes the entire environment down at once and requires manual redeployment if there are any issues caused by the update.References:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.rollingversiondeploy.html47:T557,You can launch a cluster of multicontainer instances in a singleinstance or autoscaling Elastic Beanstalk environment using the Elastic Beanstalk console. The single container and multicontainer Docker platforms for Elastic Beanstalk support the use of Docker images stored in a public or private online image repository. You specify images by name in the Dockerrun.aws.json file and save it in the root of your source directory.CORRECT: \"Define the containers in the Dockerrun.aws.json file in JSON format and save at the root of the source directory\" is the correct answer.INCORRECT: \"Create a Docker.config file and save it in the .ebextensions folder at the root of the source directory\" is incorrect as the you need to create a Dockerrun.aws.json file, not a Dokcer.config file and it should be saved at the root of the source directory not in the .ebextensions folder.INCORRECT: \"Define the containers in the Dockerrun.aws.json file in YAML format and save at the root of the source directory\" is incorrect because the contents of the file should be in JSON format, not YAML format.INCORRECT: \"Create a buildspec.yml file and save it at the root of the source directory\" is incorrect as the buildspec.yml file is used with AWS CodeBuild, not Elastic Beanstalk.References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_v2config.html48:T631,You can record additional information about requests, the environment, or your application with annotations and metadata. You can add annotations and metadata to the segments that the XRay SDK creates, or to custom subsegments that you create.Annota"])</script><script>self.__next_f.push([1,"tions are keyvalue pairs with string, number, or Boolean values. Annotations are indexed for use with filter expressions.Use annotations to record data that you want to use to group traces in the console, or when calling the GetTraceSummaries API.Metadata are keyvalue pairs that can have values of any type, including objects and lists, but are not indexed for use with filter expressions. Use metadata to record additional data that you want stored in the trace but don't need to use with search.Annotations can be used with filter expressions, so this is the best solution for this requirement. The Developer can add annotations to the custom subsegments and will then be able to use filter expressions to filter the results in AWS XRay.CORRECT: \"Add annotations to the custom subsegments\" is the correct answer.INCORRECT: \"Add metadata to the custom subsegments\" is incorrect as though you can add metadata to custom subsegments it is not indexed and cannot be used with filters.INCORRECT: \"Add the keyvalue pairs to the Trace ID\" is incorrect as this is not something you can do.INCORRECT: \"Setup sampling for the custom subsegments \" is incorrect as this is a mechanism used by XRay to send only statistically significant data samples to the API.References: https://docs.aws.amazon.com/xray/latest/devguide/xraysdkjavasegment.html49:T5f0,There are two key requirements in this scenario. Firstly the company wants to manage user accounts using a system that allows users to reset their own passwords. The company also wants to authorize users to access AWS services.The first requirement is provided by an Amazon Cognito User Pool. With a Cognito user pool you can add signup and signin to mobile and web apps and it also offers a user directory so user accounts can be created directly within the user pool. Users also have the ability to reset their passwords.To access AWS services you need a Cognito Identity Pool. An identity pool can be used with a user pool and enables a user to obtain temporary limitedprivilege credentials to access "])</script><script>self.__next_f.push([1,"AWS services.Therefore, the best answer is to use Amazon Cognito user pools and identity pools.CORRECT: \"Amazon Cognito user pools and identity pools\" is the correct answer.INCORRECT: \"Amazon Cognito identity pools and AWS STS\" is incorrect as there is no user directory in this solution. A Cognito user pool is required.INCORRECT: \"Amazon Cognito identity pools and AWS IAM\" is incorrect as a Cognito user pool should be used as the directory source for creating and managing users. IAM is used for accounts that are used to administer AWS services, not for application user access.INCORRECT: \"Amazon Cognito user pools and AWS KMS\" is incorrect as KMS is used for encryption, not for authentication to AWS services.References: https://docs.aws.amazon.com/cognito/latest/developerguide/cognitouseridentitypools.html4a:T4b5,An event source mapping is an AWS Lambda resource that reads from an event source and invokes a Lambda function. You can use event source mappings to process items from a stream or queue in services that don't invoke Lambda functions directly. Lambda provides event source mappings for the following services.Services That Lambda Reads Events From• Amazon Kinesis• Amazon DynamoDB• Amazon Simple Queue ServiceAn event source mapping uses permissions in the function's execution role to read and manage items in the event source.Permissions, event structure, settings, and polling behavior vary by event source.CORRECT: \"Amazon Kinesis, Amazon DynamoDB, and Amazon Simple Queue Service (SQS)\" are the correct answers.INCORRECT: \"Amazon Simple Notification Service (SNS)\" is incorrect as SNS should be used as destination for asynchronous invocation.INCORRECT: \"Amazon Simple Storage Service (S3)\" is incorrect as Lambda does not read from Amazon S3, you must configure the event notification on the S3 side.INCORRECT: \"Another Lambda function\" is incorrect as another function should be invoked asynchronously.References: https://docs.aws.amazon.com/lambda/latest/dg/invocationeventsourcemapping.html4b:T643,If you woul"])</script><script>self.__next_f.push([1,"d like to run a Docker image that is available in Amazon ECR, you can pull it to your local environment with the docker pull command. You can do this from either your default registry or from a registry associated with another AWS account.Docker CLI does not support standard AWS authentication methods, so client authentication must be handled so that ECR knows who is requesting to push or pull an image. To do this you can issue the aws ecr getlogin or aws ecr getloginpassword (AWS CLI v2) and then use the output to login using docker login and then issue a docker pull command specifying the image name using registry/repository[:tag].CORRECT: \"Run the output of the following: aws ecr getlogin, and then run docker pull REPOSITORY URI : TAG\" is the correct answer.INCORRECT: \"Run the following: docker pull REPOSITORY URI : TAG\" is incorrect as the Developers first need to authenticate before they can pull the image.INCORRECT: \"Run the following: aws ecr getlogin, and then run: docker pull REPOSITORY URI : TAG\" is incorrect. TheDevelopers need to not just run the login command but run the output of the login command which contains the authentication token required to log in.INCORRECT: \"Run the output of the following: aws ecr getdownloadurlforlayer, and then run docker pull REPOSITORY URI : TAG\" is incorrect as this command retrieves a presigned Amazon S3 download URL corresponding to an image layer.References:https://docs.aws.amazon.com/AmazonECR/latest/userguide/Registries.html#registry_auth https://docs.aws.amazon.com/AmazonECR/latest/userguide/dockerpullecrimage.html"])</script><script>self.__next_f.push([1,"5:[\"$\",\"$Le\",null,{\"quiz\":{\"id\":\"aws-developer-3\",\"title\":\"AWS Developer Associate (DVA-C02) Practice Test Latest 2025 3\",\"description\":\"Additional practice questions covering AWS development topics.\",\"questions\":[{\"question\":\"A developer is completing the configuration for an Amazon ECS cluster. Which task placement strategy will MINIMIZE the number of instances in use?\",\"answers\":[{\"text\":\"binpack\",\"isCorrect\":true},{\"text\":\"random\",\"isCorrect\":false},{\"text\":\"spread\",\"isCorrect\":false},{\"text\":\"Canary\",\"isCorrect\":false}],\"explanation\":\"$f\"},{\"question\":\"A serverless application requires a storage location for log files. Which storage solution is the BEST fit?\",\"answers\":[{\"text\":\"Amazon S3\",\"isCorrect\":true},{\"text\":\"Amazon EBS\",\"isCorrect\":false},{\"text\":\"Amazon EFS\",\"isCorrect\":false},{\"text\":\"Amazon EC2 instance store\",\"isCorrect\":false}],\"explanation\":\"Amazon S3 is an objectbased storage system. The serverless application can use the REST API or AWS SDK to write data to an S3 bucket. This is a suitable solution for storing log files from a serverless app at low cost.CORRECT: \\\"Amazon S3\\\" is the correct answer.INCORRECT: \\\"Amazon EBS \\\" is incorrect as this is a blockbased storage solution in which volumes are attached to EC2 instances so it is not suitable for a serverless application.INCORRECT: \\\"Amazon EFS\\\" is incorrect as this is filebased storage solution that is mounted from EC2 instances using the NFS protocol. This is not suitable for a serverless application.INCORRECT: \\\"Amazon EC2 instance store\\\" is incorrect as this is an ephemeral storage volume that is locally attached to EC2 instances from the same physical hardware. It is not suitable for a serverless application.References: https://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html\"},{\"question\":\"An application will use AWS Lambda and an Amazon RDS database. The Developer needs to secure the database connection string and enable automatic rotation every 30 days. What is the SIMPLEST way to achieve this requirement?\",\"answers\":[{\"text\":\"Store the connection string in an encrypted Amazon S3 bucket and use a scheduled CloudWatch Event to update the connection string every 30 days\",\"isCorrect\":false},{\"text\":\"Store the connection string as an encrypted environment variable in Lambda and create a separate function that rotates the connection string every 30 days\",\"isCorrect\":false},{\"text\":\"Store a secret in AWS Secrets Manager and enable automatic rotation every 30 days\",\"isCorrect\":true},{\"text\":\"Store a SecureString in Systems Manager Parameter Store and enable automatic rotation every 30 days\",\"isCorrect\":false}],\"explanation\":\"$10\"},{\"question\":\"Fault tolerance needs to be increased for a stateless application that runs on Amazon EC2 instances. The application runs in an Auto Scaling group of EC2 instances in a single subnet behind an Application Load Balancer. How can the application be made more fault tolerant?\",\"answers\":[{\"text\":\"Add a subnet in another VPC to the ASG and add the same subnet to the ALB\",\"isCorrect\":false},{\"text\":\"Add an Elastic IP to each instance and use Amazon Route 53 Alias records to distribute incoming connections\",\"isCorrect\":false},{\"text\":\"Add a subnet in another AZ to the ASG and add the same subnet to the ALB\",\"isCorrect\":true},{\"text\":\"Add a subnet in another region to the ASG and add the same subnet to the ALB\",\"isCorrect\":false}],\"explanation\":\"$11\"},{\"question\":\"A web application is using Amazon Kinesis Data Streams for ingesting IoT data that is then stored before processing for up to 24 hours. How can the Developer implement encryption at rest for data stored in Amazon Kinesis Data Streams?\",\"answers\":[{\"text\":\"Enable serverside encryption on Kinesis Data Streams with an AWS KMS CMK\",\"isCorrect\":true},{\"text\":\"Use the Amazon Kinesis Consumer Library (KCL) to encrypt the data\",\"isCorrect\":false},{\"text\":\"Add a certificate and enable SSL/TLS connections to Kinesis Data Streams\",\"isCorrect\":false},{\"text\":\"Encrypt the data once it is at rest with an AWS Lambda function\",\"isCorrect\":false}],\"explanation\":\"$12\"},{\"question\":\"A financial application is hosted on an Auto Scaling group of EC2 instance with an Elastic Load Balancer. A Developer needs to capture information about the IP traffic going to and from network interfaces in the VPC. How can the Developer capture this information?\",\"answers\":[{\"text\":\"Create a flow log in the VPC and publish data to Amazon S3\",\"isCorrect\":true},{\"text\":\"Capture the information directly into Amazon CloudWatch Logs\",\"isCorrect\":false},{\"text\":\"Capture the information using a Network ACL\",\"isCorrect\":false},{\"text\":\"Create a flow log in the VPC and publish data to Amazon CloudTrail\",\"isCorrect\":false}],\"explanation\":\"$13\"},{\"question\":\"To include objects defined by the AWS Serverless Application Model (SAM) in an AWS CloudFormation template, in addition to Resources, what section MUST be included in the document root?\",\"answers\":[{\"text\":\"Globals\",\"isCorrect\":false},{\"text\":\"Properties\",\"isCorrect\":false},{\"text\":\"Transform\",\"isCorrect\":true},{\"text\":\"Conditions\",\"isCorrect\":false}],\"explanation\":\"$14\"},{\"question\":\"An application running on a fleet of EC2 instances use the AWS SDK for Java to copy files into several AWS buckets using access keys stored in environment variables. A Developer has modified the instances to use an assumed IAM role with a more restrictive policy that allows access to only one bucket.However, after applying the change the Developer logs into one of the instances and is still able to write to all buckets. What is the MOST likely explanation for this situation?\",\"answers\":[{\"text\":\"The AWS CLI is corrupt and needs to be reinstalled\",\"isCorrect\":false},{\"text\":\"The AWS credential provider looks for instance profile credentials last\",\"isCorrect\":true},{\"text\":\"An IAM inline policy is being used on the IAM role\",\"isCorrect\":false},{\"text\":\"An IAM managed policy is being used on the IAM role\",\"isCorrect\":false}],\"explanation\":\"$15\"},{\"question\":\"A company is using AWS Lambda for processing small images that are uploaded to Amazon S3. This was working well until a large number of small files (several thousand) were recently uploaded and an error was generated by AWS Lambda (status code 429). What is the MOST likely cause?\",\"answers\":[{\"text\":\"Lambda cannot process multiple files simultaneously\",\"isCorrect\":false},{\"text\":\"The event source mapping has not been configured\",\"isCorrect\":false},{\"text\":\"The concurrency execution limit for the account has been exceeded\",\"isCorrect\":true},{\"text\":\"Amazon S3 could not handle the sudden burst in traffic\",\"isCorrect\":false}],\"explanation\":\"$16\"},{\"question\":\"A customer requires a schemaless, key/value database that can be used for storing customer orders. Which type of AWS database is BEST suited to this requirement?\",\"answers\":[{\"text\":\"Amazon ElastiCache\",\"isCorrect\":false},{\"text\":\"Amazon S3\",\"isCorrect\":false},{\"text\":\"Amazon RDS\",\"isCorrect\":false},{\"text\":\"Amazon DynamoDB\",\"isCorrect\":true}],\"explanation\":\"Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It is a nonrelational (schemaless), keyvalue type of database. This is the most suitable solution for this requirement.CORRECT: \\\"Amazon DynamoDB\\\" is the correct answer.INCORRECT: \\\"Amazon RDS\\\" is incorrect as this a relational database that has a schema.INCORRECT: \\\"Amazon ElastiCache\\\" is incorrect as this is a key/value database but it is used to cache the contents of other databases (including DynamoDB and RDS) for better performance for reads.INCORRECT: \\\"Amazon S3\\\" is incorrect as this is an objectbased storage system not a database. It is a key/value store but DynamoDB is a better fit for a customer order database.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html\"},{\"question\":\"A company needs to ingest several terabytes of data every hour from a large number of distributed sources. The messages are delivered continually 24 hrs a day. Messages must be delivered in real-time for security analysis and live operational dashboards. Which approach will meet these requirements?\",\"answers\":[{\"text\":\"Use the Amazon S3 API to write messages to an S3 bucket, then process the messages by using Amazon RedShift\",\"isCorrect\":false},{\"text\":\"Use AWS Data Pipeline to automate the movement and transformation of data\",\"isCorrect\":false},{\"text\":\"Use Amazon Kinesis Data Streams with Kinesis Client Library to ingest and deliver messages\",\"isCorrect\":true},{\"text\":\"Send the messages to an Amazon SQS queue, then process the messages by using a fleet of Amazon EC2 instances\",\"isCorrect\":false}],\"explanation\":\"$17\"},{\"question\":\"A Developer wants to debug an application by searching and filtering log data. The application logs are stored in Amazon CloudWatch Logs. The Developer creates a new metric filter to count exceptions in the application logs. However, no results are returned from the logs. What is the reason that no filtered results are being returned?\",\"answers\":[{\"text\":\"Metric data points to logs groups can be filtered only after they are exported to an Amazon S3 bucket\",\"isCorrect\":false},{\"text\":\"CloudWatch Logs only publishes metric data for events that happen after the filter is created\",\"isCorrect\":true},{\"text\":\"The log group for CloudWatch Logs should be first streamed to Amazon Elasticsearch Service before filtering returns the results\",\"isCorrect\":false},{\"text\":\"A setup of the Amazon CloudWatch interface VPC endpoint is required for filtering the CloudWatch Logs in the VPC\",\"isCorrect\":false}],\"explanation\":\"$18\"},{\"question\":\"A Developer needs to choose the best data store for a new application. The application requires a data store that supports key/value pairs and optimistic locking. Which of the following would provide the MOST suitable solution?\",\"answers\":[{\"text\":\"Amazon S3\",\"isCorrect\":false},{\"text\":\"Amazon DynamoDB\",\"isCorrect\":true},{\"text\":\"Amazon RedShift\",\"isCorrect\":false},{\"text\":\"Amazon RDS\",\"isCorrect\":false}],\"explanation\":\"$19\"},{\"question\":\"A Development team has deployed several applications running on an Auto Scaling fleet of Amazon EC2 instances. The Operations team have asked for a display that shows a key performance metric for each application on a single screen for monitoring purposes. What steps should a Developer take to deliver this capability using Amazon CloudWatch?\",\"answers\":[{\"text\":\"Create a custom namespace with a unique metric name for each application\",\"isCorrect\":true},{\"text\":\"Create a custom alarm with a unique metric name for each application\",\"isCorrect\":false},{\"text\":\"Create a custom dimension with a unique metric name for each application\",\"isCorrect\":false},{\"text\":\"Create a custom event with a unique metric name for each application\",\"isCorrect\":false}],\"explanation\":\"$1a\"},{\"question\":\"A mobile application is being developed that will use AWS Lambda, Amazon API Gateway and Amazon DynamoDB. A developer would like to securely authenticate the users of the mobile application and then grant them access to the API. What is the BEST way to achieve this?\",\"answers\":[{\"text\":\"Create a COGNITO_USER_POOLS authorizer in API Gateway\",\"isCorrect\":true},{\"text\":\"Create a COGNITO_IDENTITY_POOLS authorizer in API Gateway\",\"isCorrect\":false},{\"text\":\"Create an IAM authorizer in API Gateway\",\"isCorrect\":false},{\"text\":\"Create a Lambda authorizer in API Gateway\",\"isCorrect\":false}],\"explanation\":\"$1b\"},{\"question\":\"A developer needs to implement a caching layer in front of an Amazon RDS database. If the caching layer fails, it is time consuming to repopulate cached data so the solution should be designed for maximum uptime. Which solution is best for this scenario?\",\"answers\":[{\"text\":\"Implement Amazon ElastiCache Memcached\",\"isCorrect\":false},{\"text\":\"Migrate the database to Amazon RedShift\",\"isCorrect\":false},{\"text\":\"Implement Amazon DynamoDB DAX\",\"isCorrect\":false},{\"text\":\"Implement Amazon ElastiCache Redis\",\"isCorrect\":true}],\"explanation\":\"$1c\"},{\"question\":\"Messages produced by an application must be pushed to multiple Amazon SQS queues. What is the BEST solution for this requirement?\",\"answers\":[{\"text\":\"Create and AWS Step Functions state machine that uses multiple Lambda functions to process and push the messages into multiple SQS queues\",\"isCorrect\":false},{\"text\":\"Create an Amazon SWF workflow that receives the messages and pushes them to multiple SQS queues\",\"isCorrect\":false},{\"text\":\"Publish the messages to an Amazon SNS topic and subscribe each SQS queue to the topic\",\"isCorrect\":true},{\"text\":\"Publish the messages to an Amazon SQS queue and configure an AWS Lambda function to duplicate the message into multiple queues\",\"isCorrect\":false}],\"explanation\":\"$1d\"},{\"question\":\"A Development team is creating a microservices application running on Amazon ECS. The release process workflow of the application requires a manual approval step before the code is deployed into the production environment. What is the BEST way to achieve this using AWS CodePipeline?\",\"answers\":[{\"text\":\"Disable a stage just prior the deployment stage\",\"isCorrect\":false},{\"text\":\"Disable the stage transition to allow manual approval\",\"isCorrect\":false},{\"text\":\"Use an Amazon SNS notification from the deployment stage\",\"isCorrect\":false},{\"text\":\"Use an approval action in a stage before deployment\",\"isCorrect\":true}],\"explanation\":\"$1e\"},{\"question\":\"A company uses an Amazon Simple Queue Service (SQS) Standard queue for an application. An issue has been identified where applications are picking up messages from the queue that are still being processed causing duplication. What can a Developer do to resolve this issue?\",\"answers\":[{\"text\":\"Increase the ReceiveMessageWaitTimeSeconds API action on the queue\",\"isCorrect\":false},{\"text\":\"Create a RedrivePolicy for the queue\",\"isCorrect\":false},{\"text\":\"Increase the DelaySeconds API action on the queue\",\"isCorrect\":false},{\"text\":\"Increase the VisibilityTimeout API action on the queue\",\"isCorrect\":true}],\"explanation\":\"$1f\"},{\"question\":\"An application has been instrumented to use the AWS XRay SDK to collect data about the requests the application serves. The Developer has set the user field on segments to a string that identifies the user who sent the request. How can the Developer search for segments associated with specific users?\",\"answers\":[{\"text\":\"Use a filter expression to search for the user field in the segment annotations\",\"isCorrect\":false},{\"text\":\"By using the GetTraceSummaries API with a filter expression\",\"isCorrect\":true},{\"text\":\"By using the GetTraceGraph API with a filter expression\",\"isCorrect\":false},{\"text\":\"Use a filter expression to search for the user field in the segment metadata\",\"isCorrect\":false}],\"explanation\":\"$20\"},{\"question\":\"A Developer has created the code for a Lambda function saved the code in a file named lambda_function.py. He has also created a template that named template.yaml. The following code is included in the template file:AWSTemplateFormatVersion: '20100909'Transform: 'AWS::Serverless20161031'Resources:microservicehttpendpointpython3:Type: 'AWS::Serverless::Function'Properties:Handler: lambda_function.lambda_handlerCodeUri: .What commands can the Developer use to prepare and then deploy this template? (Select TWO.)\",\"answers\":[{\"text\":\"Run sam build and then sam package\",\"isCorrect\":false},{\"text\":\"Run sam package and then sam deploy\",\"isCorrect\":true},{\"text\":\"Run aws cloudformation package and then aws cloudformation deploy\",\"isCorrect\":true},{\"text\":\"Run aws cloudformation compile and then aws cloudformation deploy\",\"isCorrect\":false},{\"text\":\"Run aws serverless package and then aws serverless deploy\",\"isCorrect\":false}],\"explanation\":\"$21\"},{\"question\":\"A company is migrating a stateful web service into the AWS cloud. The objective is to refactor the application to realize the benefits of cloud computing. How can the Developer leading the project refactor the application to enable more elasticity? (Select TWO.)\",\"answers\":[{\"text\":\"Use Amazon CloudFormation and the Serverless Application Model\",\"isCorrect\":true},{\"text\":\"Use Amazon CloudFront with a Web Application Firewall\",\"isCorrect\":true},{\"text\":\"Use an Elastic Load Balancer and Auto Scaling Group\",\"isCorrect\":false},{\"text\":\"Store the session state in an Amazon RDS database\",\"isCorrect\":false},{\"text\":\"Store the session state in an Amazon DynamoDB table\",\"isCorrect\":false}],\"explanation\":\"$22\"},{\"question\":\"A developer is building a multitier web application that accesses an Amazon RDS MySQL database. The application must use a credentials to connect and these need to be stored securely. The application will take care of secret rotation. Which AWS service represents the LOWEST cost solution for storing credentials?\",\"answers\":[{\"text\":\"AWS Systems Manager Parameter Store\",\"isCorrect\":true},{\"text\":\"AWS IAM with the Security Token Service (STS)\",\"isCorrect\":false},{\"text\":\"AWS Secrets Manager\",\"isCorrect\":false},{\"text\":\"AWS Key Management Service (KMS)\",\"isCorrect\":false}],\"explanation\":\"$23\"},{\"question\":\"A website is running on a single Amazon EC2 instance. A Developer wants to publish the website on the Internet and is creating an A record on Amazon Route 53 for the website's public DNS name. What type of IP address MUST be assigned to the EC2 instance and used in the A record to ensure ongoing connectivity?\",\"answers\":[{\"text\":\"Elastic IP address\",\"isCorrect\":true},{\"text\":\"Private IP address\",\"isCorrect\":false},{\"text\":\"Dynamic IP address\",\"isCorrect\":false},{\"text\":\"Public IP address\",\"isCorrect\":false}],\"explanation\":\"$24\"},{\"question\":\"A developer is creating a serverless application that will use a DynamoDB table. The average item size is 7KB. The application will make 3 strongly consistent reads/sec, and 1 standard write/sec. How many RCUs/WCUs are required?\",\"answers\":[{\"text\":\"6 RCU and 7 WCU\",\"isCorrect\":true},{\"text\":\"12 RCU and 14 WCU\",\"isCorrect\":false},{\"text\":\"6 RCU and 14 WCU\",\"isCorrect\":false},{\"text\":\"3 RCU and 7 WCU\",\"isCorrect\":false}],\"explanation\":\"$25\"},{\"question\":\"A Developer needs to create an instance profile for an Amazon EC2 instance using the AWS CLI. How can this be achieved? (Select THREE.)\",\"answers\":[{\"text\":\"Run the aws iam addroletoinstanceprofile command\",\"isCorrect\":true},{\"text\":\"Run the aws ec2 associateinstanceprofile command\",\"isCorrect\":true},{\"text\":\"Run the AssignInstanceProfile API\",\"isCorrect\":false},{\"text\":\"Run the AddRoleToInstanceProfile API\",\"isCorrect\":false},{\"text\":\"Run the aws iam createinstanceprofile command\",\"isCorrect\":true},{\"text\":\"Run the CreateInstanceProfile API\",\"isCorrect\":false}],\"explanation\":\"$26\"},{\"question\":\"An application uses an Auto Scaling group of Amazon EC2 instances, an Application Load Balancer (ALB), and an Amazon Simple Queue Service (SQS) queue. An Amazon CloudFront distribution caches content for global users. A Developer needs to add in transit encryption to the data by configuring end-to-end SSL between the CloudFront Origin and the end users. How can the Developer meet this requirement? (Select TWO.)\",\"answers\":[{\"text\":\"Configure the Viewer Protocol Policy\",\"isCorrect\":true},{\"text\":\"Create an encrypted distribution\",\"isCorrect\":false},{\"text\":\"Create an Origin Access Identity (OAI)\",\"isCorrect\":false},{\"text\":\"Add a certificate to the Auto Scaling Group\",\"isCorrect\":false},{\"text\":\"Configure the Origin Protocol Policy\",\"isCorrect\":true}],\"explanation\":\"$27\"},{\"question\":\"A developer is planning the deployment of a new version of an application to AWS Elastic Beanstalk. The new version of the application should be deployed only to new EC2 instances. Which deployment methods will meet these requirements? (Select TWO.)\",\"answers\":[{\"text\":\"Rolling\",\"isCorrect\":false},{\"text\":\"Rolling with additional batch\",\"isCorrect\":false},{\"text\":\"Immutable\",\"isCorrect\":true},{\"text\":\"All at once\",\"isCorrect\":false},{\"text\":\"Blue/green\",\"isCorrect\":true}],\"explanation\":\"$28\"},{\"question\":\"A company is developing a game for the Android and iOS platforms. The mobile game will securely store user game history and other data locally on the device. The company would like users to be able to use multiple mobile devices and synchronize data between devices. Which service can be used to synchronize the data across mobile devices without the need to create a backend application?\",\"answers\":[{\"text\":\"AWS Lambda\",\"isCorrect\":false},{\"text\":\"Amazon DynamoDB\",\"isCorrect\":false},{\"text\":\"Amazon API Gateway\",\"isCorrect\":false},{\"text\":\"Amazon Cognito\",\"isCorrect\":true}],\"explanation\":\"$29\"},{\"question\":\"A web application has been deployed on AWS. A developer is concerned about exposure to common exploits that could affect application availability or compromise security. Which AWS service can protect from these threats?\",\"answers\":[{\"text\":\"Amazon Cognito\",\"isCorrect\":false},{\"text\":\"AWS CloudFront\",\"isCorrect\":false},{\"text\":\"AWS CloudHSM\",\"isCorrect\":false},{\"text\":\"AWS Web Application Firewall (WAF)\",\"isCorrect\":true}],\"explanation\":\"$2a\"},{\"question\":\"A serverless application is used to process customer information and outputs a JSON file to an Amazon S3 bucket. AWS Lambda is used for processing the data. The data is sensitive and should be encrypted. How can a Developer modify the Lambda function to ensure the data is encrypted before it is uploaded to the S3 bucket?\",\"answers\":[{\"text\":\"Use the S3 managed key and call the GenerateDataKey API to encrypt the file\",\"isCorrect\":false},{\"text\":\"Use the default KMS key for S3 and encrypt the file using the Lambda code\",\"isCorrect\":false},{\"text\":\"Use the GenerateDataKey API, then use the data key to encrypt the file using the Lambda code\",\"isCorrect\":true},{\"text\":\"Enable serverside encryption on the S3 bucket and create a policy to enforce encryption\",\"isCorrect\":false}],\"explanation\":\"$2b\"},{\"question\":\"A Developer is troubleshooting an issue with a DynamoDB table. The table is used to store order information for a busy online store and uses the order date as the partition key. During busy periods writes to the table are being throttled despite the consumed throughput being well below the provisioned throughput. According to AWS best practices, how can the Developer resolve the issue at the LOWEST cost?\",\"answers\":[{\"text\":\"Increase the read and write capacity units for the table\",\"isCorrect\":false},{\"text\":\"Add a random number suffix to the partition key values\",\"isCorrect\":true},{\"text\":\"Use an Amazon SQS queue to buffer the incoming writes\",\"isCorrect\":false},{\"text\":\"Add a global secondary index to the table\",\"isCorrect\":false}],\"explanation\":\"$2c\"},{\"question\":\"A company is in the process of migrating an application from a monolithic architecture to a microservicesbased architecture. The developers need to refactor the application so that the many microservices can asynchronously communicate with each other in a decoupled manner. Which AWS services can be used for asynchronous message passing? (Select TWO.)\",\"answers\":[{\"text\":\"Amazon Kinesis\",\"isCorrect\":false},{\"text\":\"Amazon SQS\",\"isCorrect\":true},{\"text\":\"Amazon SNS\",\"isCorrect\":true},{\"text\":\"AWS Lambda\",\"isCorrect\":false},{\"text\":\"Amazon ECS\",\"isCorrect\":false}],\"explanation\":\"$2d\"},{\"question\":\"A company needs a version control system for collaborative software development. The solution must include support for batches of changes across multiple files and parallel branching. Which AWS service will meet these requirements?\",\"answers\":[{\"text\":\"Amazon S3\",\"isCorrect\":false},{\"text\":\"AWS CodePipeline\",\"isCorrect\":false},{\"text\":\"AWS CodeCommit\",\"isCorrect\":true},{\"text\":\"AWS CodeBuild\",\"isCorrect\":false}],\"explanation\":\"$2e\"},{\"question\":\"A static website is hosted on Amazon S3 using the bucket name of dctlabs.com. Some HTML pages on the site use JavaScript to download images that are located in the bucket https://dctlabsimages.s3.amazonaws.com/. Users have reported that the images are not being displayed. What is the MOST likely cause?\",\"answers\":[{\"text\":\"The dctlabsimages bucket is not in the same region as the dctlabs.com bucket\",\"isCorrect\":false},{\"text\":\"Amazon S3 Transfer Acceleration should be enabled on the dctlabs.com bucket\",\"isCorrect\":false},{\"text\":\"Cross Origin Resource Sharing is not enabled on the dctlabs.com bucket\",\"isCorrect\":false},{\"text\":\"Cross Origin Resource Sharing is not enabled on the dctlabsimages bucket\",\"isCorrect\":true}],\"explanation\":\"$2f\"},{\"question\":\"A company uses continuous integration and continuous delivery (CI/CD) systems. A Developer needs to automate the deployment of a software package to Amazon EC2 instances as well as to onpremises virtual servers. Which AWS service can be used for the software deployment?\",\"answers\":[{\"text\":\"AWS CodeDeploy\",\"isCorrect\":true},{\"text\":\"AWS CloudBuild\",\"isCorrect\":false},{\"text\":\"AWS CodePipeline\",\"isCorrect\":false},{\"text\":\"AWS Elastic Beanstalk\",\"isCorrect\":false}],\"explanation\":\"$30\"},{\"question\":\"A highly secured AWS environment has strict policies for granting access to Developers. A Developer requires the ability to use the API to call ec2:StartInstances and ec2:StopInstances. Which element of an IAM policy statement should be used to specify which APIs can be called?\",\"answers\":[{\"text\":\"Effect\",\"isCorrect\":false},{\"text\":\"Action\",\"isCorrect\":true},{\"text\":\"Condition\",\"isCorrect\":false},{\"text\":\"Resource\",\"isCorrect\":false}],\"explanation\":\"The Action element describes the specific action or actions that will be allowed or denied. Statements must include either an Action or NotAction element. Each AWS service has its own set of actions that describe tasks that you can perform with that service.For this scenario, the Action element might include the following JSON”\\\"Action\\\": [ \\\"ec2:StartInstances\\\", \\\"ec2:StopInstances\\\" ]CORRECT: \\\"Action\\\" is the correct answer.INCORRECT: \\\"Effect\\\" is incorrect. The Effect element is required and specifies whether the statement results in an allow or an explicit deny.INCORRECT: \\\"Resource\\\" is incorrect. The Resource element specifies the object or objects that the statement covers.INCORRECT: \\\"Condition\\\" is incorrect. The Condition element (or Condition block) lets you specify conditions for when a policy is in effect.References: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_action.html\"},{\"question\":\"A company wants to implement authentication for its new REST service using Amazon API Gateway. To authenticate the calls, each request must include HTTP headers with a client ID and user ID. These credentials must be compared to authentication data in an Amazon DynamoDB table. What MUST the company do to implement this authentication in API Gateway?\",\"answers\":[{\"text\":\"Implement an AWS Lambda authorizer that references the DynamoDB authentication table\",\"isCorrect\":true},{\"text\":\"Create a model that requires the credentials, then grant API Gateway access to the authentication table\",\"isCorrect\":false},{\"text\":\"Modify the integration requests to require the credentials, then grant API Gateway access to the authentication table\",\"isCorrect\":false},{\"text\":\"Implement an Amazon Cognito authorizer that references the DynamoDB authentication table\",\"isCorrect\":false}],\"explanation\":\"$31\"},{\"question\":\"A development team are creating a mobile application that customers will use to receive notifications and special offers. Users will not be required to log in. What is the MOST efficient method to grant users access to AWS resources?\",\"answers\":[{\"text\":\"Use an IAM SAML 2.0 identity provider to establish trust\",\"isCorrect\":false},{\"text\":\"Embed access keys in the application that have limited access to resources\",\"isCorrect\":false},{\"text\":\"Use Amazon Cognito Federated Identities and setup authentication using a Cognito User Pool\",\"isCorrect\":false},{\"text\":\"Use Amazon Cognito to associate unauthenticated users with an IAM role that has limited access to resources\",\"isCorrect\":true}],\"explanation\":\"$32\"},{\"question\":\"A Developer is storing sensitive documents in Amazon S3. The documents must be encrypted at rest and company policy mandates that the encryption keys must be rotated annually. What is the EASIEST way to achieve this?\",\"answers\":[{\"text\":\"Use AWS KMS with automatic key rotation\",\"isCorrect\":true},{\"text\":\"Export a key from AWS KMS to encrypt the data\",\"isCorrect\":false},{\"text\":\"Encrypt the data before sending it to Amazon S3\",\"isCorrect\":false},{\"text\":\"Import a custom key into AWS KMS with annual rotation enabled\",\"isCorrect\":false}],\"explanation\":\"$33\"},{\"question\":\"An application is being migrated into the cloud. The application is stateless and will run on a fleet of Amazon EC2 instances. The application should scale elastically. How can a Developer ensure that the number of instances available is sufficient for current demand?\",\"answers\":[{\"text\":\"Create a launch configuration and use Amazon CodeDeploy\",\"isCorrect\":false},{\"text\":\"Create a task definition and use an Amazon ECS cluster\",\"isCorrect\":false},{\"text\":\"Create a launch configuration and use Amazon EC2 Auto Scaling\",\"isCorrect\":true},{\"text\":\"Create a task definition and use an AWS Fargate cluster\",\"isCorrect\":false}],\"explanation\":\"$34\"},{\"question\":\"A Developer needs to configure an Elastic Load Balancer that is deployed through AWS Elastic Beanstalk. Where should the Developer place the loadbalancer.config file in the application source bundle?\",\"answers\":[{\"text\":\"In the bin folder\",\"isCorrect\":false},{\"text\":\"In the .ebextensions folder\",\"isCorrect\":true},{\"text\":\"In the loadbalancer.config.root\",\"isCorrect\":false},{\"text\":\"In the root of the source code\",\"isCorrect\":false}],\"explanation\":\"$35\"},{\"question\":\"A Developer is designing a faulttolerant environment where client sessions will be saved. How can the Developer ensure that no sessions are lost if an Amazon EC2 instance fails?\",\"answers\":[{\"text\":\"Use Elastic Load Balancer connection draining to stop sending requests to failing instances\",\"isCorrect\":false},{\"text\":\"Use Amazon DynamoDB to perform scalable session handling\",\"isCorrect\":true},{\"text\":\"Use sticky sessions with an Elastic Load Balancer target group\",\"isCorrect\":false},{\"text\":\"Use Amazon SQS to save session data\",\"isCorrect\":false}],\"explanation\":\"$36\"},{\"question\":\"A company runs a booking system for a medical practice. The AWS SDK is used to communicate with between several AWS services. Due to compliance requirements, the security department has requested that a record is made of all API calls. How can this requirement be met?\",\"answers\":[{\"text\":\"Use AWS XRay to trace the API calls and keep a record\",\"isCorrect\":false},{\"text\":\"Use Amazon CloudWatch logs to keep a history of API calls\",\"isCorrect\":false},{\"text\":\"Use an AWS Lambda to function to continually monitor API calls and log them to an Amazon S3 bucket\",\"isCorrect\":false},{\"text\":\"Use Amazon CloudTrail to keep a history of API calls\",\"isCorrect\":true}],\"explanation\":\"$37\"},{\"question\":\"The following permissions policy is applied to an IAM user account:{\\\"Version\\\": \\\"20121017\\\",\\\"Statement\\\": [{\\\"Effect\\\": \\\"Allow\\\",\\\"Action\\\": \\\"sqs:*\\\",\\\"Resource\\\": \\\"arn:aws:sqs:*:513246782345:stagingqueue*\\\"}]}Due to this policy, what Amazon SQS actions will the user be able to perform?\",\"answers\":[{\"text\":\"The user will be able to apply a resourcebased policy to the Amazon SQS queue named “stagingqueue”\",\"isCorrect\":false},{\"text\":\"The user will be able to create a queue named “stagingqueue“\",\"isCorrect\":false},{\"text\":\"The user will be granted crossaccount access from account number “513246782345” to queue “stagingqueue”\",\"isCorrect\":false},{\"text\":\"The user will be able to use all Amazon SQS actions, but only for queues with names begin with the string “stagingqueue“\",\"isCorrect\":true}],\"explanation\":\"$38\"},{\"question\":\"A company uses Amazon SQS to decouple an online application that generates memes. The SQS consumers poll the queue regularly to keep throughput high and this is proving to be costly and resource intensive. A Developer has been asked to review the system and propose changes that can reduce costs and the number of empty responses. What would be the BEST approach to MINIMIZING cost?\",\"answers\":[{\"text\":\"Set the Imaging queue ReceiveMessageWaitTimeSeconds attribute to 20 seconds\",\"isCorrect\":true},{\"text\":\"Set the imaging queue MessageRetentionPeriod attribute to 20 seconds\",\"isCorrect\":false},{\"text\":\"Set the DelaySeconds parameter of a message to 20 seconds\",\"isCorrect\":false},{\"text\":\"Set the imaging queue visibility Timeout attribute to 20 seconds\",\"isCorrect\":false}],\"explanation\":\"$39\"},{\"question\":\"A developer is planning to launch as serverless application composed of AWS Lambda, Amazon API Gateway, and Amazon DynamoDB. What is the EASIEST way to deploy the application using simple syntax?\",\"answers\":[{\"text\":\"Use AWS Elastic Beanstalk\",\"isCorrect\":false},{\"text\":\"Use the Serverless Application Model\",\"isCorrect\":true},{\"text\":\"Use AWS CloudFormation\",\"isCorrect\":false},{\"text\":\"Use the Serverless Application Repository\",\"isCorrect\":false}],\"explanation\":\"$3a\"},{\"question\":\"A team of Developers have been assigned to a new project. The team will be collaborating on the development and delivery of a new application and need a centralized private repository for managing source code. The repository should support updates from multiple sources. Which AWS service should the development team use?\",\"answers\":[{\"text\":\"AWS CodePipeline\",\"isCorrect\":false},{\"text\":\"AWS CodeCommit\",\"isCorrect\":true},{\"text\":\"AWS CodeBuild\",\"isCorrect\":false},{\"text\":\"AWS CodeDeploy\",\"isCorrect\":false}],\"explanation\":\"$3b\"},{\"question\":\"A customer-facing web application runs on Amazon EC2 with an Application Load Balancer and an Amazon RDS database back end. Recently, the security team noticed some SQL injection attacks and cross-site scripting attacks targeting the web application. Which service can a Developer use to protect against future attacks?\",\"answers\":[{\"text\":\"Security Groups\",\"isCorrect\":false},{\"text\":\"Network ACLs\",\"isCorrect\":false},{\"text\":\"AWS WAF\",\"isCorrect\":true},{\"text\":\"AWS KMS\",\"isCorrect\":false}],\"explanation\":\"AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources.AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or crosssite scripting, and rules that filter out specific traffic patterns you define.CORRECT: \\\"AWS WAF\\\" is the correct answer.INCORRECT: \\\"AWS KMS\\\" is incorrect as this service is used for creating and managing encryption keys.INCORRECT: \\\"Security Groups\\\" is incorrect as they are an instancelevel firewall. They do not have the ability to prevent SQL injection or crosssite scripting attacks.INCORRECT: \\\"Network ACLs\\\" is incorrect as this is a subnetlevel firewall. It doesn’t not have the ability to prevent SQL injection or crosssite scripting attacks.References: https://aws.amazon.com/waf/\"},{\"question\":\"A team of developers are adding an API layer to a multi-container Docker environment running on AWS Elastic Beanstalk. The clients-submitted method requests should be passed directly to the backend, without modification. Which integration type is MOST suitable for this solution?\",\"answers\":[{\"text\":\"AWS\",\"isCorrect\":false},{\"text\":\"HTTP\",\"isCorrect\":false},{\"text\":\"AWS_PROXY\",\"isCorrect\":false},{\"text\":\"HTTP_PROXY\",\"isCorrect\":true}],\"explanation\":\"$3c\"},{\"question\":\"A Developer is creating a microservices architecture for a modern application. The application will run on Docker containers. The Developer requires a serverless service. Which AWS service is MOST suitable?\",\"answers\":[{\"text\":\"AWS Elastic Beanstalk\",\"isCorrect\":false},{\"text\":\"AWS Fargate\",\"isCorrect\":true},{\"text\":\"Amazon ECS\",\"isCorrect\":false},{\"text\":\"AWS Lambda\",\"isCorrect\":false}],\"explanation\":\"$3d\"},{\"question\":\"An application is using Amazon DynamoDB as its data store and needs to be able to read 200 items per second as eventually consistent reads. Each item is 12 KB in size. What value should be set for the table's provisioned throughput for reads?\",\"answers\":[{\"text\":\"300 Read Capacity Units\",\"isCorrect\":true},{\"text\":\"600 Read Capacity Units\",\"isCorrect\":false},{\"text\":\"1200 Read Capacity Units\",\"isCorrect\":false},{\"text\":\"150 Read Capacity Units\",\"isCorrect\":false}],\"explanation\":\"$3e\"},{\"question\":\"A Developer manages a monitoring service for a fleet of IoT sensors in a major city. The monitoring application uses an Amazon Kinesis Data Stream with a group of EC2 instances processing the data. Amazon CloudWatch custom metrics show that the instances a reaching maximum processing capacity and there are insufficient shards in the Data Stream to handle the rate of data flow. What course of action should the Developer take to resolve the performance issues?\",\"answers\":[{\"text\":\"Increase the EC2 instance size\",\"isCorrect\":false},{\"text\":\"Increase the number of EC2 instances to match the number of shards\",\"isCorrect\":false},{\"text\":\"Increase the number of open shards\",\"isCorrect\":false},{\"text\":\"Increase the EC2 instance size and add shards to the stream\",\"isCorrect\":true}],\"explanation\":\"$3f\"},{\"question\":\"A Developer is using AWS SAM to create a template for deploying a serverless application. The Developer plans deploy a Lambda function using the template. Which resource type should the Developer specify?\",\"answers\":[{\"text\":\"AWS::Serverless:API\",\"isCorrect\":false},{\"text\":\"AWS::Serverless:Function\",\"isCorrect\":true},{\"text\":\"AWS::Serverless::Application\",\"isCorrect\":false},{\"text\":\"AWS::Serverless:LayerVersion\",\"isCorrect\":false}],\"explanation\":\"$40\"},{\"question\":\"A Developer has updated an AWS Lambda function and published a new version. To ensure the code is working as expected the Developer needs to initially direct a percentage of traffic to the new version and gradually increase this over time. It is important to be able to rollback if there are any issues reported. What is the BEST way the Developer can implement the migration to the new version SAFELY?\",\"answers\":[{\"text\":\"Use an Amazon Elastic Load Balancer to direct a percentage of traffic to each target group containing the Lambda function versions\",\"isCorrect\":false},{\"text\":\"Use an immutable update with a new ASG to deploy the new version in parallel, following testing cutover to the new version\",\"isCorrect\":false},{\"text\":\"Create an Amazon Route 53 weighted routing policy pointing to the current and new versions, assign a lower weight to the new version\",\"isCorrect\":false},{\"text\":\"Create an Alias, assign the current and new versions and use traffic shifting to assign a percentage of traffic to the new version\",\"isCorrect\":true}],\"explanation\":\"$41\"},{\"question\":\"An application uses AWS Lambda to process many files. The Lambda function takes approximately 3 minutes to process each file and does not return any important data. A Developer has written a script that will invoke the function using the AWS CLI. What is the FASTEST way to process all the files?\",\"answers\":[{\"text\":\"Invoke the Lambda function asynchronously with the invocation type RequestResponse and process the files sequentially\",\"isCorrect\":false},{\"text\":\"Invoke the Lambda function synchronously with the invocation type Event and process the files in parallel\",\"isCorrect\":false},{\"text\":\"Invoke the Lambda function synchronously with the invocation type RequestResponse and process the files sequentially\",\"isCorrect\":false},{\"text\":\"Invoke the Lambda function asynchronously with the invocation type Event and process the files in parallel\",\"isCorrect\":true}],\"explanation\":\"$42\"},{\"question\":\"A development team have deployed a new application and users have reported some performance issues. The developers need to enable monitoring for specific metrics with a data granularity of one second. How can this be achieved?\",\"answers\":[{\"text\":\"Create custom metrics and enable detailed monitoring\",\"isCorrect\":false},{\"text\":\"Do nothing, CloudWatch uses standard resolution metrics by default\",\"isCorrect\":false},{\"text\":\"Create custom metrics and configure them as standard resolution\",\"isCorrect\":false},{\"text\":\"Create custom metrics and configure them as high resolution\",\"isCorrect\":true}],\"explanation\":\"$43\"},{\"question\":\"A team of Developers are building a continuous integration and delivery pipeline using AWS Developer Tools. Which services should they use for running tests against source code and installing compiled code on their AWS resources? (Select TWO.)\",\"answers\":[{\"text\":\"AWS CodeBuild for running tests against source code\",\"isCorrect\":true},{\"text\":\"AWS Cloud9 for running tests against source code\",\"isCorrect\":false},{\"text\":\"AWS CodeDeploy for installing compiled code on their AWS resources\",\"isCorrect\":true},{\"text\":\"AWS CodeCommit for installing compiled code on their AWS resources\",\"isCorrect\":false},{\"text\":\"AWS CodePipeline for running tests against source code\",\"isCorrect\":false}],\"explanation\":\"$44\"},{\"question\":\"An application writes items to an Amazon DynamoDB table. As the application scales to thousands of instances, calls to the DynamoDB API generate occasional ThrottlingException errors. The application is coded in a language incompatible with the AWS SDK. How should the error be handled?\",\"answers\":[{\"text\":\"Pass API calls through Amazon API Gateway\",\"isCorrect\":false},{\"text\":\"Add exponential backoff to the application logic\",\"isCorrect\":true},{\"text\":\"Send the items to DynamoDB through Amazon Kinesis Data Firehose\",\"isCorrect\":false},{\"text\":\"Use Amazon SQS as an API message bus\",\"isCorrect\":false}],\"explanation\":\"$45\"},{\"question\":\"A Developer has completed some code updates and needs to deploy the updates to an Amazon Elastic Beanstalk environment. Due to the criticality of the application, the ability to quickly roll back must be prioritized of any other considerations. Which deployment policy should the Developer choose?\",\"answers\":[{\"text\":\"Rolling\",\"isCorrect\":false},{\"text\":\"Immutable\",\"isCorrect\":true},{\"text\":\"Rolling with additional batch\",\"isCorrect\":false},{\"text\":\"All at once\",\"isCorrect\":false}],\"explanation\":\"$46\"},{\"question\":\"A developer is preparing the resources for creating a multicontainer Docker environment on AWS Elastic Beanstalk. How can the developer define the Docker containers?\",\"answers\":[{\"text\":\"Define the containers in the Dockerrun.aws.json file in JSON format and save at the root of the source directory\",\"isCorrect\":true},{\"text\":\"Create a buildspec.yml file and save it at the root of the source directory\",\"isCorrect\":false},{\"text\":\"Define the containers in the Dockerrun.aws.json file in YAML format and save at the root of the source directory\",\"isCorrect\":false},{\"text\":\"Create a Docker.config file and save it in the .ebextensions folder at the root of the source directory\",\"isCorrect\":false}],\"explanation\":\"$47\"},{\"question\":\"An application is instrumented to generate traces using AWS XRay and generates a large amount of trace data. A Developer would like to use filter expressions to filter the results to specific key-value pairs added to custom subsegments. How should the Developer add the key-value pairs to the custom subsegments?\",\"answers\":[{\"text\":\"Add metadata to the custom subsegments\",\"isCorrect\":false},{\"text\":\"Add the keyvalue pairs to the Trace ID\",\"isCorrect\":false},{\"text\":\"Setup sampling for the custom subsegments\",\"isCorrect\":false},{\"text\":\"Add annotations to the custom subsegments\",\"isCorrect\":true}],\"explanation\":\"$48\"},{\"question\":\"A company is creating an application that will require users to access AWS services and allow them to reset their own passwords. Which of the following would allow the company to manage users and authorization while allowing users to reset their own passwords?\",\"answers\":[{\"text\":\"Amazon Cognito identity pools and AWS IAM\",\"isCorrect\":false},{\"text\":\"Amazon Cognito user pools and AWS KMS\",\"isCorrect\":false},{\"text\":\"Amazon Cognito identity pools and AWS STS\",\"isCorrect\":false},{\"text\":\"Amazon Cognito user pools and identity pools\",\"isCorrect\":true}],\"explanation\":\"$49\"},{\"question\":\"A Developer is designing a cloud native application. The application will use several AWS Lambda functions that will process items that the functions read from an event source. Which AWS services are supported for Lambda event source mappings? (Select THREE.)\",\"answers\":[{\"text\":\"Amazon Kinesis\",\"isCorrect\":true},{\"text\":\"Another Lambda function\",\"isCorrect\":false},{\"text\":\"Amazon DynamoDB\",\"isCorrect\":true},{\"text\":\"Amazon Simple Notification Service (SNS)\",\"isCorrect\":false},{\"text\":\"Amazon Simple Storage Service (S3)\",\"isCorrect\":false},{\"text\":\"Amazon Simple Queue Service (SQS)\",\"isCorrect\":true}],\"explanation\":\"$4a\"},{\"question\":\"AWS CodeBuild builds code for an application, creates a Docker image, pushes the image to Amazon Elastic Container Registry (ECR), and tags the image with a unique identifier. If the Developers already have AWS CLI configured on their workstations, how can the Docker images be pulled to the workstations?\",\"answers\":[{\"text\":\"Run the following: docker pull REPOSITORY URI : TAG\",\"isCorrect\":false},{\"text\":\"Run the output of the following: aws ecr getdownloadurlforlayer, and then run docker pull REPOSITORY URI : TAG\",\"isCorrect\":false},{\"text\":\"Run the output of the following: aws ecr getlogin, and then run docker pull REPOSITORY URI : TAG\",\"isCorrect\":true},{\"text\":\"Run the following: aws ecr getlogin, and then run: docker pull REPOSITORY URI : TAG\",\"isCorrect\":false}],\"explanation\":\"$4b\"}]}}]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n7:null\n"])</script><script>self.__next_f.push([1,"8:null\nc:[[\"$\",\"title\",\"0\",{\"children\":\"v0 App\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Created with v0\"}],[\"$\",\"meta\",\"2\",{\"name\":\"generator\",\"content\":\"v0.app\"}]]\n"])</script></body></html>