<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/dva-c02-quiz-app/_next/static/css/31d717072bdb6d85.css" data-precedence="next"/><link rel="stylesheet" href="/dva-c02-quiz-app/_next/static/css/14ea25f3945b7722.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/dva-c02-quiz-app/_next/static/chunks/webpack-b20bf3550849aea8.js"/><script src="/dva-c02-quiz-app/_next/static/chunks/4bd1b696-05e7186cde350658.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/684-c670c892a0ebb4b8.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/main-app-01726a8d9af88025.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/app/layout-7d8a5f63f536cdcf.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/261-2d9b76ccba401937.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/app/quiz/%5Bid%5D/page-d7cb26530e4508b3.js" async=""></script><title>v0 App</title><meta name="description" content="Created with v0"/><meta name="generator" content="v0.app"/><script src="/dva-c02-quiz-app/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans __variable_1f39b6 __variable_c20681"><div class="min-h-screen bg-gradient-to-br from-background via-background to-primary/5 flex items-center justify-center"><div class="text-center"><div class="w-16 h-16 border-4 border-primary border-t-transparent rounded-full animate-spin mx-auto mb-4"></div><p class="text-muted-foreground">Loading quiz...</p></div></div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><script src="/dva-c02-quiz-app/_next/static/chunks/webpack-b20bf3550849aea8.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7555,[],\"\"]\n3:I[1295,[],\"\"]\n4:I[9742,[\"177\",\"static/chunks/app/layout-7d8a5f63f536cdcf.js\"],\"Analytics\"]\n6:I[9665,[],\"OutletBoundary\"]\n9:I[9665,[],\"ViewportBoundary\"]\nb:I[9665,[],\"MetadataBoundary\"]\nd:I[6614,[],\"\"]\n:HL[\"/dva-c02-quiz-app/_next/static/css/31d717072bdb6d85.css\",\"style\"]\n:HL[\"/dva-c02-quiz-app/_next/static/css/14ea25f3945b7722.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"m2gRVUmWgr8EoZ9_W7ed7\",\"p\":\"/dva-c02-quiz-app\",\"c\":[\"\",\"quiz\",\"aws-developer-12\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"quiz\",{\"children\":[[\"id\",\"aws-developer-12\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/dva-c02-quiz-app/_next/static/css/31d717072bdb6d85.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/dva-c02-quiz-app/_next/static/css/14ea25f3945b7722.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"font-sans __variable_1f39b6 __variable_c20681\",\"children\":[[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"$L4\",null,{}]]}]}]]}],{\"children\":[\"quiz\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"id\",\"aws-developer-12\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",\"$undefined\",null,[\"$\",\"$L6\",null,{\"children\":[\"$L7\",\"$L8\",null]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"zkQ5hzixAnuxSiDlp6VEU\",{\"children\":[[\"$\",\"$L9\",null,{\"children\":\"$La\"}],null]}],[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$d\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"e:I[1184,[\"261\",\"static/chunks/261-2d9b76ccba401937.js\",\"200\",\"static/chunks/app/quiz/%5Bid%5D/page-d7cb26530e4508b3.js\"],\"default\"]\nf:T90a,"])</script><script>self.__next_f.push([1,"Each time you upload a new version of your application with the Elastic Beanstalk console or the EB CLI, Elastic Beanstalk creates an application version. If you don't delete versions that you no longer use, you will eventually reach the application version quota and be unable to create new versions of that application. You can avoid hitting the quota by applying an application version lifecycle policy to your applications. A lifecycle policy tells Elastic Beanstalk to delete application versions that are old or to delete application versions when the total number of versions for an application exceeds a specified number. Elastic Beanstalk applies an application's lifecycle policy each time you create a new application version and deletes up to 100 versions each time the lifecycle policy is applied. Elastic Beanstalk deletes old versions after creating the new version and does not count the new version towards the maximum number of versions defined in the policy. In the Retention section of the application version lifecycle settings, you can either choose to delete the source bundle in S3 or retain it. To solve the problem in the scenario, the Retention option must be configured to the Retain source bundle in S3. Hence, the correct answer is: Configure the retention setting to retain the source bundle in S3. The option that says: Modify the value of the Set application versions limit by the total count option to zero is incorrect because the minimum number that you can set for the application versions limit by total count is 1. The option that says: Trigger a Lambda function to copy the source code to another S3 bucket is incorrect. Although this is possible, this solution entails unnecessary configurations as you can just change the Retention of the application version lifecycle settings. The option that says: Modify the value of the Set the application versions limit by age option to zero is incorrect. You can't set an application version limit by age that is less than 1. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html https://aws.amazon.com/about-aws/whats-new/2017/05/aws-elastic-beanstalk-supports-version-lifecycle-management/ Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/"])</script><script>self.__next_f.push([1,"10:T7e6,The GenerateDataKey generates a unique symmetric data key for client-side encryption. This operation returns a plaintext copy of the data key and a copy that is encrypted under a customer master key (CMK) that you specify. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data. GenerateDataKey returns a unique data key for each request. The bytes in the plaintext key are not related to the caller or the CMK. Hence, the correct answer is: Use the GenerateDataKey API command to generate a data key for each file to encrypt them. Store the encrypted data key and the file. The option that says: Use the CreateKey API command to generate a CMK for each file to encrypt them is incorrect. A CMK costs $1/month, which means you'd have to spend over $1,000 just to generate unique encryption keys. In addition, a CMK can only encrypt 4 KB of data, which does not make it suitable for encrypting media files. The option that says: Enable the SSE-S3 for the S3 bucket. Directly store the files in the bucket is incorrect. When you use Server-Side Encryption with SSE-S3, each object is encrypted with a unique key. However, the scenario requires you to generate a unique key for each file prior to uploading. This means that the encryption must be done at the client-side. The option that says: Use an open-source key generator to produce a unique key. Use the key to encrypt the files is incorrect. Although this will save costs, it'll be difficult to manage the keys and you'd have to write your own logic for encrypting and decrypting the files. AWS KMS makes it a lot easier to manage the data keys. Also, you don't have to write your own code as every KMS function is wrapped in an API. References: https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html https://docs.aws.amazon.com/cli/latest/reference/kms/generate-data-key.html Check out this AWS KMS Cheat Sheet: https://tutorialsdojo.com/aws-key-management-service-aws-kms/11:Tddc,"])</script><script>self.__next_f.push([1,"AWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that maintain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by your business logic. Task and Parallel states can have a field named Retry, whose value must be an array of objects known as retriers. An individual retrier represents a certain number of retries, usually at increasing time intervals. A retrier contains the following fields: ErrorEquals A non-empty array of strings that match error names. When a state reports an error, Step Functions scans through the retriers. When the error name appears in this array, it implements the retry policy described in this retrier. IntervalSeconds An integer that represents the number of seconds before the first retry attempt (1 by default). MaxAttempts A positive integer that represents the maximum number of retry attempts (3 by default). If the error recurs more times than specified, retries cease and normal error handling resumes. A value of 0 specifies that the error or errors are never retried. BackoffRate The multiplier by which the retry interval increases during each attempt (2.0 by default) Task and Parallel states can have a field named Catch. This field's value must be an array of objects, known as catchers. A catcher contains the following fields. ErrorEquals A non-empty array of strings that match error names, specified exactly as they are with the retrier field of the same name. Next A string that must exactly match one of the state machine's state names. ResultPath A path that determines what input is sent to the state specified in the Next field. When a state reports an error and either there is no Retry field, or if retries fail to resolve the error, Step Functions scans through the catchers in the order listed in the array. When the error name appears in the value of a catcher's ErrorEquals field, the state machine transitions to the state named in the Next field. In the given scenario, you can use the Catch and Retry fields inside the state machine definition to capture an exception error and attempt to recover from it by automatically retrying the state. Hence, the correct answer is: Use Catch and Retry fields in the state machine definition. The option that says: Use a try and catch logic inside the application code to capture the exception error. Then, use a Retry field in the state definition is incorrect because you don't have to implement a try and catch logic yourself in the application code as this can already be easily done by declaring a Catch field in your state machine definition. The option that says: Use Catch and Retry fields inside the application code is incorrect because these fields are only applicable to the Amazon States Language. The option that says: Use a Catch field inside the application code to capture the exception error. Then, use a Retry field in the state definition is incorrect because the Catch field is explicitly a function of the Amazon States Language and will not work if implemented in the application code. References: https://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html https://aws.amazon.com/step-functions/ Check out this AWS Step Functions Cheat Sheet: https://tutorialsdojo.com/aws-step-functions/"])</script><script>self.__next_f.push([1,"12:T78d,A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the Cache-Control: max-age=0 header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint. To grant permission for a client, attach a policy of the following to an IAM execution role for the user. This policy allows the API Gateway execution service to invalidate the cache for requests on the specified resource (or resources). Hence, the correct answers are: - Include Cache-Control: max-age=0 HTTP header on the API request. - Grant permission to the client to invalidate caching when there’s a request using the IAM execution role. The option that says: Create a new REST API endpoint and disable caching is incorrect. You don’t have to create a new REST API endpoint so you can disable caching. Invalidate cache is already a built-in feature on API Gateway. The option that says: Include Cache-Control: no-cache HTTP header on the API request is incorrect. Although “no-cache” is a valid value for the Cache-Control HTTP header, it is not the right value when invalidating API Gateway cache. The Cache-Control: max-age=0 header must be used. The option that says: Set the new endpoint as a trigger for the lambda function is incorrect. There is no sense in setting a new endpoint as a trigger for the lambda function as you can use the existing endpoint. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html Check out this Amazon API Gateway Cheat Sheet: https://aws.amazon.com/api-gateway/13:T912,"])</script><script>self.__next_f.push([1,"When you update existing files in a CloudFront distribution, AWS recommends that you include some sort of version identifier either in your file names or in your directory names to give yourself better control over your content. This identifier might be a date-time stamp, a sequential number, or some other method of distinguishing two versions of the same object. For example, instead of naming a graphic file image.jpg, you might call it image_1.jpg. When you want to start serving a new version of the file, you'd name the new file image_2.jpg, and you'd update the links in your web application or website to point to image_2.jpg. Alternatively, you might put all graphics in an images_v1 directory and, when you want to start serving new versions of one or more graphics, you'd create a new images_v2 directory, and you'd update your links to point to that directory. With versioning, you don't have to wait for an object to expire before CloudFront begins to serve a new version of it, and you don't have to pay for object invalidation. Hence, the correct answer is: Update the images by using versioned file names. The option that says: Update the images by invalidating them from the edge caches is incorrect. While this will update the images, it is not a cost-efficient solution as you have to pay for the additional invalidation requests. The option that says: Disable the CloudFront distribution and re-enable it to update the images in all edge locations is incorrect. CloudFront distributes files to edge locations only when the files are requested, not when you put new or updated files in your origin. Therefore, this is not the proper approach to update the images. The option that says: Upload the new images in the S3 bucket and wait for the objects in the edge locations to expire to reflect the changes is incorrect. This a time-consuming solution as you have to wait for the objects to expire just to have them updated. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/UpdatingExistingObjects.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html https://aws.amazon.com/blogs/aws/simplified-multiple-object-invalidation-for-amazon-cloudfront/ Check out this Amazon CloudFront Cheat Sheet: https://tutorialsdojo.com/amazon-cloudfront/"])</script><script>self.__next_f.push([1,"14:T8b0,"])</script><script>self.__next_f.push([1,"As an alternative to using IAM roles and policies or Lambda Authorizers (formerly known as custom authorizers), you can use an Amazon Cognito User Pool to control who can access your API in Amazon API Gateway. To use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn't authorized to make the call because the client did not have credentials that could be authorized. Hence, the correct steps to implement the solution are as follows: - Create an Amazon Cognito User Pool. - On the API Gateway Console, create an authorizer using the Cognito User Pool ID. - Set the name of the header that will be used from the request to the Cognito User Pool as a token source for authorization. The option that says: Create an Amazon Cognito Identity Pool is incorrect because you can not use Cognito Identity Pool as an authorizer for API Gateway. The only valid authorizers for API Gateway are AWS Lambda and Amazon Cognito User Pool. The option that says: Choose and set the authentication provider for your website is incorrect because this step is done during the creation of a Cognito Identity Pool. The option that says: Set the name of the header that will be used from the request to the Cognito Identity Pool as a token source for authorization is incorrect because Cognito Identity Pool cannot be used as an authorizer for API Gateway. You should use the Cognito User Pool. References: https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/ https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html Check out this Amazon Cognito Cheat Sheet: https://tutorialsdojo.com/amazon-cognito/ https://tutorialsdojo.com/amazon-cognito-user-pools-and-identity-pools-explained/"])</script><script>self.__next_f.push([1,"15:Ta56,"])</script><script>self.__next_f.push([1,"Several AWS services, such as Amazon Simple Storage Service (Amazon S3) and Amazon Simple Notification Service (Amazon SNS), invoke functions asynchronously to process events. When you invoke a function asynchronously, you don't wait for a response from the function code. You hand off the event to Lambda and Lambda handles the rest. You can configure how Lambda handles errors and can send invocation records to a downstream resource to chain together components of your application. For asynchronous invocation, Lambda places the event in a queue and returns a success response without additional information. A separate process reads events from the queue and sends them to your function. To invoke a function asynchronously, set the invocation type parameter to Event. Since we are not concerned about waiting for any important data, we can just invoke the Lambda function asynchronously. Because of its non-blocking design, data processing in parallel would be a lot faster than running them in sequence in this kind of scenario. For example, a Lambda function that processes three files for four minutes each would take a total of 12 minutes processing time. If run asynchronously, ideally, the processing time would only take about four minutes. Hence, the correct answer is: Use asynchronous Event Lambda invocations. Configure the function to process the files in parallel. The option that says: Compress the files to reduce their size, then process them with synchronous RequestResponse Lambda invocations is incorrect. Compression may reduce the amount of time it takes to process a file, but the effect won't be as significant as when running them in parallel. The option that says: Use synchronous RequestResponse Lambda invocations. Process the files one by one is incorrect because this will not make any improvement in the current processing speed of the Lambda function. Additionally, there is no sense in running the Lambda function synchronously because it is not expected to return any important data. The option that says: Merge all of the files into a single file then process them all at once with an asynchronous Event Lambda invocation is incorrect because doing this negates the advantage of using an asynchronous invocation, which is parallelizing workloads. Additionally, there'll be a higher chance of the Lambda function hitting its maximum execution time. References: https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html https://aws.amazon.com/blogs/architecture/understanding-the-different-ways-to-invoke-lambda-functions/ Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"])</script><script>self.__next_f.push([1,"16:Ta26,"])</script><script>self.__next_f.push([1,"The AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to model and provision your cloud application resources using familiar programming languages. The AWS CDK has first-class support for TypeScript, JavaScript, Python, Java, and C#. The AWS CDK can also update your deployed resources after you modify your app using the appropriate CDK commands. You can think of the CDK as a cloud infrastructure \"compiler\". It provides a set of high-level class libraries, called Constructs, that abstract AWS cloud resources and encapsulate AWS best practices. Constructs can be snapped together into object-oriented CDK applications that precisely define your application infrastructure and take care of all the complex boilerplate logic. When you run your CDK application, it is compiled into a CloudFormation Template, the \"assembly language\" for AWS cloud infrastructure. The template is then ready for processing by the CloudFormation provisioning engine for deployment into your AWS account. The CDK tools make it easy to define your application infrastructure stack, while the CloudFormation service takes care of the safe and dependable provisioning of your stack. Since the scenario requires the provisioning of cloud resources using a programming language (Python), the correct answer is: Use AWS CDK to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda. The option that says: Use CloudFormation to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda is incorrect because CloudFormation only allows JSON and YAML in defining cloud resources in a stack. The option that says: Use AWS SDK for Python (boto3) to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda is incorrect because boto3 is just a library for Python that lets you use AWS resources programmatically, allowing easy integration with your application. The option that says: Use AWS CloudShell to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda is incorrect because AWS CloudShell simply provides a browser-based terminal for managing AWS resources. It does not offer the ability to define and manage infrastructure as code in a reusable way, as required by the scenario. References: https://aws.amazon.com/blogs/developer/aws-cdk-developer-preview/ https://aws.amazon.com/cdk/ Check out this AWS Cloud Development Kit Cheat Sheet: https://tutorialsdojo.com/aws-cloud-development-kit-cdk/"])</script><script>self.__next_f.push([1,"17:T979,"])</script><script>self.__next_f.push([1,"AWS X-ray analyzes and debugs production, distributed applications, such as those built using a microservices architecture. With, X-Ray, you can identify performance bottlenecks, edge case errors, and other hard to detect issues. A segment can break down the data about the work done into subsegments. Subsegments provide more granular timing information and details about downstream calls that your application made to fulfill the original request. A subsegment can contain additional details about a call to an AWS service, an external HTTP API, or an SQL database. You can define arbitrary subsegments to instrument specific functions or lines of code in your application. Subsegments extend a trace's segment with details about work done in order to serve a request. Each time you make a call with an instrumented client, the X-Ray SDK records the information generated in a subsegment. You can create additional subsegments to group other subsegments, to measure the performance of a section of code, or to record annotations and metadata. Hence, the correct answer is: Using AWS X-Ray, define an arbitrary subsegment inside the code to instrument the function. The option that says: Using AWS X-Ray, disable sampling to efficiently trace all requests for calls is incorrect because sampling is simply used to ensure efficient tracing and to provide a representative sample of the requests that your application serves. Additionally, sampling will help you save money by reducing the amount of traces for high-volume and unimportant requests. The option that says: Using CloudWatch, troubleshoot the issue by checking the logs is incorrect because CloudWatch is not suited for debugging applications. CloudWatch is just used to capture performance metrics and log data. But, it can not help you debug the applications' internal logic flow or determine where the potential bottlenecks are. The option that says: Use CloudTrail to record and store event logs for actions made by your function is incorrect because CloudTrail just tracks user activity and API usage to enable governance, compliance, operational auditing, and risk auditing of your AWS account. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-annotations https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-nodejs-subsegments.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/"])</script><script>self.__next_f.push([1,"18:T980,"])</script><script>self.__next_f.push([1,"A cold start happens when a system needs to create a new resource in response to an event/request. Cold starts are not unique to Lambda. There are also cold starts in container orchestration, high-performance computing, or any places where IT resources need to be spun up. In AWS Lambda, whenever you execute a helper function/pre-handler code where you need to do things like pulling data from an S3 bucket, connecting to a database, pulling configuration information and dependencies, or anything of the sorts, it gets executed on the INIT code where the partial cold start occurs. It's important to note that basically everything that you're doing outside of the handler function will block its execution. When it comes to thinking about pre handler code dependencies that you want to use, remember that less is more. The more targeted you are at the resource that you include, the better the overall performance your function will have during its execution. You also have the option to tweak the power of the resources that run your function by increasing the memory allocated to your function to optimize its overall performance. Hence, the correct answers are: - Reduce the deployment package’s size by including only the needed modules from the AWS SDK for Java. - Increase the memory allocation setting for the Lambda function. The option that says: Increase the timeout setting for the Lambda function is incorrect as this will not reduce the cold start time. This is usually done in solving the problem of execution timeout. The option that says: Run the Lambda function in a VPC to gain access to Amazon's high-end infrastructure is incorrect because this solution will give no performance gain that will reduce the cold start of the Lambda function. This is usually done to a Lambda function to access private resources in a VPC. The option that says: Add the Spring Framework to the project and enable dependency injection is incorrect as doing this can have a significant increase in your function's cold start time. References: https://aws.amazon.com/blogs/compute/new-for-aws-lambda-predictable-start-up-times-with-provisioned-concurrency/ https://github.com/awslabs/aws-serverless-java-container/wiki/Quick-start---Spring-Boot Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/ End Cold Starts in Your Serverless Apps with AWS Lambda Provisioned Concurrency: https://youtu.be/EML6FKBdsNU"])</script><script>self.__next_f.push([1,"19:T607,A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Google, Facebook, Amazon, or Apple, and through SAML identity providers. Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limited-privilege AWS credentials to access other AWS services. Hence, the correct answer is: Amazon Cognito Identity Pools and User Pools. Amazon Cognito Identity Pools and IAM Role are incorrect. The solution is not enough to meet the requirements as you have to use Cognito User pools to allow users to sign in to your application. Amazon Cognito User Pools and AWS Key Management Service (KMS) are incorrect because AWS KMS is just a service that is used to encrypt data. Amazon User Pools and AWS Security Token Service (STS) are incorrect. While it is true that you need AWS STS to allow users to access Amazon S3, it is already abstracted by the Amazon Cognito Identity Pools. That being said, you have to configure an Identity Pool to accept users federated with your Cognito User Pool. References: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html Check out this Amazon Cognito Cheat Sheet: https://tutorialsdojo.com/amazon-cognito/1a:T628,Amazon S3 Glacier Deep Archive is an S3 storage class that provides secure and durable object storage for long-term retention of data that is accessed once or twice in a year. From just $0.00099 per GB-month (less than one-tenth of one cent, or about $1 per TB-month), S3 Glacier Deep Archive offers the lowest cost storage in the cloud, at prices significantly lower than storing and maintaining data in on-premises magnetic tape libraries or archiving data off-site. Amazon S3 Glacier an"])</script><script>self.__next_f.push([1,"d S3 Glacier Deep Archive are designed to be the lowest-cost Amazon S3 storage classes, allowing you to archive large amounts of data at a very low cost. This makes it feasible to retain all the data you want for use cases like data lakes, analytics, IoT, machine learning, compliance, and media asset archiving. You pay only for what you need, with no minimum commitments or up-front fees. Hence, the correct answer is: Amazon S3 Glacier Deep Archive. Amazon S3 Glacier is incorrect. Although it is a valid solution for archiving data records, it is more expensive than the Amazon S3 Glacier Deep Archive. With Amazon S3 Glacier, storage is priced from $0.004 per gigabyte per month, while Amazon S3 Glacier Deep Archive is priced from $0.00099 per GB-month. Amazon S3 Infrequent Access and Amazon S3 One-Zone are both incorrect because these services are not suitable for data archiving. References: https://aws.amazon.com/about-aws/whats-new/2019/03/S3-glacier-deep-archive/ https://aws.amazon.com/s3/pricing/ Check out this Amazon S3 Cheat Sheet: https://tutorialsdojo.com/amazon-s3/1b:Ta38,"])</script><script>self.__next_f.push([1,"Amazon DynamoDB Accelerator (DAX) is a fully managed, in-memory caching service designed to enhance the performance of DynamoDB by delivering microsecond response times for read-intensive applications. By offloading read operations to DAX, applications can achieve significantly reduced latency. It is particularly beneficial for use cases such as real-time bidding, social gaming, and trading platforms that demand rapid data access. DAX seamlessly integrates with existing DynamoDB applications, requiring minimal code changes, and supports API compatibility, simplifying the process of adding caching to DynamoDB tables. In addition to performance improvements, DynamoDB Accelerator reduces the operational complexity of managing cache invalidation, data consistency, and cluster maintenance. It operates as a write-through cache, ensuring that data written to DynamoDB is automatically synchronized with the cache, maintaining consistency between the two. DynamoDB Accelerator also supports encryption at rest and transit, enhancing security for sensitive data. With its ability to handle millions of requests per second and its managed infrastructure, DynamoDB Accelerator (DAX) provides a scalable and reliable solution for high-performance data retrieval applications. Hence, the correct answer is: Amazon DynamoDB Accelerator (DAX). The option that says: Amazon CloudFront is incorrect because it is primarily designed as a Content Delivery Network (CDN) to cache and deliver static content like videos, images, or web pages with low latency. While it can simply enhance the performance of S3-hosted video lectures, it does not directly address DynamoDB read operations or latency issues. The option that says:Amazon Elastic Load Balancing is incorrect. This option only distributes incoming traffic across multiple targets, such as EC2 instances or containers to ensure high availability and fault tolerance. It is unrelated to DynamoDB performance optimization in a serverless architecture and cannot reduce read latency on DynamoDB. The option that says: Amazon DynamoDB Streams is incorrect because it is typically used to capture table data changes for event-driven workflows. It simply provides a mechanism to process data updates but does not optimize or improve the performance of read operations, which is the issue described in this scenario. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"1c:T75a,The AWS STS DecodeAuthorizationMessage API decodes additional information about the authorization status of a request from an encoded message returned in response to an AWS request. For example, if a user is not authorized to perform an operation that he or she has requested, the request returns a Client.UnauthorizedOperation response (an HTTP 403 response). Some AWS operations additionally return an encoded message that can provide details about this authorization failure. The message is encoded because the details of the authorization status can constitute privileged information that the user who requested the operation should not see. To decode an authorization status message, a user must be granted permissions via an IAM policy to request the DecodeAuthorizationMessage (sts:DecodeAuthorizationMessage) action. Hence, the correct answer is: Decode the message by calling the AWS STS decode-authorization-message command. The option that says: Decode the message by calling the AWS KMS decrypt command is incorrect. This will fail because the encoded message isn't encrypted by an AWS KMS key; therefore, you won't be able to decrypt it via KMS. The option that says: Decode the message by calling the AWS IAM decode-authorization-message command is incorrect because this command is not available to AWS IAM. The option that says: Decode the message using an external cryptography library is incorrect because the encoded message is generated from the AWS STS. Therefore, only the AWS STS service can decode it. You won't be able to decrypt it with a KMS key or any external key. References: https://docs.aws.amazon.com/STS/latest/APIReference/API_DecodeAuthorizationMessage.html https://docs.aws.amazon.com/cli/latest/reference/sts/decode-authorization-message.html Check out this AWS IAM Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/1d:T750,AWS X-Ray analyzes and debugs production, distributed applications, such as those built using a microservices architecture. With, X-Ray, you can ident"])</script><script>self.__next_f.push([1,"ify performance bottlenecks, edge case errors, and other hard to detect issues. When you instrument your application, the X-Ray SDK records information about incoming and outgoing requests, the AWS resources used, and the application itself. You can add other information to the segmented document as annotations and metadata. Annotations and metadata are aggregated at the trace level and can be added to any segment or subsegment. Annotations are simple key-value pairs that are indexed for use with filter expressions. Use annotations to record data that you want to use to group traces in the console, or when calling the GetTraceSummaries API. Hence, the correct answer is: Annotations. Metadata is incorrect because you can not group traces with it. Metadata are key-value pairs with values of any type, including objects and lists, but that is not indexed. You commonly use metadata to record data that you want to store in the trace but don't need to search for traces. Sampling is incorrect because it is just used to ensure efficient tracing and to provide a representative sample of the requests that your application serves. Additionally, sampling will help you save money by reducing the amount of traces for high-volume and unimportant requests. Subsegment is incorrect because it is only used to provide more granular timing information and details about downstream calls that your application made to fulfill the original request. It cannot group traces from recorded data. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-nodejs-segment.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/1e:T6a2,A usage plan specifies who can access one or more deployed API stages and methods—and also how much and how fast they can access them. The plan uses API keys to identify API clients and meters access to the associated API stages for each key. It also lets you configure throttling limits and quota limits that are en"])</script><script>self.__next_f.push([1,"forced on individual client API keys. This feature allows developers to build and monetize APIs and to create ecosystems around them. You can create usage plans for different levels of access (Bronze, Silver, and Gold), different categories of users (Student, Individual, Professional, or Enterprise), and so forth. Hence, the correct answer is: Create three Usage Plans. Specify a quota and throttle requests according to the level of access. The option that says: Create three stages. Specify a quota and throttle requests according to the level of access is incorrect because you have to create a usage plan to monetize APIs. Moreover, you can't specify a quota in a stage. The option that says: Create three Authorizers to control API access is incorrect as this will just deny unauthorized users. The solution must regulate API access from authorized users. The option that says: Create three stages and enable CloudWatch metrics. Set up an alarm for each stage according to the ApiName, Method, Resource, and Stage dimensions is incorrect. You can't regulate and monetize API access through CloudWatch. References: https://aws.amazon.com/blogs/aws/new-usage-plans-for-amazon-api-gateway/ https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/1f:T75f,To manage large Amazon Simple Queue Service (Amazon SQS) messages, you can use Amazon Simple Storage Service (Amazon S3) and the Amazon SQS Extended Client Library for Java. This is especially useful for storing and consuming messages up to 2 GB. Unless your application requires repeatedly creating queues and leaving them inactive or storing large amounts of data in your queues, consider using Amazon S3 for storing your data. You can use the Amazon SQS Extended Client Library for Java to do the following: - Specify whether messages are always stored in Amazon S3 or only when the size of a message exceeds 256 KB - Send a message that references a single"])</script><script>self.__next_f.push([1," message object stored in an S3 bucket - Retrieve the message object from an S3 bucket - Delete the message object from an S3 bucket You can use the Amazon SQS Extended Client Library for Java to manage Amazon SQS messages using Amazon S3 only with the AWS SDK for Java. You can't do this with the AWS CLI, the Amazon SQS console, the Amazon SQS HTTP API, or any of the other AWS SDKs. Hence, the correct answer is: Use Amazon S3 and the Amazon SQS Extended Client Library for Java. Using Amazon S3 and the Amazon SQS Console is incorrect because the SQS Console does not support the storing of SQS messages that exceed 256 KB. Using Amazon S3 and the Amazon SQS HTTP API is incorrect because you can't store large SQS messages in an S3 bucket using the SQS HTTP API. Using Amazon S3 and the Amazon SQS CLI is incorrect because the SQS CLI does not support storing of SQS messages to Amazon S3 via the Amazon SQS Extended Client Library for Java. References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/quotas-messages.html Check out this Amazon SQS Cheat Sheet: https://tutorialsdojo.com/amazon-sqs/20:Ta52,"])</script><script>self.__next_f.push([1,"DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real-time. Amazon DynamoDB is integrated with AWS Lambda so that you can create triggers—pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables. If you enable DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with an AWS Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records. The Lambda function can perform any actions you specify, such as sending a notification or initiating a workflow. For example, you can write a Lambda function to persist changes from one DynamoDB database to another. Hence, the correct answer is: Enable DynamoDB Streams to stream all the changes from the Customer service table and trigger a Lambda function to update the Payment service table. The option that says: Create a Firehose stream to stream all the changes from the Customer service table and trigger a Lambda function to update the Payment service table is incorrect because DynamoDB does not support Data Firehose, hence, this solution is not possible. The option that says: Use a Kinesis data stream to stream all the changes from the Customer service database directly into the Payment service table is incorrect. Kinesis Data Stream is a valid service for streaming changes from a DynamoDB table. However, it cannot directly write stream records to a DynamoDB table. You need a processing layer that will poll records from the stream and update the other DynamoDB table. The option that says: Create a scheduled Amazon EventBridge (Amazon CloudWatch Events) rule that invokes a Lambda function every minute to update the changes from the Customer service table into the Payment service table is incorrect. With this method, you would have to update new items in batches. Take note that the requirement is to stream changes to the Payment service table in near real-time. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/ Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"21:T80c,"])</script><script>self.__next_f.push([1,"If you terminate a container instance in the RUNNING state, that container instance is automatically removed or deregistered from the cluster. However, if you terminate a container instance in the STOPPED state, that container instance isn't automatically removed from the cluster. To deregister your container instance from the cluster, you should deregister it after terminating it in the STOPPED state by using the Amazon ECS Console or AWS Command Line Interface. The deregistered container instance will no longer appear as a resource in your Amazon ECS cluster. Hence, the correct answer is: When a container instance is terminated in the stopped state, the container instance is not automatically deregistered from the cluster. The option that says: When a container instance is terminated in the running state, the container instance is not automatically deregistered from the cluster is incorrect because terminating a container instance in the RUNNING state will automatically deregister it from the cluster. The option that says: After terminating the container instance in the running state, the container instance must be manually deregistered in the Amazon ECS Console is incorrect because you do not have to manually deregister a container instance terminated in the RUNNING state. It will automatically be deregistered from the cluster. The option that says: After terminating the container instance in the stopped state, the container instance must be manually deregistered in the Amazon EC2 Console since it was launched using the the EC2 launch type is incorrect because while it is true that you should manually deregister the container instance terminated in the STOPPED state, this should be done in the Amazon ECS Console and not on the Amazon EC2 Console. References: https://aws.amazon.com/premiumsupport/knowledge-center/deregister-ecs-instance/ https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html Check out this Amazon ECS Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/"])</script><script>self.__next_f.push([1,"22:T67f,ModuleNotFoundError and Module cannot be loaded are common errors for Lambda functions. These errors are usually due to an incorrect folder structure or file permissions with the deployment package .zip file. To fix this error: 1. Install all dependency modules local to the function project. 2. Build the deployment package by zipping up the project folder for deployment to Lambda. 3. Upload the deployment package. Hence, the correct answer is: Install the missing modules locally to your application’s folder. Package the folder into a ZIP file and upload it to AWS Lambda. The option that says: Import the missing modules in the Lambda code. The Lambda function will automatically install them when invoked is incorrect. This will still result in the same error because the Lambda function won't be able to recognize the modules that are defined in your code. The option that says: Download the missing modules in the lib directory. Package all files under the lib directory into a ZIP file and upload it to AWS Lambda is incorrect. This won't work. The modules and your Lambda code must be under the same directory level before packaging them into a ZIP file. The option that says: Run a Linux command inside the Lambda function to install the missing modules is incorrect. Although you can run Linux commands via custom runtimes in AWS Lambda, you can't install dependencies directly on a Lambda function. References: https://aws.amazon.com/premiumsupport/knowledge-center/build-python-lambda-deployment-package/ https://docs.aws.amazon.com/lambda/latest/dg/python-package.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/23:T82b,"])</script><script>self.__next_f.push([1,"With Amazon DynamoDB transactions, you can group multiple actions together and submit them as a single all-or-nothing TransactWriteItems or TransactGetItems operation. TransactWriteItems is a synchronous and idempotent write operation that groups up to 25 write actions in a single all-or-nothing operation. These actions can target up to 25 distinct items in one or more DynamoDB tables within the same AWS account and in the same Region. The aggregate size of the items in the transaction cannot exceed 4 MB. The actions are completed atomically so that either all of them succeed or none of them succeeds. You can add the following types of actions to a transaction: Put — Initiates a PutItem operation to create a new item or replace an old item with a new item, conditionally or without specifying any condition. Update — Initiates an UpdateItem operation to edit an existing item's attributes or add a new item to the table if it does not already exist. Use this action to add, delete, or update attributes on an existing item conditionally or without a condition. Delete — Initiates a DeleteItem operation to delete a single item in a table identified by its primary key. ConditionCheck — Checks that an item exists or checks the condition of specific attributes of the item. Hence, the correct answer is: TransactWriteItems BatchWriteItem is incorrect because this is not a single all-or-nothing operation, which means some requests in a BatchWriteItem operation can succeed or fail. Also, this DynamoDB operation does not support UpdateItem , which is one of the requirements in the scenario. Scan is incorrect because this just reads all of the items in a table. Query is incorrect because this simply enables you to get items in a table based on primary key values. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html https://aws.amazon.com/blogs/aws/new-amazon-dynamodb-transactions/ Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ Quick DynamoDB Overview: https://youtu.be/3ZOyUNIeorU"])</script><script>self.__next_f.push([1,"24:Te5b,"])</script><script>self.__next_f.push([1,"Amazon API Gateway Caching is a powerful feature that enables users to improve the performance and scalability of its APIs by storing responses in memory. By enabling caching, API Gateway can store responses to API requests for a specified period. With subsequent requests with the same parameters, API Gateway can quickly serve the cached response, reducing the need to invoke the backend Lambda function. This leads to faster response times, less load on backend systems, and a more cost-effective solution by minimizing the executions required for frequently accessed data. API Gateway caching is especially beneficial when the data served is relatively static or changes infrequently, such as aggregated metrics that update once daily. API Gateway caching is highly configurable, allowing users to specify the time-to-live (TTL) for cached data, meaning how long the data should be stored before being refreshed. This TTL can be adjusted based on the data's nature and the API's specific needs. For instance, if the video performance metrics are updated once every 24 hours, a TTL of 24 hours ensures that cached responses remain valid for each update cycle. Additionally, caching can be applied at both the stage and method levels, providing flexibility in managing which API endpoints should be cached and for how long. By using API Gateway's built-in caching feature, users can significantly reduce the latency experienced by end-users, enhance overall API performance, and improve user experience. This feature also integrates seamlessly with AWS's broader environment, ensuring reliable and scalable API management without additional infrastructure changes. Hence, the correct answer is: Enable Amazon API Gateway caching. The option that says: Enable cross-origin resource sharing (CORS) using the API Gateway is incorrect. Cross-origin resource sharing (CORS) is primarily used to enable secure resource sharing between different domains, typically for web applications running in the browser. It does not directly impact the performance or responsiveness of the API itself. While CORS is crucial for handling cross-origin requests, it doesn’t address the latency issues caused by frequent backend invocations and thus does not serve as a solution to improve the responsiveness of the API. The option that says: Use Amazon ElastiCache to store frequently requested data in memory is incorrect. While ElastiCache is an effective tool for caching, it typically requires changes to the backend architecture to integrate the cache layer with the Lambda functions. This approach would involve additional configuration and management overhead, which goes against the goal of improving responsiveness without altering the existing backend architecture. The option that says: Configure Amazon CloudFront as a caching layer in front of API Gateway is incorrect. Amazon CloudFront is primarily a content delivery network (CDN) that caches content at edge locations to speed up delivery for users across the globe. While it can help with reducing latency by caching API responses, it typically requires more complex configuration compared to API Gateway’s built-in caching. CloudFront is more suited for static content or large-scale applications where global distribution is essential. Still, for API performance improvements, API Gateway’s own caching is a more direct and simpler solution. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"])</script><script>self.__next_f.push([1,"25:Ta53,"])</script><script>self.__next_f.push([1,"For general use, the aws configure command is the fastest way to set up your AWS CLI installation. When you enter this command, the AWS CLI prompts you for four pieces of information: - Access Key ID - Secret Access Key - AWS Region - Output format Access keys consist of an access key ID and secret access keys are used to sign programmatic requests that you make to AWS. AWS Region identifies the AWS Region whose servers you want to send your requests to by default. This is typically the Region closest to you, but it can be any Region. For example, you can type us-west-2 to use US West (Oregon). This is the Region that all later requests are sent to unless you specify otherwise in an individual command. Output format specifies how the results are formatted. The value can be any of the values in the following list. If you don't specify an output format, JSON is used as the default. The InvalidInstanceID.NotFound error suggests that an instance does not exist. Ensure that you have indicated the AWS Region in which the instance is located if it's not in the default Region. This error may occur because the ID of a recently created instance has not propagated through the system.Since it was mentioned in the scenario that the EC2 instance already exists, we can conclude that there is a mismatch in the AWS Region configured in the CLI. It means that the EC2 instance is located in another Region which is why the developer got the error message. Hence, the correct answer is: The AWS Region name used to configure the AWS CLI does not match the region where the instance lives. The option that says: The AWS Region, where the programmatic access for the AWS CLI is created, does not match with the region where the instance lives is incorrect because the programmatic access is just another way of presenting yourself as an IAM User. IAM users are global entities which means it can not be associated with a particular region. The option that says: The AWS Access Key Id used to configure the AWS CLI is invalid is incorrect because you will get an InvalidAccessKeyId error as a response if you do not have the correct AWS Access Key Id. The scenario's issue is about the InvalidInstanceID.NotFound error. The option that says: The Image Id used in running the command for creating a snapshot is incorrect because the error is about the instance Id and not the Image Id. References: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html https://aws.amazon.com/getting-started/hands-on/backup-to-s3-cli/ Check out this AWS IAM Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"])</script><script>self.__next_f.push([1,"26:Tae4,"])</script><script>self.__next_f.push([1,"Here are the SAM CLI commands needed to deploy serverless applications: sam init - Initializes a serverless application with an AWS SAM template. The template provides a folder structure for your Lambda functions and is connected to an event source such as APIs, S3 buckets, or DynamoDB tables. This application includes everything you need to get started and to eventually extend it into a production-scale application. sam build - The sam build command builds any dependencies that your application has, and copies your application source code to folders under .aws-sam/build to be zipped and uploaded to Lambda. sam deploy - performs the functionality of sam package. You can use the sam deploy command to directly package and deploy your application. Since the application's runtime and dependencies are already defined, the next step is to call the sam build command to install and build the dependencies of the application. After running a series of local tests, you can now package and deploy the SAM template into an S3 bucket via the sam deploy command. Hence, the correct answer is: Build the SAM template in the local machine and call the sam deploy command to package and deploy the SAM template from an S3 bucket. The option that says: Build the SAM template in the local machine. Run the sam deploy command to package and deploy the SAM template from AWS CodePipeline is incorrect because it suggests using AWS CodePipeline directly with the sam deploy command. While CodePipeline can primarily be used for CI/CD, the sam deploy command itself does not interact directly with CodePipeline. Instead, it packages and deploys the application using an S3 bucket and CloudFormation. The option that says: Run the sam init command. Build the SAM template in the local machine and call the sam deploy command to package and deploy the SAM template from an S3 bucket is incorrect. You don't have to run the sam init command because from the conditions given, it is assumed that the runtime and the folder structure of the application have already been established. The option that says: Upload and build the SAM template in an EC2 instance. Run the sam deploy command to package and deploy the SAM template is incorrect because you can build the SAM template from your local computer by using the SAM CLI. Creating an EC2 instance just adds unnecessary costs. Also, SAM can only deploy from Amazon S3. References: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-getting-started-hello-world.html https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-command-reference.html Check out this AWS Serverless Application Model Cheat Sheet: https://tutorialsdojo.com/aws-serverless-application-model-sam/"])</script><script>self.__next_f.push([1,"27:Tccf,"])</script><script>self.__next_f.push([1,"Before you can access any AWS services via CLI/API from an EC2 instance, you must first configure and specify your access credentials. A more secure approach is by allowing the EC2 to assume an IAM role so it can access AWS services on your behalf. This way, your credentials are never stored or exposed. According to the scenario, the EC2 instances (that will be launched by the developers) need access to a DynamoDB table. First, we need to create an IAM role with permission that will allow access to the DynamoDB table. After creating the role, you must add the EC2 service as a trusted entity in the role's trust policy. You need to do this so EC2 instances can assume the IAM role. Afterwhich, you have to attach the following policy to the IAM Group: If the developers don’t have iam:PassRole permission, he or she can’t associate a role with the instance during launch. The PassRole permission helps you make sure that a user doesn’t pass a role to an EC2 instance where the role has more permissions than you want the user to have. For example, Alice might be allowed to perform only EC2 and S3 actions. If Alice could pass a role to the EC2 instance that allows additional actions, she could log into the instance, get temporary security credentials via the role she passed, and make calls to AWS that you don’t intend. Hence, the correct answer is: Create an IAM role with an IAM policy that will allow access to the DynamoDB table. Add the EC2 service to the trust policy of the role. Create a custom policy with iam:PassRole permission. Attach the policy to the IAM group. The option that says: Create an IAM role with an IAM policy that will allow access to the DynamoDB table. Add the EC2 service to the trust policy of the role. Create a custom policy with iam:GetRolePolicy and iam:PutRolePolicy permissions. Attach the policy to the IAM group is incorrect as you have to add the iam:PassRole permission instead. The iam:GetRolePolicy and iam:PutRolePolicy permissions are just used to grant permissions to retrieve and create an inline policy document that is embedded with a specified IAM role. The option that says: Create an IAM role with an IAM policy that will allow access to the EC2 instances. Add the DynamoDB service to the trust policy of the role. Create a custom policy with iam:GetRole permission. Attach the policy to the IAM group is incorrect because the scenario requires you to allow access to DynamoDB from the EC2 instance and not the other way around. Also, you need to have iam:PassRole permission. The option that says: Create an IAM role with an IAM policy that will allow access to the EC2 instances. Add the DynamoDB service to the trust policy of the role. Create a custom policy with iam:PassRole permission. Attach the policy to the IAM group is incorrect because the IAM policy of the role should allow access to DynamoDB and not EC2. The reason for this is to allow EC2 instances to call DynamoDB requests on the user's behalf. References: https://aws.amazon.com/blogs/security/granting-permission-to-launch-ec2-instances-with-iam-roles-passrole-permission/ https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html Check out this AWS IAM Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"])</script><script>self.__next_f.push([1,"28:Ta93,"])</script><script>self.__next_f.push([1,"AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). Below is the summary of the available STS API: AssumeRole - is useful for allowing existing IAM users to access AWS resources that they don't already have access to. For example, the user might need access to resources in another AWS account. It is also useful as a means to temporarily gain privileged access—for example, to provide multi-factor authentication (MFA). You must call this API using existing IAM user credentials. AssumeRoleWithWebIdentity - returns a set of temporary security credentials for federated users who are authenticated through a public identity provider. Examples of public identity providers include Login with Amazon, Facebook, Google, or any OpenID Connect (OIDC)-compatible identity provider. AssumeRoleWithSAML - returns a set of temporary security credentials for federated users who are authenticated by your organization's existing identity system. The users must also use SAML 2.0 (Security Assertion Markup Language) to pass authentication and authorization information to AWS. This API operation is useful in organizations that have integrated their identity systems (such as Windows Active Directory or OpenLDAP) with software that can produce SAML assertions. GetFederationToken - returns a set of temporary security credentials (consisting of an access key ID, a secret access key, and a security token) for a federated user. A typical use is in a proxy application that gets temporary security credentials on behalf of distributed applications inside a corporate network. You must call the GetFederationToken operation using the long-term security credentials of an IAM user. GetSessionToken - returns a set of temporary security credentials to an existing IAM user. This is useful for providing enhanced security, such as allowing AWS requests only when MFA is enabled for the IAM user. Because the credentials are temporary, they provide enhanced security when you have an IAM user who accesses your resources through a less secure environment. All of the options given provide temporary credentials to make API calls against AWS resources, but GetSessionToken is the only API that supports MFA. Hence, the correct answer is GetSessionToken. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html#stsapi_comparison https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html Check out this AWS IAM Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"])</script><script>self.__next_f.push([1,"29:Tccf,"])</script><script>self.__next_f.push([1,"AWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that maintain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by your business logic. Step Functions can help solve the problem of timeout errors of Lambda functions. Imagine a Lambda function that has four utility functions that are run sequentially. Each of those functions takes 5 minutes to finish which translates to a total execution time of 20 minutes. This is a problem since Lambda can only run for a maximum of 15 minutes. To solve this, we can refactor the functions inside the Lambda function into individual Step Functions states. This way, each function is contained in a separate Lambda function, which has its own execution timeout. Individual states can make decisions based on their input, perform actions, and pass output to other states. In AWS Step Functions, you define your workflows in the Amazon States Language. The Step Functions console provides a graphical representation of that state machine to help visualize your application logic. States are elements in your state machine. A state is referred to by its name, which can be any string, but must be unique within the scope of the entire state machine. States can perform a variety of functions in your state machine: Task State - Do some work in your state machine Choice State - Make a choice between branches of execution Fail or Succeed State - Stop execution with failure or success Pass State - Simply pass its input to its output or inject some fixed data, without performing work. Wait State - Provide a delay for a certain amount of time or until a specified time/date. Parallel State - Begin parallel branches of execution. Map State - Dynamically iterate steps. Out of all the types of State, only the Task State and the Parallel State can be used to run processes in the state machine. In the given scenario, the application logic inside the Lambda function process data synchronously. In this case, Task State should be used. Pass State is incorrect because this type of state cannot perform work as it simply passes its input data to its output. Pass State is mainly used for constructing and debugging state machines. Parallel State is incorrect. Although it can be used to run processes in a state machine, this type of state should only be used when you want to run processes asynchronously. Parallel state executes each branch concurrently and independently. In the given scenario, the Lambda function processes data synchronously. This means that each output of a function is piped as an input to the next function. The Task State is much more applicable in this scenario. Wait State is incorrect because this type of state just provides a delay mechanism to your state machine. References: https://aws.amazon.com/step-functions/ https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-task-state.html Check out this AWS Step Functions Cheat Sheet: https://tutorialsdojo.com/aws-step-functions/"])</script><script>self.__next_f.push([1,"2a:T58e,You can use Amazon Cognito to deliver temporary, limited-privilege credentials to your application so that your users can access AWS resources. Amazon Cognito identity pools support both authenticated and unauthenticated identities. You can retrieve a unique Amazon Cognito identifier (identity ID) for your end-user immediately if you're allowing unauthenticated users or after you've set the login tokens in the credentials provider if you're authenticating users. When your mobile app authenticates with the Identity Provider (IdP) using Amazon Cognito, the token returned from the IdP is passed to Amazon Cognito, which then returns a Cognito ID for the user. This Cognito ID is used to provide a set of temporary, limited-privilege AWS credentials through the Cognito Identity Pool. Hence, the correct answer is: Cognito ID. Cognito SDK is incorrect because this is not a unique Amazon Cognito identifier but a software development kit that is available in various programming languages. Cognito Key Pair is incorrect because this is not a unique Amazon Cognito identifier but a cryptography key. Cognito API is incorrect because this is not a unique Amazon Cognito identifier and is primarily used as an Application Programming Interface. Reference: http://docs.aws.amazon.com/cognito/latest/developerguide/getting-credentials.html Check out this Amazon Cognito Cheat Sheet: https://tutorialsdojo.com/amazon-cognito/2b:Ta4e,"])</script><script>self.__next_f.push([1,"A launch template is an instance configuration template that an Auto Scaling group uses to launch EC2 instances. When you create a launch template, you specify information for the instances. Include the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping. If you've launched an EC2 instance before, you specified the same information in order to launch the instance. You can specify your launch template with multiple Auto Scaling groups. However, you can only specify one launch template for an Auto Scaling group at a time, and you can't modify a launch template after you've created it. To change the launch template for an Auto Scaling group, you must create a launch template and then update your Auto Scaling group with it. Access to AWS resources requires permissions. You can create IAM roles and users that include the permissions that you need for the CloudWatch agent to write metrics to CloudWatch and for the CloudWatch agent to communicate with Amazon EC2 and AWS Systems Manager. You use IAM roles on Amazon EC2 instances, and you use IAM users with on-premises servers. In the given scenario, we can create a launch template and specify an IAM role with cloudwatch:PutMetricData permission. Then, use that launch template to create an auto-scaling group. Hence, the correct answer is: Create an IAM role with cloudwatch:PutMetricData permission for the new Auto Scaling launch template from which you launch instances. The option that says: Create an IAM user with programmatic access. Attach a cloudwatch:PutMetricData permission and store the access key and secret key in the instance's configuration file is incorrect because using IAM roles for applications that run on Amazon EC2 instances to provide credentials to the application is more secure. The option that says: Create an IAM user with programmatic access. Attach a cloudwatch:PutMetricData permission and update the Auto Scaling launch template to insert the access key and secret key into the instance via user data is incorrect because you can't update a launch template after you've created it. The option that says: Modify the existing Auto Scaling launch template to use an IAM role with the cloudwatch:PutMetricData permission for the instances is incorrect because modifying an existing launch template is not possible. References: https://docs.aws.amazon.com/autoscaling/ec2/userguide/change-launch-config.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates.html Check out this Amazon CloudWatch Cheat Sheet: https://tutorialsdojo.com/amazon-cloudwatch/"])</script><script>self.__next_f.push([1,"2c:Td00,"])</script><script>self.__next_f.push([1,"In DynamoDB, you can control access to individual data items and attributes in a table. For example, you can do the following: - Grant permissions on a table, but restrict access to specific items in that table based on certain primary key values. An example might be a social networking app for games, where all users' saved game data is stored in a single table, but no users can access data items that they do not own, as shown in the following illustration: - Hide information so that only a subset of attributes is visible to the user. An example might be an app that displays flight data for nearby airports, based on the user's location. Airline names, arrival and departure times, and flight numbers are all displayed. However, attributes such as pilot names or the number of passengers are hidden, as shown in the following illustration: To implement this kind of fine-grained access control, you write an IAM permissions policy that specifies conditions for accessing security credentials and the associated permissions. You then apply the policy to IAM users, groups, or roles that you create using the IAM console. Your IAM policy can restrict access to individual items in a table, access to the attributes in those items, or both at the same time. You can optionally use web identity federation to control access by users who are authenticated by Login with Amazon, Facebook, or Google. You use the IAM Condition element to implement a fine-grained access control policy. By adding a Condition element to a permissions policy, you can allow or deny access to items and attributes in DynamoDB tables and indexes, based upon your particular business requirements. In the given scenario, we are only required to restrict access to specific items in the table based on User Id which is the partition key. We can achieve this by inserting a dynamodb:LeadingKeys condition key to the IAM policy associated with the Identity provider's role. Hence, the correct answer is: Modify the IAM Policy associated with the Identity provider’s role by adding a dynamodb:LeadingKeys condition key. The option that says: Modify the IAM Policy associated with the Identity provider's role by adding a dynamodb:Attributes condition key to the user IDs is incorrect because this type of condition key is used for granting permissions that will limit access to specific attributes in the table. Note that the question is requiring access control to the items. The option that says: Modify the IAM Policy associated with the Identity provider's role by adding a dynamodb:ReturnValues condition key to the user IDs is incorrect because this type of condition key is just used for getting the item attributes as they appear before or after they are updated. The option that says: Modify the IAM Policy associated with the Identity provider's role by adding a dynamodb:Select condition key to the user IDs is incorrect because this type of condition key is mainly used for specifying attributes to be returned in the result of a Query or Scan request. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html#FGAC_DDB.ConditionKeys https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WIF.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"2d:Tb83,"])</script><script>self.__next_f.push([1,"AWS Key Management Service (AWS KMS) is a managed service that makes it easy for you to create and control KMS keys, the encryption keys used to encrypt your data. KMS keys are protected by hardware security modules (HSMs) that are validated by the FIPS 140-2 Cryptographic Module Validation Program except in the China (Beijing) and China (Ningxia) Regions. AWS KMS is integrated with most other AWS services that encrypt your data. AWS KMS is also integrated with AWS CloudTrail to log the use of your KMS keys for auditing, regulatory, and compliance needs. You can perform the following key management functions in AWS KMS: - Create symmetric and asymmetric keys where the key material is only ever used within the service - Create symmetric keys where the key material is generated and used within a custom key store under your control. - Import your own symmetric key for use within the service. - Create both symmetric and asymmetric data key pairs for local use within your applications. - Define which IAM users and roles can manage keys. - Define which IAM users and roles can use keys to encrypt and decrypt data. - Choose to have keys that were generated by the service to be automatically rotated on an annual basis. - Temporarily disable keys so they cannot be used by anyone. - Re-enable disabled keys. - Schedule the deletion of keys that you no longer use. - Audit the use of keys by inspecting logs in AWS CloudTrail. By default, AWS KMS creates the key material for a KMS key. You cannot extract, export, view, or manage this key material. Also, you cannot delete this key material; you must delete the KMS key. However, you can import your own key material into a KMS key or create the key material for it in the AWS CloudHSM cluster associated with an AWS KMS custom key store. There are also types of KMS keys that are not eligible for automatic key rotation such as asymmetric keys, keys in custom key stores, and keys with imported key material. Hence, the correct answers are: - Re-enabling disabled keys. - Creation of symmetric encryption and asymmetric KMS keys. The option that says: Using AWS Certificate Manager as a custom key store is incorrect because you can simply use AWS CloudHSM as a custom key store for AWS KMS. The option that says: Importing a custom key material to an asymmetric KMS key is incorrect because you can only import your own key material into symmetric keys, not asymmetric keys. The option that says: Automatic key rotation for KMS keys in custom key stores is incorrect because automatic key rotation is only supported in symmetric encryption KMS keys. Automatic key rotation is not available for asymmetric keys, keys in custom key stores, and keys with imported key material. References: https://docs.aws.amazon.com/kms/latest/developerguide/overview.html https://aws.amazon.com/kms/faqs/ Check out this AWS KMS Cheat Sheet: https://tutorialsdojo.com/aws-key-management-service-aws-kms/"])</script><script>self.__next_f.push([1,"2e:T94b,"])</script><script>self.__next_f.push([1,"AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources. When you first create an AWS account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user and is accessed by signing in with the email address and password that you used to create the account. AWS strongly recommends that you do not use the root user for your everyday tasks, even the administrative ones. Access keys provide programmatic access to AWS. Do not embed access keys within unencrypted code or share these security credentials between users in your AWS account. For applications that need access to AWS, configure the program to retrieve temporary security credentials using an IAM role. Hence, the correct answers are: - Delete any access keys to your AWS account root user. - Use IAM roles for applications that need access to AWS services. The option that says: Save the access key in your application code for convenience is incorrect. Since access keys are long-term credentials, anyone who might get hold of your application code could easily use the access key inside it to use AWS services on behalf of your account as long as the credentials are valid. The option that says: Regularly rotate the credentials for all the account users except for the administrator user for tracking purposes is incorrect. AWS recommends changing your own passwords and access keys regularly. Make sure that all IAM users in your account do as well. That way, if a password or access key is compromised without your knowledge, you limit how long the credentials can be used to access your resources. The option that says: Maintain at least one access key for your AWS account root user is incorrect because AWS recommends deleting all access keys to root users. You can create a separate IAM user instead if you want to have full admin access over your AWS resources. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#delegate-using-roles https://docs.aws.amazon.com/IAM/latest/UserGuide/intro-structure.html Check out this AWS IAM Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"])</script><script>self.__next_f.push([1,"2f:Te27,"])</script><script>self.__next_f.push([1,"Amazon Cognito supports developer authenticated identities, in addition to web identity federation through Facebook (Identity Pools), Google (Identity Pools), Login with Amazon (Identity Pools), and Sign in with Apple (Identity Pools). With developer authenticated identities, you can register and authenticate users via your own existing authentication process, while still using Amazon Cognito to synchronize user data and access AWS resources. Using developer authenticated identities involves interaction between the end-user device, your backend for authentication, and Amazon Cognito. Developers can use their own authentication system with Cognito. What this means is that your app can benefit from all of the features of Amazon Cognito while utilizing your own authentication system. This works by your app requesting a unique identity ID for your end-users based on the identifier you use in your own authentication system. You can use the Cognito identity ID to save and synchronize user data across devices with the Cognito sync service or retrieve temporary, limited-privilege AWS credentials to securely access your AWS resources. The process is simple, you first request a token for your users by using the server-side Cognito API for developer authenticated identities. Cognito then creates a valid token for your users. You can then exchange this token with Amazon Secure Token Service for AWS credentials. With developer authenticated identities, a new API, GetOpenIdTokenForDeveloperIdentity, was introduced. This API call replaces the use of GetId and GetOpenIdToken (APIs needed in the basic authflow) from the device and should be called from your backend as part of your own authentication API. Because this API call is signed by your AWS credentials, Cognito can trust that the user identifier supplied in the API call is valid. This replaces the token validation Cognito performs with public providers. Hence, the correct answer is: Use developer-authenticated identities in Amazon Cognito to generate unique identifiers for the users. The option that says: Generate a universally unique identifier (UUID) for each device. Store the UUID with the user in a DynamoDB table is incorrect because this would produce multiple identifiers for a single user. The scenario requires you to identify users via a unique identifier regardless of the device they use. The option that says: Generate a unique IAM access key for each user and use the access key ID as the unique identifier is incorrect because this means that you'll have to create a unique IAM user (with programmatic access) for each user just for the sake of identification, which is impractical. IAM user is mainly used for accessing services in an AWS account and not for web application authentication. The option that says: Create an IAM Role for each user and use its Amazon Resource Name (ARN) as unique identifiers is incorrect because this requires creating an IAM Role for each user. An IAM Role is not suited for web application authentication, not to mention, that it has a limit per account. Using it to identify game users is infeasible and will not scale well, especially for an application with millions of users. References: https://docs.aws.amazon.com/cognito/latest/developerguide/developer-authenticated-identities.html https://aws.amazon.com/blogs/mobile/understanding-amazon-cognito-authentication-part-2-developer-authenticated-identities/ https://aws.amazon.com/blogs/mobile/amazon-cognito-announcing-developer-authenticated-identities/ Check out this Amazon Cognito Cheat Sheet: https://tutorialsdojo.com/amazon-cognito/"])</script><script>self.__next_f.push([1,"30:T4bc,You can add AWS Elastic Beanstalk configuration files (.ebextensions) to your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle. Hence, the correct answer is: Inside the .ebextensions folder. The option that says: Inside the package.json is incorrect. This is just the file that contains the libraries needed by the Node.js application to run. The option that says: Inside the .elasticbeanstalk folder is incorrect because this is simply the folder that contains the environment configuration settings for the current running environment. The option that says: Inside the MyApp folder at the root level is incorrect. Documents with a .config file extension should be placed in the .ebextensions folder. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/31:Tac1,"])</script><script>self.__next_f.push([1,"With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints. For example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, http://example.com). In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage, and calls a different web host (for example, beta.example.com). Similarly, stage variables can be used to specify a different AWS Lambda function name for each stage in your API. In the scenario, by creating a new stage named 'Beta', the company can safely test updates by routing internal traffic to this stage, which will reference the AccountService:Beta version of the Lambda function. This will allow testers to invoke the new version of the function while end users continue to access the stable, production-ready AccountService:Prod version via the 'Prod' stage. Hence, the correct answer is: Create a new stage named 'Beta' in Amazon API Gateway and use stage variables to reference the Lambda functions in Prod and Beta. The option that says: Modify the existing 'Prod' stage in Amazon API Gateway to create a new stage variable that references the Lambda functions in Prod and Beta is incorrect. The 'Prod' stage contains settings critical for the production environment, such as API keys and rate limits. Testing on this stage could disrupt live traffic and affect real users even though you're hitting a non-prod Lamdba function. The option that says: Update the integration request settings in the 'Prod' stage of the Amazon API Gateway to select the Lambda function alias based on a query parameter is incorrect. This is risky since it can expose the beta environment to end users as well. The option that says: Create a 'Beta' stage in Amazon API Gateway. In the Lambda function configuration, add a condition to the AccountService:Prod function that checks for an environment variable and, if set to 'Beta', invokes the AccountService:Beta alias instead is incorrect. This approach would require deploying code changes to the production function, which adds complexity and inadvertently affects the production environment. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html https://aws.amazon.com/blogs/compute/using-api-gateway-stage-variables-to-manage-lambda-functions/ Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"])</script><script>self.__next_f.push([1,"32:Te74,"])</script><script>self.__next_f.push([1,"If you need a simple way to configure an HTTPS endpoint in front of your Lambda function without having to learn and configure additional services besides Lambda, you can use Lambda function URLs. This can be useful in cases where you need to implement a simple webhook handler or form validator that runs within an individual Lambda function and does not require additional functionality beyond processing incoming requests. By using Lambda function URLs, you can directly invoke your Lambda function using a simple HTTPS request without needing to set up and configure additional services like API Gateway. This approach can be a simple and efficient way to handle incoming requests and integrate with other services or third-party platforms that require a publicly accessible HTTPS endpoint. There are two types of authorization available for Lambda function URLs: AWS_IAM - the function URL can only be invoked by an IAM user or role with the necessary permissions. This can be useful in cases where you need to restrict access to the Lambda function to a specific set of users or roles within your organization. NONE - anyone can invoke the Lambda function using the URL. This approach can be useful in cases where you want to make the Lambda function publicly accessible and do not require any additional authentication or authorization beyond the URL. However, you may still need to validate the incoming requests in the Lambda function to ensure that the request comes from a trusted source. By setting the \"lambda:FunctionUrlAuthType\" condition to \"NONE,\" the function will be publicly accessible without requiring any additional authentication. However, you still need to write custom authorization logic to verify the signature provided in the HTTP headers and ensure that the request is coming from a valid user. Hence the correct answer is: Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the \"lambda:FunctionUrlAuthType\": \"NONE\" condition is present. Write a custom authorization logic based on a signature provided in the HTTP headers. The option that says: Configure API Gateway to connect with the Lambda function using a Lambda proxy integration. Create a Lambda function authorizer to validate incoming requests based on a signature provided in the HTTP headers is incorrect. While it's a valid solution, this is not the best choice because it involves additional setup and configuration of API Gateway to only invoke a single Lambda function. The option that says: Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the \"lambda:FunctionUrlAuthType\": \"AWS_IAM\" condition is present is incorrect. This authentication type means that the Lambda function can only be invoked by an authorized IAM user or role. The scenario specifically mentions that each request are signed before they are received by the Lambda function The option that says: Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the \"lambda:CodeSigningConfigArn\": \"arn:aws:lambda:\u003cAWS_REGION\u003e:\u003cACCOUNT_NUMBER\u003e:code-signing-config:csc-\u003cSIGNING_SECRET\u003e\" condition is present is incorrect, as code signing is a security feature that verifies the integrity of code running in your Lambda functions and helps ensure that only trusted code is deployed. References: https://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html https://aws.amazon.com/blogs/aws/announcing-aws-lambda-function-urls-built-in-https-endpoints-for-single-function-microservices/ Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"])</script><script>self.__next_f.push([1,"33:T800,Standard generic and preconfigured Docker platforms on Elastic Beanstalk support only a single Docker container per Elastic Beanstalk environment. In order to get the most out of Docker, Elastic Beanstalk lets you create an environment where your Amazon EC2 instances run multiple Docker containers side by side. The following diagram shows an example Elastic Beanstalk environment configured with three Docker containers running on each Amazon EC2 instance in an Auto Scaling group: Container instances—Amazon EC2 instances running Multicontainer Docker in an Elastic Beanstalk environment—require a configuration file named Dockerrun.aws.json. This file is specific to Elastic Beanstalk and can be used alone or combined with source code and content in a source bundle to create an environment on a Docker platform. Hence, the correct answer is: Configure the container definitions in the Dockerrun.aws.json file. The option that says: Configure the container definitions in the Amazon ECS Console when building the Docker environment is incorrect because the application must be deployed using Elastic Beanstalk. Therefore, you must configure the container definitions in the Dockerrun.aws.json file. The option that says: Configure the container definitions in the Dockerrun.aws.json.config and put it inside the .ebextensions folder is incorrect because the Dockerrun.aws.json file should be placed on the same level where your application file resides. The option that says: Use the eb config command to configure the container definitions is incorrect. This is just a command that you can use to change the configuration settings of your environment. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecstutorial.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/single-container-docker-configuration.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-be"])</script><script>self.__next_f.push([1,"anstalk/34:Ta8e,"])</script><script>self.__next_f.push([1,"A Lambda function's execution role is an AWS Identity and Access Management (IAM) role that grants the function permission to access AWS services and resources. For example, you might create an execution role that has permission to send logs to Amazon CloudWatch and upload trace data to AWS X-Ray. The AWSLambdaBasicExecutionRole is a managed policy provided by AWS that includes permissions essential for a Lambda function to create and write logs to Amazon CloudWatch Logs. These permissions include permissions to Log actions such as logs:CreateLogGroup, logs:CreateLogStream, and logs:PutLogEvents. When you create a Lambda function using the AWS Management Console, the execution role that AWS automatically creates for you often includes this managed policy. However, when defining a Lambda function via a CloudFormation template or other Infrastructure as Code (IAC) methods, you might need to explicitly attach this policy to the function's execution role to ensure that it has the appropriate logging permissions. Hence, the correct answer is: Update the execution role by adding the AWSLambdaBasicExecutionRole managed policy. The option that says: Associate the function with a resource-based policy that contains the logs:CreateLogGroup, logs:CreateLogStream, and logs:PutLogEvents permissions is incorrect. While this option contains the required permissions, they are defined as resource-based policies rather than as part of the execution role's policy. Resource-based policies just define what AWS services or users are allowed to do with the Lambda function (like invoking the function). The permissions required to write to CloudWatch Logs should be associated with the execution role. The option that says: Update the execution role by adding the CloudWatchLambdaInsightsExecutionRolePolicy managed policy is incorrect. Although this policy might have permissions similar to AWSLambdaBasicExecutionRole, its target resource is specific to Lambda Insights and not for Lambda functions. The option that says: Associate the function with a resource-based policy that contains the logs:PutLogEvents permissions is incorrect. This option suggests associating the Lambda function with a resource-based policy containing only the logs:PutLogEvents permission, which is insufficient. This permission alone allows the function to send log data to existing streams but does not permit the creation of new log groups or streams in Amazon CloudWatch Logs. References: https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html https://repost.aws/knowledge-center/lambda-cloudwatch-log-streams-error Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"])</script><script>self.__next_f.push([1,"35:T618,When Lambda runs your function, it passes a context object to the handler. This object provides methods and properties that provide information about the invocation, function, and execution environment. One of the properties that you can get from the context object is the log_stream_name which gives the log location of a function instance. As shown in the screenshot above, we can easily retrieve the corresponding log stream of a request by returning the value of context.log_stream_name. For the full list of context methods and properties, see this link. Hence, the correct answer is: Extract the log stream name from the Context object of the handler function. The option that says: Extract the invocation request id from the Context object of the handler function. Then, call the FilterLogEvents API and pass the request id to filter results is incorrect because this adds unnecessary steps to meet the requirement. The log stream name is directly available in the Context object. The following options can be eliminated because the log stream name and request-id are not properties of the event object: - Extract the log stream name from the Event object of the handler function. - Extract the invocation request id from the Event object of the handler function. Call the FilterLogEvents API and use the request id to filter results. References: https://docs.aws.amazon.com/lambda/latest/dg/nodejs-context.html https://docs.aws.amazon.com/lambda/latest/dg/python-handler.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/36:T8ea,"])</script><script>self.__next_f.push([1,"In AWS CodeDeploy, during an in-place deployment, the previous version of the application on each compute resource is stopped, and then the latest application revision is installed. After installation, the new version of the application is started and validated. You can configure a deployment group or deployment to automatically roll back when a deployment fails or when a monitoring threshold you specify is met. In this case, CodeDeploy redeploys the last known good version of an application revision. These rolled-back deployments are technically new deployments, with new deployment IDs, rather than restored versions of a previous deployment. Hence, the correct answer is: CodeDeploy redeploys the last known good version of an application with a new deployment ID. The option that says: CodeDeploy pauses the current deployment, restores the last stable version from S3, and uses the deployment ID of the most recent deployment with a SUCCEEDED status is incorrect. CodeDeploy doesn't \"pause\" deployments during a rollback. Instead, it initiates a new deployment with a new unique deployment ID for the last known good revision. It does not reuse the deployment ID of the previous successful deployment. The option that says: CodeDeploy reroutes traffic back to the blue environment and terminates the green environment is incorrect because this describes a \"blue/green\" deployment strategy. While AWS CodeDeploy does support blue/green deployments, the question explicitly mentioned \"in-place deployments.\", where the application is directly updated on the existing instances. The option that says: CodeDeploy reverts the EC2 instance to a previous AMI snapshot taken during the last successful deployment is incorrect. AWS CodeDeploy does not take or revert to AMI snapshots as part of its deployment process. Instead, it focuses on deploying application revisions and keeps a record of these revisions. The process of managing AMIs and snapshots is outside the scope of CodeDeploy. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html https://docs.aws.amazon.com/whitepapers/latest/introduction-devops-aws/in-place-deployments.html Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/"])</script><script>self.__next_f.push([1,"37:T662,You can install an Apache web server with PHP and MySQL support on your Amazon Linux instance (sometimes called a LAMP web server or LAMP stack). You can use this server to host a static website or deploy a dynamic PHP application that reads and writes information to a database. To decouple the database from the application, you can choose from the AWS Database services that support MySQL (e.g., Amazon RDS, Amazon Aurora) From the options given, we can deploy a LAMP web server by using an EC2 instance and an Amazon Aurora database for MySQL. Hence, the correct answer is Amazon EC2 and Amazon Aurora. Amazon S3 and Amazon CloudFront are incorrect because Amazon S3 is only capable of serving static websites. You cannot use an S3 bucket to host a LAMP web server. Amazon ECS and Amazon EFS are incorrect. Although it is possible to containerize a LAMP web server and host it on Amazon ECS, it's not suitable for the scenario because the web server is just a monolithic application rather than a microservice. Configuring ECS entails more effort than EC2 and Amazon EFS is only used for POSIX-compliant storage. Amazon API Gateway and Amazon RDS are incorrect. While you can host a MySQL database using Amazon RDS, you can't use API Gateway to host the Apache web server. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/install-LAMP.html https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Tutorials.WebServerDB.CreateWebServer.html Check out these Amazon EC2 and Amazon Aurora Cheat Sheets: https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/ https://tutorialsdojo.com/amazon-aurora/38:T7a2,The --dry-run parameter checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is DryRun-Operation. Otherwise, it is UnauthorizedOperation. With the IAM policy simulator, you can test and troubleshoot identity-based policies, IAM permissions boundaries, Organiza"])</script><script>self.__next_f.push([1,"tions service control policies (SCPs), and resource-based policies. You can test policies that are attached to IAM users, groups, or roles in your AWS account. If more than one policy is attached to the user, group, or role, you can test all the policies or select individual policies to test. You can test which actions are allowed or denied by the selected policies for specific resources. Hence, the correct answers are: - Run the describe-instances command with the --dry-run parameter. - Use the IAM Policy Simulator to validate the permission for the IAM role. The option that says: Validate the IAM role’s permission by querying the in-line policies within the EC2 instance metadata is incorrect because in-line policies are not available in the EC2 instance metadata. The option that says: Run the get-group-policy command is incorrect because this command just retrieves the specified inline policy document that is embedded in the specified IAM group. Since the developer is using IAM roles, this command won't work because IAM roles can't be associated with an IAM group. The option that says: Run the get-role command is incorrect. While this command retrieves information about an IAM role, it does not provide information about the permissions attached to that IAM role. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-help.html Check out this AWS IAM Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/39:T7e9,Allocating more memory to a Lambda function also increases the amount of CPU, network, and other resources allocated to it. By provisioning more memory, you can improve the performance and speed of your function while potentially reducing your costs. You should benchmark your use case to determine where the breakeven point is for running faster and using more memory vs running slower and using less memory. In the scenario, by increasing the memory allocation of the function, the "])</script><script>self.__next_f.push([1,"CPU power and network throughput available to the function are also increased, which can speed up the execution of the function and result in faster processing times for the images. Hence, the correct answer is: Increase the memory allocation of the function. The option that says: Increase the timeout value of the function is incorrect. This can provide more time for the function to execute before it times out, which may be useful if the function is being terminated prematurely. However, simply increasing the timeout value may not necessarily improve the processing time of the function. The option that says: Configure the S3 bucket to send notifications to an SQS queue. Use the SQS queue with the Lambda function to process the image is incorrect. Adding an SQS queue to the solution won't necessarily make the Lambda function run faster but can improve the overall fault tolerance of the system. The option that says: Run the function with Lambda@Edge which will run the code closer to the users of your application, reducing your application’s latency is incorrect. This may improve latency, but it may not necessarily improve the processing time of the function. Additionally, this solution will require you to set up a CloudFront distribution, which introduces additional costs. References: https://aws.amazon.com/blogs/architecture/best-practices-for-developing-on-aws-lambda/ https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/3a:T869,"])</script><script>self.__next_f.push([1,"AWS KMS establishes quotas for the number of API operations requested in each second. You can make API requests directly or by using an integrated AWS service that makes API requests to AWS KMS on your behalf. The quota applies to both kinds of requests. For example, you might store data in Amazon S3 using server-side encryption with AWS KMS (SSE-KMS). Each time you upload or download an S3 object that's encrypted with SSE-KMS, Amazon S3 makes a GenerateDataKey (for uploads) or Decrypt (for downloads) request to AWS KMS on your behalf. These requests count toward your quota, so AWS KMS throttles the requests if you exceed a combined total of 5,500 (or 10,000 or 30,000 depending upon your AWS Region) uploads or downloads per second of S3 objects encrypted with SSE-KMS. Hence, the correct answer is: The API request rate has exceeded the quota for AWS KMS API operations. The option that says: The Amazon S3 throttles the PutObject operation for objects encrypted with SSE-KMS is incorrect because Amazon S3 can automatically scale to high request rates with or without server-side encryption through parallelization. The option that says: The AWK KMS key uses an AES 256 algorithm, which makes the encryption process slower. AES 128 should be used instead is incorrect. While it is true that AES 256 is technically slower than AES 128 (because it has a larger key size), the performance difference is hardly noticeable. That being said, this is unlikely to be the cause of the problem. The option that says: The KMS key does not use an alias required for the server-side encryption with AWS KMS (SSE-KMS) is incorrect because an alias is simply a reference name that points to a key. It is not required for using a KMS key in SSE-KMS. The key can be referenced directly by its ARN or an alias, but the absence of an alias wouldn't cause performance degradation. References: https://docs.aws.amazon.com/kms/latest/developerguide/requests-per-second.html#rps-from-service https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html Check out this AWS KMS Cheat Sheet: https://tutorialsdojo.com/aws-key-management-service-aws-kms/"])</script><script>self.__next_f.push([1,"3b:T721,You can use AWS CodeBuild with a proxy server to regulate HTTP and HTTPS traffic to and from the Internet. To run CodeBuild with a proxy server, you install a proxy server in a public subnet and CodeBuild in a private subnet in a VPC. Below are possible causes of error when running CodeBuild with a proxy server: ssl-bump is not configured properly. Your organization's security policy does not allow you to use ssl-bump. Your buildspec.yml file does not have proxy settings specified using a proxy element. If you do not use ssl-bump for an explicit proxy server, add a proxy configuration to your buildspec.yml using a proxy element. version: 0.2proxy:upload-artifacts: yeslogs: yes Hence, the correct answer is: Modify the proxy element of the buildspec.yml file on the source code root directory. The following options are both incorrect because the AppSpec file is used in AWS CodeDeploy and not in AWS CodeBuild: - Modify the phases element of the AppSpec.yml file on the source code root directory. - Modify the proxy element of the AppSpec.yml file on the source code root directory. The option that says: Modify the artifacts element of the buildspec.yml file on the source code root directory is incorrect because the artifacts element represents information about where CodeBuild can find the build output and how CodeBuild prepares it for uploading to the S3 output bucket. You can not configure proxy settings here. Use the proxy element if you want to run CodeBuild in an explicit proxy server. References: https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html#build-spec.proxy Check out this AWS CodeBuild Cheat Sheet: https://tutorialsdojo.com/aws-codebuild/ AWS CodeBuild Overview: https://youtu.be/1zA6mK9BdA43c:Ta44,"])</script><script>self.__next_f.push([1,"By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data. The larger the table or index being scanned, the more time the Scan takes to complete. In addition, a sequential Scan might not always be able to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table's data across multiple physical partitions, a Scan operation can only read one partition at a time. For this reason, the throughput of a Scan is constrained by the maximum throughput of a single partition. To address these issues, the Scan operation can logically divide a table or secondary index into multiple segments, with multiple application workers scanning the segments in parallel. Each worker can be a thread (in programming languages that support multithreading) or an operating system process. To perform a parallel scan, each worker issues its own Scan request with the following parameters: To make the most of your table’s provisioned throughput, you’ll want to use the Parallel Scan API operation so that your scan is distributed across your table’s partitions. But be careful that your scan doesn’t consume your table’s provisioned throughput and cause the critical parts of your application to be throttled. To avoid throttling, you need to rate-limit your client application. Hence, the correct answer is: Perform a rate-limited parallel scan operation. The option that says: Perform a rate-limited sequential scan operation is incorrect because a DynamoDB scan operation is sequential by default, therefore, there will be no improvement in the execution time of the current scan operation. The option that says: Use a parallel scan operation is incorrect because running a parallel scan alone might consume all of your table's provisioned throughput which may affect your application's normal workload. You must use a rate-limiter along with it. The option that says: Use eventually consistent reads for the scan operation instead of strongly consistent reads is incorrect. You might reduce the cost of your provisioned throughput but the scan operation will still run sequentially, making no improvements at all. References: https://aws.amazon.com/blogs/developer/rate-limited-scans-in-amazon-dynamodb/ https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Scan.html https://amazon-dynamodb-labs.com/design-patterns/ex2scan/step2.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"3d:T789,A task definition is required to run Docker containers in Amazon ECS. The following are some of the parameters you can specify in a task definition: - The Docker image to use with each container in your task - How much CPU and memory to use with each task or each container within a task - The launch type to use, which determines the infrastructure on which your tasks are hosted - The Docker networking mode to use for the containers in your task - The logging configuration to use for your tasks - Whether the task should continue to run if the container finishes or fails - The command the container should run when it is started - Any data volumes that should be used with the containers in the task - The IAM role that your tasks should use Before you can run Docker containers on Amazon ECS, you must create a task definition. You can define multiple containers and data volumes in a single task definition. Hence, the correct answer is: Specify the containers in a single task definition and configure EFS as its volume type. The option that says: Use two task definitions for each container and mount an EFS volume between the tasks is incorrect because you can define two containers in a task definition. Creating two task definitions is unnecessary. A pod is an execution unit specifically used in Kubernetes. Since the scenario mentioned that the containers are to be orchestrated using Amazon ECS, The following options are both incorrect: - Use two pod specifications for each container and mount an EFS volume between the pods. - Specify the containers in a single pod specification and configure EFS as its volume type. References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/example_task_definitions.html Check out this Amazon ECS Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/3e:Tac7,"])</script><script>self.__next_f.push([1,"CodeCommit is a secure, highly scalable, managed source control service that hosts private Git repositories. CodeCommit eliminates the need for you to manage your own source control system or worry about scaling its infrastructure. AWS CodeCommit is designed for collaborative software development. You can easily commit, branch, and merge your code allowing you to easily maintain control of your team's projects. CodeCommit also supports pull requests, which provide a mechanism to request code reviews and discuss code with collaborators. You can create a repository from the AWS Management Console, AWS CLI, or AWS SDKs and start working with the repository using Git. You can use the EB CLI to deploy your application directly from your AWS CodeCommit repository. With CodeCommit, you can upload only your changes to the repository when you deploy, instead of uploading your entire project. Hence, the correct answer is: Create an AWS CodeCommit repository and allow access to all developers. Deploy the code to Elastic Beanstalk. The option that says: Host the code repository on an EC2 instance and allow access to all the developers. Write a script that will automate the deployment process to Elastic Beanstalk is incorrect because this solution entails a lot of effort — from selecting the AMI to managing the instance, setting up a code repository, and configuring scalability. AWS CodeCommit is already a managed and highly scalable source control service that has native integration with Elastic Beanstalk. The option that says: Configure event notifications on a central S3 bucket and allow access to all developers. Invoke a Lambda Function that will deploy the code to Elastic Beanstalk when a PUT event occurs is incorrect. Amazon S3 does not have the functionality to modify stored objects. Incremental code updates are not possible with S3, which defies the requirement for the scenario. The option that says: Upload the code to an Amazon EFS mounted on an EC2 instance. Write a script that will automate the deployment process to Elastic Beanstalk is incorrect. Although EFS is a good use case for concurrent access, it is not a suitable solution for a source control service. EFS is less performant to workloads that require random access over large files. Use EFS if you want to distribute highly parallelized workloads like analytical workloads and media processing across several machines. References: https://aws.amazon.com/premiumsupport/knowledge-center/deploy-codecommit-elastic-beanstalk/ https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli-codecommit.html Check out these AWS CodeCommit and AWS Elastic Beanstalk Cheat Sheets: https://tutorialsdojo.com/aws-elastic-beanstalk/ https://tutorialsdojo.com/aws-codecommit/"])</script><script>self.__next_f.push([1,"3f:Td6f,"])</script><script>self.__next_f.push([1,"The Amazon CloudWatch embedded metric format allows you to generate custom metrics asynchronously in the form of logs written to CloudWatch Logs. You can embed custom metrics alongside detailed log event data, and CloudWatch automatically extracts the custom metrics so that you can visualize and alarm on them, for real-time incident detection. Additionally, the detailed log events associated with the extracted metrics can be queried using CloudWatch Logs Insights to provide deep insights into the root causes of operational events. Embedded metric format helps you generate actionable custom metrics from ephemeral resources such as Lambda functions and containers. By using the embedded metric format to send logs from these ephemeral resources, you can now easily create custom metrics without having to instrument or maintain separate code, while gaining powerful analytical capabilities on your log data. The Amazon CloudWatch embedded metric format (EMF) is a JSON specification used to instruct CloudWatch Logs to automatically extract metric values embedded in structured log events. You can use CloudWatch to graph and create alarms on the extracted metric values. The following is an example of embedded metric format. By using EMF, you can extract and monitor these custom metrics directly from your logs, enabling you to gain operational insights and set alarms based on these metrics. Additionally, Amazon's open-source libraries provide a convenient way to format logs in the EMF. These libraries can be integrated into your application to structure the log events with the necessary metric data, ensuring that CloudWatch can accurately extract and process these custom metrics. Hence, the correct answer is: Use Amazon's open-source libraries to format logs in the Amazon CloudWatch embedded metric format. Then, use CloudWatch to monitor, view, and set alarms for the custom metrics. The option that says: Send custom metric data directly to Amazon EventBridge using the PutMetricData API operation. Create EventBridge rules to trigger actions based on the metrics is incorrect because Amazon EventBridge is typically used for event-driven architectures, not for monitoring or custom metrics. EventBridge rules are used to respond to events, not to monitor metrics. The option that says: Use Amazon CloudWatch Lambda Insights to extract and monitor custom metrics for processing times. Set up alarms and dashboards in Lambda Insights for real-time analysis is incorrect because Lambda Insights is primarily designed for monitoring and troubleshooting Lambda performance but does not directly extract custom metrics from logs. The option that says: Configure Amazon Data Firehose to stream logs from the Lambda function to an Amazon Redshift cluster. Use SQL queries to analyze metrics and set alerts for anomalies is incorrect. While this approach can be used for log analysis, it is overly complex for the requirement of extracting custom metrics from logs in real-time. Data Firehose and Amazon Redshift are more suited for large-scale data analysis rather than real-time monitoring and alerting. References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Embedded_Metric_Format_Specification.html https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Embedded_Metric_Format.html Check out this Amazon CloudWatch Cheat Sheet: https://tutorialsdojo.com/amazon-cloudwatch/"])</script><script>self.__next_f.push([1,"40:T9b4,"])</script><script>self.__next_f.push([1,"To deploy an API, you create an API deployment and associate it with a stage. A stage is a logical reference to a lifecycle state of your API (for example, dev, prod, beta, v2). API stages are identified by the API ID and stage name. They're included in the URL that you use to invoke the API. Each stage is a named reference to an API deployment and is made available for client applications to call. In the scenario, you can apply the new updates to the backend Lambda function and publish it as a new version. Then, update the integration request of the target API resource by replacing the old Lambda function ARN with the new version's ARN. Finally, deploy the resource to a new stage and use the new Invoke URL in your application. This way, existing users will be able to access both versions. You can retire the old version eventually after all users have migrated to the new one. Hence, the correct answer is: Implement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target resource then redeploy it to a new stage. The option that says: Implement the updates and publish a new version of the Lambda function. Then, issue the new Lambda invocation URL is incorrect. Although you can invoke a Lambda function by its ARN, you can't directly invoke a Lambda by its URL because AWS does not provide one. However, you can invoke it through a URL when integrated with API Gateway. The option that says: Implement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target resource then redeploy it to the same stage is incorrect because the objective is to publish two live endpoints (one for version 1 and another for version 2) that the users can access. Therefore, we need to create two stages. Deploying the new version to the same stage would overwrite the old version. The option that says: Implement the updates on the Lambda function. Create a CloudFront distribution and use the function as the origin is incorrect because a Lambda function is not a valid origin for a CloudFront distribution. Moreover, CloudFront is for content delivery and not used for exposing backend services as API endpoints. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"])</script><script>self.__next_f.push([1,"41:Ta01,"])</script><script>self.__next_f.push([1,"You (or your organization) probably has some existing web services that respond to older protocols such as XML-RPC or SOAP. You can use the API Gateway to modernize these services. In API Gateway, an API's method request can take a payload in a different format from the corresponding integration request payload, as required in the backend. Similarly, the backend may return an integration response payload different from the method response payload, as expected by the frontend. API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response. Hence, the correct answer is: Use API Gateway to create a RESTful API. Transform the incoming JSON into XML using mapping templates. Forward the request into the SOAP interface by using a Lambda function and parse the response (XML) into JSON before sending back to API Gateway. The option that says: Use API Gateway to create a WebSocket API. Transform the incoming JSON into XML using mapping templates. Forward the request into the SOAP interface by using a Lambda function and parse the response (XML) into JSON before sending back to API Gateway is incorrect. The WebSocket protocol is mainly used for applications that require bidirectional persistent connection such as push notifications and chat messaging applications. For this scenario, using a REST-based approach is the more appropriate solution. The option that says: Use API Gateway to create a RESTful API. Transform the incoming JSON into XML for the SOAP interface through an Application Load Balancer and vice versa. Put the legacy web service behind the ALB is incorrect because ALB is not capable of transforming data. The option that says: Use API Gateway to create a RESTful API. Send the incoming JSON to an HTTP server hosted on an EC2 instance and have it transform the data into XML and vice versa before sending it to the legacy application is incorrect. Although this could work, this means that you'll have to provision and run an EC2 instance 24/7, which is more expensive than just using a Lambda Function. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-data-transformations.html https://github.com/mwittenbols/How-to-use-Lambda-and-API-Gateway-to-consume-XML-instead-of-JSON Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"])</script><script>self.__next_f.push([1,"42:Td45,"])</script><script>self.__next_f.push([1,"A single BatchGetItem operation can retrieve up to 16 MB of data, which can contain as many as 100 items. BatchGetItem returns a partial result if: - The response size limit is exceeded - The table's provisioned throughput is exceeded - More than 1MB per partition is requested - An internal processing failure occurs. For example, if you ask to retrieve 100 items, but each individual item is 300 KB in size, the system returns 52 items (so as not to exceed the 16 MB limit). It also returns an appropriate UnprocessedKeys value so you can get the next page of results. If desired, your application can include its own logic to assemble the pages of results into one dataset. If none of the items can be processed due to insufficient provisioned throughput on all of the tables in the request, then BatchGetItem returns a ProvisionedThroughputExceededException. If at least one of the items is successfully processed, then BatchGetItem completes successfully while returning the keys of the unread items in UnprocessedKeys. If DynamoDB returns any unprocessed items, you should retry the batch operation on those items. It's recommended that you use an exponential backoff algorithm. Exponential backoff is a technique where, if a request to a server fails, you wait a bit before retrying. If it keeps failing, you wait longer each time. The main idea is to reduce the frequency of calls over time, which helps avoid overloading the server, giving it a better chance to recover and respond successfully. If you retry the batch operation immediately, the underlying read or write requests can still fail due to throttling on the individual tables. Adding progressively longer waits between retries using exponential backoff can make the individual requests in the batch much more likely to succeed. You can implement an exponential backoff yourself in your code or simply use the AWS SDK, which comes with automatic retry logic and exponential backoff. Hence, the correct answers are: - Implement an exponential backoff algorithm with a randomized delay between retries of the batch request. - Use the AWS software development kit (AWS SDK) to send batch requests. The option that says: Increase the read capacity units (RCUs) for the DynamoDB table and enable Auto Scaling is incorrect. Although this solution can help improve the throughput of the table and reduce the likelihood of throttling, it's not the most reliable method for handling partial results. The batch operation can still return partial results even if the table has sufficient read capacity due to other factors such as large response sizes. The option that says: Create a Global Secondary Index (GSI) with its own read capacity settings is incorrect. The presence of a GSI does not change how BatchGetItem behaves, and it won't help with managing UnprocessedKeys. The option that says: Implement a logic that immediately retries the batch request is incorrect. Retrying the batch operation immediately has more chance of failing than succeeding due to throttling. References: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#BatchOperations https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/ Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"43:T8ec,"])</script><script>self.__next_f.push([1,"The problem is about delegating access to the development account to use the S3 Bucket on the production account. The steps to execute this are as follows: You use the AWS Management Console to establish trust between the Production account (ID number XXXXXXXXXXXX) and the Development account (ID number YYYYYYYYYYYY) by creating an IAM role. When you create the role, you define the Development account as a trusted entity and specify a permissions policy that allows trusted users to access the S3 bucket. On the development account, create an STS policy to assume the role created on the production account. This can be done by referencing the ARN of the role that was created to establish trust between the Production account and the Development account. Hence, the correct answers are: - On the production account, create an IAM role and specify the development account as a trusted entity. - Set the policy that will grant access to S3 for the IAM role created in the production account - Log in to the development account and create a policy that will use STS to assume the IAM role in the production account. Attach the policy to corresponding IAM users. The option that says: On the development account, create an IAM role and specify the production account as a trusted entity is incorrect. Since the S3 bucket is in the production account, the role should also be created in the production account. The option that says: Set the policy that will grant access to S3 for the IAM role created in the development account is incorrect because the policy associated with the role that will grant access to S3 should be created on the production account. The option that says: Log in to the production account and create a policy that will use STS to assume the IAM role in the development account. Attach the policy to the IAM users is incorrect. The policy that will use STS should be created on the account you are delegating access to, which is the development account. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html https://aws.amazon.com/blogs/security/how-to-enable-cross-account-access-to-the-aws-management-console/ Check out this AWS IAM Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"])</script><script>self.__next_f.push([1,"44:T795,When you launch an instance in Amazon EC2, you have the option of passing user data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. You can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives. You can also pass this data into the launch wizard as plain text, as a file (this is useful for launching instances using the command line tools), or as base64-encoded text (for API calls). Hence, the correct answer is: Configure the user data at the creation of the EC2 instance to run a script that will install and create the Apache webserver after the instance starts. The option that says: Connect to the instance via port 22. Run the commands that will install and create the Apache webserver is incorrect because the scenario mentioned that the setting of the instance security group is only open to port 80. The developer will get a timeout error if he tries to connect to the instance via port 22. The option that says: Connect to the instance via port 80. Run the commands that will install and create the Apache webserver is incorrect. Although the security group's port 80 is open to the public, you can not establish a remote connection to the EC2 instance thru port 80. Port number 80 is used for sending and receiving web pages from an HTTP server. The option that says: Configure the metadata at the creation of the EC2 instance to run a script that will install the Apache webserver after the instance starts is incorrect because you can not run scripts on metadata. Metadata is just a list of details about your instance. You should add a User Data instead. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/launching-instance.html Check out this Amazon EC2 Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/45:T771,AWS Lambda natively supports Java, Go, PowerShell, Node.js, C#, Python, and Ruby code, and "])</script><script>self.__next_f.push([1,"provides a Runtime API, which allows you to use any additional programming languages to author your functions. You can use the custom runtime to create a Lambda function if your preferred language is not available. You can implement an AWS Lambda runtime in any programming language. A runtime is a program that runs a Lambda function's handler method when the function is invoked. You can include a runtime in your function's deployment package in the form of an executable file named bootstrap. Because AWS Lambda supports Ruby by default, there's no additional configuration needed. Hence, the correct answer is: Create a Lambda function with a supported runtime version for Ruby. The option that says: Create a Lambda function with a custom runtime to use Ruby. Then include the runtime in the function's deployment package. Migrate it to a layer that you manage independently from the function is incorrect because a custom runtime is used to run programming languages that are not readily available to AWS Lambda. The option that says: Create a Lambda function on Ruby with a custom runtime and use the AWS SDK for Ruby is incorrect because the application will make calls to an external API and not to AWS Resources. AWS SDK is used for making API calls to different AWS services. Additionally, you don't have to make use of the custom runtime because AWS Lambda natively supports Ruby. The option that says: Create a Lambda function using the AWS SDK for Ruby is incorrect because the AWS SDK for Ruby just provides Ruby classes to access AWS services. It is not a valid runtime environment. References: https://aws.amazon.com/lambda/faqs/ https://docs.aws.amazon.com/lambda/latest/dg/runtimes-custom.html?icmpid=docs_lambda_help Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/46:T72d,Cross-origin resource sharing (CORS) is a browser security feature that restricts cross-origin HTTP requests that are initiated from scripts running in the browser. If your REST API's resources receive non-simple cross-origin "])</script><script>self.__next_f.push([1,"HTTP requests, you need to enable CORS support. For a Lambda custom (non-proxy) integration, HTTP custom (non-proxy) integration, or AWS service integration, you can set up the required headers by using API Gateway method response and integration response settings. When you enable CORS by using the AWS Management Console, API Gateway creates an OPTIONS method and attempts to add the Access-Control-Allow-Origin header to your existing method integration responses. Hence, the correct answer is: In the API Gateway Console, enable cross-origin resource sharing (CORS) for the method in the specified resource. The option that says: In the Amazon S3 Console, enable cross-origin resource sharing (CORS) on the S3 bucket where the website is hosted is incorrect. This could be a possible solution if the website is interacting with resources from another website-enabled S3 bucket. The option that says: Set the value of the Access-Control-Max-Age header to 0 is incorrect because this header is simply used to indicate how long the results of a preflight request can be cached. The option that says: Set the value of the Access-Control-Allow-Credentials header to true is incorrect because this is just a response header that tells browsers whether to expose the response to frontend JavaScript code when the request's credentials mode (Request.credentials) is include. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/47:T7f8,The Lambda execution environment provides ephemeral storage for your code to use at /tmp. This space has a size that can be set between 512 MB (free) and 10,240 MB. For this feature, you are charged for the storage you configure over the 512 MB free limit for the duration of your function invokes. In the scenario, the /tmp directory can be used as a staging area for unzipping the file. Also, since the file size is relatively "])</script><script>self.__next_f.push([1,"small (50MB), even when unzipped, the default 512 MB should be enough for the job, making the solution the best option in terms of effort and cost. Hence, the correct answer is: Download the file to the /tmp directory. From there, consume and process the data before sending it to the S3 bucket. The option that says: Download the file to Amazon Elastic File System. From there, consume and process the data before sending it to the S3 bucket is incorrect. Although this is a valid solution, using EFS entails additional effort for this type of problem. For example, the Lambda function must be configured first to connect to the VPC where the EFS is located. The option that says: Download the file to a separate S3 bucket. From there, consume and process the data before sending it to the S3 bucket that contains the logs is incorrect. This solves nothing as the file will still remain uncompressed. Optionally, you can load the file into the Lambda function's memory and decompress it from there, but this method eats up a lot of memory and is subjected to the memory limits of the Lambda function. The option that says: Download the file to an Amazon Elastic Block Store (EBS) volume. From there, consume and process the data before sending it to the S3 bucket is incorrect. Using Amazon EBS as storage for unzipping a file is a costly and overkill solution. References: https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html https://aws.amazon.com/blogs/aws/aws-lambda-now-supports-up-to-10-gb-ephemeral-storage/ Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/48:Tbe6,"])</script><script>self.__next_f.push([1,"The optional Mappings section matches a key to a corresponding set of named values. For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. You use the Fn::FindInMap intrinsic function to retrieve values in a map. Hence, the correct answer is: Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::FindInMap function to retrieve the desired Image Id from the region key. The option that says: Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::ImportValue function to retrieve the desired Image Id from the region key is incorrect because the Fn::ImportValue function is just used to return the value of an output exported by another stack. It can’t be used to retrieve values from a Mappings section. The option that says: Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::GetAtt function to retrieve the desired Image Id from the region key is incorrect because the Fn::GetAtt function is simply used to return the value of an attribute from a resource in the template and not in a Mappings section. The option that says: Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Parameters section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Ref function to retrieve the desired Image Id from the region key is incorrect because the Parameters section is mainly used to declare values within a specified parameter. For example, you can specify the allowed Amazon EC2 instance type for the stack to use when you create or update the stack. Although you can specify the values for the Image Id in the Parameters section, it does not give you the flexibility to map the Image Ids according to its correct region. The Mappings section is more suited for this type of use case. References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-findinmap.html Check out this AWS CloudFormation Cheat Sheet: https://tutorialsdojo.com/aws-cloudformation/"])</script><script>self.__next_f.push([1,"49:T883,"])</script><script>self.__next_f.push([1,"Application Load Balancers provide two advanced options that you may want to configure when you use ALBs with AWS Lambda: support for multi-value headers and health check configurations. You can set up these options in Target Groups section on the Amazon EC2 console. If requests from a client or responses from a Lambda function contain headers with multiple values or contains the same header multiple times or query parameters with multiple values for the same key, you can enable support for multi-value header syntax. After you enable multi-value headers, the headers and query parameters exchanged between the load balancer and the Lambda function use arrays instead of strings. For example, suppose the client supplies a query string like: ?name=foo\u0026name=bar If you’ve enabled multi-value headers, ALB supplies these duplicate parameters in the event object as: ‘name’: [‘foo’, ‘bar’] ALB applies the same processing to duplicate HTTP headers. If you do not enable multi-value header syntax and a header or query parameter has multiple values, the load balancer uses the last value that it receives. Hence, the correct answer is: Enable the multi-value headers on the Application Load Balancer. The option that says: Set a custom HTTP response header in the Lambda function is incorrect because this can only be done when integrated with API Gateway. The option that says: Replace the Application Load Balancer with a Network Load Balancer and enable multi-value headers is incorrect. The Network Load Balancer does not support Lambda functions as a target type. The option that says: Decode the URL encoded query string values in the Lambda function is incorrect. This won't change anything. The load balancer will still use the last value of the query parameter it receives regardless if its URL is encoded or not. References: https://aws.amazon.com/blogs/networking-and-content-delivery/lambda-functions-as-targets-for-application-load-balancers/ https://docs.aws.amazon.com/elasticloadbalancing/latest/application/lambda-functions.html Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet: https://tutorialsdojo.com/aws-elastic-load-balancing-elb/"])</script><script>self.__next_f.push([1,"4a:T726,You can use the AWS Management Console, or the AWS CLI or API, to specify customization settings for the built-in app UI experience. You can upload a custom logo image to be displayed in the app. You can also choose many CSS customizations. You can specify app UI customization settings for a single client (with a specific clientId) or for all clients (by setting the clientId to ALL). If you specify ALL, the default configuration will be used for every client that has no UI customization set previously. If you specify UI customization settings for a particular client, it will no longer fall back to the ALL configuration. In the given scenario, you can include the product logo on the webpage by uploading the logo in the Cognito app settings under UI customization. Hence, the correct answer is: Upload the logo to the Amazon Cognito app settings and use that logo on the custom login page. The option that says: Upload the logo to an S3 bucket and point the S3 endpoint on the custom login page is incorrect as this procedure simply won't work. The logo should be uploaded on the Cognito app settings and not on the S3 bucket. The option that says: Create a login page with the product logo and upload it to Amazon Cognito is incorrect. You don't have to create a login page as this is already hosted by Cognito. The option that says: Create a login page with the product logo and upload it to an S3 bucket. Point the S3 endpoint in the Cognito app settings is incorrect because there is no such option in the Cognito app settings. References: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-app-ui-customization.html https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html Check out this Amazon Cognito Cheat Sheet: https://tutorialsdojo.com/amazon-cognito/4b:T91b,"])</script><script>self.__next_f.push([1,"In an EC2/On-Premises deployment, a deployment group is a set of individual instances targeted for deployment. A deployment group contains individually tagged instances, Amazon EC2 instances in Amazon EC2 Auto Scaling groups, or both. You can associate more than one deployment group with an application in CodeDeploy. This makes it possible to deploy an application revision to different sets of instances at different times. For example, you might use one deployment group to deploy an application revision to a set of instances tagged Test where you ensure the code's quality. Next, you deploy the same application revision to a deployment group with instances tagged Staging for additional verification. Finally, when you are ready to release the latest application to customers, you deploy to a deployment group that includes instances tagged Production. Hence, the correct answer is: Create multiple deployment groups for each environment using AWS CodeDeploy. The option that says: Create separate S3 buckets for each environment to deploy the application is incorrect. While S3 buckets can be used to store application artifacts, they are not designed for managing deployments. This approach would require additional scripting and manual steps to deploy the application from S3 to the respective environments, making it less convenient and more error-prone compared to using a dedicated deployment service like CodeDeploy. The option that says: Create, configure, and deploy multiple application projects for each environment using CodeBuild is incorrect because you can't use AWS CodeBuild to perform code deployments. CodeBuild is simply a service that allows you to compile and run tests on your code before deployment. The option that says: Create separate CloudFormation templates for each environment to deploy the application is incorrect because CloudFormation is primarily used for infrastructure as code, but it's not designed specifically for managing code deployments in a sequential manner across environments. This approach would be inefficient and overly complex compared to using CodeDeploy. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html https://aws.amazon.com/codedeploy/faqs/ Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/"])</script><script>self.__next_f.push([1,"4c:T668,Amazon EventBridge (Amazon CloudWatch Events) help you to respond to state changes in your AWS resources. When your resources change state, they automatically send events into an event stream. You can create rules that match selected events in the stream and route them to your AWS Lambda function to take action. To solve the given problem, we can set up a Schedule event source that will invoke the Lambda function responsible for sending a newsletter every 7 days. Hence, the correct answer is: Configure a scheduled Amazon EventBridge (Amazon CloudWatch Events) that triggers every week to invoke the Lambda function. The option that says: Run a cron job in an Amazon EC2 instance that will trigger the Lambda function every week is incorrect because this approach does not comply with the required solution. Amazon EC2 is not a serverless compute service. The option that says: Add an environment variable named DAYS for the Lambda function and set its value to 7 is incorrect because an environment variable is just an optional key-value pair that is stored in the Lambda function's version-specific configuration. The option that says: Implement a task timer using Step Functions that will send a newsletter every week is incorrect. Although serverless, using Step Functions for a basic application that pushes data is unnecessarily complex and may incur additional costs. References: https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents.html https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html Check out this Amazon CloudWatch Cheat Sheet: https://tutorialsdojo.com/amazon-cloudwatch/4d:Td00,"])</script><script>self.__next_f.push([1,"The output of a state can be a copy of its input, the result it produces (for example, the output from a Task state’s Lambda function), or a combination of its input and result. Use ResultPath to control which combination of these is passed to the state output. You can use a Catch field to capture the error in a Task and Parallel State. This field's value must be an array of objects, known as catchers. A catcher contains the following fields: ErrorEquals - A non-empty array of strings that match error names. Next - A string that must exactly match one of the state machine's state names. ResultPath - A path that determines what input is sent to the state specified in the Next field. When a state reports an error and either there is no Retry field, or if retries fail to resolve the error, Step Functions scans through the catchers in the order listed in the array. When the error name appears in the value of a catcher's ErrorEquals field, the state machine transitions to the state named in the Next field. Below is the visual representation of the workflow that is given in the scenario: The four states that handle the application logic and error handling are as follows: Choice State - \"Yes or No\" Task State - \"YesMessage\" and \"NoMessage\" Pass State - \"Cause Of Error\" The workflow is initiated by passing an input of values \"yes\" or \"no\". On the left side, we can see that an error has occurred during the \"NoMessage\" task state (as labeled by its orange color) when a \"no\" value was passed as an input. On the right side, we can see that all data that passes through the nodes (input, error, output) are aggregated in a single step output. This can be done by including a Catch field in the state machine definition to capture the error in a state and the ResultPath to include each node's input with its output. Hence, the correct answer is: Include a Catch field in the state machine definition to capture the errors. Then, use ResultPath to include each node’s input data with its output. The option that says: Include a Parameters field in the state machine definition to capture the errors. Then, use ResultPath to include each node’s input data with its output is incorrect because the Parameters field is mainly used for passing a collection of key-value pairs, where the values are either static values that you define in your state machine definition, or that are selected from the input using a path. It can't be used to capture errors in a state. The option that says: Include a Catch field in the state machine definition to capture the errors. Then, use ItemsPath to include each node’s input data with its output is incorrect because the ItemsPath is only applicable in a Map state. The option that says: Include a Parameters field in the state machine definition to capture the errors. Then, use ItemsPath to include each node’s input data with its output is incorrect because the Parameters field cannot be used to capture errors in a state. ItemsPath is also incorrect because this is only applicable in a Map state. References: https://docs.aws.amazon.com/step-functions/latest/dg/input-output-resultpath.html https://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html Check out this AWS Step Functions Cheat Sheet: https://tutorialsdojo.com/aws-step-functions/"])</script><script>self.__next_f.push([1,"4e:T788,An EC2/On-Premises deployment hook is executed once per deployment to an instance. You can specify one or more scripts to run in a hook. Each hook for a lifecycle event is specified with a string on a separate line. Here are descriptions of the hooks available for use in your AppSpec file. DownloadBundle deployment lifecycle event will throw an error whenever: - The EC2 instance’s IAM profile does not have permission to access the application code in the Amazon S3. - An Amazon S3 internal error occurs. - The instances you deploy to are associated with one AWS Region (for example, US West Oregon), but the Amazon S3 bucket that contains the application revision is related to another AWS Region (for example, US East N. Virginia). Hence, the correct answer is: The EC2 instance’s IAM profile does not have the permissions to access the application code in Amazon S3. The option that says: Wrong configuration of the DownloadBundle lifecycle event in the AppSec file is incorrect because you can not manually configure DownloadBundle in the Appsec file. The CodeDeploy Agent installed on the EC2 instance manages the DownloadBundle lifecycle event. The option that says: The DownloadBundle deployment lifecycle event is not supported in the N. Virginia region is incorrect because CodeDeploy is supported in N. Virginia. The option that says: Versioning is not enabled on the Amazon S3 Bucket where the application code resides is incorrect because versioning on Amazon S3 Bucket is just used to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-downloadbundle https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/4f:T8bd,"])</script><script>self.__next_f.push([1,"The sam local invoke command allows you to test AWS Lambda functions locally by emulating the Lambda execution environment. However, to test resources defined in AWS CDK, you must first convert the CDK constructs into a format that SAM can understand. This is where the cdk synth command comes into play. It synthesizes or \"compiles\" your CDK application into an AWS CloudFormation template. By running cdk synth, you generate the necessary CloudFormation template that sam local invoke can be used to locally execute and test your Lambda functions. Hence, the correct answers are: - Run the cdk synth command and indicate the stack name of Lambda functions to be tested. - Execute the sam local invoke command and specify the location of the synthesized CloudFormation template and identifier of each function. The option that says: Run the cdk bootstrap command to prepare the staging of CDK assets is incorrect. This command simply sets up the necessary resources in the AWS account to manage the deployments of CDK applications. It's a prerequisite step for deploying resources, not for local testing. The option that says: Execute the sam package and specify the S3 bucket where the Lambda code will be uploaded for local testing is incorrect. This just packages a CloudFormation template for deployment to AWS. The option that says: Execute the sam local start-lambda and specify the location of the synthesized CloudFormation template along with function identifiers is incorrect. While this command is indeed used for local testing, it starts a local endpoint that emulates the AWS Lambda service. This is more suitable for scenarios where you want to test the Lambda function as it would be invoked by other AWS services. Since the developer wants to test only some of the Lambda functions, the sam local invoke command is a more direct and appropriate choice for invoking specific Lambda functions one at a time. References: https://docs.aws.amazon.com/cdk/v2/guide/troubleshooting.html https://repost.aws/knowledge-center/cdk-customize-bootstrap-cfntoolkit https://docs.aws.amazon.com/cdk/v2/guide/cli.html Check out this Amazon Cloud Development Kit (CDK) Cheat Sheet: https://tutorialsdojo.com/aws-cloud-development-kit-cdk/"])</script><script>self.__next_f.push([1,"5:[\"$\",\"$Le\",null,{\"quiz\":{\"id\":\"aws-developer-12\",\"title\":\"AWS Certified Developer Associate Practice Exams 6\",\"description\":\"Additional practice questions covering AWS development topics.\",\"questions\":[{\"question\":\"A developer has enabled the lifecycle policy of an application deployed in Elastic Beanstalk. The lifecycle is set to limit the application version to 15 versions. The developer wants to keep the source code in an S3 bucket, yet, it gets deleted. What change should the developer do?\",\"answers\":[{\"text\":\"Modify the value of the Set the application versions limit by age option to zero.\",\"isCorrect\":false},{\"text\":\"Configure the Retention setting to retain the source bundle in the S3 bucket.\",\"isCorrect\":true},{\"text\":\"Modify the value of the Set application versions limit by the total count option to zero.\",\"isCorrect\":false},{\"text\":\"Trigger a Lambda function to copy the source code to another S3 bucket.\",\"isCorrect\":false}],\"explanation\":\"$f\"},{\"question\":\"A developer is writing an application that will download hundreds of media files. Each file must be encrypted with a unique encryption key within the application before storing it in an S3 bucket. The developer needs a cost-effective solution with low management overhead. Which of the following is the most suitable solution?\",\"answers\":[{\"text\":\"Enable the SSE-S3 for the S3 bucket. Directly store the files in the bucket.\",\"isCorrect\":false},{\"text\":\"Use the CreateKey API command to generate a CMK for each file to encrypt them.\",\"isCorrect\":false},{\"text\":\"Use an open-source key generator to produce a unique key. Use the key to encrypt the files.\",\"isCorrect\":false},{\"text\":\"Use the GenerateDataKey API command to generate a data key for each file to encrypt them. Store the encrypted data key and the file.\",\"isCorrect\":true}],\"explanation\":\"$10\"},{\"question\":\"A developer is building a serverless workflow using Step Functions. The developer has to implement a solution that will help the State Machine handle and recover from State exception errors. Which of the following actions should the developer do?\",\"answers\":[{\"text\":\"Use a try and catch logic inside the application code to capture the exception error. Then, use a Retry field in the state definition.\",\"isCorrect\":false},{\"text\":\"Use Catch and Retry fields in the state machine definition.\",\"isCorrect\":true},{\"text\":\"Use a Catch field inside the application code to capture the exception error. Then, use a Retry field in the state definition.\",\"isCorrect\":false},{\"text\":\"Use Catch and Retry fields inside the application code.\",\"isCorrect\":false}],\"explanation\":\"$11\"},{\"question\":\"An update was made on an AWS Lambda-based application. It is invoked by an API Gateway endpoint with caching enabled to improve latency requests. The developer expected to get the latest data as a response when he tested the application. However, he kept getting stale data upon trying many times. What should the developer do that will require the LEAST amount of effort to resolve the issue? (Select TWO.)\",\"answers\":[{\"text\":\"Include Cache-Control: max-age=0 HTTP header on the API request.\",\"isCorrect\":true},{\"text\":\"Create a new REST API endpoint and disable caching.\",\"isCorrect\":false},{\"text\":\"Set the new endpoint as a trigger for the lambda function.\",\"isCorrect\":false},{\"text\":\"Grant permission to the client to invalidate caching when there’s a request using the IAM execution role.\",\"isCorrect\":true},{\"text\":\"Include Cache-Control: no-cache HTTP header on the API request.\",\"isCorrect\":false}],\"explanation\":\"$12\"},{\"question\":\"A developer is building an application that uses Amazon CloudFront to distribute thousands of images stored in an S3 bucket. The developer needs a fast and cost-efficient solution that will allow him to update the images immediately without waiting for the object’s expiration date. Which solution meets the requirements?\",\"answers\":[{\"text\":\"Update the images by invalidating them from the edge caches.\",\"isCorrect\":false},{\"text\":\"Upload the new images in the S3 bucket and wait for the objects in the edge locations to expire to reflect the changes.\",\"isCorrect\":false},{\"text\":\"Update the images by using versioned file names.\",\"isCorrect\":true},{\"text\":\"Disable the CloudFront distribution and re-enable it to update the images in all edge locations.\",\"isCorrect\":false}],\"explanation\":\"$13\"},{\"question\":\"A developer is building a ReactJS application that will be hosted on Amazon S3. Amazon Cognito handles the registration and signing of users using the AWS Software Development Kit (SDK) for JavaScript. The JSON Web Token (JWT) received upon authentication will be stored on the browser's local storage. After signing in, the application will use the JWT as an authorizer to access an API Gateway endpoint. What are the steps needed to implement the scenario above? (Select THREE.)\",\"answers\":[{\"text\":\"Create an Amazon Cognito User Pool.\",\"isCorrect\":true},{\"text\":\"Set the name of the header that will be used from the request to the Cognito User Pool as a token source for authorization.\",\"isCorrect\":true},{\"text\":\"Set the name of the header that will be used from the request to the Cognito Identity Pool as a token source for authorization.\",\"isCorrect\":false},{\"text\":\"Create an Amazon Cognito Identity Pool.\",\"isCorrect\":false},{\"text\":\"On the API Gateway Console, create an authorizer using the Cognito User Pool ID.\",\"isCorrect\":true},{\"text\":\"Choose and set the authentication provider for your website.\",\"isCorrect\":false}],\"explanation\":\"$14\"},{\"question\":\"A serverless application is created to process numerous files. Each invocation takes 5 minutes to complete. The Lambda function's execution time is too slow for the application. Considering that the Lambda function does not return any important data, which method will accelerate data processing the most?\",\"answers\":[{\"text\":\"Use asynchronous Event Lambda invocations. Configure the function to process the files in parallel.\",\"isCorrect\":true},{\"text\":\"Use synchronous RequestResponse Lambda invocations. Process the files one by one.\",\"isCorrect\":false},{\"text\":\"Merge all of the files into a single file then process them all at once with an asynchronous Event Lambda invocation.\",\"isCorrect\":false},{\"text\":\"Compress the files to reduce their size, then process them with synchronous RequestResponse Lambda invocations.\",\"isCorrect\":false}],\"explanation\":\"$15\"},{\"question\":\"A developer is building a serverless URL shortener using Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. The application code as well as the stack that defines the cloud resources should be written in Python. The code should also be reusable in case an update must be done to the stack. Which of the following actions must be done by the developer to meet the requirements above?\",\"answers\":[{\"text\":\"Use AWS CloudFormation to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.\",\"isCorrect\":false},{\"text\":\"Use AWS SDK for Python (boto3) to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.\",\"isCorrect\":false},{\"text\":\"Use AWS CDK to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.\",\"isCorrect\":true},{\"text\":\"Use AWS CloudShell to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.\",\"isCorrect\":false}],\"explanation\":\"$16\"},{\"question\":\"An application has a feature that displays GIFs based on keyword inputs. The code streams random GIF links from an external API to your local machine. When run, the application's process takes longer than expected. You are suspecting that the new function sendRequest() you added is the culprit. Which of the following actions should you do to determine the latency of the function?\",\"answers\":[{\"text\":\"Use CloudTrail to record and store event logs for actions made by your function.\",\"isCorrect\":false},{\"text\":\"Using AWS X-Ray, disable sampling to efficiently trace all requests for calls.\",\"isCorrect\":false},{\"text\":\"Using CloudWatch, troubleshoot the issue by checking the logs.\",\"isCorrect\":false},{\"text\":\"Using AWS X-Ray, define an arbitrary subsegment inside the code to instrument the function.\",\"isCorrect\":true}],\"explanation\":\"$17\"},{\"question\":\"A developer is building an AWS Lambda-based Java application that optimizes pictures uploaded to an S3 bucket. Upon running several tests, the Lambda function shows a cold start of about 5 seconds. Which of the following could the developer do to reduce the cold start time? (Select TWO.)\",\"answers\":[{\"text\":\"Add the Spring Framework to the project and enable dependency injection.\",\"isCorrect\":false},{\"text\":\"Run the Lambda function in a VPC to gain access to Amazon’s high-end infrastructure.\",\"isCorrect\":false},{\"text\":\"Reduce the deployment package’s size by including only the needed modules from the AWS SDK for Java.\",\"isCorrect\":true},{\"text\":\"Increase the memory allocation setting for the Lambda function.\",\"isCorrect\":true},{\"text\":\"Increase the timeout setting for the Lambda function.\",\"isCorrect\":false}],\"explanation\":\"$18\"},{\"question\":\"A developer is writing a web application that will allow users to save and retrieve images in an Amazon S3 bucket. The users are required to register and log in to access the application. Which combination of AWS Services should the Developer utilize for implementing the user authentication module of the application?\",\"answers\":[{\"text\":\"Amazon Cognito Identity Pools and IAM Role.\",\"isCorrect\":false},{\"text\":\"Amazon Cognito Identity Pools and User Pools.\",\"isCorrect\":true},{\"text\":\"Amazon Cognito User Pools and AWS Key Management Service (KMS)\",\"isCorrect\":false},{\"text\":\"Amazon User Pools and AWS Security Token Service (STS)\",\"isCorrect\":false}],\"explanation\":\"$19\"},{\"question\":\"A university is gradually migrating some of its physical documents to the AWS cloud. They will start by moving their alumnus' historical records to Amazon S3. The storage solution should provide a secure and durable object storage with the lowest cost. Which of the following types of S3 storage should you recommend?\",\"answers\":[{\"text\":\"Amazon S3 One-Zone\",\"isCorrect\":false},{\"text\":\"Amazon S3 Infrequent Access\",\"isCorrect\":false},{\"text\":\"Amazon S3 Glacier Deep Archive\",\"isCorrect\":true},{\"text\":\"Amazon S3 Glacier\",\"isCorrect\":false}],\"explanation\":\"$1a\"},{\"question\":\"A developer is tasked with enhancing the performance of an online learning platform that uses a serverless architecture. The platform relies on Amazon API Gateway to handle user requests, AWS Lambda to process quiz submissions, Amazon DynamoDB to store course progress data, and Amazon S3 to host video lectures. During peak hours, the platform experiences high latency caused by increased read operations on DynamoDB, leading to a poor user experience. Which AWS service or feature should the developer implement to optimize DynamoDB read performance and reduce latency?\",\"answers\":[{\"text\":\"Amazon CloudFront\",\"isCorrect\":false},{\"text\":\"Amazon DynamoDB Streams\",\"isCorrect\":false},{\"text\":\"Amazon Elastic Load Balancing\",\"isCorrect\":false},{\"text\":\"Amazon DynamoDB Accelerator (DAX)\",\"isCorrect\":true}],\"explanation\":\"$1b\"},{\"question\":\"A developer is using the AWS CLI to interact with different AWS services. An UnauthorizedOperation error, as shown below, is received after running the stop-instance command: Along with the response is an additional failure message displayed in ciphertext format. How can the developer decode the message?\",\"answers\":[{\"text\":\"Decode the message by calling the AWS IAM decode-authorization-message command.\",\"isCorrect\":false},{\"text\":\"Decode the message by calling the AWS KMS decrypt command.\",\"isCorrect\":false},{\"text\":\"Decode the message by calling the AWS STS decode-authorization-message command.\",\"isCorrect\":true},{\"text\":\"Decode the message using an external cryptography library.\",\"isCorrect\":false}],\"explanation\":\"$1c\"},{\"question\":\"An application executes GET operations to various AWS services. The development team is using AWS X-Ray to trace all the calls made to AWS. As one of the developers, you are responsible for maintaining a particular block of code on the application. To save time, you only want to record data associated with the code to group the traces in the AWS console. Which of the following X-Ray features should you use?\",\"answers\":[{\"text\":\"Subsegment\",\"isCorrect\":false},{\"text\":\"Metadata\",\"isCorrect\":false},{\"text\":\"Annotations\",\"isCorrect\":true},{\"text\":\"Sampling\",\"isCorrect\":false}],\"explanation\":\"$1d\"},{\"question\":\"A company is running an Artificial Intelligence (AI) software for its automotive clients using the AWS Cloud. The software is used for identifying road obstructions for autonomous driving and predicting failure on vehicle components. The company wants to extend its usage and access based on different levels (students, professionals, and hobbyist developers) by exposing an API through API Gateway. The company should regulate access to the API and monetize it by charging based on usage. What should the company do?\",\"answers\":[{\"text\":\"Create three stages and enable CloudWatch metrics. Set up an alarm for each stage according to the ApiName, Method, Resource, and Stage dimensions.\",\"isCorrect\":false},{\"text\":\"Create three stages. Specify a quota and throttle requests according to the level of access.\",\"isCorrect\":false},{\"text\":\"Create three Authorizers to control API access.\",\"isCorrect\":false},{\"text\":\"Create three Usage Plans. Specify a quota and throttle requests according to the level of access.\",\"isCorrect\":true}],\"explanation\":\"$1e\"},{\"question\":\"A developer needs to build a queueing mechanism for an application that will run on AWS. The application is expected to consume SQS messages that are larger than 256 KB and up to 1 GB in size. How should the developer manage the SQS messages?\",\"answers\":[{\"text\":\"Use Amazon S3 and the Amazon SQS Extended Client Library for Java\",\"isCorrect\":true},{\"text\":\"Use Amazon S3 and the Amazon SQS CLI\",\"isCorrect\":false},{\"text\":\"Use Amazon S3 and the Amazon SQS HTTP API\",\"isCorrect\":false},{\"text\":\"Use Amazon S3 and the Amazon SQS Console\",\"isCorrect\":false}],\"explanation\":\"$1f\"},{\"question\":\"A microservices application's Customer and Payment service components have two separate DynamoDB tables. New items inserted into the Customer service table must be dynamically updated in the Payment service table. How can the Payment service get near real-time update\",\"answers\":[{\"text\":\"Create a Firehose stream to stream all the changes from the Customer service table and trigger a Lambda function to update the Payment service table.\",\"isCorrect\":false},{\"text\":\"Use a Kinesis data stream to stream all the changes from the Customer service database directly into the Payment service table.\",\"isCorrect\":false},{\"text\":\"Create a scheduled Amazon EventBridge (Amazon CloudWatch Events) rule that invokes a Lambda function every minute to update the changes from the Customer service table into the Payment service table.\",\"isCorrect\":false},{\"text\":\"Enable DynamoDB Streams to stream all the changes from the Customer service table and trigger a Lambda function to update the Payment service table.\",\"isCorrect\":true}],\"explanation\":\"$20\"},{\"question\":\"A company wants to know how its monolithic application will perform on a microservice architecture. The Lead Developer has deployed the application on Amazon ECS using the EC2 launch type. He terminated the container instance after testing; however, the container instance still appears as a resource in the ECS cluster. What is the possible cause of this?\",\"answers\":[{\"text\":\"When a container instance is terminated in the running state, the container instance is not automatically deregistered from the cluster.\",\"isCorrect\":false},{\"text\":\"After terminating the container instance in the running state, the container instance must be manually deregistered in the Amazon ECS Console.\",\"isCorrect\":false},{\"text\":\"When a container instance is terminated in the stopped state, the container instance is not automatically deregistered from the cluster.\",\"isCorrect\":true},{\"text\":\"After terminating the container instance in the stopped state, the container instance must be manually deregistered in the Amazon EC2 Console since it was launched using the EC2 launch type.\",\"isCorrect\":false}],\"explanation\":\"$21\"},{\"question\":\"An application that runs on a local Linux server is migrated to a Lambda function to save costs. The Lambda function shows an Unable to import module error when invoked. As the developer, how can you fix the error?\",\"answers\":[{\"text\":\"Download the missing modules in the lib directory. Package all files under the lib directory into a ZIP file and upload it to AWS Lambda.\",\"isCorrect\":false},{\"text\":\"Import the missing modules in the Lambda code. The Lambda function will automatically install them when invoked.\",\"isCorrect\":false},{\"text\":\"Install the missing modules locally to your application’s folder. Package the folder into a ZIP file and upload it to AWS Lambda.\",\"isCorrect\":true},{\"text\":\"Run a Linux command inside the Lambda function to install the missing modules.\",\"isCorrect\":false}],\"explanation\":\"$22\"},{\"question\":\"A developer is building an application with Amazon DynamoDB as its database. The application needs to group the PUT, UPDATE, and DELETE actions into a single all-or-nothing operation to make changes against multiple items in the DynamoDB table. Which DynamoDB operation should the developer use?\",\"answers\":[{\"text\":\"Query\",\"isCorrect\":false},{\"text\":\"Scan\",\"isCorrect\":false},{\"text\":\"TransactWriteItems\",\"isCorrect\":true},{\"text\":\"BatchWriteItem\",\"isCorrect\":false}],\"explanation\":\"$23\"},{\"question\":\"A media analytics company provides APIs as a service that enables users to retrieve aggregated, daily-updated data on video performance metrics, such as views, likes, and watch times. These APIs are built using Amazon API Gateway and AWS Lambda, with data sourced from a pre-computed file in Amazon S3 that updates every 24 hours. Due to a surge in traffic, users have reported increased latency when accessing the API. The company aims to enhance the API's responsiveness without changing the backend architecture. Which approach would improve the responsiveness of the APIs?\",\"answers\":[{\"text\":\"Enable cross-origin resource sharing (CORS) using the API Gateway.\",\"isCorrect\":false},{\"text\":\"Configure Amazon CloudFront as a caching layer in front of API Gateway.\",\"isCorrect\":false},{\"text\":\"Enable Amazon API Gateway caching.\",\"isCorrect\":true},{\"text\":\"Use Amazon ElastiCache to store frequently requested data in memory.\",\"isCorrect\":false}],\"explanation\":\"$24\"},{\"question\":\"A developer has been instructed to automate the creation of the snapshot of an existing Amazon EC2 instance. The engineer created a script that uses the AWS Command Line Interface (CLI) to run the necessary API call. He is getting an InvalidInstanceID.NotFound error whenever the script is run. What is the most likely cause of the error?\",\"answers\":[{\"text\":\"The AWS Access Key Id used to configure the AWS CLI is invalid.\",\"isCorrect\":false},{\"text\":\"The Image Id used in running the command for creating a snapshot is incorrect.\",\"isCorrect\":false},{\"text\":\"The AWS Region, where the programmatic access for the AWS CLI is created, does not match the region where the instance lives.\",\"isCorrect\":false},{\"text\":\"The AWS Region name used to configure the AWS CLI does not match the region where the instance lives.\",\"isCorrect\":true}],\"explanation\":\"$25\"},{\"question\":\"A developer uses AWS Serverless Application Model (SAM) in a local machine to create a serverless Python application. After defining the required dependencies in the requirements.txt file, the developer is now ready to test and deploy. What are the steps to successfully deploy the application?\",\"answers\":[{\"text\":\"Upload and build the SAM template in an EC2 instance. Run the sam deploy command to package and deploy the SAM template.\",\"isCorrect\":false},{\"text\":\"Build the SAM template in the local machine. Run the sam deploy command to package and deploy the SAM template from AWS CodePipeline.\",\"isCorrect\":false},{\"text\":\"Run the sam init command. Build the SAM template in the local machine and call the sam deploy command to package and deploy the SAM template from an S3 bucket.\",\"isCorrect\":false},{\"text\":\"Build the SAM template in the local machine and call the sam deploy command to package and deploy the SAM template from an S3 bucket.\",\"isCorrect\":true}],\"explanation\":\"$26\"},{\"question\":\"A team of developers needs permission to launch EC2 instances with an instance role that will allow them to update items in a DynamoDB table. Each developer has access to IAM users that belongs in the same IAM group. Which of the following steps must be done to implement the solution?\",\"answers\":[{\"text\":\"Create an IAM role with an IAM policy that will allow access to the EC2 instances. Add the DynamoDB service to the trust policy of the role. Create a custom policy with the iam:GetRole permission. Attach the policy to the IAM group.\",\"isCorrect\":false},{\"text\":\"Create an IAM role with an IAM policy that will allow access to the DynamoDB table. Add the EC2 service to the trust policy of the role. Create a custom policy with the iam:PassRole permission. Attach the policy to the IAM group.\",\"isCorrect\":true},{\"text\":\"Create an IAM role with an IAM policy that will allow access to the DynamoDB table. Add the EC2 service to the trust policy of the role. Create a custom policy with iam:GetRolePolicy and iam:PutRolePolicy permissions. Attach the policy to the IAM group.\",\"isCorrect\":false},{\"text\":\"Create an IAM role with an IAM policy that will allow access to the EC2 instances. Add the DynamoDB service to the trust policy of the role. Create a custom policy with the iam:PassRole permission. Attach the policy to the IAM group.\",\"isCorrect\":false}],\"explanation\":\"$27\"},{\"question\":\"A company is running an e-commerce application on an Amazon EC2 instance. A newly hired developer has been tasked to monitor and handle the necessary updates on the EC2 instance every Saturday. The developer is working from home and needs remote access to the webserver. As the system administrator, you’re looking to use the AWS STS API to give the developer temporary credentials and enforce Multi-factor Authentication (MFA) to protect specific programmatic calls against the instance that could adversely affect the server. Which of the following STS API should you use?\",\"answers\":[{\"text\":\"AssumeRoleWithSAML\",\"isCorrect\":false},{\"text\":\"GetSessionToken\",\"isCorrect\":true},{\"text\":\"AssumeRoleWithWebIdentity\",\"isCorrect\":false},{\"text\":\"GetFederationToken\",\"isCorrect\":false}],\"explanation\":\"$28\"},{\"question\":\"A Lamba function has multiple sub-functions that are chained together to process large data synchronously. When invoked, the function tends to exceed its maximum timeout limit. This has prompted the developer to break the Lambda function into manageable coordinated states using Step Functions, enabling each sub-function to run in separate processes. Which of the following type of states should the developer use to run processes?\",\"answers\":[{\"text\":\"Pass State\",\"isCorrect\":false},{\"text\":\"Parallel State\",\"isCorrect\":false},{\"text\":\"Task State\",\"isCorrect\":true},{\"text\":\"Wait State\",\"isCorrect\":false}],\"explanation\":\"$29\"},{\"question\":\"A San Francisco-based tech startup is building a cross-platform mobile app that can notify the user of upcoming astronomical events. Your mobile app authenticates with the Identity Provider (IdP) using the provider's SDK and Amazon Cognito. Once the end-user is authenticated with the IdP, the OAuth or OpenID Connect token returned from the IdP is passed by your app to Amazon Cognito. Which of the following is returned for the user to provide a set of temporary, limited-privilege AWS credentials?\",\"answers\":[{\"text\":\"Cognito ID\",\"isCorrect\":true},{\"text\":\"Cognito Key Pair\",\"isCorrect\":false},{\"text\":\"Cognito API\",\"isCorrect\":false},{\"text\":\"Cognito SDK\",\"isCorrect\":false}],\"explanation\":\"$2a\"},{\"question\":\"A developer needs to view the percentage of used memory and the number of TCP connections of instances inside an Auto Scaling Group. To achieve this, the developer must send the metrics to Amazon CloudWatch. Which approach provides the MOST secure way of authenticating a CloudWatch PUT request?\",\"answers\":[{\"text\":\"Create an IAM user with programmatic access. Attach a cloudwatch:PutMetricDatapermission and store the access key and secret key in the instance’s configuration file.\",\"isCorrect\":false},{\"text\":\"Create an IAM role with cloudwatch:PutMetricData permission for the new Auto Scaling launch template from which you launch instances.\",\"isCorrect\":true},{\"text\":\"Create an IAM user with programmatic access. Attach a cloudwatch:PutMetricData permission and update the Auto Scaling launch template to insert the access key and secret key into the instance user data.\",\"isCorrect\":false},{\"text\":\"Modify the existing Auto Scaling launch template to use an IAM role with the cloudwatch:PutMetricData permission for the instances.\",\"isCorrect\":false}],\"explanation\":\"$2b\"},{\"question\":\"A mobile game developer is using DynamoDB as a data store and a Web Identity Federation for authorization and authentication. Each item in the DynamoDB table contains the attributes for individual user's game data such as user ID, game scores, and top score where the user ID is the partition key. The developer must control user access to specific data items based on their IDs. In doing so, users will only be able to obtain items that they own. Which of the following solutions must be implemented by the developer?\",\"answers\":[{\"text\":\"Modify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:Select condition key to the user IDs.\",\"isCorrect\":false},{\"text\":\"Modify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:Attributes condition key to the user IDs.\",\"isCorrect\":false},{\"text\":\"Modify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:LeadingKeys condition key to the user IDs.\",\"isCorrect\":true},{\"text\":\"Modify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:ReturnValues condition key to the user IDs.\",\"isCorrect\":false}],\"explanation\":\"$2c\"},{\"question\":\"A developer has an application that stores sensitive data to an Amazon DynamoDB table. AWS KMS must be used to encrypt the data before sending it to the table and to manage the encryption keys. Which of the following features are supported when using KMS? (Select TWO.)\",\"answers\":[{\"text\":\"Creation of symmetric encryption and asymmetric KMS keys\",\"isCorrect\":true},{\"text\":\"Automatic key rotation for KMS keys in custom key stores\",\"isCorrect\":false},{\"text\":\"Importing a custom key material to an asymmetric KMS key\",\"isCorrect\":false},{\"text\":\"Using AWS Certificate Manager as a custom key store\",\"isCorrect\":false},{\"text\":\"Re-enabling disabled keys\",\"isCorrect\":true}],\"explanation\":\"$2d\"},{\"question\":\"A startup has recently opened an AWS account to develop a cloud-native web application. The CEO wants to improve the security of the account by implementing the best practices in managing access keys in AWS. Which actions follow the security best practices in IAM? (Select TWO.)\",\"answers\":[{\"text\":\"Use IAM roles for applications that need access to AWS services.\",\"isCorrect\":true},{\"text\":\"Delete any access keys to your AWS account root user.\",\"isCorrect\":true},{\"text\":\"Save the access key in your application code for convenience.\",\"isCorrect\":false},{\"text\":\"Maintain at least one access key for your AWS account root user\",\"isCorrect\":false},{\"text\":\"Regularly rotate the credentials for all the account users except for the administrator user for tracking purposes.\",\"isCorrect\":false}],\"explanation\":\"$2e\"},{\"question\":\"A company is planning to launch an online cross-platform game that expects millions of users. The developer wants to use an in-house authentication system for user identification. Each user identifier must be kept consistent across devices and platforms. How can the developer achieve this?\",\"answers\":[{\"text\":\"Generate a unique IAM access key for each user and use the access key ID as the unique identifier.\",\"isCorrect\":false},{\"text\":\"Generate a universally unique identifier (UUID) for each device. Store the UUID with the user in a DynamoDB table.\",\"isCorrect\":false},{\"text\":\"Use developer-authenticated identities in Amazon Cognito to generate unique identifiers for the users.\",\"isCorrect\":true},{\"text\":\"Create an IAM Role for each user and use its Amazon Resource Name (ARN) as unique identifiers.\",\"isCorrect\":false}],\"explanation\":\"$2f\"},{\"question\":\"A Software Engineer is developing a Node.js application that will be deployed using Elastic Beanstalk. The application source code is currently inside a folder called MyApp. He wants to add a configuration file named tutorialsdojo.config to the application. Where should the file be placed?\",\"answers\":[{\"text\":\"Inside the MyApp folder at the root level\",\"isCorrect\":false},{\"text\":\"Inside the package.json\",\"isCorrect\":false},{\"text\":\"Inside the .ebextensions folder\",\"isCorrect\":true},{\"text\":\"Inside the .elasticbeanstalk folder\",\"isCorrect\":false}],\"explanation\":\"$30\"},{\"question\":\"A company has a serverless application on AWS. They are using AWS Lambda for business logic and Amazon API Gateway for handling client requests. They have published a version of the AccountService:Prod function with the alias AccountService:Beta. The internal team wants to test these updates before promoting them to production without impacting live users. Which configuration should the company take?\",\"answers\":[{\"text\":\"Create a new stage named 'Beta' in Amazon API Gateway and use stage variables to reference the Lambda functions in Prod and Beta.\",\"isCorrect\":true},{\"text\":\"Modify the existing 'Prod' stage in Amazon API Gateway to create a new stage variable that references the Lambda functions in Prod and Beta.\",\"isCorrect\":false},{\"text\":\"Create a 'Beta' stage in Amazon API Gateway. In the Lambda function configuration, add a condition to the AccountService:Prod function that checks for an environment variable and, if set to 'Beta', invokes the AccountService:Beta alias instead\",\"isCorrect\":false},{\"text\":\"Update the integration request settings in the 'Prod' stage of the Amazon API Gateway to select the Lambda function alias based on a query parameter.\",\"isCorrect\":false}],\"explanation\":\"$31\"},{\"question\":\"A startup is integrating an event-driven alerting tool with a third-party platform. The platform requires a publicly accessible HTTPS endpoint to receive webhook requests, which will be processed by a Lambda function. Given that the platform signs each request with a secret key and includes it in the headers, the developer must ensure that the Lambda function executes the domain logic only when a webhook request comes from a valid user. Which action would satisfy the requirement with the least amount of development effort?\",\"answers\":[{\"text\":\"Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the \\\"lambda:FunctionUrlAuthType\\\": \\\"NONE\\\" condition is present. Write a custom authorization logic based on a signature provided in the HTTP headers.\",\"isCorrect\":true},{\"text\":\"Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the \\\"lambda:CodeSigningConfigArn\\\": \\\"arn:aws:lambda:\u003cAWS_REGION\u003e:\u003cACCOUNT_NUMBER\u003e:code-signing-config:csc-\u003cSIGNING_SECRET\u003e\\\" condition is present.\",\"isCorrect\":false},{\"text\":\"Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the \\\"lambda:FunctionUrlAuthType\\\": \\\"AWS_IAM\\\" condition is present.\",\"isCorrect\":false},{\"text\":\"Configure API Gateway to connect with the Lambda function using a Lambda proxy integration. Create a Lambda function authorizer to validate incoming requests based on a signature provided in the HTTP headers.\",\"isCorrect\":false}],\"explanation\":\"$32\"},{\"question\":\"A developer plans to use AWS Elastic Beanstalk to deploy a microservice application. The application will be implemented in a multi-container Docker environment. How should the developer configure the container definitions in the environment?\",\"answers\":[{\"text\":\"Use the eb config command to configure the container definitions.\",\"isCorrect\":false},{\"text\":\"Configure the container definitions in the Dockerrun.aws.json.config and put it inside the .ebextensions folder.\",\"isCorrect\":false},{\"text\":\"Configure the container definitions in the Amazon ECS Console when building the Docker environment.\",\"isCorrect\":false},{\"text\":\"Configure the container definitions in the Dockerrun.aws.json file.\",\"isCorrect\":true}],\"explanation\":\"$33\"},{\"question\":\"A developer is testing a Lambda function that was created from a CloudFormation template. While the function executes without errors, it isn't generating logs in Amazon CloudWatch Logs, and the developer cannot find associated log streams or log groups. Upon inspection, the following observations were made: The function's code contains appropriate logging statements. The Lambda function is associated with an execution role that establishes a trusted relationship with the Lambda service; however, this role has no permissions assigned. The Lambda function does not have any resource-based policies. Which configuration must be done to resolve the issue?\",\"answers\":[{\"text\":\"Update the execution role by adding the AWSLambdaBasicExecutionRole managed policy.\",\"isCorrect\":true},{\"text\":\"Update the execution role by adding the CloudWatchLambdaInsightsExecutionRolePolicy managed policy.\",\"isCorrect\":false},{\"text\":\"Associate the function with a resource-based policy that contains the logs:CreateLogGroup, logs:CreateLogStream, and logs:PutLogEvents permissions.\",\"isCorrect\":false},{\"text\":\"Associate the function with a resource-based policy that contains the logs:PutLogEvents permissions.\",\"isCorrect\":false}],\"explanation\":\"$34\"},{\"question\":\"A developer is debugging an issue in an AWS Lambda-based application. To save time searching through logs, the developer wants the function to return the corresponding log location of an invocation request. Which approach should the developer take with the least amount of effort?\",\"answers\":[{\"text\":\"Extract the log stream name from the Event object of the handler function.\",\"isCorrect\":false},{\"text\":\"Extract the log stream name from the Context object of the handler function.\",\"isCorrect\":true},{\"text\":\"Extract the invocation request id from the Context object of the handler function. Then, call the FilterLogEvents API and pass the request id to filter results.\",\"isCorrect\":false},{\"text\":\"Extract the invocation request id from the Event object of the handler. Call the FilterLogEvents API and use the request id to filter results.\",\"isCorrect\":false}],\"explanation\":\"$35\"},{\"question\":\"A company uses AWS CodeDeploy in their CI/CD pipeline to handle in-place deployments of their web application on EC2 instances. Recently, a new version of the application was pushed, which contained a code regression. The deployment group is configured with automatic rollback. What happens if the deployment of the new version fails?\",\"answers\":[{\"text\":\"CodeDeploy pauses the current deployment, restores the last stable version from S3, and uses the deployment ID of the most recent deployment with a SUCCEEDED status.\",\"isCorrect\":false},{\"text\":\"CodeDeploy redeploys the last known good version of an application with a new deployment ID.\",\"isCorrect\":true},{\"text\":\"CodeDeploy reroutes traffic back to the blue environment and terminates the green environment.\",\"isCorrect\":false},{\"text\":\"CodeDeploy reverts the EC2 instance to a previous AMI snapshot taken during the last successful deployment.\",\"isCorrect\":false}],\"explanation\":\"$36\"},{\"question\":\"A company uses a Linux, Apache, MySQL, and PHP (LAMP) web service stack to host an on-premises application for its car rental business. The manager wants to move its operation into the Cloud using Amazon Web Services. Which combination of services could be used to run the application that will require the least amount of configuration?\",\"answers\":[{\"text\":\"Amazon API Gateway and Amazon RDS\",\"isCorrect\":false},{\"text\":\"Amazon EC2 and Amazon Aurora\",\"isCorrect\":true},{\"text\":\"Amazon ECS and Amazon EFS\",\"isCorrect\":false},{\"text\":\"Amazon S3 and Amazon CloudFront\",\"isCorrect\":false}],\"explanation\":\"$37\"},{\"question\":\"A developer needs to use IAM roles to list all EC2 instances that belong to the development environment in an AWS account. Which methods could be done to verify IAM access to describe instances? (Select TWO.)\",\"answers\":[{\"text\":\"Use the IAM Policy Simulator to validate the permission for the IAM role.\",\"isCorrect\":true},{\"text\":\"Run the describe-instances command with the --dry-run parameter.\",\"isCorrect\":true},{\"text\":\"Run the get-group-policy command.\",\"isCorrect\":false},{\"text\":\"Validate the IAM role’s permission by querying the in-line policies within the EC2 instance metadata.\",\"isCorrect\":false},{\"text\":\"Run the get-role command.\",\"isCorrect\":false}],\"explanation\":\"$38\"},{\"question\":\"An application is used to upload images to an Amazon S3 bucket. Once an event occurs, a Lambda function is triggered to compress the photos. However, it has been discovered that the processing time of the function is longer than expected. Which change will improve the processing time of the function most effectively?\",\"answers\":[{\"text\":\"Configure the S3 bucket to send notifications to an SQS queue. Use the SQS queue with the Lambda function to process the image.\",\"isCorrect\":false},{\"text\":\"Increase the timeout value of the function.\",\"isCorrect\":false},{\"text\":\"Increase the memory allocation of the function.\",\"isCorrect\":true},{\"text\":\"Run the function with Lambda@Edge which will run the code closer to the users of your application, reducing your application’s latency.\",\"isCorrect\":false}],\"explanation\":\"$39\"},{\"question\":\"An application uses the PutObject operation in parallel to upload hundreds of thousands of objects per second to an S3 bucket. To meet security compliance, the developer uses the server-side encryption in AWS KMS (SSE-KMS) to encrypt objects as they get stored in the S3 bucket. There is a noticeable performance degradation after making the change. Which of the following is the most likely cause of the problem?\",\"answers\":[{\"text\":\"The KMS key does not use an alias required for the server-side encryption with AWS KMS (SSE-KMS).\",\"isCorrect\":false},{\"text\":\"The Amazon S3 throttles the PutObject operation for objects encrypted with SSE-KMS.\",\"isCorrect\":false},{\"text\":\"The AWK KMS key uses an AES 256 algorithm, which makes the encryption process slower. AES 128 should be used instead.\",\"isCorrect\":false},{\"text\":\"The API request rate has exceeded the quota for AWS KMS API operations.\",\"isCorrect\":true}],\"explanation\":\"$3a\"},{\"question\":\"A development team is building a website that displays an analytics dashboard. The team uses AWS CodeBuild to compile the website from a source code residing on Github. A member was instructed to configure CodeBuild to run with a proxy server for privacy and security reasons. A RequestError timeout error appears on CloudWatch whenever CodeBuild is accessed. Which is a possible solution to resolve the issue?\",\"answers\":[{\"text\":\"Modify the proxy element of the buildspec.yml file on the source code root directory.\",\"isCorrect\":true},{\"text\":\"Modify the artifacts element of the buildspec.yml file on the source code root directory.\",\"isCorrect\":false},{\"text\":\"Modify the proxy element of the AppSpec.yml file on the source code root directory.\",\"isCorrect\":false},{\"text\":\"Modify the phases element of the AppSpec.yml file on the source code root directory.\",\"isCorrect\":false}],\"explanation\":\"$3b\"},{\"question\":\"A developer wants to cut down the execution time of the scan operation on a DynamoDB table during periods of low demand without interfering with typical workloads. The operation consumes half of the strongly consistent read capacity units within regular operating hours. How can the developer improve this scan operation?\",\"answers\":[{\"text\":\"Perform a rate-limited parallel scan operation.\",\"isCorrect\":true},{\"text\":\"Use a parallel scan operation.\",\"isCorrect\":false},{\"text\":\"Use eventually consistent reads for the scan operation instead of strongly consistent reads.\",\"isCorrect\":false},{\"text\":\"Perform a rate-limited sequential scan operation.\",\"isCorrect\":false}],\"explanation\":\"$3c\"},{\"question\":\"A developer uses Amazon ECS to orchestrate two Docker containers. He needs to configure ECS to allow the two containers to share log data. Which configuration should the developer do?\",\"answers\":[{\"text\":\"Use two task definitions for each container and mount an EFS volume between the tasks.\",\"isCorrect\":false},{\"text\":\"Specify the containers in a single pod specification and configure EFS as its volume type.\",\"isCorrect\":false},{\"text\":\"Specify the containers in a single task definition and configure EFS as its volume type.\",\"isCorrect\":true},{\"text\":\"Use two pod specifications for each container and mount an EFS volume between the pods.\",\"isCorrect\":false}],\"explanation\":\"$3d\"},{\"question\":\"Several development teams worldwide will be collaboratively working on a project hosted on an AWS Elastic Beanstalk environment. The developers need to be able to deploy incremental code updates without re-uploading the entire project. Which of the following actions will reduce the upload and deployment time with the LEAST amount of effort?\",\"answers\":[{\"text\":\"Configure event notifications on a central S3 bucket and allow access to all developers. Invoke a Lambda Function that will deploy the code to Elastic Beanstalk when a PUT event occurs.\",\"isCorrect\":false},{\"text\":\"Host the code repository on an EC2 instance and allow access to all the developers. Write a script that will automate the deployment process to Elastic Beanstalk.\",\"isCorrect\":false},{\"text\":\"Create an AWS CodeCommit repository and allow access to all developers. Deploy the code to Elastic Beanstalk.\",\"isCorrect\":true},{\"text\":\"Upload the code to an Amazon EFS mounted on an EC2 instance. Write a script that will automate the deployment process to Elastic Beanstalk.\",\"isCorrect\":false}],\"explanation\":\"$3e\"},{\"question\":\"A development team uses AWS Step Functions to orchestrate a serverless workflow that processes incoming data streams with AWS Lambda. The team requires a solution to extract custom metrics, such as processing times, directly from the logs generated by a Lambda function. These metrics must be analyzed for operational insights, with alarms set up to detect anomalies in real-time. Which approach should be used to meet this requirement?\",\"answers\":[{\"text\":\"Configure Amazon Data Firehose to stream logs from the Lambda function to an Amazon Redshift cluster. Use SQL queries to analyze metrics and set alerts for anomalies.\",\"isCorrect\":false},{\"text\":\"Use Amazon's open-source libraries to format logs in the Amazon CloudWatch embedded metric format. Then, use CloudWatch to monitor, view, and set alarms for the custom metrics.\",\"isCorrect\":true},{\"text\":\"Use Amazon CloudWatch Lambda Insights to extract and monitor custom metrics for processing times. Set up alarms and dashboards in Lambda Insights for real-time analysis.\",\"isCorrect\":false},{\"text\":\"Send custom metric data directly to Amazon EventBridge using the PutMetricData API operation. Create EventBridge rules to trigger actions based on the metrics.\",\"isCorrect\":false}],\"explanation\":\"$3f\"},{\"question\":\"A developer is managing several microservices built using API Gateway and AWS Lambda. The Developer wants to deploy new updates to one of the APIs. He wants to ensure a smooth transition between the versions by giving users enough time to migrate to the new version before retiring the previous one. Which solution should the developer implement?\",\"answers\":[{\"text\":\"Implement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target resource then redeploy it to the same stage.\",\"isCorrect\":false},{\"text\":\"Implement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target resource then redeploy it to a new stage.\",\"isCorrect\":true},{\"text\":\"Implement the updates and publish a new version of the Lambda function. Then, issue the new Lambda invocation URL.\",\"isCorrect\":false},{\"text\":\"Implement the updates on the Lambda function. Create a CloudFront distribution and use the function as the origin.\",\"isCorrect\":false}],\"explanation\":\"$40\"},{\"question\":\"A developer wants to expose a legacy web service that uses an XML-based Simple Object Access Protocol (SOAP) interface through API Gateway. However, there is a compatibility issue since most modern applications communicate data in JSON format. Which is the most cost-effective method that will overcome this issue?\",\"answers\":[{\"text\":\"Use API Gateway to create a WebSocket API. Transform the incoming JSON into XML using mapping templates. Forward the request into the SOAP interface by using a Lambda function and parse the response (XML) into JSON before sending back to API Gateway.\",\"isCorrect\":false},{\"text\":\"Use API Gateway to create a RESTful API. Send the incoming JSON to an HTTP server hosted on an EC2 instance and have it transform the data into XML and vice versa before sending it to the legacy application.\",\"isCorrect\":false},{\"text\":\"Use API Gateway to create a RESTful API. Transform the incoming JSON into XML for the SOAP interface through an Application Load Balancer and vice versa. Put the legacy web service behind the ALB.\",\"isCorrect\":false},{\"text\":\"Use API Gateway to create a RESTful API. Transform the incoming JSON into XML using mapping templates. Forward the request into the SOAP interface by using a Lambda function and parse the response (XML) into JSON before sending back to API Gateway.\",\"isCorrect\":true}],\"explanation\":\"$41\"},{\"question\":\"A developer has a Python script that relies on the low-level BatchGetItem API to fetch large amounts of data from a DynamoDB table. The script often encounters responses with partial results. A significant portion of the data appears under UnprocessedKeys. Which approaches can the developer implement to handle data retrieval MOST reliably? (Select TWO.)\",\"answers\":[{\"text\":\"Implement an exponential backoff algorithm with a randomized delay between retries of the batch request.\",\"isCorrect\":true},{\"text\":\"Use the AWS software development kit (AWS SDK) to send batch requests.\",\"isCorrect\":true},{\"text\":\"Implement a logic that immediately retries the batch request.\",\"isCorrect\":false},{\"text\":\"Increase the read capacity units (RCUs) for the DynamoDB table and enable Auto Scaling.\",\"isCorrect\":false},{\"text\":\"Create a Global Secondary Index (GSI) with its own read capacity settings.\",\"isCorrect\":false}],\"explanation\":\"$42\"},{\"question\":\"Some static assets stored in an S3 bucket need to be accessed by a user on the development account. The S3 bucket is in the production account. According to the company policy, the sharing of full credentials between accounts is prohibited. What steps should be done to delegate access across the two accounts? (Select THREE.)\",\"answers\":[{\"text\":\"Log in to the production account and create a policy that will use STS to assume the IAM role in the development account. Attach the policy to the IAM users.\",\"isCorrect\":false},{\"text\":\"On the development account, create an IAM role and specify the production account as a trusted entity.\",\"isCorrect\":false},{\"text\":\"Set the policy that will grant access to S3 for the IAM role created in the production account.\",\"isCorrect\":true},{\"text\":\"Set the policy that will grant access to S3 for the IAM role created in the development account.\",\"isCorrect\":false},{\"text\":\"Log in to the development account and create a policy that will use STS to assume the IAM role in the production account. Attach the policy to the IAM users.\",\"isCorrect\":true},{\"text\":\"On the production account, create an IAM role and specify the development account as a trusted entity.\",\"isCorrect\":true}],\"explanation\":\"$43\"},{\"question\":\"A developer plans to launch an EC2 instance, with Amazon Linux 2 as its AMI, using the AWS Console. A security group with port 80 that is open to public access will be associated with the instance. He wants to quickly build and test an Apache webserver with an index.html displaying a hello world message. Which of the following should the developer do?\",\"answers\":[{\"text\":\"Configure the metadata at the creation of the EC2 instance to run a script that will install the Apache webserver after the instance starts.\",\"isCorrect\":false},{\"text\":\"Configure the user data at the creation of the EC2 instance to run a script that will install and create the Apache webserver after the instance starts.\",\"isCorrect\":true},{\"text\":\"Connect to the instance via port 80. Run the commands that will install and create the Apache webserver.\",\"isCorrect\":false},{\"text\":\"Connect to the instance via port 22. Run the commands that will install and create the Apache webserver.\",\"isCorrect\":false}],\"explanation\":\"$44\"},{\"question\":\"A Ruby developer is looking to offload some of the processing on his application to the AWS cloud without managing any servers. The submodules must be written in Ruby, which mainly invokes API calls to an external web service. The response from the API call is parsed and stored in a MongoDB database. What should he do to develop the Lambda function in his preferred programming language?\",\"answers\":[{\"text\":\"Create a Lambda function on Ruby with a custom runtime and use the AWS SDK for Ruby.\",\"isCorrect\":false},{\"text\":\"Create a Lambda function using the AWS SDK for Ruby.\",\"isCorrect\":false},{\"text\":\"Create a Lambda function with a supported runtime version for Ruby.\",\"isCorrect\":true},{\"text\":\"Create a Lambda function with a custom runtime to use Ruby. Then include the runtime in the function's deployment package. Migrate it to a layer that you manage independently from the function.\",\"isCorrect\":false}],\"explanation\":\"$45\"},{\"question\":\"A developer is hosting a static website from an S3 bucket. The website makes requests to an API Gateway endpoint integrated with a Lambda function (non-proxy). The developer noticed that the requests were failing. Upon debugging, he found a: \\\"No 'Access-Control-Allow-Origin' header is present on the requested resource\\\" error message. What should the developer do to resolve this issue?\",\"answers\":[{\"text\":\"In the API Gateway Console, enable cross-origin resource sharing (CORS) for the method in the specified resource.\",\"isCorrect\":true},{\"text\":\"Set the value of the Access-Control-Max-Age header to 0.\",\"isCorrect\":false},{\"text\":\"Set the value of the Access-Control-Allow-Credentials header to true.\",\"isCorrect\":false},{\"text\":\"In the Amazon S3 Console, enable cross-origin resource sharing (CORS) on the S3 bucket where the website is hosted.\",\"isCorrect\":false}],\"explanation\":\"$46\"},{\"question\":\"A Lambda function is being developed to process a 50MB gzip-compressed file that will be uploaded to an S3 bucket on a daily basis. The function must have access to a storage location where it can load and unzip the file. After processing, the file will be delivered to another S3 bucket. Which solution can the developer implement that requires the LEAST effort and cost?\",\"answers\":[{\"text\":\"Download the file to the /tmp directory. From there, consume and process the data before sending it to the S3 bucket.\",\"isCorrect\":true},{\"text\":\"Download the file to an Amazon Elastic Block Store (EBS) volume. From there, consume and process the data before sending it to the S3 bucket.\",\"isCorrect\":false},{\"text\":\"Download the file to a separate S3 bucket. From there, consume and process the data before sending it to the S3 bucket that contains the logs.\",\"isCorrect\":false},{\"text\":\"Download the file to Amazon Elastic File System. From there, consume and process the data before sending it to the S3 bucket.\",\"isCorrect\":false}],\"explanation\":\"$47\"},{\"question\":\"An application is hosted in the us-east-1 region. The app needs to be recreated on the us-east-2, ap-northeast-1, and ap-southeast-1 region using the same Amazon Machine Image (AMI). As the developer, you have to use AWS CloudFormation to rebuild the application using a template. Which of the following actions is the most suitable way to configure the CloudFormation template for the scenario?\",\"answers\":[{\"text\":\"Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Parameters section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Ref function to retrieve the desired Image Id from the region key.\",\"isCorrect\":false},{\"text\":\"Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::FindInMap function to retrieve the desired Image Id from the region key.\",\"isCorrect\":true},{\"text\":\"Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::ImportValue function to retrieve the desired Image Id from the region key.\",\"isCorrect\":false},{\"text\":\"Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::GetAtt function to retrieve the desired Image Id from the region key.\",\"isCorrect\":false}],\"explanation\":\"$48\"},{\"question\":\"A developer is managing an Application Load Balancer that targets a Lambda function. The developer needs to obtain all values of identical query parameters key that is supplied in a request. How can the developer implement this?\",\"answers\":[{\"text\":\"Replace the Application Load Balancer with a Network Load Balancer and enable multi-value headers.\",\"isCorrect\":false},{\"text\":\"Enable the multi-value headers on the Application Load Balancer.\",\"isCorrect\":true},{\"text\":\"Decode the URL encoded query string values in the Lambda function.\",\"isCorrect\":false},{\"text\":\"Set a custom HTTP response header in the Lambda function.\",\"isCorrect\":false}],\"explanation\":\"$49\"},{\"question\":\"A startup plans to use Amazon Cognito User Pools to easily manage their users' sign-up and sign-in workflows to an application. To save time from designing the User Interface (UI) for the login page, the development team has decided to use Cognito's built-in UI. However, the product manager finds the UI bland and instructed the developer to include the product logo on the web page. How should the developer meet the above requirements?\",\"answers\":[{\"text\":\"Upload the logo to an S3 bucket and point the S3 endpoint on the custom login page.\",\"isCorrect\":false},{\"text\":\"Create a login page with the product logo and upload it to an S3 bucket. Point the S3 endpoint in the Cognito app settings.\",\"isCorrect\":false},{\"text\":\"Create a login page with the product logo and upload it to Amazon Cognito.\",\"isCorrect\":false},{\"text\":\"Upload the logo to the Amazon Cognito app settings and use that logo on the custom login page.\",\"isCorrect\":true}],\"explanation\":\"$4a\"},{\"question\":\"A development team needs to deploy an application revision into three environments: Test, Staging, and Production. The application should be deployed into the Test environment first, then Staging, and then Production. Which approach will conveniently allow the team to deploy the application into different environments?\",\"answers\":[{\"text\":\"Create multiple deployment groups for each environment using AWS CodeDeploy.\",\"isCorrect\":true},{\"text\":\"Create separate S3 buckets for each environment to deploy the application.\",\"isCorrect\":false},{\"text\":\"Create separate CloudFormation templates for each environment to deploy the application.\",\"isCorrect\":false},{\"text\":\"Create, configure, and deploy multiple application projects for each environment using CodeBuild.\",\"isCorrect\":false}],\"explanation\":\"$4b\"},{\"question\":\"A developer is building a serverless application that will send out a newsletter to customers using AWS Lambda. The Lambda function will be invoked at a 7-day interval. Which method will provide an automated and serverless approach to trigger the function?\",\"answers\":[{\"text\":\"Configure a scheduled Amazon EventBridge (Amazon CloudWatch Events) that triggers every week to invoke the Lambda function.\",\"isCorrect\":true},{\"text\":\"Run a cron job in an Amazon EC2 instance that will trigger the Lambda function every week.\",\"isCorrect\":false},{\"text\":\"Implement a task timer using Step Functions that will send a newsletter every week.\",\"isCorrect\":false},{\"text\":\"Add an environment variable named DAYS for the Lambda function and set its value to 7.\",\"isCorrect\":false}],\"explanation\":\"$4c\"},{\"question\":\"A company plans to conduct an online survey to distinguish the users who bought its product from those who didn't. The survey will be processed by Step Functions which comprises four states that will manage the application logic and error handling of the state machine. It is required to aggregate all the data that passes through the nodes if the process fails. What should the company do to meet the requirements?\",\"answers\":[{\"text\":\"Include a Catch field in the state machine definition to capture the errors. Then, use ItemsPath to include each node's input data with its output.\",\"isCorrect\":false},{\"text\":\"Include a Catch field in the state machine definition to capture the error. Then, use ResultPath to include each node’s input data with its output.\",\"isCorrect\":true},{\"text\":\"Include a Parameters field in the state machine definition to capture the errors. Then, use ResultPath to include each node's input data with its output.\",\"isCorrect\":false},{\"text\":\"Include a Parameters field in the state machine definition to capture the errors. Then, use ItemsPath to include each node's input data with its output.\",\"isCorrect\":false}],\"explanation\":\"$4d\"},{\"question\":\"A developer is building a new feature for an application deployed on an EC2 instance in the N. Virginia region. A co-developer suggests to upload the code on Amazon S3 and use CodeDeploy to deploy the new version of the application. The deployment fails during the DownloadBundle deployment lifecycle event with the UnknownError: not opened for reading error. What is the possible cause of this?\",\"answers\":[{\"text\":\"The EC2 instance’s IAM profile does not have the permissions to access the application code in Amazon S3.\",\"isCorrect\":true},{\"text\":\"Versioning is not enabled on the Amazon S3 Bucket where the application code resides.\",\"isCorrect\":false},{\"text\":\"The DownloadBundle deployment lifecycle event is not supported in the N. Virginia region.\",\"isCorrect\":false},{\"text\":\"Wrong configuration of the DownloadBundle lifecycle event in the AppSec file.\",\"isCorrect\":false}],\"explanation\":\"$4e\"},{\"question\":\"A developer is building a serverless API composed of an API Gateway and several Lambda functions. All resources are defined using Cloud Development Kit (CDK) L2 constructs. The developer wants to test some of the Lambda functions in their local environment. Given that AWS SAM and AWS CDK are already configured locally, what combination of actions must the developer do? (Select TWO.)\",\"answers\":[{\"text\":\"Execute the sam local invoke command and specify the location of the synthesized CloudFormation template and identifier of each function.\",\"isCorrect\":true},{\"text\":\"Run the cdk bootstrap command to prepare the staging of CDK assets.\",\"isCorrect\":false},{\"text\":\"Run the cdk synth command and indicate the stack name of Lambda functions to be tested.\",\"isCorrect\":true},{\"text\":\"Execute the sam package and specify the S3 bucket where the Lambda code will be uploaded for local testing.\",\"isCorrect\":false},{\"text\":\"Execute the sam local start-lambda and specify the location of the synthesized CloudFormation template along with function identifiers.\",\"isCorrect\":false}],\"explanation\":\"$4f\"}]}}]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n7:null\n"])</script><script>self.__next_f.push([1,"8:null\nc:[[\"$\",\"title\",\"0\",{\"children\":\"v0 App\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Created with v0\"}],[\"$\",\"meta\",\"2\",{\"name\":\"generator\",\"content\":\"v0.app\"}]]\n"])</script></body></html>