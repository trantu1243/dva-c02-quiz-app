<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/dva-c02-quiz-app/_next/static/css/31d717072bdb6d85.css" data-precedence="next"/><link rel="stylesheet" href="/dva-c02-quiz-app/_next/static/css/14ea25f3945b7722.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/dva-c02-quiz-app/_next/static/chunks/webpack-b20bf3550849aea8.js"/><script src="/dva-c02-quiz-app/_next/static/chunks/4bd1b696-65f2dd90969a3b23.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/684-499f1a03f1824ea2.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/main-app-01726a8d9af88025.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/app/layout-7d8a5f63f536cdcf.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/261-2d9b76ccba401937.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/app/quiz/%5Bid%5D/page-d7cb26530e4508b3.js" async=""></script><title>v0 App</title><meta name="description" content="Created with v0"/><meta name="generator" content="v0.app"/><script src="/dva-c02-quiz-app/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans __variable_1f39b6 __variable_c20681"><div class="min-h-screen bg-gradient-to-br from-background via-background to-primary/5 flex items-center justify-center"><div class="text-center"><div class="w-16 h-16 border-4 border-primary border-t-transparent rounded-full animate-spin mx-auto mb-4"></div><p class="text-muted-foreground">Loading quiz...</p></div></div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><script src="/dva-c02-quiz-app/_next/static/chunks/webpack-b20bf3550849aea8.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7555,[],\"\"]\n3:I[1295,[],\"\"]\n4:I[9742,[\"177\",\"static/chunks/app/layout-7d8a5f63f536cdcf.js\"],\"Analytics\"]\n6:I[9665,[],\"OutletBoundary\"]\n9:I[9665,[],\"ViewportBoundary\"]\nb:I[9665,[],\"MetadataBoundary\"]\nd:I[6614,[],\"\"]\n:HL[\"/dva-c02-quiz-app/_next/static/css/31d717072bdb6d85.css\",\"style\"]\n:HL[\"/dva-c02-quiz-app/_next/static/css/14ea25f3945b7722.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"_vngiHLbu7MHbT5WeOEMF\",\"p\":\"/dva-c02-quiz-app\",\"c\":[\"\",\"quiz\",\"aws-developer-9\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"quiz\",{\"children\":[[\"id\",\"aws-developer-9\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/dva-c02-quiz-app/_next/static/css/31d717072bdb6d85.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/dva-c02-quiz-app/_next/static/css/14ea25f3945b7722.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"font-sans __variable_1f39b6 __variable_c20681\",\"children\":[[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"$L4\",null,{}]]}]}]]}],{\"children\":[\"quiz\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"id\",\"aws-developer-9\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",\"$undefined\",null,[\"$\",\"$L6\",null,{\"children\":[\"$L7\",\"$L8\",null]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"sP4MAywB71wDBt2SuY2PK\",{\"children\":[[\"$\",\"$L9\",null,{\"children\":\"$La\"}],null]}],[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$d\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"e:I[1184,[\"261\",\"static/chunks/261-2d9b76ccba401937.js\",\"200\",\"static/chunks/app/quiz/%5Bid%5D/page-d7cb26530e4508b3.js\"],\"default\"]\nf:Tbce,"])</script><script>self.__next_f.push([1,"Amazon Kinesis Data Streams is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. You can configure hundreds of thousands of data producers to continuously put data into a Kinesis data stream. Data will be available within milliseconds to your Amazon Kinesis applications, and those applications will receive data records in the order they were generated. The purpose of resharding in Amazon Kinesis Data Streams is to enable your stream to adapt to changes in the rate of data flow. You split shards to increase the capacity (and cost) of your stream. You merge shards to reduce the cost (and capacity) of your stream. One approach to resharding could be to split every shard in the stream—which would double the stream's capacity. However, this might provide more additional capacity than you actually need and therefore create unnecessary costs. You can also use metrics to determine which are your \"hot\" or \"cold\" shards, that is, shards that are receiving much more data, or much less data, than expected. You could then selectively split the hot shards to increase capacity for the hash keys that target those shards. Similarly, you could merge cold shards to make better use of their unused capacity. You can obtain some performance data for your stream from the Amazon CloudWatch metrics that Kinesis Data Streams publishes. However, you can also collect some of your own metrics for your streams. One approach would be to log the hash key values generated by the partition keys for your data records. Recall that you specify the partition key at the time that you add the record to the stream. Hence, the correct statements regarding Kinesis resharding are: - You can decrease the stream's capacity by merging shards - You can increase the stream's capacity by splitting shards The option that says: you have to merge the hot shards to increase the capacity of the stream is incorrect because a hot shard is the one that receives more data in the stream, which you should split rather than merge. The option that says: you have to split the cold shards to decrease the capacity of the stream is incorrect because a cold shard is the one that receives fewer data in the stream, which you should merge rather than split. The option that says: the data records that are flowing to the parent shards will be lost when you reshard is incorrect because the data records are actually re-routed to flow to the child shards based on the hash key values that the data-record partition keys map to. Hence, this is incorrect because the records are not lost. References: https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/ Kinesis Scaling, Resharding and Parallel Processing: https://tutorialsdojo.com/kinesis-scaling-resharding-and-parallel-processing/"])</script><script>self.__next_f.push([1,"10:Tc07,"])</script><script>self.__next_f.push([1,"Amazon SQS FIFO First-In-First-Out queues are designed to enhance messaging between applications when the order of operations and events is critical or where duplicates can't be tolerated. Amazon SQS FIFO queues follow exactly-once processing. It introduces a parameter called Message Deduplication ID, which is the token used for deduplication of sent messages. Suppose a message with a particular message deduplication ID is sent successfully. In that case, any messages sent with the same message deduplication ID are accepted successfully but aren't delivered during the 5-minute deduplication interval. SQS remembers the MessageDeduplicationId values it’s seen for at least five minutes, which means deduplication Ids can only reduce, not completely eliminate, the chances of duplication occurring. For example, if a producer was unable to receive an acknowledgment after sending a message due to a network issue and then regains connection after 10 minutes and attempts to resend the message, there is a risk of duplication occurring. In this scenario, you can lessen the chances of the Lambda function processing duplicate messages by storing data in an SQS FIFO queue. You may provide a MessageDeduplicationId value so SQS can distinguish one message from another. Optionally, you may enable ContentBasedDeduplication to let SQS create an SHA-256 hash based on the message body and use it as the value for MessageDeduplicationId. Hence, in this scenario, the correct answer is to: Add a MessageDeduplicationId parameter to the SendMessage API request. Refactoring the Lambda function to store the message's content and dropping the incoming messages with similar content within a 5-minute period is incorrect because Lambda functions do not share data amongst themselves during a scale-up event. Therefore, if a function is processing a message and another function handles the succeeding message, it would not be able to compare if it is indeed a duplicate or not. You have to configure the SQS FIFO queue to use a Message Deduplication ID in order to avoid having duplicate messages. Configuring the Amazon SQS queue to automatically drop a duplicate message whenever it arrives within the message's VisibilityTimeout is incorrect because the visibility timeout is primarily used to prevent other consumers from processing the message again and not for detecting duplicate messages. Using an Amazon SQS Standard queue instead of a FIFO queue to avoid any duplicate messages is incorrect because using standard queues will actually introduce duplicate messages. Take note that FIFO queues help you avoid sending duplicates and not the Standard-type queue. References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html https://docs.amazonaws.cn/en_us/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html#FIFO-queues-exactly-once-processing https://aws.amazon.com/blogs/developer/how-the-amazon-sqs-fifo-api-works/ Check out this Amazon SQS Cheat Sheet: https://tutorialsdojo.com/amazon-sqs/"])</script><script>self.__next_f.push([1,"11:T779,The GetSessionToken API returns a set of temporary credentials for an AWS account or IAM user. The credentials consist of an access key ID, a secret access key, and a security token. Typically, you use GetSessionToken if you want to use MFA to protect programmatic calls to specific AWS API operations like Amazon EC2 StopInstances. MFA-enabled IAM users would need to call GetSessionToken and submit an MFA code that is associated with their MFA device. Using the temporary security credentials that are returned from the call, IAM users can then make programmatic calls to API operations that require MFA authentication. If you do not supply a correct MFA code, then the API returns an access denied error. Thus, the correct answer is to use the GetSessionToken API in this scenario. AssumeRoleWithWebIdentity is incorrect because this only returns a set of temporary security credentials for federated users who are authenticated through public identity providers such as Amazon, Facebook, Google, or OpenID, which were not mentioned in the scenario. This API does not support MFA. AssumeRoleWithSAML is incorrect because this just returns a set of temporary security credentials for users who have been authenticated via a SAML authentication response. This operation provides a mechanism for tying an enterprise identity store or directory to role-based AWS access without user-specific credentials or configuration. This API does not support MFA. GetFederationToken is incorrect because it does not support MFA. The appropriate STS API that the developer should use is GetSessionToken. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html#stsapi_comparison https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html Check out this AWS Identity \u0026 Access Management (IAM) Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/12:T9bf,"])</script><script>self.__next_f.push([1,"AWS SAM uses AWS CloudFormation as the underlying deployment mechanism. You can deploy your application by using AWS SAM command line interface (CLI) commands. You can also use other AWS services that integrate with AWS SAM to automate your deployments. When deploying a SAM application, it's vital to ensure that all components of the application are properly packaged and that all resources are provisioned correctly in the AWS environment. The sam build command serves this purpose by resolving any dependencies the application might have and constructing deployment artifacts for all functions and layers specified in the SAM template. This is especially important when the SAM template references local file paths, such as CodeUri pointing to local Lambda function codes. Once the application has been successfully built, the next step is to deploy it. The sam deploy command allows the application to be deployed using AWS CloudFormation, ensuring that all resources defined in the SAM template are provisioned and configured correctly in the target environment. Hence, the correct answers are: - Execute sam build to resolve dependencies and construct deployment artifacts for all functions and layers in the SAM template. - Use the sam deploy command to deploy the application with a specified CloudFormation stack. The option that says: Run sam init to initialize a new SAM project is incorrect because this command is simply used when creating a new SAM project, not to deploy an existing one. The option that says: Execute sam publish to make the application available in the AWS Serverless Application Repository is incorrect. This SAM CLI is used to publish applications to the AWS Serverless Application Repository, which is not a mandatory step for deploying SAM applications. The option that says: Use the sam sync command to synchronize the local changes to the application in AWS is incorrect. This command is typically used for quick syncing of local changes to AWS and is more suitable for rapid development testing. In a Production setting, a full deployment using sam deploy is more appropriate to ensure that all configurations and resources are correctly provisioned. References: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html https://docs.aws.amazon.com/serverlessrepo/latest/devguide/what-is-serverlessrepo.html Check out this AWS SAM Cheat Sheet: https://tutorialsdojo.com/aws-serverless-application-model-sam/"])</script><script>self.__next_f.push([1,"13:T8b4,"])</script><script>self.__next_f.push([1,"For commands that can return a large list of items, the AWS Command Line Interface (AWS CLI) adds three options that you can use to control the number of items included in the output when the AWS CLI calls a service's API to populate the list. By default, the AWS CLI uses a page size of 1000 and retrieves all available items. If you see issues when running list commands on a large number of resources, the default page size of 1000 might be too high. This can cause calls to AWS services to exceed the maximum allowed time and generate a \"timed out\" error. You can use the --page-size option to specify that the AWS CLI request a smaller number of items from each call to the AWS service. The CLI still retrieves the full list, but performs a larger number of service API calls in the background and retrieves a smaller number of items with each call. This gives the individual calls a better chance of succeeding without a timeout. To include fewer items at a time in the AWS CLI output, use the --max-items option. The AWS CLI still handles pagination with the service as described above, but prints out only the number of items at a time that you specify. If the number of items output is fewer than the total number of items returned by the underlying API calls, the output includes a NextToken that you can pass to a subsequent command to retrieve the next set of items. Hence, the correct ones that you should include in the AWS CLI command are the --page-size and --max-items parameters. The --size-only parameter is incorrect because this just accepts a boolean value and is typically used along with \"s3 sync\" command. It makes the size of each key the only criteria to use to decide whether to sync from source to destination. The --exclude parameter is incorrect because it simply makes Amazon S3 exclude all files or objects that match a specified pattern from the result of the command. The --summary parameter is incorrect because this only displays the summary information (number of objects, total size) of objects returned from an \"s3 ls\" command. References: https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html https://docs.aws.amazon.com/cli/latest/reference/s3/index.html#cli-aws-s3"])</script><script>self.__next_f.push([1,"14:T842,"])</script><script>self.__next_f.push([1,"In ElasticBeanstalk, you can choose from a variety of deployment methods: -All at once – Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time while the deployment occurs. This is the method that provides the least amount of time for deployment. -Rolling – Deploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch. -Rolling with additional batch – Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during the deployment process. -Immutable – Deploy the new version to a fresh group of instances by performing an immutable update. -Blue/Green - Deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly. Refer to the table below for the characteristics of each deployment method as well as the amount of time it takes to do the deployment, as seen in the Deploy Time column: Hence, the correct answer in this scenario is All at once. Rolling is incorrect because this will deploy the new version in batches only to existing instances, without provisioning new resources. Immutable is incorrect because this will deploy the new version to a fresh group of instances by performing an immutable update. Considering the time to deploy additional instances and installing the new version, this option does not provide the least amount of deployment time. Rolling with additional batch is incorrect because this just deploys the new version in batches. It first launches a new batch of instances to ensure full capacity during the deployment process then proceeds in deploying the new version. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/"])</script><script>self.__next_f.push([1,"15:T9c2,"])</script><script>self.__next_f.push([1,"AWS Lambda supports synchronous and asynchronous invocation of a Lambda function. You can control the invocation type only when you invoke a Lambda function (referred to as on-demand invocation). The following examples illustrate on-demand invocations: -Your custom application invokes a Lambda function. -You manually invoke a Lambda function (for example, using the AWS CLI) for testing purposes. In both cases, you invoke your Lambda function using the Invoke operation, and you can specify the invocation type as synchronous or asynchronous. When you use AWS services as a trigger, the invocation type is predetermined for each service. You have no control over the invocation type that these event sources use when they invoke your Lambda function. In the Invoke API, you have 3 options to choose from for the InvocationType: RequestResponse (default) - Invoke the function synchronously. Keep the connection open until the function returns a response or times out. The API response includes the function response and additional data. Event - Invoke the function asynchronously. Send events that fail multiple times to the function's dead-letter queue (if it's configured). The API response only includes a status code. DryRun - Validate parameter values and verify that the user or role has permission to invoke the function. Hence, the correct answer is the option that says: Use the Invoke API to call the Lambda function and set the invocation type request parameter to Event. The option that says: Use the InvokeAsync API to call the Lambda function and set the invocation type request parameter to RequestResponse is incorrect because the InvokeAsync API is already deprecated. In addition, using the RequestResponse type will invoke the Lambda function synchronously. The option that says: Use the Invoke API to call the Lambda function and set the invocation type request parameter to RequestResponse is incorrect because this is the default value of the invocation type that will invoke the Lambda function synchronously. The option that says: Use the InvokeAsync API to call the Lambda function and set the invocation type request parameter to Event is incorrect. Although it uses the correct invocation type, the InvokeAsync API that it uses is already deprecated. Reference: https://docs.aws.amazon.com/lambda/latest/dg/invocation-options.html https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"])</script><script>self.__next_f.push([1,"16:Tc91,"])</script><script>self.__next_f.push([1,"One read request unit represents one strongly consistent read request, or two eventually consistent read requests, for an item up to 4 KB in size. Transactional read requests require 2 read request units to perform one read for items up to 4 KB. If you need to read an item that is larger than 4 KB, DynamoDB needs additional read request units. The total number of read request units required depends on the item size, and whether you want an eventually consistent or strongly consistent read. For example, suppose that you create a table with 10 provisioned read capacity units. This allows you to perform 10 strongly consistent reads per second, or 20 eventually consistent reads per second, for items up to 4 KB. Reading an item larger than 4 KB consumes more read capacity units. For example, a strongly consistent read of an item that is 8 KB (4 KB × 2) consumes 2 read capacity units. An eventually consistent read on that same item consumes only 1 read capacity unit. Item sizes for reads are rounded up to the next 4 KB multiple. For example, reading a 3,500-byte item consumes the same throughput as reading a 4 KB item. To get the number of strong and eventual consistent read requests that your table can accommodate per second, you simply have to do the following steps: Step #1 Multiply the value of the provisioned RCU by 4 KB = 10 RCU x 4 KB = 40 Step #2 To get the number of strong consistency requests, just divide the result of step 1 by 4 KB = 40 / 4 KB = 10 strongly consistent read requests Step #3 To get the number of eventual consistency requests, just divide the result of step 1 by 2 KB =40 / 2 KB = 20 eventually consistent read requests Hence, the correct answer is that the table can handle 10 strongly consistent reads and 20 eventually consistent reads per second. 10 strongly consistent reads and 10 eventually consistent reads per second is incorrect. Although the former value is correct, the latter one is not. Take note that one strongly consistent read request is equivalent to two eventually consistent read request as these two consistency types are quite different from each other. 5 strongly consistent reads and 20 eventually consistent reads per second is incorrect. Although the latter value is correct, the former one is not. If the scenario says that it uses transaction read requests, then it is correct that it will provide 5 strongly consistent reads. However, it is explicitly mentioned to get both strong and eventual consistency. 20 strongly consistent reads and 10 eventually consistent reads per second is incorrect because it should be the other way around. The table can provide 10 strongly consistent reads and 20 eventually consistent reads per second. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ItemSizeCalculations.Reads Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ Calculating the Required Read and Write Capacity Unit for your DynamoDB Table: https://tutorialsdojo.com/calculating-the-required-read-and-write-capacity-unit-for-your-dynamodb-table/"])</script><script>self.__next_f.push([1,"17:T988,"])</script><script>self.__next_f.push([1,"A role specifies a set of permissions that you can use to access AWS resources. In that sense, it is similar to an IAM User. A principal (person or application) assumes a role to receive temporary permissions to carry out required tasks and interact with AWS resources. The role can be in your own account or any other AWS account. To assume a role, an application calls the AWS STS AssumeRole API operation and passes the ARN of the role to use. The operation creates a new session with temporary credentials. This session has the same permissions as the identity-based policies for that role. The given problem is an example of a scenario where cross-account access is required. For cross-account access, imagine that you own multiple accounts and need to access resources in a particular account. You could create long-term credentials in each account to access those resources. However, managing all those credentials and remembering which one can access which account can be time-consuming. Instead, you can create one set of long-term credentials in one account. Then use temporary security credentials to access all the other accounts by assuming roles in those accounts. Thus, the correct answer is AssumeRole. AssumeRoleWithWebIdentity is incorrect because this only returns a set of temporary security credentials for federated users who are authenticated through public identity providers such as Amazon, Facebook, Google, or OpenID, which were not mentioned in the scenario. The AssumeRole API is good for account access AssumeRoleWithSAML is incorrect because this just returns a set of temporary security credentials for users who have been authenticated via a SAML authentication response. This operation provides a mechanism for tying an enterprise identity store or directory to role-based AWS access without user-specific credentials or configuration. GetSessionToken is incorrect because this is primarily used to return a set of temporary credentials for an AWS account or IAM user only. References: https://aws.amazon.com/blogs/security/how-to-use-a-single-iam-user-to-easily-access-all-your-accounts-by-using-the-aws-cli/ https://aws.amazon.com/premiumsupport/knowledge-center/s3-instance-access-bucket/ https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html Check out this AWS Identity \u0026 Access Management (IAM) Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"])</script><script>self.__next_f.push([1,"18:T8db,"])</script><script>self.__next_f.push([1,"The content in the 'hooks' section of the AppSpec file varies, depending on the compute platform for your deployment. The 'hooks' section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts. The 'hooks' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment. An AWS Lambda hook is one Lambda function specified with a string on a new line after the name of the lifecycle event. Each hook is executed once per deployment. Following are descriptions of the hooks that are available for use in your AppSpec file. BeforeAllowTraffic – Use to run tasks before traffic is shifted to the deployed Lambda function version. AfterAllowTraffic – Use to run tasks after all traffic is shifted to the deployed Lambda function version. In a serverless Lambda function version deployment, event hooks run in the following order: ake note that Start, AllowTraffic, and End events in the deployment cannot be scripted, which is why they appear in gray in the above diagram. Hence, the correct answer is BeforeAllowTraffic. Start is incorrect because this deployment lifecycle event in Lambda cannot be scripted or used. In this scenario, the correct event that you should configure is the BeforeAllowTraffic event. BeforeInstall is incorrect because this event is only applicable for ECS, EC2 or On-Premises compute platforms and not for Lambda deployments. Install is incorrect because this uses the CodeDeploy agent to copy the revision files from the temporary location to the final destination folder of the EC2 or On-Premises server. This deployment lifecycle event is not available for Lambda as well as for ECS deployments. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps.html#deployment-steps-lambda Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/"])</script><script>self.__next_f.push([1,"19:Tdf6,"])</script><script>self.__next_f.push([1,"DynamoDB Streams provides a time-ordered sequence of item-level changes in any DynamoDB table. The changes are de-duplicated and stored for 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real time. The Kinesis Adapter is the recommended way to consume streams from DynamoDB for real-time processing. The DynamoDB Streams API is intentionally similar to that of Kinesis Streams, a service for real-time processing of streaming data at a massive scale. You can write applications for Kinesis Streams using the Kinesis Client Library (KCL). The KCL simplifies coding by providing useful abstractions above the low-level Kinesis Streams API. As a DynamoDB Streams user, you can leverage the design patterns found within the KCL to process DynamoDB Streams shards and stream records. To do this, you use the DynamoDB Streams Kinesis Adapter. The Kinesis Adapter implements the Kinesis Streams interface, so that the KCL can be used for consuming and processing records from DynamoDB Streams. When an item in the table is modified, StreamViewType determines what information is written to the stream for this table. Valid values for StreamViewType are: KEYS_ONLY - Only the key attributes of the modified item are written to the stream. NEW_IMAGE - The entire item, as it appears after it was modified, is written to the stream. OLD_IMAGE - The entire item, as it appeared before it was modified, is written to the stream. NEW_AND_OLD_IMAGES - Both the new and the old item images of the item are written to the stream. Hence, the correct answer is: Enable DynamoDB Streams and set the value of StreamViewType to NEW_IMAGE then use Kinesis Adapter in the application to consume streams from DynamoDB. The option that says: Enable DynamoDB Streams and set the value of StreamViewType to NEW_AND_OLD_IMAGE. Create a trigger in AWS Lambda to capture stream data and forward it to your application is incorrect. Using Lambda for real-time data analytics is not a suitable solution for this scenario since it reads records in batches. A more appropriate service to use is the Kinesis service. In addition, using the StreamViewType of NEW_AND_OLD_IMAGE is wrong since this will send both the old and the new values of the item. Remember that it is specifically mentioned in the scenario that only the new values should be tracked. The option that says: Enable DynamoDB Streams and set the value of StreamViewType to NEW_IMAGE. Create a trigger in AWS Lambda to capture stream data and forward it to your application is incorrect because just like what is mentioned above, it is better to use Kinesis instead of Lambda for the real-time data analytics application. The option that says: Enable DynamoDB Streams and set the value of StreamViewType to NEW_AND_OLD_IMAGE. Use Kinesis Adapter in the application to consume streams from DynamoDB is incorrect because this will send both the old and the new values of the item to the data analytics application. The correct StreamViewType to use here should be NEW_IMAGE. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.KCLAdapter.html https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_StreamSpecification.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ AWS Lambda Integration with Amazon DynamoDB Streams: https://tutorialsdojo.com/aws-lambda-integration-with-amazon-dynamodb-streams/"])</script><script>self.__next_f.push([1,"1a:T11ff,"])</script><script>self.__next_f.push([1,"Applications that run on an EC2 instance must include AWS credentials in their AWS API requests. You could have your developers store AWS credentials directly within the EC2 instance and allow applications in that instance to use those credentials. But developers would then have to manage the credentials and ensure that they securely pass the credentials to each instance and update each EC2 instance when it's time to rotate the credentials. That's a lot of additional work. Instead, you can and should use an IAM role to manage temporary credentials for applications that run on an EC2 instance. When you use a role, you don't have to distribute long-term credentials (such as a username and password or access keys) to an EC2 instance. Instead, the role supplies temporary permissions that applications can use when they make calls to other AWS resources. When you launch an EC2 instance, you specify an IAM role to associate with the instance. Applications that run on the instance can then use the role-supplied temporary credentials to sign API requests. Using roles to grant permissions to applications that run on EC2 instances requires a bit of extra configuration. An application running on an EC2 instance is abstracted from AWS by the virtualized operating system. Because of this extra separation, an additional step is needed to assign an AWS role and its associated permissions to an EC2 instance and make them available to its applications. This extra step is the creation of an instance profile that is attached to the instance. The instance profile contains the role and can provide the role's temporary credentials to an application that runs on the instance. Those temporary credentials can then be used in the application's API calls to access resources and to limit access to only those resources that the role specifies. Note that only one role can be assigned to an EC2 instance at a time, and all applications on the instance share the same role and permissions. Using roles in this way has several benefits. Because role credentials are temporary and rotated automatically, you don't have to manage credentials, and you don't have to worry about long-term security risks. In addition, if you use a single role for multiple instances, you can make a change to that one role and the change is propagated automatically to all the instances. If you use the IAM console, the instance profile is managed for you and is mostly transparent to you. However, if you use the AWS CLI or API to create and manage the role and EC2 instance, then you must create the instance profile and assign the role to it as separate steps. Then, when you launch the instance, you must specify the instance profile name instead of the role name. In this scenario, the instance has both an attached IAM Role and AWS CLI. Although the Instance Profile role has been updated to only access the development environment, the application running in the instance might still use the access credentials or the old IAM Role that is attached in the AWS CLI. Take note that you have to configure both of your Instance Profile and AWS CLI in this scenario. Hence, the correct answer is that the application is still using the IAM role that is configured for the AWS CLI Key. The option that says: the new IAM Role has an attached inline policy is incorrect because an inline policy is just a policy that's embedded in a principal entity (a user, group, or role). This is irrelevant in this scenario as the main issue here is the profile/IAM Role that still exists and used, by the AWS CLI. The option that says: due to eventual consistency, you must wait 24 hours for the change to appear across all of AWS is incorrect because although there is eventual consistency in EC2, there is no exact time limit for the changes to be reflected, which is contrary to what this option is saying. Moreover, the existing AWS CLI profile would be a more compelling root cause in this scenario rather than what this option suggests. The option that says: the instance profile role of a running EC2 instance is static and can't be replaced at all is incorrect because the instance profile role can be changed anytime. You can remove the existing role and then add a different role to an instance profile. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html Check out this AWS Identity \u0026 Access Management (IAM) Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"])</script><script>self.__next_f.push([1,"1b:Tbb7,"])</script><script>self.__next_f.push([1,"CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can use CodeDeploy. CodeDeploy supports the following deployment configurations: -In-place (for EC2/On-premises) - the application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. -Canary (for Lambda/ECS) - traffic is shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function or ECS task set in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment. -Linear (for Lambda/ECS) - traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment. -All-at-once (for Lambda/ECS) - all traffic is shifted from the original Lambda function or ECS task set to the updated function or task set all at once. In a Linear deployment configuration, the traffic will be shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment. Hence, the is the correct answer is: Deploy the functions using a Linear deployment configuration. The option that says: Deploy the functions using a Canary deployment configuration is incorrect because this will cause the traffic to be shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment. The option that says: Deploy the functions using an All-at-once deployment configuration is incorrect because, with this deployment configuration, the traffic is shifted from the original Lambda function to the updated Lambda function version all at once. The option that says: Deploy the functions using an Immutable deployment configuration is incorrect because this is only applicable in Elastic Beanstalk and not to Lambda. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/"])</script><script>self.__next_f.push([1,"1c:Tf75,"])</script><script>self.__next_f.push([1,"Applications that run on an EC2 instance must include AWS credentials in their AWS API requests. You could have your developers store AWS credentials directly within the EC2 instance and allow applications in that instance to use those credentials. But developers would then have to manage the credentials and ensure that they securely pass the credentials to each instance and update each EC2 instance when it's time to rotate the credentials. That's a lot of additional work. Instead, you can and should use an IAM role to manage temporary credentials for applications that run on an EC2 instance. When you use a role, you don't have to distribute long-term credentials (such as a username and password or access keys) to an EC2 instance. Instead, the role supplies temporary permissions that applications can use when they make calls to other AWS resources. When you launch an EC2 instance, you specify an IAM role to associate with the instance. Applications that run on the instance can then use the role-supplied temporary credentials to sign API requests. Using roles to grant permissions to applications that run on EC2 instances requires a bit of extra configuration. An application running on an EC2 instance is abstracted from AWS by the virtualized operating system. Because of this extra separation, an additional step is needed to assign an AWS role and its associated permissions to an EC2 instance and make them available to its applications. This extra step is the creation of an instance profile that is attached to the instance. The instance profile contains the role and can provide the role's temporary credentials to an application that runs on the instance. Those temporary credentials can then be used in the application's API calls to access resources and to limit access to only those resources that the role specifies. Note that only one role can be assigned to an EC2 instance at a time, and all applications on the instance share the same role and permissions. Using roles in this way has several benefits. Because role credentials are temporary and rotated automatically, you don't have to manage credentials, and you don't have to worry about long-term security risks. In addition, if you use a single role for multiple instances, you can make a change to that one role and the change is propagated automatically to all the instances. Hence, using an IAM Role to grant the application the necessary permissions to upload data to S3 is the correct answer for this scenario as this provides the safest way to integrate your application hosted in EC2 and S3. Storing the access keys in the instance and then using the AWS SDK to upload the results to S3 is incorrect because this will expose the AWS access credentials to all users who have access to the EC2 instance. Since this option entails a security risk, this is incorrect as is not the safest method. Installing the AWS CLI then using it to upload the results to S3 is incorrect. Although this option is valid, this method also presents a security risk just as shown above. By default, an AWS CLI requires you to store the AWS access keys in your instance which will be used in executing the commands. Hence, this option is incorrect. Using an IAM Inline Policy to grant the application the necessary permissions to upload data to S3 is incorrect because inline policies are useful if you want to maintain a strict one-to-one relationship between a policy and the principal entity that it's applied to. This option doesn't provide a secure way of allowing the application that is hosted in EC2 to upload data to an S3 bucket. You should use an IAM Role instead. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html Check out this AWS Identity \u0026 Access Management (IAM) Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"])</script><script>self.__next_f.push([1,"1d:Tf89,"])</script><script>self.__next_f.push([1,"CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can use CodeDeploy. CodeDeploy provides two deployment type options: In-place deployment: The application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete. Only deployments that use the EC2/On-Premises compute platform can use in-place deployments. AWS Lambda compute platform deployments cannot use an in-place deployment type. Blue/green deployment: The behavior of your deployment depends on which compute platform you use: - Blue/green on an EC2/On-Premises compute platform: The instances in a deployment group (the original environment) are replaced by a different set of instances (the replacement environment). If you use an EC2/On-Premises compute platform, be aware that blue/green deployments work with Amazon EC2 instances only. - Blue/green on an AWS Lambda compute platform: Traffic is shifted from your current serverless environment to one with your updated Lambda function versions. You can specify Lambda functions that perform validation tests and choose the way in which the traffic shift occurs. All AWS Lambda compute platform deployments are blue/green deployments. For this reason, you do not need to specify a deployment type. - Blue/green on an Amazon ECS compute platform: Traffic is shifted from the task set with the original version of a containerized application in an Amazon ECS service to a replacement task set in the same service. The protocol and port of a specified load balancer listener are used to reroute production traffic. During deployment, a test listener can be used to serve traffic to the replacement task set while validation tests are run. The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The CodeDeploy agent communicates outbound using HTTPS over port 443. It is also important to note that the CodeDeploy agent is required only if you deploy to an EC2/On-Premises compute platform. The agent is not required for deployments that use the Amazon ECS or AWS Lambda compute platform. Therefore, the valid considerations in CodeDeploy in this scenario are: - AWS Lambda compute platform deployments cannot use an in-place deployment type. - CodeDeploy can deploy applications to both your EC2 instances as well as your on-premises servers. The option that says: CodeDeploy can deploy applications to EC2, AWS Lambda, and Amazon ECS only is incorrect because it can also deploy to your on-premises servers. The option that says: The CodeDeploy agent communicates using HTTP over port 80 is incorrect because it is actually using HTTPS over port 443 and not HTTP. The option that says: You have to install and use the CodeDeploy agent installed on your EC2 instances and ECS cluster is incorrect. Although this statement is true for EC2 instances, it is wrong for the latter as the CodeDeploy agent is not required for deployments that use the Amazon ECS or AWS Lambda compute platform. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/ Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/"])</script><script>self.__next_f.push([1,"1e:Tb77,"])</script><script>self.__next_f.push([1,"To create a Lambda function, you first create a Lambda function deployment package, a .zip or .jar file consisting of your code and any dependencies. When creating the zip, include only the code and its dependencies, not the containing folder. You will then need to set the appropriate security permissions for the zip package. If you are using a CloudFormation template, you can configure the AWS::Lambda::Function resource which creates a Lambda function. To create a function, you need a deployment package and an execution role. The deployment package contains your function code. The execution role grants the function permission to use AWS services, such as Amazon CloudWatch Logs for log streaming and AWS X-Ray for request tracing. Under the AWS::Lambda::Function resource, you can use the Code property which contains the deployment package for a Lambda function. For all runtimes, you can specify the location of an object in Amazon S3. For Node.js and Python functions, you can specify the function code inline in the template. Changes to a deployment package in Amazon S3 are not detected automatically during stack updates. To update the function code, change the object key or version in the template. Hence, including your function source inline in the ZipFile parameter of the AWS::Lambda::Function resource in the CloudFormation template is the easiest way to deploy the Lambda function to AWS. Uploading the code in S3 then specifying the S3Key and S3Bucket parameters under the AWS::Lambda::Function resource in the CloudFormation template is incorrect. Although this is a valid deployment step, you still have to upload the code in S3 instead of just including the function source inline in the ZipFile parameter. Take note that the scenario explicitly mentions that you have to pick the easiest way. Including your function source inline in the Code parameter of the AWS::Lambda::Function resource in the CloudFormation template is incorrect because you should use the ZipFile parameter instead. Take note that the Code property is the parent property of the ZipFile parameter. Uploading the code in S3 as a ZIP file then specifying the S3 path in the ZipFile parameter of the AWS::Lambda::Function resource in the CloudFormation template is incorrect because contrary to its name, the ZipFile parameter directly accepts the source code of your Lambda function and not an actual zip file. If you include your function source inline with this parameter, AWS CloudFormation places it in a file named index and zips it to create a deployment package. This is the reason why it is called the \"ZipFile\" parameter. References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-function.html Check out this AWS CloudFormation Cheat Sheet: https://tutorialsdojo.com/aws-cloudformation/"])</script><script>self.__next_f.push([1,"1f:T9f4,"])</script><script>self.__next_f.push([1,"AWS X-Ray receives data from services as segments. X-Ray then groups segments that have a common request into traces. X-Ray processes the traces to generate a service graph that provides a visual representation of your application. The compute resources running your application logic send data about their work as segments. A segment provides the resource's name, details about the request, and details about the work done. For example, when an HTTP request reaches your application, it can record the following data about: The host – hostname, alias or IP address The request – method, client address, path, user agent The response – status, content The work done – start and end times, subsegments Issues that occur – errors, faults and exceptions, including automatic capture of exception stacks. The X-Ray SDK gathers information from request and response headers, the code in your application, and metadata about the AWS resources on which it runs. You choose the data to collect by modifying your application configuration or code to instrument incoming requests, downstream requests, and AWS SDK clients. If a load balancer or other intermediary forwards a request to your application, X-Ray takes the client IP from the X-Forwarded-For header in the request instead of from the source IP in the IP packet. The client IP that is recorded for a forwarded request can be forged, so it should not be trusted. Hence, the correct answer in this scenario is that, AWS X-Ray will fetch the client IP address from the X-Forwarded-For header of the request. The option that says: from the X-Forwarded-Host header of the request is incorrect because this header is primarily used in identifying the original host where the request originated from and not the IP address. The option that says: from the ipAddress query parameter of the request if it exists is incorrect because query parameters are primarily used in the application layer and not for the network layer. Take note that AWS X-Ray uses the X-Forwarded-For header of the request and not any query parameter. The option that says: from the source IP of the IP packet is incorrect because AWS X-Ray uses the X-Forwarded-For header of the request and not the source IP of the IP packet since the application is using an Application Load Balancer. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html https://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/"])</script><script>self.__next_f.push([1,"20:T99e,"])</script><script>self.__next_f.push([1,"You can integrate an API method in your API Gateway with a custom HTTP endpoint of your application in two ways: - HTTP proxy integration - HTTP custom integration In your API Gateway console, you can define the type of HTTP integration of your resource by checking, or not checking, the \"Configure as proxy resource\" checkbox. For example, this API Resource configuration is a type of HTTP Proxy integration since the appropriate checkbox is ticked: With proxy integration, the setup is simple. You only need to set the HTTP method and the HTTP endpoint URI, according to the backend requirements, if you are not concerned with content encoding or caching. With custom integration, setup is more involved. In addition to the proxy integration setup steps, you need to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. API Gateway supports the following endpoint ports: 80, 443 and 1024-65535. Programmatically, you choose an integration type by setting the type property on the Integration resource. For the Lambda proxy integration, the value is AWS_PROXY. For the Lambda custom integration and all other AWS integrations, it is AWS. For the HTTP proxy integration and HTTP integration, the value is HTTP_PROXY and HTTP, respectively. For the mock integration, the type value is MOCK. Since the integration type that is being described in the scenario fits the definition of an HTTP proxy integration, the correct answer in this scenario is to use the HTTP_PROXY integration type. AWS is incorrect because this type is only used for Lambda custom integration. Take note that the scenario uses an application hosted in EC2 and not in Lambda. AWS_PROXY is incorrect because this type is primarily used for Lambda proxy integration. The scenario didn't mention that it uses a serverless application or Lambda. HTTP is incorrect because this type is only used for HTTP custom integration where you need to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/setup-http-integrations.html https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"])</script><script>self.__next_f.push([1,"21:T702,Lambda counts a request each time it starts executing in response to an event notification or invocation call, including test invokes from the console. You are charged based on the total number of requests processed across all of your Lambda functions. Duration is calculated from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 1ms. The price depends on the amount of memory you allocate to your function. In the AWS Lambda resource model, you choose the amount of memory you want for your function and are allocated proportional CPU power and other resources. An increase in memory size triggers an equivalent increase in CPU available to your function. Hence, the correct answer is: Increase the allocated memory of the function. The option that says: Configure the function to use unreserved account concurrency is incorrect because this configuration is primarily used for managing the number of simultaneous executions of your function as well as the capacity reservations for that concurrency level. The option that says: Increase the concurrent execution limit of the function is incorrect because this will just limit the number of simultaneous executions of your function and not increase the allocated CPU. The option that says: Use Lambda@Edge is incorrect because this is actually a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency. This will not increase the CPU of your function hence, this option does not meet the requirement. References: https://docs.aws.amazon.com/lambda/latest/dg/configuration-function-common.html https://aws.amazon.com/lambda/pricing/ Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/22:T7d6,X-Ray compiles and processes segment documents to generate queryable trace summaries and full traces that you can access by using the GetTraceSummaries and BatchGetTraces APIs, respectively. In addition to the segments and subsegments tha"])</script><script>self.__next_f.push([1,"t you send to X-Ray, the service uses information in subsegments to generate inferred segments and adds them to the full trace. Inferred segments represent downstream services and resources in the service map. In this scenario, the developer should use the GetTraceSummaries API to get the list of trace IDs of the application and then retrieve the list of traces using BatchGetTraces API in order to develop the custom debug tool The option that says: Use the GetGroup API to get the list of trace IDs of the application and then retrieving the list of traces using BatchGetTraces API is incorrect because the GetGroup API just retrieves the group resource details. The option that says: Use the GetServiceGraph API to get the list of trace IDs of the application and then retrieving the list of traces using GetTraceSummaries API is incorrect because the GetServiceGraph API just shows which services process the incoming requests, including the downstream services that they call as a result. In addition, you have to use the BatchGetTraces API instead of the GetTraceSummaries API to retrieve the list of traces. The option that says: Use the BatchGetTraces API to get the list of trace IDs of the application and then retrieving the list of traces using GetTraceSummaries API is incorrect because it should be the other way around. You have to use the GetTraceSummaries API to get the list of trace IDs of the application and then use its result as an input parameter to retrieve the list of traces using the BatchGetTraces API. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html https://docs.aws.amazon.com/xray/latest/api/API_BatchGetTraces.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/23:Tcc9,"])</script><script>self.__next_f.push([1,"Databases employ locking mechanisms to ensure that data is always updated to the latest version and is concurrent. There are multiple types of locking strategies that benefit different use cases. Some of these are: - Optimistic Locking - Pessimistic Locking - Overly Optimistic Locking Optimistic locking is a strategy to ensure that the client-side item that you are updating (or deleting) is the same as the item in DynamoDB. If you use this strategy, then your database writes are protected from being overwritten by the writes of others — and vice-versa. With optimistic locking, each item has an attribute that acts as a version number. If you retrieve an item from a table, the application records the version number of that item. You can update the item, but only if the version number on the server side has not changed. If there is a version mismatch, it means that someone else has modified the item before you did; the update attempt fails, because you have a stale version of the item. If this happens, you simply try again by retrieving the item and then attempting to update it. Optimistic locking prevents you from accidentally overwriting changes that were made by others; it also prevents others from accidentally overwriting your changes. Since the application is already using the AWS SDK for Java, it can support optimistic locking by simply adding the @DynamoDBVersionAttribute annotation to the objects. In the mapping class for your table, you designate one property to store the version number, and mark it using this annotation. When you save an object, the corresponding item in the DynamoDB table will have an attribute that stores the version number. The DynamoDBMapper assigns a version number when you first save the object, and it automatically increments the version number each time you update the item. Your update or delete requests will succeed only if the client-side object version matches the corresponding version number of the item in the DynamoDB table. Hence, implementing optimistic locking with version number is the correct answer in this scenario. Implementing pessimistic locking with read locking is incorrect because this type of locking can interrupt user operations. This is an approach where an entity is locked in the database for the entire time that it is in application memory (often in the form of an object). This can prevent certain users from reading, updating, or deleting an entry depending on the lock type. Implementing pessimistic locking with write locking is incorrect because just as explained above, pessimistic locking will significantly affect the performance of your application. Although it will ensure that your data writes are not overwritten on the fly, this type of locking will not meet the performance requirements mentioned in the scenario. Implementing overly optimistic locking (OOL) is incorrect because this strategy is completely inappropriate for multi-user systems since it is used for systems that have only one user or operation performing changes at a single time. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBContext.VersionSupport.html"])</script><script>self.__next_f.push([1,"24:Te47,"])</script><script>self.__next_f.push([1,"The unit of scale for AWS Lambda is a concurrent execution. However, scaling indefinitely is not desirable in all scenarios. For example, you may want to control your concurrency for cost reasons or to regulate how long it takes you to process a batch of events, or to simply match it with a downstream resource. To assist with this, Lambda provides a concurrent execution limit control at both the account level and the function level. The concurrent executions refers to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the concurrent execution count will differ depending on whether or not your Lambda function is processing events from a poll-based event source. If you create a Lambda function to process events from event sources that aren't poll-based (for example, Lambda can process every event from other sources, like Amazon S3 or API Gateway), each published event is a unit of work, in parallel, up to your account limits. Therefore, the number of invocations these event sources make influences the concurrency. If you set the concurrent execution limit for a function, the value is deducted from the unreserved concurrency pool. For example, if your account's concurrent execution limit is 1000 and you have 10 functions, you can specify a limit on one function at 200 and another function at 100. The remaining 700 will be shared among the other 8 functions. AWS Lambda will keep the unreserved concurrency pool at a minimum of 100 concurrent executions, so that functions that do not have specific limits set can still process requests. So, in practice, if your total account limit is 1000, you are limited to allocating 900 to individual functions. The scenario mentioned that an Application Load Balancer is used to distribute the incoming traffic to the two Lambda functions as registered targets, just as shown below: By default, an AWS account's concurrent execution limit is 1000 which will be shared by all Lambda functions. In this scenario, it is highly likely that the first function has more provisioned concurrency than the other one. Hence, the correct answer in this scenario is: the concurrency execution limit provided to the first function is significantly higher than the second function. The option that says: the first function is using the unreserved account concurrency while the second function has been set with a concurrency execution limit of 800 is incorrect because this will actually cause the first function to throttle the requests instead of the second one. The option that says: the concurrency execution limit provided to the first function is less than the second function is incorrect because what is really happening is the other way around: the concurrency execution limit provided to the first function is significantly higher than the second function, which is why the latter is throttling the requests. The option that says: the first function is using the unreserved account concurrency while the second function has been set with a concurrency execution limit of 1000 is incorrect because, in the first place, you cannot set a concurrency execution limit of 1000 since the maximum that you can allocate per function is only 900. Take note if you allocate the maximum concurrency execution to the second function, the unreserved account concurrency will actually just have a value of 100. References: https://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html https://docs.aws.amazon.com/lambda/latest/dg/scaling.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"])</script><script>self.__next_f.push([1,"25:Te13,"])</script><script>self.__next_f.push([1,"DynamoDB supports two types of secondary indexes: - Global secondary index — an index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered \"global\" because queries on the index can span all of the data in the base table, across all partitions. - Local secondary index — an index that has the same partition key as the base table, but a different sort key. A local secondary index is \"local\" in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value. A local secondary index maintains an alternate sort key for a given partition key value. A local secondary index also contains a copy of some or all of the attributes from its base table; you specify which attributes are projected into the local secondary index when you create the table. The data in a local secondary index is organized by the same partition key as the base table, but with a different sort key. This lets you access data items efficiently across this different dimension. For greater query or scan flexibility, you can create up to five local secondary indexes per table. Suppose that an application needs to find all of the threads that have been posted within the last three months. Without a local secondary index, the application would have to Scan the entire Thread table and discard any posts that were not within the specified time frame. With a local secondary index, a Query operation could use LastPostDateTime as a sort key and find the data quickly. In the provided scenario, you can create a local secondary index named LastPostIndex to meet the requirements. Note that the partition key is the same as that of the Thread table, but the sort key is LastPostDateTime as shown in the diagram below: Hence, the most effective solution in this scenario is to: Add a local secondary index while creating the new Thread table. Use the Query operation to utilize the LastPostDateTime attribute as the sort key in order to find the data quickly. Configuring the application to Scan the entire Thread table and discarding any posts that were not within the specified time frame is incorrect. Although this option is valid, this solution will consume a large amount of provisioned read-throughput and will take a significant amount of time to complete. This is not a scalable solution, and the time it takes to fetch the data will continue to increase as the table grows. Configuring the application to Query the entire Thread table and discarding any posts that were not within the specified time frame is incorrect because using the Query operation is not sufficient to meet this requirement. You have to create a local secondary index when you create the table to narrow down the results and improve the performance of your application. Creating a global secondary index and using the Query operation to utilize the LastPostDateTime attribute as the sort key is incorrect because using a local secondary index is a more appropriate solution to be used in this scenario. Take note that in this scenario, it is still using the same partition key (ForumName), but with an alternate sort key (LastPostDateTime), which warrants the use of a local secondary index. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html Amazon DynamoDB Overview: https://youtu.be/3ZOyUNIeorU Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"26:Td34,"])</script><script>self.__next_f.push([1,"Elastic Beanstalk regularly releases new platform versions to update all Linux-based and Windows Server-based platforms. New platform versions provide updates to existing software components and support for new features and configuration options. You can use the Elastic Beanstalk console or the EB CLI to update your environment's platform version. Depending on the platform version you'd like to update to, Elastic Beanstalk recommends one of two methods for performing platform updates. Method 1 – Update your Environment's Platform Version - This is the recommended method when you're updating to the latest platform version, without a change in runtime, web server, or application server versions, and without a change in the major platform version. This is the most common and routine platform update. Method 2 – Perform a Blue/Green Deployment - This is the recommended method when you're updating to a different runtime, web server, or application server version or to a different major platform version. This is a good approach when you want to take advantage of new runtime capabilities or the latest Elastic Beanstalk functionality. Because AWS Elastic Beanstalk performs an in-place update when you update your application versions, your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment and then swap CNAMEs of the two environments to redirect traffic to the new version instantly. Blue/green deployments require that your environment runs independently of your production database if your application uses one. If your environment has an Amazon RDS DB instance attached to it, the data will not transfer over to your second environment and will be lost if you terminate the original environment. Hence, the correct answer is to perform a Blue/Green deployment to safely upgrade the application's runtime environment from Java 7 to Java 8. The option that says: Update the environment's platform version to Java 8 is incorrect because using this method is only recommended when you're updating to the latest platform version without a change in the runtime environment. The option that says: Manually upgrade the Java runtime environment of the EC2 instances in the Elastic Beanstalk environment is incorrect. Although this method will work, this entails a lot of configuration to implement compared with just performing a blue/green deployment. In addition, this method may introduce operational risk because the environment may go down while the developer is doing the updates manually. The option that says: Perform a Traffic splitting deployment is incorrect because the scenario requires all user traffic to be immediately directed towards the new version. Take note that in Traffic splitting, updates are released incrementally to a subset of users. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.platform.upgrade.html#using-features.platform.upgrade.bluegreen https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/ Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"])</script><script>self.__next_f.push([1,"27:T928,"])</script><script>self.__next_f.push([1,"For applications that need to read or write multiple items, DynamoDB provides the BatchGetItem and BatchWriteItem operations. Using these operations can reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the individual read or write operations in parallel. Your applications benefit from this parallelism without having to manage concurrency or threading. The batch operations are essentially wrappers around multiple read or write requests. For example, if a BatchGetItem request contains five items, DynamoDB performs five GetItem operations on your behalf. Similarly, if a BatchWriteItem request contains two put requests and four delete requests, DynamoDB performs two PutItem and four DeleteItem requests. Hence, the correct answer is to use DynamoDB's BatchGetItem and BatchWriteItem API operations. Using DynamoDB conditional writes is incorrect because conditional writes are only helpful in cases where multiple users attempt to modify the same item. This is because write operations will succeed only if the item attributes meet one or more expected conditions. Otherwise, it returns an error. This method does not decrease the time it takes to process the entries. Modifying your application to use multithreading is incorrect because although refactoring your application to concurrently send multiple requests to the table is a valid solution, this entails a lot of effort to refactor your code in order to support multithreading. Without proper locking mechanisms, threads can mistakenly introduce redundancies hence, using the DynamoDB's batch API operations is still a better solution. Deploying your application into a cluster of EC2 instances is incorrect because you will need a solution that can track which application is currently handling which item. Although it could work if executed properly, it is not the simplest to do among the choices given. Take note that the scenario explicitly asks for a solution that you can implement with minimal configuration. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"28:Tbc1,"])</script><script>self.__next_f.push([1,"In Lambda non-proxy (or custom) integration, you can specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. For an AWS service action, you have the AWS integration of the non-proxy type only. API Gateway also supports the mock integration, where API Gateway serves as an integration endpoint to respond to a method request. The Lambda custom integration is a special case of the AWS integration, where the integration endpoint corresponds to the function-invoking action of the Lambda service. The Lambda proxy integration type (AWS_PROXY) lets an API method be integrated with the Lambda function invocation action with a flexible, versatile, and streamlined integration setup. This integration relies on direct interactions between the client and the integrated Lambda function. With this type of integration, also known as the Lambda proxy integration, you do not set the integration request or the integration response. API Gateway passes the incoming request from the client as the input to the backend Lambda function. In this scenario, you have to enforce the use of a required courseType query string parameter in the /getcourses resource in API Gateway. In order to do this, you can configure the method request of your resource just as shown in the diagram above. Hence, the correct answer is to configure the method request of the resource. Configuring the integration request of the resource is incorrect. Although configuring the integration request may also be valid, the client traffic will hit the method request first before it goes to the integration request down to the underlying Lambda function. This is why you should configure the method request first so it won't be necessary to check the required parameters in the Lambda integration. In addition, the integration request does not have the capability to enforce a request to include certain query string parameter nor enable API caching, unlike the method request. Configuring the integration response of the resource is incorrect because you have to use the method request. Configuring the response, either the method-type or the integration-type, is irrelevant in this scenario. Configuring the method response of the resource is incorrect because you should configure the method request instead. Take note that the scenario explicitly mentioned about the required query parameter which needs to be present before the processing can proceed. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-method-settings-method-request.html#setup-method-request-model https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-simple-proxy-for-lambda-input-format Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"])</script><script>self.__next_f.push([1,"29:Tfa1,"])</script><script>self.__next_f.push([1,"The unit of scale for AWS Lambda is a concurrent execution. However, scaling indefinitely is not desirable in all scenarios. For example, you may want to control your concurrency for cost reasons or to regulate how long it takes you to process a batch of events, or to simply match it with a downstream resource. To assist with this, Lambda provides a concurrent execution limit control at both the account level and the function level. The concurrent executions refers to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the concurrent execution count will differ depending on whether or not your Lambda function is processing events from a poll-based event source. If you create a Lambda function to process events from event sources that aren't poll-based (for example, Lambda can process every event from other sources, like Amazon S3 or API Gateway), each published event is a unit of work, in parallel, up to your account limits. Therefore, the number of invocations these event sources make influences the concurrency. If you set the concurrent execution limit for a function, the value is deducted from the unreserved concurrency pool. For example, if your account's concurrent execution limit is 1000 and you have 10 functions, you can specify a limit on one function at 200 and another function at 100. The remaining 700 will be shared among the other 8 functions. AWS Lambda will keep the unreserved concurrency pool at a minimum of 100 concurrent executions, so that functions that do not have specific limits set can still process requests. So, in practice, if your total account limit is 1000, you are limited to allocating 900 to individual functions. By default, an AWS account's concurrent execution limit is 1000 which will be shared by all Lambda functions. In this scenario, it is highly likely that the first function has more provisioned concurrency than the other one. It is possible that the concurrency execution limit of the first function is set to a significantly high value (e.g. 900) and the second function is set to use the unreserved account concurrency which may only contain the last 100 units out of the AWS account's concurrent execution limit of 1000. Hence, the correct solutions in this scenario are: - Set the concurrency execution limit of both functions to 450 - Decrease the concurrency execution limit of the first function. Setting the concurrency execution limit of both functions to 500 is incorrect because by default, a newly created AWS account has a concurrent execution limit of only 1000. Take note that AWS Lambda will keep the unreserved concurrency pool at a minimum of 100 concurrent executions so that functions that do not have specific limits set can still process requests. Hence, you can only allocate a concurrent execution limit of 900 for a single Lambda function or 450 for two functions. Configuring the second function to use an unreserved account concurrency is incorrect because this may possibly be the current setting of this function, which is why the requests are being throttled. The total number of concurrent execution limits that you allocated to all Lambda functions affect the value of the unreserved concurrency limit. Since the second function is being throttled, it is highly likely that it is already using an unreserved account concurrency, which only has a low value since the units were already exhausted by the first function. Take note that the unreserved concurrency pool has a minimum value of 100 concurrent executions. Setting the concurrency execution limit of the second function to 0 is incorrect because this will throttle all future invocations of this function and will make the problem worse. References: https://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html https://docs.aws.amazon.com/lambda/latest/dg/scaling.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"])</script><script>self.__next_f.push([1,"2a:Td02,"])</script><script>self.__next_f.push([1,"AWS Lambda is a serverless compute service that allows developers to run code without provisioning or managing servers. It automatically scales based on the workload and charges only for the compute time consumed. Developers can use Lambda to execute code in response to events such as changes in data, HTTP requests, or system state changes, making it ideal for event-driven architectures. Lambda supports multiple programming languages and integrates seamlessly with other AWS services, enabling flexible and scalable application development. AWS Lambda functions operate within a highly available infrastructure and manage resources automatically, ensuring reliability and performance. Lambda can execute specific business logic by using triggers like S3 events, DynamoDB streams, or API Gateway, making it a key component for building modern, agile applications. AWS Lambda allows developers to configure memory allocation for 128 MB to 10,240 MB functions. This memory setting directly influences the CPU resources available to the function, as Lambda allocates CPU power proportionally to the configured memory. For instance, at 1,769 MB, a function has the equivalent of one vCPU. Increasing the memory allocation provides more RAM and enhances CPU capacity, which can lead to significant performance improvements for compute-intensive tasks. Hence, the correct answer is: Optimize memory allocation for the Lambda function. The option that says: Use AWS Step Functions to split tasks into smaller workflows is incorrect. AWS Step Functions are primarily used for orchestrating workflows and breaking down complex processes into smaller, manageable steps. However, this approach does not directly improve the execution performance of the Lambda function itself. The issue lies in the Lambda function's CPU resources, which Step Functions simply cannot address. While they can enhance task coordination, they typically do not optimize the speed of underlying tasks within a single function. The option that says: Increase the timeout setting of the Lambda function is incorrect. This option primarily focuses on extending the maximum runtime for the Lambda function. Increasing the timeout setting would allow the function to run longer but not address the underlying inefficiencies caused by insufficient memory or CPU resources. Timeout adjustments are typically useful for handling long-running tasks, not optimizing compute-intensive workloads. The option that says: Utilize Amazon S3 Transfer Acceleration for image uploads is incorrect. Amazon S3 Transfer Acceleration is designed to improve the upload and download speed of objects to and from S3 by using Amazon's global edge network. However, this feature is only relevant when data transfer speed between the client and S3 is a bottleneck. In this case, the issue lies with the processing of images within the Lambda function. Transfer Acceleration simply cannot influence the performance of compute tasks, as it is unrelated to the Lambda execution environment. References: https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtime-environment.html https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"])</script><script>self.__next_f.push([1,"2b:T88d,"])</script><script>self.__next_f.push([1,"The EB CLI is a command line interface for Elastic Beanstalk that provides interactive commands that simplify creating, updating and monitoring environments from a local repository. It is recommended that you use the EB CLI as part of your everyday development and testing cycle as an alternative to the AWS Management Console. You can tell the EB CLI to deploy a ZIP file or WAR file that you generate as part of a separate build process by adding the following lines to .elasticbeanstalk/config.yml in your project folder: deploy: artifact: path/to/buildartifact.zip If you configure the EB CLI in your Git repository, and you don't commit the artifact to source, use the --staged option to deploy the latest build: ~/eb$ eb deploy --staged Hence, packaging your application as a zip file and deploying it using the eb deploy command is the correct answer. Packaging your application as a tar file and deploying it using the eb deploy command is incorrect because tar is not supported. You can only deploy a ZIP or WAR file. Packaging your application as a tar file and deploying it using the aws elasticbeanstalk update-application command is incorrect because this CLI command just updates the specified properties of the application. This command does not allow you to upload packages to Elastic Beanstalk. Packaging your application as a zip file and deploying it using the aws elasticbeanstalk update-application command is incorrect because although you have a valid file type for your application bundle (ZIP), the CLI command that was used is wrong. Remember that the update-application command does not allow you to upload packages to Elastic Beanstalk. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-configuration.html#eb-cli3-artifact Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/ Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy: https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/ Comparison of AWS Services Cheat Sheets: https://tutorialsdojo.com/comparison-of-aws-services/"])</script><script>self.__next_f.push([1,"2c:Te34,"])</script><script>self.__next_f.push([1,"Server-side encryption is about protecting data at rest. Using server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your own encryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it writes to disks, and decryption, when you access your objects. Therefore, you don't need to maintain any code to perform data encryption and decryption. The only thing you do is manage the encryption keys you provide. When you upload an object, Amazon S3 uses the encryption key you provide to apply AES-256 encryption to your data and removes the encryption key from memory. It is important to note that Amazon S3 does not store the encryption key you provide. Instead, it is stored in a randomly salted HMAC value of the encryption key in order to validate future requests. The salted HMAC value cannot be used to derive the value of the encryption key or to decrypt the contents of the encrypted object. That means, if you lose the encryption key, you lose the object. When you retrieve an object, you must provide the same encryption key as part of your request. Amazon S3 first verifies that the encryption key you provided matches, and then decrypts the object before returning the object data to you. When using server-side encryption with customer-provided encryption keys (SSE-C), you must provide encryption key information using the following request headers: x-amz-server-side-encryption-customer-algorithm - This header specifies the encryption algorithm. The header value must be \"AES256\". x-amz-server-side-encryption-customer-key - This header provides the 256-bit, base64-encoded encryption key for Amazon S3 to use to encrypt or decrypt your data. x-amz-server-side-encryption-customer-key-MD5 - This header provides the base64-encoded 128-bit MD5 digest of the encryption key according to RFC 1321. Amazon S3 uses this header for a message integrity check to ensure the encryption key was transmitted without error. Hence, the correct answer is: x-amz-server-side​-encryption​-customer-algorithm, x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers. The option that says: Including the x-amz-server-side-encryption and x-amz-server-side-encryption-aws-kms-key-id headers in the upload request is incorrect because these headers are primarily used in Server-Side Encryption with AWS KMS Keys (SSE-KMS) and not for Server-Side Encryption with Customer-Provided Keys (SSE-C). The option that says: Including the x-amz-server-side-encryption, x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers is incorrect because the x-amz-server-side-encryption header is not used in SSE-C encryption. This should be replaced with the x-amz-server-side​-encryption​-customer-algorithm header. The option that says: Including just the x-amz-server-side-encryption-customer-key header only is incorrect because you have to include the x-amz-server-side​-encryption​-customer-algorithm and x-amz-server-side-encryption-customer-key-MD5 headers as well to upload the objects to the S3 bucket with SSE-C encryption. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html#RESTObjectPUT-responses-examples https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/ Check out these Amazon S3 and AWS KMS Cheat Sheets: https://tutorialsdojo.com/amazon-s3/ https://tutorialsdojo.com/aws-key-management-service-aws-kms/"])</script><script>self.__next_f.push([1,"2d:T8ab,"])</script><script>self.__next_f.push([1,"Time To Live (TTL) for DynamoDB allows you to define when items in a table expire so that they can be automatically deleted from the database. TTL is provided at no extra cost as a way to reduce storage usage and reduce the cost of storing irrelevant data without using provisioned throughput. With TTL enabled on a table, you can set a timestamp for deletion on a per-item basis, allowing you to limit storage usage to only those records that are relevant. TTL is useful if you have continuously accumulated data that lose relevance after a specific time period. For example session data, event logs, usage patterns, and other temporary data. If you have sensitive data that must be retained only for a certain amount of time according to contractual or regulatory obligations, TTL helps you ensure that it is removed promptly and as scheduled. Hence, the correct answer is to: Turn on Time To Live (TTL) in the table. The option that says: Use a Lambda function with CloudWatch Events to schedule a purge of stale items in the table on a daily basis is incorrect. Although this solution can work, it entails a lot of configuration to implement and can incur an additional cost. The option that says: Implement a Write-Through caching strategy in your application is incorrect because this strategy simply adds data or updates data in the cache whenever data are written to the database, and not in the event of a cache miss. Since the scenario is about deleting the stale items in the table and not improving the cache performance, this option is incorrect. The option that says: Implement a Lazy Loading caching strategy to your application is incorrect because this strategy just loads data into the cache only when necessary. Just like the Write-Through caching strategy, this is not applicable in this scenario since the objective is to automatically expire and delete the stale session data in the DynamoDB table to improve the application performance. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"2e:Ta50,"])</script><script>self.__next_f.push([1,"When you create a global secondary index on a provisioned mode table, you must specify read and write capacity units for the expected workload on that index. The provisioned throughput settings of a global secondary index are separate from those of its base table. A Query operation on a global secondary index consumes read capacity units from the index, not the base table. When you put, update, or delete items in a table, the global secondary indexes on that table are also updated; these index updates consume write capacity units from the index, not from the base table. For example, if you Query a global secondary index and exceed its provisioned read capacity, your request will be throttled. If you perform heavy write activity on the table but a global secondary index on that table has insufficient write capacity, then the write activity on the table will be throttled. To avoid potential throttling, the provisioned write capacity for a global secondary index should be equal or greater than the write capacity of the base table since new updates will write to both the base table and global secondary index. To view the provisioned throughput settings for a global secondary index, use the DescribeTable operation; detailed information about all of the table's global secondary indexes will be returned. Hence, the most likely cause of this issue is that the provisioned write capacity for the global secondary index is less than the write capacity of the base table. The option that says: The provisioned write capacity for the global secondary index is greater than the write capacity of the base table is incorrect because it should be the other way around, just as mentioned above. If the provisioned WCU of the global secondary index is greater than its base table then this issue is unlikely to happen. The option that says: The provisioned throughput exceeds the current throughput limit for your account is incorrect because this will only happen if DynamoDB returns a RequestLimitExceeded exception. The option that says: The rate of requests exceeds the allowed throughput is incorrect because this will only happen if DynamoDB returns a ThrottlingException. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html#GSI.ThroughputConsiderations https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.MessagesAndCodes Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"])</script><script>self.__next_f.push([1,"2f:Tce1,"])</script><script>self.__next_f.push([1,"CloudFormation drift detection is a feature that allows you to identify differences between the actual configuration of your AWS resources and their expected configuration as defined in your CloudFormation stack template. Some of its capabilities are: - Compares a stack's current state of resources with their expected state as defined in the CloudFormation template. - Identifies any discrepancies or \"drift\" between the actual and expected configurations of resources. - Assists in identifying potential issues affecting stack operations or overall infrastructure integrity. - Helps maintain consistency and compliance by detecting unintended or unauthorized changes outside of CloudFormation. Running a drift detection check on the CloudFormation stack would allow the organization to identify these unauthorized changes. CloudFormation would detect that the actual configuration of the IAM roles no longer matches the expected configuration defined in the stack template, and it would report these differences as drifts. By identifying the drifts, the organization can take appropriate actions to remediate the unauthorized changes and bring the resources back into compliance with the CloudFormation stack template. This could involve updating the IAM roles through CloudFormation or reverting the manual changes made to the roles. Additionally, CloudFormation's drift detection feature provides detailed information about the specific properties or attributes of the resources that have drifted, making it easier to understand the nature of the changes and take targeted actions to address them. Hence, the correct answer is: Run a drift detection check on the CloudFormation stack. The option that says: Use AWS Config to monitor updates made to the Lambda functions and IAM roles is incorrect. While AWS Config can track configuration changes and rule compliance for AWS resources, it wouldn't directly show which resources have deviated from their CloudFormation stack configuration. AWS Config might identify some changes, but it lacks the direct stack synchronization context provided by drift detection. The option that says: Review CloudTrail logs to trace IAM role updates for the Lambda functions is incorrect. AWS CloudTrail simply records API calls made within an AWS account, including changes made to IAM roles. Reviewing the CloudTrail logs, the organization can trace the API calls that modified the IAM roles associated with the Lambda functions. However, this approach alone does not identify whether the changes were authorized or unauthorized, nor does it ensure that all resources are in sync with the CloudFormation stack. The option that says: Analyze CloudWatch Logs to identify changes to the IAM role permissions is incorrect. CloudWatch Logs is primarily used for monitoring and collecting log data from various AWS services. It does not directly provide information about changes to IAM role permissions or help identify unauthorized changes in the context of the CloudFormation stack. References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-drift.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/detect-drift-stack.html Check out this CloudFormation Cheat Sheet: https://tutorialsdojo.com/aws-cloudformation/"])</script><script>self.__next_f.push([1,"30:Te35,"])</script><script>self.__next_f.push([1,"In the traditional data center-based model of IT, once the infrastructure is deployed, it typically runs whether it is needed or not, and all the capacity is paid for, regardless of how much it gets used. In the cloud, resources are elastic, meaning they can instantly grow or shrink to match the requirements of a specific application. Elasticity allows you to match the supply of resources—which cost money—to demand. Because cloud resources are paid for based on usage, matching needs to utilization is critical for cost optimization. Demand includes both external usage, such as the number of customers who visit a website over a given period, and internal usage, such as an application team using development and test environments. There are two basic types of elasticity: 1. Time-based2. Volume-based Time-based elasticity means turning off resources when they are not being used, such as a development environment that is needed only during business hours. Volume-based elasticity means matching scale to the intensity of demand, whether that’s compute cores, storage sizes, or throughput. By combining monitoring, tagging, and automation, you can get the most value out of your AWS resources and optimize costs. By taking advantage of volume-based elasticity, you can scale resources to match capacity. The best tool for accomplishing this task is Auto Scaling, which you can use to optimize performance by automatically increasing the number of EC2 instances during demand spikes and decreasing capacity during lulls to reduce costs. Auto Scaling is well-suited for applications that have stable demand patterns and for ones that experience hourly, daily, or weekly variability in usage. You can also use a combination of ELB and Auto Scaling to maximize the elasticity of your architecture. Beyond Auto Scaling for Amazon EC2, you can use Application Auto Scaling to automatically scale resources for other AWS services, including: - Amazon ECS - Amazon EC2 Spot Fleets - Amazon EMR clusters - Amazon AppStream 2.0 stacks and fleets - Amazon DynamoDB For Amazon EC2 Spot Fleets, it can either launch instances (scale out) or terminate instances (scale in), within the range that you choose, in response to one or more scaling policies. For Amazon DynamoDB, you can dynamically adjust provisioned throughput capacity in response to actual traffic patterns. This enables a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic without throttling. When the workload decreases, Application Auto Scaling decreases the throughput so that you don't pay for unused provisioned capacity. Hence, the correct answers are: - Amazon EC2 Spot Fleet. - Amazon DynamoDB. Amazon CloudFront is incorrect because this is primarily helpful for scaling out your application. Moreover, the scenario says that the internal application will only be used by about a hundred employees, which clearly doesn't warrant the use of a CDN or CloudFront. AWS WAF is incorrect because it only improves the security of your architecture and not its elasticity. Amazon RDS is incorrect. While Amazon RDS offers robust features and scalability, its cost structure can be higher due to the need for continuous instance uptime, storage, and backup costs. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html Check out these cheat sheets for Amazon EC2 and Amazon DyamoDB: https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/ https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"31:T9d7,"])</script><script>self.__next_f.push([1,"When you create a table, in addition to the table name, you must specify the primary key of the table. The primary key uniquely identifies each item in the table so that no two items can have the same key. DynamoDB supports two different kinds of primary keys: 1. Partition key 2. Partition key and sort key Partition key – A simple primary key, composed of one attribute known as the partition key. DynamoDB uses the partition key's value as input to an internal hash function. The output from the hash function determines the partition (physical storage internal to DynamoDB) in which the item will be stored. In a table that has only a partition key, no two items can have the same partition key value. Partition key and sort key – Referred to as a composite primary key, this type of key is composed of two attributes. The first attribute is the partition key, and the second attribute is the sort key. DynamoDB uses the partition key value as input to an internal hash function. The output from the hash function determines the partition (physical storage internal to DynamoDB) in which the item will be stored. All items with the same partition key are stored together, in sorted order by sort key value. In a table that has a partition key and a sort key, it's possible for two items to have the same partition key value. However, those two items must have different sort key values. A composite primary key gives you additional flexibility when querying data. For example, if you provide only the value for Artist, DynamoDB retrieves all of the songs by that artist. To retrieve only a subset of songs by a particular artist, you can provide a value for Artist along with a range of values for SongTitle. Thus, in this scenario, the correct answer is to use employee_id because each employee ID is unique. Using high-cardinality attributes are recommended when creating primary partition keys. Examples of these unique attributes are email, employee_no, customerid, and so on. Both department_id and position_id are incorrect because these values are not unique per employee. Using employee_name is not recommended because in big organizations, somebody may share the same name as someone else. References: https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/ https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html#HowItWorks.CoreComponents.PrimaryKey Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"32:T933,"])</script><script>self.__next_f.push([1,"You can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package. Layers let you keep your deployment package small, which makes development easier. You can avoid errors that can occur when you install and package dependencies with your function code. For Node.js, Python, and Ruby functions, you can develop your function code in the Lambda console as long as you keep your deployment package under 3 MB. A function can use up to 5 layers at a time. The total unzipped size of the function and all layers can't exceed the unzipped deployment package size limit of 250 MB. You can create layers, or use layers published by AWS and other AWS customers. Layers support resource-based policies for granting layer usage permissions to specific AWS accounts, AWS Organizations, or all accounts. Layers are extracted to the /opt directory in the function execution environment. Each runtime looks for libraries in a different location under /opt, depending on the language. Structure your layer so that function code can access libraries without additional configuration. Hence, the correct answer is to upload the other dependencies of your function as a separate Lambda Layer instead. Uploading the deployment package to S3 is incorrect. Although you can upload large deployment packages of over 50 MB in size via S3, your function will still be in a single layer. This doesn't meet the requirement of making the deployment package small and modularized. You have to use Lambda Layers instead. Zipping the deployment package again to further compress the zip file is incorrect because doing this will not significantly make the ZIP file smaller. Compressing the deployment package as TAR file instead is incorrect. Although it may decrease the size of the deployment package, it is still not enough to totally solve the issue. A compressed TAR file is not significantly smaller as compared to a ZIP file. References: https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html https://docs.aws.amazon.com/lambda/latest/dg/limits.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"])</script><script>self.__next_f.push([1,"33:Ta33,"])</script><script>self.__next_f.push([1,"Amazon Kinesis Data Streams supports changes to the data record retention period of your stream. A Kinesis data stream is an ordered sequence of records meant to be written to and read from in real-time. Data records are therefore stored in shards in your stream temporarily. The time period from when a record is added to when it is no longer accessible is called the retention period. A Kinesis data stream stores record for up to 24 hours by default. You can increase the retention period up to 8760 hours (365 days) using the IncreaseStreamRetentionPeriod operation. You can decrease the retention period to a minimum of 24 hours using the DecreaseStreamRetentionPeriod operation. The request syntax for both operations includes the stream name and the retention period in hours. Finally, you can check the current retention period of a stream by calling the DescribeStream operation. Kinesis Data Streams supports changes to the data record retention period of your stream. A Kinesis data stream is an ordered sequence of records meant to be written to and read from in real-time. Data records are therefore stored in shards in your stream temporarily. In this scenario, the consumer of the data stream is configured to process the data every other day. Since the default data retention of the Kinesis data stream is only 24 hours, the data from the day before is already lost prior to the scheduled processing. Hence, the root cause of the problem in this scenario is that by default, the data records are only accessible for 24 hours from the time they are added to a Kinesis stream. The option that says: the sensors are having intermittent connection issues is incorrect because the sensors have been verified to be working properly, hence, this is not the root cause. The option that says: the Amazon Kinesis Data Stream has too many open shards is incorrect because having this configuration is irrelevant in this scenario as it just increases the data stream's rate of data flow. The option that says: the Amazon Kinesis Data Stream automatically deletes duplicate data is incorrect because Amazon Kinesis does not do this by default. If the sensors send two records with identical data, these will have unique sequence numbers in the stream. This does not have anything to do with data retention; hence, it is irrelevant in this scenario. Reference: http://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/"])</script><script>self.__next_f.push([1,"34:T9f1,"])</script><script>self.__next_f.push([1,"Concurrent executions refers to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the concurrent execution count will differ depending on whether or not your Lambda function is processing events from a poll-based event source. If you create a Lambda function to process events from event sources that aren't poll-based (for example, Lambda can process every event from other sources, like Amazon S3 or API Gateway), each published event is a unit of work, in parallel, up to your account limits. Therefore, the number of invocations these event sources make influences the concurrency. You can use this formula to estimate the capacity used by your function: concurrent executions = invocations per second * average execution duration in seconds Supposed you have 10 events coming in per second and it takes 3 seconds for each event to be processed, then 10 instances of the function will be spawned to handle those requests. AWS Lambda won't be able to reuse those functions for 3 seconds. To accommodate the 2nd batch (another 10 events), AWS Lambda has to spawn another 10 instances of the function, which become busy for another 3 seconds. At the 3rd second, another batch comes in, invoking additional 10 functions. Suppose there's a continuous stream of events, the concurrent executions that you'll get at any given time would be 30. In this scenario, a Lambda function processes Amazon S3 events where the function takes on an average of three seconds to complete the processing (average execution duration in seconds) and Amazon S3 publishes 10 events per second (invocations per second). By using the formula given above: concurrent executions = 10 * 3 = 30 Hence, you will have 30 concurrent executions of your Lambda function. 3 is incorrect because the value of the concurrent executions is not equivalent to the average execution duration of your Lambda function in seconds. 10 is incorrect because the value of the concurrent executions is not equivalent to the number of invocations your Lambda function receives per second. 13 is incorrect because the value of the concurrent executions is not equivalent to the sum of the average execution duration and the number of invocations your Lambda function receives per second. References: https://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html https://docs.aws.amazon.com/lambda/latest/dg/scaling.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"])</script><script>self.__next_f.push([1,"35:Te6e,"])</script><script>self.__next_f.push([1,"A segment document conveys information about a segment to X-Ray. A segment document can be up to 64 kB and contain a whole segment with subsegments, a fragment of a segment that indicates that a request is in progress, or a single subsegment that is sent separately. You can send segment documents directly to X-Ray by using the PutTraceSegments API. X-Ray compiles and processes segment documents to generate queryable trace summaries and full traces that you can access by using the GetTraceSummaries and BatchGetTraces APIs, respectively. In addition to the segments and subsegments that you send to X-Ray, the service uses information in subsegments to generate inferred segments and adds them to the full trace. Inferred segments represent downstream services and resources in the service map. A subset of segment fields is indexed by X-Ray for use with filter expressions. For example, if you set the user field on a segment to a unique identifier, you can search for segments associated with specific users in the X-Ray console or by using the GetTraceSummaries API. Below are the optional subsegment fields: namespace - aws for AWS SDK calls; remote for other downstream calls. http - http object with information about an outgoing HTTP call. aws - aws object with information about the downstream AWS resource that your application called. error, throttle, fault, and cause - error fields that indicate an error occurred and that include information about the exception that caused the error. annotations - annotations object with key-value pairs that you want X-Ray to index for search. metadata - metadata object with any additional data that you want to store in the segment. subsegments - array of subsegment objects. precursor_ids - array of subsegment IDs that identify subsegments with the same parent that was completed prior to this subsegment. You can use the \"metadata\" field in the segment section to add custom data for your tracing. If you want to trace all the AWS SDK calls of your application, then you can add a subsegment and set the \"namespace\" field to \"AWS\". Alternatively, you can set the \"namespace\" value to \"remote\" if you want to trace other downstream calls. Hence, the valid considerations in this scenario are: - Set the namespace subsegment field to aws for AWS SDK calls and remote for other downstream calls. - Set the metadata object with any additional custom data that you want to store in the segment. Setting the annotations object with any additional custom data that you want to store in the segment is incorrect because this should be the metadata object and not the annotations object. Segments and subsegments can include an annotations object containing one or more fields that X-Ray indexes for use with filter expressions. Fields can have string, number, or Boolean values (no objects or arrays). X-Ray indexes up to 50 annotations per trace. Setting the namespace subsegment field to remote for AWS SDK calls and aws for other downstream calls is incorrect because this should be the other way around. You should set the namespace subsegment field to aws for AWS SDK calls and remote for other downstream calls Setting the metadata object with key-value pairs that you want X-Ray to index for search is incorrect because this should be the annotations object, and not the metadata object. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/ Instrumenting your Application with AWS X-Ray: https://tutorialsdojo.com/instrumenting-your-application-with-aws-x-ray/"])</script><script>self.__next_f.push([1,"36:T83d,"])</script><script>self.__next_f.push([1,"A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. When a client makes a request to one of your API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output. There are two types of Lambda authorizers: - A token-based Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token. - A request parameter-based Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of headers, query string parameters, stageVariables, and $context variables. It is possible to use an AWS Lambda function from an AWS account that is different from the one in which you created your Lambda authorizer function by using a Cross-Account Lambda Authorizer. Therefore, the correct answer in this scenario is to use a token-based authorization since this is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML. Requesting Parameter-based Lambda Authorization is incorrect because this does not use tokens to identify a caller but through a combination of headers, query string parameters, stageVariables, and $context variables. AWS STS-based authentication is incorrect because this is not a valid type of API Gateway Lambda Authorizer. Cross-Account Lambda Authorizer is incorrect because this just enables you to use an AWS Lambda function from a different AWS account as your API authorizer function. Moreover, this is not a valid Lambda authorizer type. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-lambda-authorizer-input.html https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-lambda-authorizer-cross-account-lambda-authorizer.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"])</script><script>self.__next_f.push([1,"37:T980,"])</script><script>self.__next_f.push([1,"The AWS X-Ray SDK does not send trace data directly to AWS X-Ray. To avoid calling the service every time your application serves a request, the SDK sends the trace data to a daemon, which collects segments for multiple requests and uploads them in batches. Use a script to run the daemon alongside your application. To properly instrument your applications in Amazon ECS, you have to create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster. You can use port mappings and network mode settings in your task definition file to allow your application to communicate with the daemon container. The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon works in conjunction with the AWS X-Ray SDKs and must be running so that data sent by the SDKs can reach the X-Ray service. Hence, the correct steps to properly instrument the application is to create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster. In addition, you also have to configure the port mappings and network mode settings in your task definition file to allow traffic on UDP port 2000. Configuring the port mappings and network mode settings in the container agent to allow traffic on TCP port 2000 is incorrect because this should be done in the task definition and not in the container agent. Moreover, X-Ray is primarily using the UDP port 2000, so this should also be added alongside with the TCP port mapping. Manually installing the X-Ray daemon to the instances via a user data script is incorrect because this is only applicable if your application is hosted in an EC2 instance. Adding the xray-daemon.config configuration file in your Docker image is incorrect because this step is not suitable for ECS. The xray-daemon.config configuration file is primarily used in Elastic Beanstalk. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html https://docs.aws.amazon.com/xray/latest/devguide/scorekeep-ecs.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/ Instrumenting your Application with AWS X-Ray: https://tutorialsdojo.com/instrumenting-your-application-with-aws-x-ray/"])</script><script>self.__next_f.push([1,"38:T1084,"])</script><script>self.__next_f.push([1,"AWS Elastic Beanstalk provides support for running Amazon Relational Database Service (Amazon RDS) instances in your Elastic Beanstalk environment. This works great for development and testing environments. However, it isn't ideal for a production environment because it ties the lifecycle of the database instance to the lifecycle of your application's environment. If you haven't used a DB instance with your application before, try adding one to a test environment with the Elastic Beanstalk console first. This lets you verify that your application is able to read environment properties, construct a connection string, and connect to a DB instance before you add Amazon Virtual Private Cloud (Amazon VPC) and security group configuration to the mix. To decouple your database instance from your environment, you can run a database instance in Amazon RDS and configure your application to connect to it on launch. This enables you to connect multiple environments to a database, terminate an environment without affecting the database, and perform seamless updates with blue-green deployments. To allow the Amazon EC2 instances in your environment to connect to an outside database, you can configure the environment's Auto Scaling group with an additional security group. The security group that you attach to your environment can be the same one that is attached to your database instance, or a separate security group from which the database's security group allows ingress. You can connect your environment to a database by adding a rule to your database's security group that allows ingress from the autogenerated security group that Elastic Beanstalk attaches to your environment's Auto Scaling group. However, doing so creates a dependency between the two security groups. Subsequently, when you attempt to terminate the environment, Elastic Beanstalk will be unable to delete the environment's security group because the database's security group is dependent on it. Hence, the option that says: Use the blue/green deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance. Before terminating the old Elastic Beanstalk environment, remove its security group rule first before proceeding is the correct answer in this scenario. The option that says: Use the blue/green deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance and delete the old environment is incorrect. Although the deployment strategy being used here is valid, the existing security group rule is not yet removed, which hinders the deletion of the old environment. The option that says: Use a Canary deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance and delete the old environment is incorrect because there is no Canary deployment configuration in Elastic Beanstalk. This type of deployment strategy is usually used in Lambda. The option that says: Use a Canary deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and then create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance is incorrect because you should use a blue/green deployment strategy instead. This will also cause a data loss since the deletion protection for the database is not enabled. References: https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/ https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/"])</script><script>self.__next_f.push([1,"39:T999,"])</script><script>self.__next_f.push([1,"Amazon CloudWatch is basically a metrics repository. An AWS service—such as Amazon EC2—puts metrics into the repository, and you retrieve statistics based on those metrics. If you put your own custom metrics into the repository, you can retrieve statistics on these metrics as well. A namespace is a container for CloudWatch metrics. Metrics in different namespaces are isolated from each other so that metrics from different applications are not mistakenly aggregated into the same statistics. There is no default namespace. You must specify a namespace for each data point you publish to CloudWatch. You can specify a namespace name when you create a metric. These names must contain valid XML characters and be fewer than 256 characters in length. Possible characters are: alphanumeric characters (0-9A-Za-z), period (.), hyphen (-), underscore (_), forward slash (/), hash (#), and colon (:). The AWS namespaces typically use the following naming convention: AWS/service. For example, Amazon EC2 uses the AWS/EC2 namespace. Hence, the correct answer is: Set up a custom CloudWatch namespace with a unique metric name for each application. The option that says: Set up a custom CloudWatch Alarm with a unique metric name for each application is incorrect because a CloudWatch Alarm simply watches a single metric over a specified time period and performs one or more specified actions based on the value of the metric relative to a threshold over time. The option that says: Set up a custom CloudWatch Event with a unique metric name for each application is incorrect because a CloudWatch Event is primarily used to deliver a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources, and not for segregating metrics of various applications. The option that says: Set up a custom CloudWatch dimension with a unique metric name for each application is incorrect because a CloudWatch dimension is only a name/value pair that is part of the identity of a metric. You have to use a CloudWatch namespace instead. References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html#Namespace https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/viewing_metrics_with_cloudwatch.html https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/aws-services-cloudwatch-metrics.html Check out this Amazon CloudWatch Cheat Sheet: https://tutorialsdojo.com/amazon-cloudwatch/"])</script><script>self.__next_f.push([1,"3a:T892,"])</script><script>self.__next_f.push([1,"Amazon RDS Read Replicas provide enhanced performance and durability for the database (DB) instances. This feature makes it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read. You can reduce the load on your source DB instance by routing read queries from your applications to the read replica. Read replicas allow you to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Because read replicas can be promoted to master status, they are useful as part of a sharding implementation. To shard your database, add a read replica and promote it to master status, then, from each of the resulting DB Instances, delete the data that belongs to the other shard. Hence, the correct solution in this scenario is to: create an RDS Read Replica instance and configure the application to use this for read queries. The option that says: Launch a large ElastiCache Cluster as a database cache for RDS and apply the required code change is incorrect. Although this will improve the read performance of the application, this solution entails a lot of code changes in the application as compared with just using RDS Read Replicas. It is specifically mentioned in the scenario that you need to solve the issue with the minimal code change. The option that says: Set up a multi-AZ deployment configuration in RDS is incorrect because configuring a Multi-AZ RDS just improves the availability of the database but does not drastically improve the read performance. The more appropriate solution for this is to use Read Replicas instead. The option that says: Upgrade the EC2 instances to a higher instance type is incorrect because the issue lies with the database, not the application servers hosted in EC2 instances. References: https://aws.amazon.com/caching/database-caching/ https://aws.amazon.com/rds/details/read-replicas/ https://aws.amazon.com/elasticache/ Check out this Amazon RDS Cheat Sheet: https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/"])</script><script>self.__next_f.push([1,"3b:Ta86,"])</script><script>self.__next_f.push([1,"In general, Scan operations are less efficient than other operations in DynamoDB. A Scan operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set. A Scan operation reads every item in a table or a secondary index. By default, a Scan operation returns all of the data attributes for every item in the table or index. You can use the ProjectionExpression parameter so that Scan only returns some of the attributes rather than all of them. On the other hand, the Query operation finds items based on primary key values. You can query any table or secondary index that has a composite primary key (a partition key and a sort key). If possible, you should avoid using a Scan operation on a large table or index with a filter that removes many results. Also, as a table or index grows, the Scan operation slows. The Scan operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation. For faster response times, design your tables and indexes so that your applications can use Query instead of Scan. DynamoDB calculates the number of read capacity units consumed based on item size, not on the amount of data that is returned to an application. For this reason, the number of capacity units consumed will be the same whether you request all of the attributes (the default behavior) or just some of them (using a projection expression). The number will also be the same whether or not you use a filter expression. Hence, using the Query operation with eventual consistency reads is the correct answer. Using the Scan operation with eventual consistency reads is incorrect because it doesn't find items based on primary key values but by reading every item in a table. Using the Scan operation with strong consistency reads is incorrect because in addition to what is mentioned above about the Scan operation, using strong consistency will consume more RCU. Remember that the scenario requires a solution that uses the LEAST amount of RCU. Using the Query operation with strong consistency reads is incorrect because this consumes more RCU in comparison to eventual consistency. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Query.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ DynamoDB Scan vs Query: https://tutorialsdojo.com/dynamodb-scan-vs-query/"])</script><script>self.__next_f.push([1,"3c:Tc8b,"])</script><script>self.__next_f.push([1,"In Elastic Beanstalk, you can include a YAML formatted environment manifest in the root of your application source bundle to configure the environment name, solution stack and environment links to use when creating your environment. An environment manifest uses the same format as Saved Configurations. This file format includes support for environment groups. To use groups, specify the environment name in the manifest with a + symbol at the end. When you create or update the environment, specify the group name with --group-name (AWS CLI) or --env-group-suffix (EB CLI). The following example manifest defines a web server environment for the tutorialsdojo frontend application, with a link to a worker environment component that it is dependent upon. The manifest uses groups to allow creating multiple environments with the same source bundle: ~/tutorialsdojo/frontend/env.yaml AWSConfigurationTemplateVersion: 1.1.0.0SolutionStack: 64bit Amazon Linux 2015.09 v2.0.6 running Multi-container Docker 1.7.1 (Generic)OptionSettings: aws:elasticbeanstalk:command: BatchSize: '30' BatchSizeType: Percentage aws:elasticbeanstalk:sns:topics: Notification Endpoint: me@example.com aws:elb:policies: ConnectionDrainingEnabled: true ConnectionDrainingTimeout: '20' aws:elb:loadbalancer: CrossZone: true aws:elasticbeanstalk:environment: ServiceRole: aws-elasticbeanstalk-service-role aws:elasticbeanstalk:application: Application Healthcheck URL: / aws:elasticbeanstalk:healthreporting:system: SystemType: enhanced aws:autoscaling:launchconfiguration: IamInstanceProfile: aws-elasticbeanstalk-ec2-role InstanceType: t2.micro EC2KeyName: workstation-uswest2 aws:autoscaling:updatepolicy:rollingupdate: RollingUpdateType: Health RollingUpdateEnabled: trueTags: Cost Center: WebApp DevCName: front-A08G28LG+EnvironmentName: front+EnvironmentLinks: \"WORKERQUEUE\" : \"worker+\" Hence, using the env.yaml is the correct configuration file to be used in this scenario. Dockerrun.aws.json is incorrect because this configuration file is primarily used in multicontainer Docker environments that are hosted in Elastic Beanstalk. This can be used alone or combined with source code and content in a source bundle to create an environment on a Docker platform. env.config is incorrect because this is just a custom configuration file which is not readily available in Elastic Beanstalk. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle. The more appropriate configuration file to use here is the env.yaml which can help you configure the environment name, solution stack, and environment links to use when creating your environment. cron.yaml is incorrect because this configuration file is primarily used to define periodic tasks that add jobs to your worker environment's queue automatically at a regular interval. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-cfg-manifest.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/"])</script><script>self.__next_f.push([1,"3d:Ta61,"])</script><script>self.__next_f.push([1,"Resharding enables you to increase or decrease the number of shards in a stream in order to adapt to changes in the rate of data flowing through the stream. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that shard. It also tracks the shards in the stream using an Amazon DynamoDB table. When new shards are created as a result of resharding, the KCL discovers the new shards and populates new rows in the table. The workers automatically discover the new shards and create processors to handle the data from them. The KCL also distributes the shards in the stream across all the available workers and record processors. By increasing the instance size and number of shards in your Kinesis stream, the developer allows the instances to handle more record processors, which are running in parallel within the instance. It also allows the stream to properly accommodate the rate of data being sent in. The data capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the capacities of its shards. Hence, the correct answer is: Increase both the instance size and the number of open shards The option that says: Increase the instance size to a larger type is incorrect. Increasing the instance size alone is incorrect because the current number of Kinesis shards is not enough to accommodate the rate of data flowing through the stream. In this scenario, the best solution is to reshard or to increase the number of shards in the stream. The option that says: Increase the number of shards is incorrect. This is not enough because the instances cannot hold any more record processors, due to CPU Utilization maxing out. You should either increase the instance size or number of instances along with the increase of shards in this scenario. The option that says: Increase the number of instances up to the number of open shards is incorrect because this scenario requires resharding to adapt to changes in the rate of data flowing through the stream. Although adding new instances will improve the compute capacity, the rate of data flowing through the stream would still be low since the number of shards is still unchanged. References: https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html https://docs.aws.amazon.com/streams/latest/dev/building-consumers.html Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/ Kinesis Scaling, Resharding and Parallel Processing: https://tutorialsdojo.com/kinesis-scaling-resharding-and-parallel-processing/"])</script><script>self.__next_f.push([1,"3e:Td4c,"])</script><script>self.__next_f.push([1,"The binpack strategy tries to fit your workloads in as few instances as possible. It gets its name from the bin packing problem where the goal is to fit objects of various sizes in the smallest number of bins. It is well suited to scenarios for minimizing the number of instances in your cluster, perhaps for cost savings, and lends itself well to automatic scaling for elastic workloads, to shut down instances that are not in use. A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service. Amazon ECS supports the following task placement strategies: binpack - Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use. random - Place tasks randomly. spread - Place tasks evenly based on the specified value. Accepted values are attribute key-value pairs, instanceId, or host. When you use the binpack strategy, you must also indicate if you are trying to make optimal use of your instances’ CPU or memory. This is done by passing an extra field parameter, which tells the task placement engine which parameter to use to evaluate how “full” your “bins” are. It then chooses the instance with the least available CPU or memory (depending on which you pick). If there are multiple instances with this CPU or memory remaining, it chooses randomly. By spreading tasks among your EC2 instances using the binpack strategy, you can minimize costs and resource consumption since this strategy maximizes available CPU/memory of your already running instances. Hence, the correct answer is: Distribute tasks among all registered EC2 instances based on the least available amount of CPU or memory using the binpack task placement strategy. The option that says: Distribute tasks evenly across all available EC2 instances using the spread task placement strategy is incorrect because this strategy is typically used to achieve high availability by making sure that multiple copies of a task are scheduled across multiple instances based on attributes such as Availability Zones. Since the scenario is focused on cost rather than availability, this option is clearly not suitable for this scenario. The option that says: Place tasks randomly using the random task placement strategy is incorrect. Random task placement just ensures tasks are run on instances with sufficient resources to complete them. Binpack has better cost-savings since it strategically places tasks in as few instances as possible. The option that says: Distribute tasks evenly across Availability Zones, and then re-distributing the tasks among EC2 instances based on the least available amount of CPU/memory within each Availability Zone is incorrect. Although it will meet the required task placement, this method will use more unnecessary EC2 instances. Take note that the scenario requires you to minimize the number of instances in use, which will keep the cost down. References: https://aws.amazon.com/blogs/compute/amazon-ecs-task-placement/ https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html Check out this Amazon ECS Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/"])</script><script>self.__next_f.push([1,"3f:Ta79,"])</script><script>self.__next_f.push([1,"With adaptive authentication, you can configure your user pool to block suspicious sign-ins or add second factor authentication in response to an increased risk level. For each sign-in attempt, Amazon Cognito generates a risk score for how likely the sign-in request is to be from a compromised source. This risk score is based on factors that include device and user information. Adaptive authentication can turn on or require multi-factor authentication (MFA) for a user in your user pool when Amazon Cognito detects risk in a user's session, and the user hasn't yet chosen an MFA method. When you activate MFA for a user, they always receive a challenge to provide or set up a second factor during authentication, regardless of how you configured adaptive authentication. From your user's perspective, your app offers to help them set up MFA, and optionally, Amazon Cognito prevents them from signing in again until they have configured an additional factor. In the given scenario, by enabling Adaptive Authentication, the company can require MFA for users only when a sign-in attempt appears suspicious. This aligns with their objective to mitigate risks from the recent data breach without affecting all users. Hence, the correct answer is: Enable Adaptive Authentication for the User Pool. The option that says: Recreate the User Pool and enable SMS text message MFA is incorrect. This option is cumbersome since the users registered in the current User Pool will be lost. This means users would have to re-register or be re-imported into the new User Pool. Additionally, enabling MFA universally would challenge all users, not just those with suspicious login attempts. The option that says: Enable the Time-based one-time password (TOTP) software token MFA for the User Pool is incorrect. Enabling TOTP MFA would apply to all users, not just those with suspicious logins. This doesn't match the company's need to enforce MFA only for suspicious attempts. The option that says: Create a subscription filter Lambda function that monitors for the CompromisedCredentialRisk metric from Advanced Security Metrics in CloudWatch Logs and triggers MFA when detected is incorrect. This approach is not feasible because it operates outside of Cognito's authentication flow. Furthermore, in Cognito, MFA settings are applied at the User Pool level, not per user. References: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pool-settings-adaptive-authentication.html https://aws.amazon.com/blogs/security/how-to-use-new-advanced-security-features-for-amazon-cognito-user-pools/ Check out this Amazon Cognito Cheat Sheet: https://tutorialsdojo.com/amazon-cognito/"])</script><script>self.__next_f.push([1,"40:T818,"])</script><script>self.__next_f.push([1,"Function invocation can result in an error for several reasons. Your code might raise an exception, time out, or run out of memory. The runtime executing your code might encounter an error and stop. You might run out of concurrency and be throttled. When an error occurs, your code might have run completely, partially, or not at all. In most cases, the client or service that invokes your function retries if it encounters an error, so your code must be able to process the same event repeatedly without unwanted effects. If your function manages resources or writes to a database, you need to handle cases where the same request is made several times. AWS Lambda directs events that cannot be processed to the specified Amazon SNS topic or Amazon SQS queue. Functions that don't specify a DLQ will discard events after they have exhausted their retries. You configure a DLQ by specifying the Amazon Resource Name TargetArn value on the Lambda function's DeadLetterConfig parameter. Hence, the setting up a Dead Letter Queue is the correct answer in this scenario. Delay Queue is incorrect because this just lets you postpone the delivery of new messages to a queue for a number of seconds. This is not relevant in this scenario since you can't use a delay queue within Lambda. FIFO Queue is incorrect because this is primarily used to enhance messaging between applications when the order of operations and events is critical, or where duplicates can't be tolerated. Although a DLQ is just a normal SQS queue, this option is still incorrect because you don't necessarily need a FIFO SQS queue since you can also use a Standard SQS queue to be the Dead Letter Queue of your Lambda function. Amazon MQ is incorrect because this is simply a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. References: https://docs.aws.amazon.com/lambda/latest/dg/dlq.html https://docs.aws.amazon.com/lambda/latest/dg/retries-on-errors.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"])</script><script>self.__next_f.push([1,"41:Te49,"])</script><script>self.__next_f.push([1,"The IAM policy simulator evaluates the policies that you choose and determines the effective permissions for each of the actions that you specify. The simulator uses the same policy evaluation engine that is used during real requests to AWS services. But the simulator differs from the live AWS environment in the following ways: - The simulator does not make an actual AWS service request, so you can safely test requests that might make unwanted changes to your live AWS environment. - Because the simulator does not simulate running the selected actions, it cannot report any response to the simulated request. The only result returned is whether the requested action would be allowed or denied. - If you edit a policy inside the simulator, these changes affect only the simulator. The corresponding policy in your AWS account remains unchanged. Check out this video on IAM Policy Simulator: https://youtu.be/1IIhVcXhvcE With the IAM policy simulator, you can test and troubleshoot IAM and resource-based policies in the following ways: - Test policies that are attached to IAM users, groups, or roles in your AWS account. If more than one policy is attached to the user, group, or role, you can test all the policies, or select individual policies to test. You can test which actions are allowed or denied by the selected policies for specific resources. - Test policies that are attached to AWS resources, such as Amazon S3 buckets, Amazon SQS queues, Amazon SNS topics, or Amazon S3 Glacier vaults. - If your AWS account is a member of an organization in AWS Organizations, then you can test the impact of service control policies (SCPs) on your IAM policies and resource policies. - Test new policies that are not yet attached to a user, group, or role by typing or copying them into the simulator. These are used only in the simulation and are not saved. Take note that you cannot type or copy a resource-based policy into the simulator. To use a resource-based policy in the simulator, you must include the resource in the simulation and select the checkbox to include that resource's policy in the simulation. - Test the policies with selected services, actions, and resources. For example, you can test to ensure that your policy allows an entity to perform the ListAllMyBuckets, CreateBucket, and DeleteBucket actions in the Amazon S3 service on a specific bucket. - Simulate real-world scenarios by providing context keys, such as an IP address or date, that are included in Condition elements in the policies being tested. - Identify which specific statement in a policy results in allowing or denying access to a particular resource or action. Hence, the correct answer is IAM Policy Simulator. AWS Config is incorrect because this is just a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Systems Manager is incorrect because this service just provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks across your AWS resources. Unlike IAM Policy Simulator, it can't be used to simulate your policies. Amazon Inspector is incorrect because it is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_iam_policy-sim-path-console.html Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"])</script><script>self.__next_f.push([1,"42:T86d,"])</script><script>self.__next_f.push([1,"Amazon EventBridge (Amazon CloudWatch Events) helps you respond to state changes in your AWS resources. When your resources change state, they automatically send events into an event stream. You can create rules that match selected events in the stream and route them to your AWS Lambda function to take action. For example, you can automatically invoke an AWS Lambda function to log the state of an EC2 instance or AutoScaling Group. You maintain event source mapping in Amazon CloudWatch Events by using a rule target definition. You can also create a Lambda function and direct AWS Lambda to execute it on a regular schedule. You can specify a fixed rate (for example, execute a Lambda function every hour or 15 minutes), or you can specify a Cron expression. Hence, the correct answer is: Integrate Amazon EventBridge (Amazon CloudWatch Events) with Lambda, which will automatically trigger the function every 30 minutes. The option that says: Launch an EC2 instance that has a cron job that triggers the Lambda function every 30 minutes is incorrect because provisioning a new instance incurs additional costs. There is also a possibility that the Lambda function will not be invoked in the event that the instance was stopped or terminated. The option that says: Use the Task Scheduler of your Windows PC to trigger the Lambda function every 30 minutes is incorrect because this setup is difficult to manage due to the fact that you are using your own computer to trigger the function. This may be the most cost-effective solution but it certainly is not the most manageable option. The best way is to integrate CloudWatch Events with Lambda. The option that says: Enable scheduling on the AWS Console of your Lambda function. Define a schedule to run it at 30-minute intervals is incorrect because there is no feature like this in Lambda. References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html Check out these Amazon CloudWatch and AWS Lambda Cheat Sheets: https://tutorialsdojo.com/amazon-cloudwatch/ https://tutorialsdojo.com/aws-lambda/"])</script><script>self.__next_f.push([1,"43:T91b,"])</script><script>self.__next_f.push([1,"A segment can break down the data about the work done into subsegments. Subsegments provide more granular timing information and details about downstream calls that your application made to fulfill the original request. A subsegment can contain additional details about a call to an AWS service, an external HTTP API, or an SQL database. You can even define arbitrary subsegments to instrument specific functions or lines of code in your application. For services that don't send their own segments like Amazon DynamoDB, X-Ray uses subsegments to generate inferred segments and downstream nodes on the service map. This lets you see all of your downstream dependencies, even if they don't support tracing or are external. Subsegments represent your application's view of a downstream call as a client. If the downstream service is also instrumented, the segment that it sends replaces the inferred segment generated from the upstream client's subsegment. The node on the service graph always uses information from the service's segment, if it's available, while the edge between the two nodes uses the upstream service's subsegment. Hence, the correct answer in this scenario is to use subsegments in your segment document. Using inferred segment is incorrect because this is the one generated by subsegments, which lets you see all of your downstream dependencies including the external ones even if they don't support tracing. The more appropriate solution in this scenario is to use subsegments instead. Using metadata is incorrect because this does not record the calls to AWS services and resources that are made by the application. Segments and subsegments can include a metadata object containing one or more fields with values of any type, including objects and arrays. Using annotations is incorrect because just like metadata, this also does not record the application's calls to your AWS services and resources. Segments and subsegments can include an annotations object containing one or more fields that X-Ray indexes for use with filter expressions. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html https://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html#api-segmentdocuments-subsegments Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/"])</script><script>self.__next_f.push([1,"44:Ta44,"])</script><script>self.__next_f.push([1,"You can use the UpdateItem operation to implement an atomic counter — a numeric attribute that is incremented, unconditionally, without interfering with other write requests. (All write requests are applied in the order in which they were received). With an atomic counter, the updates are not idempotent. In other words, the numeric value will increment each time you call UpdateItem. You might use an atomic counter to keep track of the number of visitors to a website. In this case, your application would increment a numeric value, regardless of its current value. If an UpdateItemoperation should fail, the application could simply retry the operation. This would risk updating the counter twice, but you could probably tolerate a slight overcounting or undercounting of website visitors. An atomic counter would not be appropriate where overcounting or undercounting cannot be tolerated (For example, in a banking application). In this case, it is safer to use a conditional update instead of an atomic counter. Hence, using atomic counters to increment the counter item in the DynamDB table for every new visitor is the most suitable solution in this scenario. Using conditional writes to update the counter item in the DynamoDB table and set the ReturnConsumedCapacity parameter to TOTAL is incorrect because using conditional writes is not required for this scenario as the counter doesn't need to be idempotent. Remember that it is indicated that they can tolerate a slight overcounting or undercounting of website visitors. In addition, the ReturnConsumedCapacity parameter simply returns the total number of write capacity units consumed hence, it is irrelevant in this scenario. Using conditional writes to update the counter item in the DynamoDB table only if the item has a unique primary key and the new value is greater than the current value is incorrect because although this is a valid solution, it entails a lot of unnecessary configuration as compared to using an atomic counter. Enabling DynamoDB Streams to track the number of new visitors is incorrect because DynamoDB streams simply captures a time-ordered sequence of item-level modifications in the table. This is not suitable if you want to track the number of website visitors with minimal configuration. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.UpdateExpressions.html#Expressions.UpdateExpressions.SET.IncrementAndDecrement Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"45:Tef8,"])</script><script>self.__next_f.push([1,"The CloudWatchAgentServerPolicy is an AWS-managed policy that grants the Amazon CloudWatch agent the necessary permission to collect and publish metrics from EC2 instances to CloudWatch. This policy allows actions such as cloudwatch:PutMetricData and logs:PutLogEvents, enabling the agent to send custom metrics and log data to CloudWatch. Attaching this policy to an IAM role associated with your EC2 instances ensures that the CloudWatch agent has the required effective permissions. An IAM role is an AWS Identity and Access Management (IAM) entity that defines a set of permissions for making AWS service requests. When you assign an IAM role to an EC2 instance, applications running on that instance can obtain temporary security credentials to interact with other AWS services without the need for embedding long-term credentials. This approach enhances security and simplifies credential management. To assign a role to an EC2 instance, users can create an instance profile that contains the role and then associate it with the instance during or after its launch. By combining the CloudWatchAgentServerPolicy with an appropriately configured IAM role, you enable the CloudWatch agent on your EC2 instances to collect and transmit custom metrics to CloudWatch seamlessly. This setup is essential for monitoring users' applications and infrastructure's performance and health, providing users with actionable insights to maintain optimal operations. Hence, the correct answer is: Attach the CloudWatchAgentServerPolicy policy to the IAM role specified in the EC2 Auto Scaling launch template to ensure proper permissions for the CloudWatch agent. The option that says: Add a user data script in the EC2 Auto Scaling launch template to install and start the CloudWatch agent during instance initialization is incorrect. While a user data script can automate the installation and startup of the CloudWatch agent, it only addresses the agent’s deployment process. This does not resolve the root cause of the issue, which is the lack of necessary IAM permissions for publishing custom metrics to Amazon CloudWatch. Even if the agent is installed and running, it requires the correct IAM role with the CloudWatchAgentServerPolicy policy attached to send metrics successfully. The option that says: Configure the IAM role for the EC2 instances with the CloudWatchAgentReadOnlyAccess policy to allow the CloudWatch agent to read default metrics and publish data is incorrect. The CloudWatchAgentReadOnlyAccess policy just provides permissions to view CloudWatch metrics and logs but does not grant the necessary permissions to publish custom metrics. This policy is intended for scenarios requiring read-only access to CloudWatch data and does not meet the requirements for the CloudWatch agent to send custom metrics to CloudWatch. The option that says: Attach the CloudWatchAgentAdminPolicy IAM policy to the IAM role specified in the EC2 Auto Scaling launch template to provide enhanced permissions for the CloudWatch agent is incorrect. Although the CloudWatchAgentAdminPolicy contains permissions for publishing custom metrics, it primarily provides excessive permissions, including administrative-level access to CloudWatch resources. This violates the principle of least privilege, which recommends granting only the permissions required for a task. References: https://docs.aws.amazon.com/aws-managed-policy/latest/reference/CloudWatchAgentServerPolicy.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html Check out these Amazon EC2,AWS Identity and Access Management ( IAM ) and Amazon CloudWatch Cheat Sheets: https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/ https://tutorialsdojo.com/aws-identity-and-access-management-iam/ https://tutorialsdojo.com/amazon-cloudwatch/"])</script><script>self.__next_f.push([1,"46:Tb9c,"])</script><script>self.__next_f.push([1,"Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. Since fast performance is one of the requirements asked in the scenario, DynamoDB should be an option to consider. In DynamoDB, an item is a collection of attributes. Each attribute has a name and a value. An attribute value can be a scalar, a set, or a document type. DynamoDB provides four operations for basic create/read/update/delete (CRUD) functionality: PutItem – create an item. GetItem – read an item. UpdateItem – update an item. DeleteItem – delete an item. You can use the UpdateItem operation to implement an atomic counter—a numeric attribute that is incremented, unconditionally, without interfering with other write requests. With an atomic counter, the numeric value will increment each time you call UpdateItem. For example, you might use an atomic counter to keep track of the number of visitors to a website. In this case, your application would increment a numeric value, regardless of its current value. If an UpdateItem operation should fail, the application could simply retry the operation. This would risk updating the counter twice, but you could probably tolerate a slight overcounting or undercounting of website visitors. Hence, the correct answer is to setup Amazon DynamoDB for the database and implement atomic counters for the UpdateItem operation of the website counter. Using Amazon RDS for the database and setting up SQL AUTO_INCREMENT on your tables is incorrect because RDS is not scalable enough to handle millions of data being submitted by readers worldwide. Auto-increment allows a unique number to be generated automatically when a new record is inserted into a table. This is often the primary key field that we would like to be created automatically every time a new record is inserted. Since you would not want to add a new database entry for every link click and immediately consume all your storage space, it would be better to use DynamoDB's atomic counter instead. Launching an Amazon Redshift for the database and applying a step count of 1 for the IDENTITY column is incorrect because Redshift is more suited for data warehousing demands that need parallel execution capabilities and columnar storage types. Taking advantage of Amazon Aurora's performance speed and AUTO_INCREMENT feature for item updates is incorrect. Although Aurora is a scalable database service, using the AUTO_INCREMENT feature of SQL does not suit the scenario's requirement. Auto-increment simply allows a unique number to be generated automatically when a new record is inserted into a table. References: https://aws.amazon.com/dynamodb/ https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.AtomicCounters https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_TABLE_NEW.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"47:T945,"])</script><script>self.__next_f.push([1,"In the simplest case, your browser script makes a GET request for a resource from a server in another domain. Depending on the CORS configuration of that server, if the request is from a domain that's authorized to submit GET requests, the cross-origin server responds by returning the requested resource. If either the requesting domain or the type of HTTP request is not authorized, the request is denied. However, CORS makes it possible to preflight the request before actually submitting it. In this case, a preflight request is made in which the OPTIONS access request operation is sent. If the cross-origin server's CORS configuration grants access to the requesting domain, the server sends back a preflight response that lists all the HTTP request types that the requesting domain can make on the requested resource. Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources. To configure your bucket to allow cross-origin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support for each origin, and other operation-specific information. You can add up to 100 rules to the configuration. You add the XML document as the cors subresource to the bucket either programmatically or by using the Amazon S3 console as shown below: Therefore, you can solve the issue in the scenario by simply enabling cross-origin resource sharing (CORS) configuration in the bucket. Enabling cross-account access is incorrect because this is just a feature in IAM that allows you to provide access to your resources to an IAM User in another AWS account. Enabling Cross-Zone Load Balancing is incorrect because this is only used in ELB and not in S3. Enabling Cross-Region Replication (CRR) is incorrect because this is just a bucket-level configuration that enables automatic, asynchronous copying of objects across buckets in different AWS Regions. Reference: http://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/cors.html"])</script><script>self.__next_f.push([1,"48:T5e3,CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can use CodeDeploy. When you deploy to an AWS Lambda compute platform, the deployment configuration specifies the way traffic is shifted to the new Lambda function versions in your application. Hence, the correct answer is AWS CodeDeploy. The option that says: AWS CodeBuild is incorrect. This service is mainly used for compiling source code, running tests, and producing software packages ready for deployment. While it is a crucial part of the CI/CD pipeline, it does not handle the deployment process itself. The option that says: AWS CloudFormation is incorrect because it only turns your infrastructure into code. This service does not deploy applications. The option that says: Amazon Kinesis is incorrect because this is just a data streaming service in AWS. This service does not deploy applications. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/49:T969,"])</script><script>self.__next_f.push([1,"A segment can break down the data about the work done into subsegments. Subsegments provide more granular timing information and details about downstream calls that your application made to fulfill the original request. A subsegment can contain additional details about a call to an AWS service, an external HTTP API, or an SQL database. You can even define arbitrary subsegments to instrument specific functions or lines of code in your application. For services that don't send their own segments like Amazon DynamoDB, X-Ray uses subsegments to generate inferred segments and downstream nodes on the service map. This lets you see all of your downstream dependencies, even if they don't support tracing, or are external. Subsegments represent your application's view of a downstream call as a client. If the downstream service is also instrumented, the segment that it sends replaces the inferred segment generated from the upstream client's subsegment. The node on the service graph always uses information from the service's segment, if it's available, while the edge between the two nodes uses the upstream service's subsegment. Hence, the correct answer in this scenario is to include subsegments in your segment document. Including tracing header is incorrect because this is added in the HTTP request header and not on the segment document. A tracing header (X-Amzn-Trace-Id) can originate from the X-Ray SDK, an AWS service, or the client request. Including metadata is incorrect because this does not record the calls to AWS services and resources that are made by the application. Segments and subsegments can include a metadata object containing one or more fields with values of any type, including objects and arrays. Including annotations is incorrect because just like metadata, this also does not record the application's calls to your AWS services and resources. Segments and subsegments can include an annotations object containing one or more fields that X-Ray indexes for use with filter expressions. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html https://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html#api-segmentdocuments-subsegments Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/ Instrumenting your Application with AWS X-Ray: https://tutorialsdojo.com/instrumenting-your-application-with-aws-x-ray/"])</script><script>self.__next_f.push([1,"4a:T76b,Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token. An identity pool is a store of user identity data specific to your account. Amazon Cognito identity pools enable you to create unique identities and assign permissions for users. Your identity pool can include: - Users in an Amazon Cognito user pool - Users who authenticate with external identity providers such as Facebook, Google, or a SAML-based identity provider - Users authenticated via your own existing authentication process With an identity pool, you can obtain temporary AWS credentials with permissions you define to directly access other AWS services or to access resources through Amazon API Gateway. Hence, the correct answer is: Use AWS Cognito Identity Pools, then enable access to unauthenticated identities. The option that says: Use AWS Cognito User Pools then enabling access to unauthenticated identities is incorrect because a user pool is just a user directory in Amazon Cognito. In addition, this doesn't enable the mobile game access to unauthenticated identities. You have to use an Identity Pool instead. The option that says: Use Amazon Cognito Sync is incorrect because this is just a client library that enables cross-device syncing of application-related user data. The option that says: Use AWS IAM Identity Center is incorrect because this service just makes it easy for you to centrally manage workforce access to multiple AWS accounts. It also does not allow any \"guest\" or unauthenticated access, unlike AWS Cognito. References: https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html https://docs.aws.amazon.com/cognito/latest/developerguide/getting-started-with-identity-pools.html Check out this Amazon Cognito Cheat Sheet: https://tutorialsdojo.com/amazon-cognito/4b:Tba8,"])</script><script>self.__next_f.push([1,"When you encrypt your data, your data is protected, but you have to protect your encryption key. One strategy is to encrypt it. Envelope encryption is the practice of encrypting plaintext data with a data key and then encrypting the data key under another key. You can even encrypt the data encryption key under another encryption key, and encrypt that encryption key under another encryption key. But, eventually, one key must remain in plaintext so you can decrypt the keys and your data. This top-level plaintext key encryption key is known as the root key. AWS KMS helps you protect your encryption keys by storing and managing them securely. Root keys stored in AWS KMS, known as AWS KMS keys, never leave the AWS KMS FIPS validated hardware security modules unencrypted. To use an AWS KMS key, you must call AWS KMS. Envelope encryption offers several benefits: Protecting data keys When you encrypt a data key, you don't have to worry about storing the encrypted data key, because the data key is inherently protected by encryption. You can safely store the encrypted data key alongside the encrypted data. Encrypting the same data under multiple keys Encryption operations can be time-consuming, particularly when the data being encrypted are large objects. Instead of re-encrypting raw data multiple times with different keys, you can re-encrypt only the data keys that protect the raw data. Combining the strengths of multiple algorithms In general, symmetric key algorithms are faster and produce smaller ciphertexts than public-key algorithms, but public-key algorithms provide inherent separation of roles and easier key management. Envelope encryption lets you combine the strengths of each strategy. Therefore, the correct answer is: Encrypt plaintext data with a data key and then encrypt the data key with a top-level plaintext key. The option that says: Encrypt plaintext data with a KMS key and then encrypt the KMS key with a top-level plaintext data key is incorrect because you have to encrypt your plaintext data with a data key and not a KMS key. Moreover, the top-level plaintext key should be the root key and not the data key. The option that says: Encrypt plaintext data with a KMS key and then encrypt the KMS key with a top-level encrypted data key is incorrect because plaintext data should be encrypted with a data key, not a KMS key. Also, the top-level key should be plaintext, not encrypted. The option that says: Encrypt plaintext data with a data key and then encrypt the data key with a top-level encrypted key is incorrect. While it is correct to encrypt plaintext data with a data key, the top-level key (root key) must be kept in plaintext and not be encrypted. References: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping https://docs.aws.amazon.com/kms/latest/developerguide/overview.html Check out this AWS Key Management Service (KMS) Cheat Sheet: https://tutorialsdojo.com/aws-key-management-service-aws-kms/"])</script><script>self.__next_f.push([1,"4c:T9ec,"])</script><script>self.__next_f.push([1,"When a Lambda function runs, AWS Lambda retains its execution context for potential subsequent invocations, allowing it to respond quickly without reinitializing the environment. This retention includes the /tmp directory, a writable temporary storage. Data saved in this directory persists across these retained invocations, making it useful for caching. In the scenario, after translating a product description, the result can be saved in /tmp. If the same translation is requested soon after, the Lambda function can retrieve the cached result instead of re-fetching from the database, improving response time. Hence, the correct answer is: Use the /tmp storage in the Lambda function to cache recently translated product descriptions. The option that says: Store the results of the TranslateText API in an Amazon DynamoDB Accelerator (DAX) cluster is incorrect because DAX is used to provide a fully managed, in-memory caching solution for Amazon DynamoDB, not for caching results from other AWS services like Amazon Translate. DAX is primarily designed to improve the performance of read-heavy DynamoDB workloads by caching data from DynamoDB tables in memory. The option that says: Use AWS Step Functions with a Parallel state to concurrently run multiple instances of the Lambda function for translation is incorrect. The AWS Step Functions Parallel state allows for the concurrent execution of multiple branches within a state machine. In this context, it could mean breaking down a single translation request into multiple smaller tasks, which doesn't make sense for translating a single product description. Such an approach could only complicate the process and potentially introduce additional delays. Additionally, running multiple Lambda instances concurrently can be more expensive. The option that says: Update the Lambda function to use asynchronous invocation. Push the translation requests to an Amazon SQS queue and then process in batches is incorrect. For on-demand translations, users expect translations to be near-instantaneous. Introducing SQS and batch processing would mean that the system would wait to accumulate several translation requests before processing them. This would add delay to the translation response time, potentially worsening the user experience. References: https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html#function-configuration Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"])</script><script>self.__next_f.push([1,"4d:T954,"])</script><script>self.__next_f.push([1,"There are two API calls available for writing records to a Kinesis Data Stream: PutRecord or PutRecords. PutRecord writes a single record to the stream, while PutRecords writes multiple records to the stream in a batch. Kinesis Data Streams attempts to process all records in each PutRecords request. A single record failure does not stop the processing of subsequent records. As a result, PutRecords doesn't guarantee the ordering of records. If you need to read records in the same order they are written to the stream, use PutRecord along with the SequenceNumberForOrdering parameter. Furthermore, to handle duplicates, you can include a unique ID in each record that you write to the stream. The consumer application can then maintain a record of the unique IDs for the records that it has already processed. This record could be stored in an external database, like DynamoDB. When the consumer application receives a record from the stream, it can check its unique ID against the list of unique IDs that it has already processed. If the unique ID has already been processed, the consumer application can skip processing the record to avoid duplicates. Hence, the correct is: Embed a unique ID in each bid record. Use Kinesis PutRecord API to write bids. Assign a timestamp-based value for the SequenceNumberForOrdering parameter. The option that says: Embed a unique ID in each bid record. Use Kinesis PutRecords API to write bids. Assign a timestamp-based value for the PartitionKey parameter is incorrect because PutRecords does not guarantee the ordering of records. Although the SQS FIFO queue guarantees ordering and can prevent duplicates, it's not designed for real-time applications. Hence, the following options are incorrect: - Replace the stream with an SQS FIFO queue and use the SendMessage API to write bids. Provide a unique id in the MessageDeduplicationId parameter for each bid request. - Replace the stream with an SQS FIFO queue and use the SendMessageBatch API to write bids. Provide a unique id in the MessageDeduplicationId parameter for each bid request. References: https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecord.html https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html#kinesis-record-processor-duplicates-producer Check out this Amazon Kinesis Cheat sheet: https://tutorialsdojo.com/amazon-kinesis/"])</script><script>self.__next_f.push([1,"4e:Tb88,"])</script><script>self.__next_f.push([1,"You can grant your IAM users permission to switch to roles within your AWS account or to roles defined in other AWS accounts that you own. Imagine that you have Amazon EC2 instances that are critical to your organization. Instead of directly granting your users permission to terminate the instances, you can create a role with those privileges. Then allow administrators to switch to the role when they need to terminate an instance. Doing this adds the following layers of protection to the instances: - You must explicitly grant your users permission to assume the role. - Your users must actively switch to the role using the AWS Management Console or assume the role using the AWS CLI or AWS API. - You can add multi-factor authentication (MFA) protection to the role so that only users who sign in with an MFA device can assume the role. You can use a role to delegate access to resources that are in different AWS accounts that you own (Production and Testing). You can share resources in one account with users in a different account by setting up cross-account access. In this way, you don't need to create individual IAM users in each account and the users don't have to sign out of one account and sign into another in order to access resources that are in different AWS accounts. Hence, the most efficient answer in this scenario is to: Grant the developer cross-account access to the resources of Accounts B and C. The option that says: Create separate identities and passwords for the developer on both the Test and Production accounts is incorrect because although this is a valid option, it is not an efficient one since the developer will have to log in to each individual AWS account in order to access the resources. A better way to do this is to grant cross-account access. The option that says: Enable AWS multi-factor authentication (MFA) to the IAM User of the developer is incorrect because although this will improve the security, it still doesn't solve the access problem of the developer. This can be used in conjunction with cross-account access, but using this alone will not suffice. The option that says: Set up AWS Organizations and attach a Service Control Policy to the developer to access the other accounts is incorrect because although it will make the multiple AWS accounts of the company more manageable, it doesn't address the access requirement of the developer. Take note that SCPs are necessary, but not sufficient, for granting access for the accounts in your organization. You still need to attach IAM policies to users and roles in your organization's accounts to actually grant permissions to them. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html Check out this AWS Identity \u0026 Access Management (IAM) Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"])</script><script>self.__next_f.push([1,"4f:Taf3,"])</script><script>self.__next_f.push([1,"By default, IAM users don't have permission to create or modify Amazon ECS resources or perform tasks using the Amazon ECS API. This means that they also can't do so using the Amazon ECS console or the AWS CLI. To allow IAM users to create or modify resources and perform tasks, you must create IAM policies. Policies grant IAM users permission to use specific resources and API actions. Then, attach those policies to the IAM users or groups that require those permissions. When you attach a policy to a user or group of users, it allows or denies the users permission to perform the specified tasks on the specified resources. Likewise, Amazon ECS container instances make calls to the Amazon ECS and Amazon EC2 APIs on your behalf, so they need to authenticate with your credentials. This authentication is accomplished by creating an IAM role for your container instances and associating that role with your container instances when you launch them. If you use an Elastic Load Balancing load balancer with your Amazon ECS services, calls to the Amazon EC2 and Elastic Load Balancing APIs are made on your behalf to register and deregister container instances with your load balancers. Hence, the most suitable solution in this scenario is: Create 4 different IAM Roles with the required permissions and attach them to each of the 4 ECS tasks. The option that says: Creating an IAM Group with all the required permissions and attaching them to each of the 4 ECS tasks is incorrect because you cannot directly attach an IAM Group to an ECS Task. Attaching an IAM Role is a more suitable solution in this scenario and not an IAM Group. The option that says: Creating 4 different Container Instance IAM Roles with the required permissions and attaching them to each of the 4 ECS tasks is incorrect because a Container Instance IAM Role only applies if you are using the EC2 launch type. Take note that the scenario says that the application will be using a Fargate launch type. The option that says: Creating 4 different Service-Linked Roles with the required permissions and attaching them to each of the 4 ECS tasks is incorrect because a service-linked role is a unique type of IAM role that is linked directly to Amazon ECS itself, not on the ECS task. Service-linked roles are predefined by Amazon ECS and include all the permissions that the service requires to call other AWS services on your behalf. References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_IAM_role.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html#create_task_iam_policy_and_role https://docs.aws.amazon.com/AmazonECS/latest/developerguide/instance_IAM_role.html Check out this Amazon ECS Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/"])</script><script>self.__next_f.push([1,"5:[\"$\",\"$Le\",null,{\"quiz\":{\"id\":\"aws-developer-9\",\"title\":\"AWS Certified Developer Associate Practice Exams 3\",\"description\":\"Additional practice questions covering AWS development topics.\",\"questions\":[{\"question\":\"A tech company has a real-time traffic monitoring system which uses Amazon Kinesis Data Stream to collect data and a group of EC2 instances that consume and process the data stream. Your development team is responsible for adjusting the number of shards in the data stream to adapt to changes in the rate of data flow. Which of the following are correct regarding Kinesis resharding which your team should consider in managing the application? (Select TWO.)\",\"answers\":[{\"text\":\"You have to split the cold shards to decrease the capacity of the stream.\",\"isCorrect\":false},{\"text\":\"You can decrease the stream's capacity by merging shards.\",\"isCorrect\":true},{\"text\":\"You can increase the stream's capacity by splitting shards.\",\"isCorrect\":true},{\"text\":\"The data records that are flowing to the parent shards will be lost when you reshard.\",\"isCorrect\":false},{\"text\":\"You have to merge the hot shards to increase the capacity of the stream.\",\"isCorrect\":false}],\"explanation\":\"$f\"},{\"question\":\"A developer monitors multiple sensors inside a data center which detects various environmental conditions that may affect their running servers. In the current architecture, the data is initially processed by an AWS Lambda function and then stored in a remote data warehouse. To make the system more durable and scalable, the developer plans to use an Amazon SQS FIFO queue to store the data, which will be polled by the Lambda function. There is a known issue with the sensor devices sending duplicate data intermittently. What action can the developer take to lessen the chances of processing duplicate messages?\",\"answers\":[{\"text\":\"Add a MessageDeduplicationId parameter to the SendMessage API request.\",\"isCorrect\":true},{\"text\":\"Configure the Amazon SQS queue to automatically drop a duplicate message whenever it arrives within the message's VisibilityTimeout.\",\"isCorrect\":false},{\"text\":\"Refactor the Lambda function to store the message's content and drop the incoming messages with similar content within a 5-minute period.\",\"isCorrect\":false},{\"text\":\"Use an Amazon SQS Standard queue instead of a FIFO queue to avoid any duplicate messages.\",\"isCorrect\":false}],\"explanation\":\"$10\"},{\"question\":\"A developer wants to use multi-factor authentication (MFA) to protect programmatic calls to specific AWS API operations like Amazon EC2 StopInstances. He needs to call an API where he can submit the MFA code that is associated with his MFA device. Using the temporary security credentials that are returned from the call, he can then make programmatic calls to API operations that require MFA authentication. Which API should the developer use to properly implement this security feature?\",\"answers\":[{\"text\":\"AssumeRoleWithWebIdentity\",\"isCorrect\":false},{\"text\":\"AssumeRoleWithSAML\",\"isCorrect\":false},{\"text\":\"GetFederationToken\",\"isCorrect\":false},{\"text\":\"GetSessionToken\",\"isCorrect\":true}],\"explanation\":\"$11\"},{\"question\":\"A development team is working on an AWS Serverless Application Model (SAM) application with its source code hosted on GitHub. A newly recruited developer clones the repository and observes that the SAM template contains references to AWS Lambda functions with CodeUri pointing to local file paths. The developer has added a new Lambda function and must redeploy the updated version to Production. Which combination of steps must be taken to satisfy the requirement? (Select Two)\",\"answers\":[{\"text\":\"Execute sam publish to make the application available in the AWS Serverless Application Repository.\",\"isCorrect\":false},{\"text\":\"Use the sam deploy command to deploy the application with a specified CloudFormation stack.\",\"isCorrect\":true},{\"text\":\"Run sam init to initialize a new SAM project.\",\"isCorrect\":false},{\"text\":\"Use the sam sync command to synchronize the local changes to the application in AWS.\",\"isCorrect\":false},{\"text\":\"Execute sam build to resolve dependencies and construct deployment artifacts for all functions and layers in the SAM template.\",\"isCorrect\":true}],\"explanation\":\"$12\"},{\"question\":\"An application is sending thousands of log files to an S3 bucket everyday. The request to retrieve the list of objects using the AWS CLI aws s3api list-objects command is timing out due to the high volume of data being fetched. In order to rectify this issue, you have to use pagination to control the number of results returned on your request. Which of the following parameters should you include in CLI command for this scenario? (Select TWO.)\",\"answers\":[{\"text\":\"--page-size\",\"isCorrect\":true},{\"text\":\"--max-items\",\"isCorrect\":true},{\"text\":\"--exclude\",\"isCorrect\":false},{\"text\":\"--summarize\",\"isCorrect\":false},{\"text\":\"--size-only\",\"isCorrect\":false}],\"explanation\":\"$13\"},{\"question\":\"An application has recently been migrated from an on-premises data center to a development Elastic Beanstalk environment. A developer will do iterative tests and therefore needs to deploy code changes and view them as quickly as possible. Which of the following options take the LEAST amount of time to complete the deployment?\",\"answers\":[{\"text\":\"Rolling with additional batch\",\"isCorrect\":false},{\"text\":\"All at once\",\"isCorrect\":true},{\"text\":\"Immutable\",\"isCorrect\":false},{\"text\":\"Rolling\",\"isCorrect\":false}],\"explanation\":\"$14\"},{\"question\":\"You are developing a serverless application in AWS composed of several Lambda functions and a DynamoDB database. The requirement is to process the requests asynchronously. Which of the following is the MOST suitable way to accomplish this?\",\"answers\":[{\"text\":\"Use the Invoke API to call the Lambda function and set the invocation type request parameter to RequestResponse.\",\"isCorrect\":false},{\"text\":\"Use the InvokeAsync API to call the Lambda function and set the invocation type request parameter to RequestResponse.\",\"isCorrect\":false},{\"text\":\"Use the Invoke API to call the Lambda function and set the invocation type request parameter to Event.\",\"isCorrect\":true},{\"text\":\"Use the InvokeAsync API to call the Lambda function and set the invocation type request parameter to Event.\",\"isCorrect\":false}],\"explanation\":\"$15\"},{\"question\":\"A company is re-architecting its legacy application to use AWS Lambda and DynamoDB. The table is provisioned to have 10 read capacity units, and each item has a size of 4 KB. How many eventual and strong consistent read requests can the table handle per second?\",\"answers\":[{\"text\":\"10 strongly consistent reads and 10 eventually consistent reads per second\",\"isCorrect\":false},{\"text\":\"10 strongly consistent reads and 20 eventually consistent reads per second\",\"isCorrect\":true},{\"text\":\"20 strongly consistent reads and 10 eventually consistent reads per second\",\"isCorrect\":false},{\"text\":\"5 strongly consistent reads and 20 eventually consistent reads per second\",\"isCorrect\":false}],\"explanation\":\"$16\"},{\"question\":\"A company has a central data repository in Amazon S3 that needs to be accessed by developers belonging to different AWS accounts. The required IAM role has been created with the appropriate S3 permissions. Given that the developers mostly interact with S3 via APIs, which API should the developers call to use the IAM role?\",\"answers\":[{\"text\":\"GetSessionToken\",\"isCorrect\":false},{\"text\":\"AssumeRoleWithSAML\",\"isCorrect\":false},{\"text\":\"AssumeRoleWithWebIdentity\",\"isCorrect\":false},{\"text\":\"AssumeRole\",\"isCorrect\":true}],\"explanation\":\"$17\"},{\"question\":\"A developer is preparing the application specification (AppSpec) file in CodeDeploy, which will be used to deploy her Lambda functions to AWS. In the deployment, she needs to configure CodeDeploy to run a task before the traffic is shifted to the deployed Lambda function version. Which deployment lifecycle event should she configure in this scenario?\",\"answers\":[{\"text\":\"BeforeInstall\",\"isCorrect\":false},{\"text\":\"BeforeAllowTraffic\",\"isCorrect\":true},{\"text\":\"Start\",\"isCorrect\":false},{\"text\":\"Install\",\"isCorrect\":false}],\"explanation\":\"$18\"},{\"question\":\"A company has a global multi-player game with a multi-master DynamoDB database topology which stores data in multiple AWS regions. You were assigned to develop a real-time data analytics application which will track and store the recent changes on all the tables from various regions. Only the new data of the recently updated item is needed to be tracked by your application. Which of the following is the MOST suitable way to configure the data analytics application to detect and retrieve the updated database entries automatically?\",\"answers\":[{\"text\":\"Enable DynamoDB Streams and set the value of StreamViewType to NEW_AND_OLD_IMAGE. Use Kinesis Adapter in the application to consume streams from DynamoDB.\",\"isCorrect\":false},{\"text\":\"Enable DynamoDB Streams and set the value of StreamViewType to NEW_IMAGE. Use Kinesis Adapter in the application to consume streams from DynamoDB.\",\"isCorrect\":true},{\"text\":\"Enable DynamoDB Streams and set the value of StreamViewType to NEW_IMAGE. Create a trigger in AWS Lambda to capture stream data and forward it to your application.\",\"isCorrect\":false},{\"text\":\"Enable DynamoDB Streams and set the value of StreamViewType to NEW_AND_OLD_IMAGE. Create a trigger in AWS Lambda to capture stream data and forward it to your application.\",\"isCorrect\":false}],\"explanation\":\"$19\"},{\"question\":\"A prototype application is hosted in an EC2 instance, which has an assigned IAM Role to store data from both the development and production S3 buckets. The instance also has AWS CLI access/secret key installed to handle other ad hoc tasks. You assigned a new IAM Role to the instance which has the permission to access the development bucket only. However, upon testing, the instance can still store files to both buckets. What is the MOST likely root cause of this issue?\",\"answers\":[{\"text\":\"Due to eventual consistency, you must wait 24 hours for the change to appear across all of AWS.\",\"isCorrect\":false},{\"text\":\"The new IAM Role has an attached inline policy.\",\"isCorrect\":false},{\"text\":\"The instance profile role of a running EC2 instance is static and can't be replaced at all.\",\"isCorrect\":false},{\"text\":\"The application is still using the IAM role that is configured for the AWS CLI key.\",\"isCorrect\":true}],\"explanation\":\"$1a\"},{\"question\":\"A serverless application, which is composed of multiple Lambda functions, has been deployed using AWS SAM. A developer was instructed to easily manage the deployments of the functions using CodeDeploy. When there is a new deployment, 10 percent of the incoming traffic should be shifted to the new version every 10 minutes until all traffic is shifted from the old version. What should the developer do to properly deploy the functions that satisfies this requirement?\",\"answers\":[{\"text\":\"Deploy the functions using a Linear deployment configuration.\",\"isCorrect\":true},{\"text\":\"Deploy the functions using a Canary deployment configuration.\",\"isCorrect\":false},{\"text\":\"Deploy the functions using an All-at-once deployment configuration.\",\"isCorrect\":false},{\"text\":\"Deploy the functions using an Immutable deployment configuration.\",\"isCorrect\":false}],\"explanation\":\"$1b\"},{\"question\":\"A developer is building the cloud architecture of an application which will be hosted in a large EC2 instance. The application will process the data and it will upload results to an S3 bucket. Which of the following is the SAFEST way to implement this architecture?\",\"answers\":[{\"text\":\"Use an IAM Inline Policy to grant the application the necessary permissions to upload data to S3.\",\"isCorrect\":false},{\"text\":\"Use an IAM Role to grant the application the necessary permissions to upload data to S3.\",\"isCorrect\":true},{\"text\":\"Install the AWS CLI then use it to upload the results to S3.\",\"isCorrect\":false},{\"text\":\"Store the access keys in the instance then use the AWS SDK to upload the results to S3.\",\"isCorrect\":false}],\"explanation\":\"$1c\"},{\"question\":\"A company is heavily using a range of AWS services to host their enterprise applications. Currently, their deployment process still has a lot of manual steps which is why they plan to automate their software delivery process using continuous integration and delivery (CI/CD) pipelines in AWS. They will use CodePipeline to orchestrate each step of their release process and CodeDeploy for deploying applications to various compute platforms in AWS. In this architecture, which of the following are valid considerations when using CodeDeploy? (Select TWO.)\",\"answers\":[{\"text\":\"CodeDeploy can deploy applications to both your EC2 instances as well as your on-premises servers.\",\"isCorrect\":true},{\"text\":\"The CodeDeploy agent communicates using HTTP over port 80.\",\"isCorrect\":false},{\"text\":\"AWS Lambda compute platform deployments cannot use an in-place deployment type.\",\"isCorrect\":true},{\"text\":\"CodeDeploy can deploy applications to EC2, AWS Lambda, and Amazon ECS only.\",\"isCorrect\":false},{\"text\":\"You have to install and use the CodeDeploy agent installed on your EC2 instances and ECS cluster.\",\"isCorrect\":false}],\"explanation\":\"$1d\"},{\"question\":\"Due to the popularity of serverless computing, your manager instructed you to share your technical expertise to the whole software development department of your company. You are planning to deploy a simple Node.js 'Hello World' Lambda function to AWS using CloudFormation. Which of the following is the EASIEST way of deploying the function to AWS?\",\"answers\":[{\"text\":\"Include your function source inline in the Code parameter of the AWS::Lambda::Function resource in the CloudFormation template.\",\"isCorrect\":false},{\"text\":\"Include your function source inline in the ZipFile parameter of the AWS::Lambda::Function resource in the CloudFormation template.\",\"isCorrect\":true},{\"text\":\"Upload the code in S3 then specify the S3Key and S3Bucket parameters under the AWS::Lambda::Function resource in the CloudFormation template.\",\"isCorrect\":false},{\"text\":\"Upload the code in S3 as a ZIP file then specify the S3 path in the ZipFile parameter of the AWS::Lambda::Function resource in the CloudFormation template.\",\"isCorrect\":false}],\"explanation\":\"$1e\"},{\"question\":\"A leading commercial bank has an online banking portal that is hosted in an Auto Scaling group of EC2 instances with an Application Load Balancer in front to distribute the incoming traffic. The application has been instrumented, and the X-Ray daemon has been installed in all instances to allow debugging and troubleshooting using AWS X-Ray. In this architecture, from which source will AWS X-Ray fetch the client IP address?\",\"answers\":[{\"text\":\"From the X-Forwarded-Host header of the request.\",\"isCorrect\":false},{\"text\":\"From the source IP of the IP packet.\",\"isCorrect\":false},{\"text\":\"From the X-Forwarded-For header of the request.\",\"isCorrect\":true},{\"text\":\"From the ipAddress query parameter of the request if it exists.\",\"isCorrect\":false}],\"explanation\":\"$1f\"},{\"question\":\"A company has a website hosted in a multicontainer Docker environment in Elastic Beanstalk. There is a requirement to integrate the website with API Gateway, where it simply passes client-submitted method requests to the backend. It is important that the client and backend interact directly with no intervention from API Gateway after the API method is set up, except for known issues such as unsupported characters. Which of the following integration types is the MOST suitable one to use to meet this requirement?\",\"answers\":[{\"text\":\"AWS\",\"isCorrect\":false},{\"text\":\"HTTP_PROXY\",\"isCorrect\":true},{\"text\":\"HTTP\",\"isCorrect\":false},{\"text\":\"AWS_PROXY\",\"isCorrect\":false}],\"explanation\":\"$20\"},{\"question\":\"You are developing a serverless application in AWS in which you have to control the code execution performance and costs of your Lambda functions. There is a requirement to increase the CPU available to your function in order to efficiently process records from an Amazon Kinesis data stream. Which of the following is the BEST way to meet this requirement?\",\"answers\":[{\"text\":\"Use Lambda@Edge.\",\"isCorrect\":false},{\"text\":\"Configure the function to use unreserved account concurrency.\",\"isCorrect\":false},{\"text\":\"Increase the concurrent execution limit of the function.\",\"isCorrect\":false},{\"text\":\"Increase the allocated memory of the function.\",\"isCorrect\":true}],\"explanation\":\"$21\"},{\"question\":\"A developer has instrumented an application using the X-Ray SDK to collect all data about the requests that an application serves. There is a new requirement to develop a custom debug tool which will enable them to view the full traces of their application without using the X-Ray console. What should the developer do to accomplish this task?\",\"answers\":[{\"text\":\"Use the BatchGetTraces API to get the list of trace IDs of the application and then retrieve the list of traces using GetTraceSummaries API.\",\"isCorrect\":false},{\"text\":\"Use the GetServiceGraph API to get the list of trace IDs of the application and then retrieve the list of traces using GetTraceSummaries API.\",\"isCorrect\":false},{\"text\":\"Use the GetTraceSummaries API to get the list of trace IDs of the application and then retrieve the list of traces using BatchGetTraces API.\",\"isCorrect\":true},{\"text\":\"Use the GetGroup API to get the list of trace IDs of the application and then retrieve the list of traces using BatchGetTraces API.\",\"isCorrect\":false}],\"explanation\":\"$22\"},{\"question\":\"A Java web application built using AWS SDK for Java with a DynamoDB database is concurrently accessed by thousands of users during peak time. The application is highly write-intensive and there are a lot of incidents where it overwrites stale data from the DynamoDB table. How can you ensure your database writes are protected from being overwritten by other write operations that are occurring at the same time without affecting the application performance?\",\"answers\":[{\"text\":\"Implement optimistic locking with version number.\",\"isCorrect\":true},{\"text\":\"Implement overly optimistic locking (OOL).\",\"isCorrect\":false},{\"text\":\"Implement pessimistic locking with write locking.\",\"isCorrect\":false},{\"text\":\"Implement pessimistic locking with read locking.\",\"isCorrect\":false}],\"explanation\":\"$23\"},{\"question\":\"A company has an AWS account with only 2 Lambda functions, which process data and store the results in an S3 bucket. An Application Load Balancer is used to distribute the incoming traffic to the two Lambda functions as registered targets. You noticed that in peak times, the first Lambda function works with optimal performance but the second one is throttling the incoming requests. Which of the following is the MOST likely root cause of this issue?\",\"answers\":[{\"text\":\"The first function is using the unreserved account concurrency while the second function has been set with a concurrency execution limit of 1000.\",\"isCorrect\":false},{\"text\":\"The concurrency execution limit provided to the first function is significantly higher than the second function.\",\"isCorrect\":true},{\"text\":\"The concurrency execution limit provided to the first function is less than the second function.\",\"isCorrect\":false},{\"text\":\"The first function is using the unreserved account concurrency while the second function has been set with a concurrency execution limit of 800.\",\"isCorrect\":false}],\"explanation\":\"$24\"},{\"question\":\"An online forum requires a new table in DynamoDB named Thread in which the partition key is ForumName and the sort key is Subject. The following diagram shows how the items in the table would be organized: For reporting purposes, the application needs to find all of the threads that have been posted in a particular forum within the last three months. Which of the following is the MOST effective solution that you should implement?\",\"answers\":[{\"text\":\"Create a global secondary index and use the Query operation to utilize the LastPostDateTime attribute as the sort key.\",\"isCorrect\":false},{\"text\":\"Add a local secondary index while creating the new Thread table. Use the Query operation to utilize the LastPostDateTime attribute as the sort key.\",\"isCorrect\":true},{\"text\":\"Configure the application to Scan the entire Thread table and discard any posts that were not within the specified time frame.\",\"isCorrect\":false},{\"text\":\"Configure the application to Query the entire Thread table and discard any posts that were not within the specified time frame.\",\"isCorrect\":false}],\"explanation\":\"$25\"},{\"question\":\"An application is hosted in Elastic Beanstalk, which is currently running in Java 7 runtime environment. A new version of the application is ready to be deployed, and the developer was tasked to upgrade the platform to Java 8 to accommodate the changes. All user traffic must be immediately directed to the new version. If problems arise, the developer should be able to quickly revert to the previous version. Which of the following is the MOST appropriate action that the developer should do to upgrade the platform?\",\"answers\":[{\"text\":\"Update the environment's platform version to Java 8.\",\"isCorrect\":false},{\"text\":\"Perform a Traffic splitting deployment.\",\"isCorrect\":false},{\"text\":\"Manually upgrade the Java runtime environment of the EC2 instances in the Elastic Beanstalk environment.\",\"isCorrect\":false},{\"text\":\"Perform a Blue/Green Deployment.\",\"isCorrect\":true}],\"explanation\":\"$26\"},{\"question\":\"You have an application that reads an individual item from a DynamoDB table, modifies it locally, and submits the changes as a new entry to a separate table before proceeding onto the next item. The process is repeated for the next 100 entries, and it consumes a lot of time performing this entire process. Which strategy can be applied to your application in order to shorten the time needed to process all the necessary entries with MINIMAL configuration?\",\"answers\":[{\"text\":\"Use DynamoDB conditional writes.\",\"isCorrect\":false},{\"text\":\"Deploy your application into a cluster of EC2 instances.\",\"isCorrect\":false},{\"text\":\"Use DynamoDB's BatchGetItem and BatchWriteItem API operations.\",\"isCorrect\":true},{\"text\":\"Modify your application to use multithreading.\",\"isCorrect\":false}],\"explanation\":\"$27\"},{\"question\":\"A serverless application is using API Gateway with a non-proxy Lambda Integration. A developer was tasked to expose a GET method on a new /getcourses resource to invoke the Lambda function, which will allow the consumers to fetch a list of online courses in JSON format. The consumers must include a query string parameter named courseType in their request to get the data. What is the MOST efficient solution that the developer should do to accomplish this requirement?\",\"answers\":[{\"text\":\"Configure the method response of the resource.\",\"isCorrect\":false},{\"text\":\"Configure the integration request of the resource.\",\"isCorrect\":false},{\"text\":\"Configure the integration response of the resource.\",\"isCorrect\":false},{\"text\":\"Configure the method request of the resource.\",\"isCorrect\":true}],\"explanation\":\"$28\"},{\"question\":\"You recently deployed an application to a newly created AWS account, which uses two identical Lambda functions to process ad-hoc requests. The first function processes incoming requests efficiently but the second one has a longer processing time even though both of the functions have exactly the same code. Based on your monitoring, the Throttles metric of the second function is greater than the first one in Amazon CloudWatch. Which of the following are possible solutions that you can implement to fix this issue? (Select TWO.)\",\"answers\":[{\"text\":\"Set the concurrency execution limit of the second function to 0.\",\"isCorrect\":false},{\"text\":\"Configure the second function to use an unreserved account concurrency.\",\"isCorrect\":false},{\"text\":\"Set the concurrency execution limit of both functions to 500.\",\"isCorrect\":false},{\"text\":\"Decrease the concurrency execution limit of the first function.\",\"isCorrect\":true},{\"text\":\"Set the concurrency execution limit of both functions to 450.\",\"isCorrect\":true}],\"explanation\":\"$29\"},{\"question\":\"A developer is building an image processing utility using an AWS Lambda function. The function processes images in parallel using multiple threads to optimize performance. The images are stored in an Amazon S3 bucket and retrieved for processing. However, the function is not performing as efficiently as expected, with the processing time taking longer than anticipated, even when handling relatively small images. Which action should the developer modify to achieve better performance in the AWS Lambda function?\",\"answers\":[{\"text\":\"Increase the timeout setting of the Lambda function.\",\"isCorrect\":false},{\"text\":\"Use AWS Step Functions to split tasks into smaller workflows.\",\"isCorrect\":false},{\"text\":\"Optimize memory allocation for the Lambda function.\",\"isCorrect\":true},{\"text\":\"Utilize Amazon S3 Transfer Acceleration for image uploads.\",\"isCorrect\":false}],\"explanation\":\"$2a\"},{\"question\":\"Your team is developing a new feature on your application which is already hosted in Elastic Beanstalk. After several weeks, the new version of the application is ready to be deployed and you were instructed to handle the deployment. What is the correct way to deploy the new version to Elastic Beanstalk via the CLI?\",\"answers\":[{\"text\":\"Package your application as a zip file and deploy it using the aws elasticbeanstalk update-application command.\",\"isCorrect\":false},{\"text\":\"Package your application as a tar file and deploy it using the eb deploy command.\",\"isCorrect\":false},{\"text\":\"Package your application as a zip file and deploy it using the eb deploy command.\",\"isCorrect\":true},{\"text\":\"Package your application as a tar file and deploy it using the aws elasticbeanstalk update-application command.\",\"isCorrect\":false}],\"explanation\":\"$2b\"},{\"question\":\"Your manager assigned you a task of implementing server-side encryption with customer-provided encryption keys (SSE-C) to your S3 bucket, which will allow you to set your own encryption keys. Amazon S3 will manage both the encryption and decryption process using your key when you access your objects, which will remove the burden of maintaining any code to perform data encryption and decryption. To properly upload data to this bucket, which of the following headers must be included in your request?\",\"answers\":[{\"text\":\"x-amz-server-side-encryption-customer-key header only\",\"isCorrect\":false},{\"text\":\"x-amz-server-side​-encryption​-customer-algorithm, x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers\",\"isCorrect\":true},{\"text\":\"x-amz-server-side-encryption and x-amz-server-side-encryption-aws-kms-key-id headers\",\"isCorrect\":false},{\"text\":\"x-amz-server-side-encryption, x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers\",\"isCorrect\":false}],\"explanation\":\"$2c\"},{\"question\":\"A social media application is using DynamoDB to manage and store the session data of its users. As the number of users grew, the number of items in the table exponentially increased as well. You have to reduce storage usage and also reduce the cost of storing irrelevant data without using provisioned throughput to rectify this issue. Which of the following is the MOST cost-effective solution that you should implement?\",\"answers\":[{\"text\":\"Turn on Time To Live (TTL) in the table.\",\"isCorrect\":true},{\"text\":\"Implement a Lazy Loading caching strategy to your application.\",\"isCorrect\":false},{\"text\":\"Use a Lambda function with CloudWatch Events to schedule a purge of stale items in the table on a daily basis.\",\"isCorrect\":false},{\"text\":\"Implement a Write-Through caching strategy in your application.\",\"isCorrect\":false}],\"explanation\":\"$2d\"},{\"question\":\"A write-heavy data analytics application is using DynamoDB database which has global secondary index. Whenever the application is performing heavy write activities on the table, the DynamoDB requests return a ProvisionedThroughputExceededException. Which of the following is the MOST likely cause of this issue?\",\"answers\":[{\"text\":\"The provisioned write capacity for the global secondary index is greater than the write capacity of the base table.\",\"isCorrect\":false},{\"text\":\"The provisioned write capacity for the global secondary index is less than the write capacity of the base table.\",\"isCorrect\":true},{\"text\":\"The provisioned throughput exceeds the current throughput limit for your account.\",\"isCorrect\":false},{\"text\":\"The rate of requests exceeds the allowed throughput.\",\"isCorrect\":false}],\"explanation\":\"$2e\"},{\"question\":\"An organization has a serverless application using AWS Lambda, Amazon API Gateway. Recently, the DevOps team discovered that the IAM roles associated with the Lambda functions had been manually modified. The organization must identify these unauthorized changes and ensure all resources are in sync with the CloudFormation stack. Which solution will help the company identify these changes?\",\"answers\":[{\"text\":\"Run a drift detection check on the CloudFormation stack.\",\"isCorrect\":true},{\"text\":\"Analyze CloudWatch Logs to identify changes to the IAM role permissions.\",\"isCorrect\":false},{\"text\":\"Use AWS Config to monitor updates made to the Lambda functions and IAM roles.\",\"isCorrect\":false},{\"text\":\"Review CloudTrail logs to trace IAM role updates for the Lambda functions.\",\"isCorrect\":false}],\"explanation\":\"$2f\"},{\"question\":\"A developer is designing the cloud architecture of an internal application which will be used by about a hundred employees. She needs to ensure that the architecture is elastic enough to adequately match the supply of resources to the demand while maintaining its cost-effectiveness. Which of the following services can provide the MOST elasticity to the architecture? (Select TWO.)\",\"answers\":[{\"text\":\"Amazon RDS\",\"isCorrect\":false},{\"text\":\"Amazon EC2 Spot Fleet\",\"isCorrect\":true},{\"text\":\"Amazon DynamoDB\",\"isCorrect\":true},{\"text\":\"AWS WAF\",\"isCorrect\":false},{\"text\":\"Amazon CloudFront\",\"isCorrect\":false}],\"explanation\":\"$30\"},{\"question\":\"You are planning to create a DynamoDB table for your employee profile website. This will be used by the Human Resources department to easily view details about each employee. When choosing the partition key of the table, which of the following is the BEST attribute to use?\",\"answers\":[{\"text\":\"department_id since employees will fall in these departments.\",\"isCorrect\":false},{\"text\":\"employee_id because each employee ID is unique.\",\"isCorrect\":true},{\"text\":\"employee_name because this will speed up searching of records.\",\"isCorrect\":false},{\"text\":\"position_id because this will help sort the records per department.\",\"isCorrect\":false}],\"explanation\":\"$31\"},{\"question\":\"A company is deploying the package of its Lambda function, which is compressed as a ZIP file, to AWS. However, they are getting an error in the deployment process because the package is too large. The manager instructed the developer to keep the deployment package small to make the development process much easier and more modularized. This should also help prevent errors that may occur when dependencies are installed and packaged with the function code. Which of the following options is the MOST suitable solution that the developer should implement?\",\"answers\":[{\"text\":\"Zip the deployment package again to further compress the zip file.\",\"isCorrect\":false},{\"text\":\"Upload the deployment package to S3.\",\"isCorrect\":false},{\"text\":\"Compress the deployment package as TAR file instead.\",\"isCorrect\":false},{\"text\":\"Upload the other dependencies of your function as a separate Lambda Layer instead.\",\"isCorrect\":true}],\"explanation\":\"$32\"},{\"question\":\"A data analytics company has installed sensors to track the number of people that goes to the mall. The data sets are collected in real-time by an Amazon Kinesis Data Stream which has a consumer that is configured to process data every other day and store the results to S3. Your team noticed that your S3 bucket is only receiving half of the data that is being sent to the Kinesis stream but after checking, you have verified that the sensors are properly sending the data to Amazon Kinesis in real-time without any issues. Which of the following is the MOST likely root cause of this issue?\",\"answers\":[{\"text\":\"The Amazon Kinesis Data Stream automatically deletes duplicate data.\",\"isCorrect\":false},{\"text\":\"The Amazon Kinesis Data Stream has too many open shards.\",\"isCorrect\":false},{\"text\":\"The sensors are having intermittent connection issues.\",\"isCorrect\":false},{\"text\":\"By default, the data records are only accessible for 24 hours from the time they are added to a Kinesis stream.\",\"isCorrect\":true}],\"explanation\":\"$33\"},{\"question\":\"A serverless application consisting of a Lambda function and a DynamoDB database is used to process Amazon S3 events. The Lambda function takes an average of three seconds to process the data and Amazon S3 publishes 10 events per second. What is the concurrent execution that the function will have?\",\"answers\":[{\"text\":\"13\",\"isCorrect\":false},{\"text\":\"30\",\"isCorrect\":true},{\"text\":\"3\",\"isCorrect\":false},{\"text\":\"10\",\"isCorrect\":false}],\"explanation\":\"$34\"},{\"question\":\"A developer is instrumenting an application that will be hosted in a large On-Demand EC2 instance in AWS. All of the downstream calls invoked by the application must be traced properly, including the AWS SDK calls. A user-defined data should also be present to expedite the troubleshooting process. Which of the following are valid considerations in AWS X-Ray that the developer should follow? (Select TWO.)\",\"answers\":[{\"text\":\"Set the namespace subsegment field to remote for AWS SDK calls and aws for other downstream calls.\",\"isCorrect\":false},{\"text\":\"Set the namespace subsegment field to aws for AWS SDK calls and remote for other downstream calls.\",\"isCorrect\":true},{\"text\":\"Set the metadata object with key-value pairs that you want X-Ray to index for search.\",\"isCorrect\":false},{\"text\":\"Set the annotations object with any additional custom data that you want to store in the segment.\",\"isCorrect\":false},{\"text\":\"Set the metadata object with any additional custom data that you want to store in the segment.\",\"isCorrect\":true}],\"explanation\":\"$35\"},{\"question\":\"A developer is using API Gateway Lambda Authorizer to provide authentication for every API request and control access to your API. The requirement is to implement an authentication strategy which is similar to OAuth or SAML. Which of the following is the MOST suitable method that the developer should use in this scenario?\",\"answers\":[{\"text\":\"Request Parameter-based Authorization\",\"isCorrect\":false},{\"text\":\"Cross-Account Lambda Authorizer\",\"isCorrect\":false},{\"text\":\"AWS STS-based Authentication\",\"isCorrect\":false},{\"text\":\"Token-based Authorization\",\"isCorrect\":true}],\"explanation\":\"$36\"},{\"question\":\"A Docker application hosted on an ECS cluster has encountered intermittent unavailability issues and timeouts. The lead DevOps engineer instructed you to instrument the application to detect where high latencies are occurring and to determine the specific services and paths impacting application performance. Which of the following steps should you take to accomplish this task properly? (Select TWO.)\",\"answers\":[{\"text\":\"Configure the port mappings and network mode settings in the container agent to allow traffic on TCP port 2000.\",\"isCorrect\":false},{\"text\":\"Add the xray-daemon.config configuration file in your Docker image.\",\"isCorrect\":false},{\"text\":\"Configure the port mappings and network mode settings in your task definition file to allow traffic on UDP port 2000.\",\"isCorrect\":true},{\"text\":\"Manually install the X-Ray daemon to the instances via a user data script.\",\"isCorrect\":false},{\"text\":\"Create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster.\",\"isCorrect\":true}],\"explanation\":\"$37\"},{\"question\":\"An application in your development account is running in an AWS Elastic Beanstalk environment which has an attached Amazon RDS database. You noticed that if you terminate the environment, it also brings down the database which hinders you from performing seamless updates with blue-green deployments. This also poses a critical security risk if the company decides to deploy the application in production. In this scenario, how can you decouple your database instance from your environment without having any data loss?\",\"answers\":[{\"text\":\"Use the blue / green deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance and delete the old environment.\",\"isCorrect\":false},{\"text\":\"Use the blue / green deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance. Before terminating the old Elastic Beanstalk environment, remove its security group rule first before proceeding.\",\"isCorrect\":true},{\"text\":\"Use a Canary deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and then create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance.\",\"isCorrect\":false},{\"text\":\"Use a Canary deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance and delete the old environment.\",\"isCorrect\":false}],\"explanation\":\"$38\"},{\"question\":\"A company has 5 different applications running on several On-Demand EC2 instances. The DevOps team is required to set up a graphical representation of the key performance metrics for each application. These system metrics must be available on a single shared screen for more effective and visible monitoring. Which of the following should the DevOps team do to satisfy this requirement using Amazon CloudWatch?\",\"answers\":[{\"text\":\"Set up a custom CloudWatch namespace with a unique metric name for each application.\",\"isCorrect\":true},{\"text\":\"Set up a custom CloudWatch Event with a unique metric name for each application.\",\"isCorrect\":false},{\"text\":\"Set up a custom CloudWatch Alarm with a unique metric name for each application.\",\"isCorrect\":false},{\"text\":\"Set up a custom CloudWatch dimension with a unique metric name for each application.\",\"isCorrect\":false}],\"explanation\":\"$39\"},{\"question\":\"An online magazine is deployed in AWS and uses an Application Load Balancer, an Auto Scaling group of EC2 instances, and an RDS MySQL Database. Some of the readers are complaining about the website's sluggish performance when loading the articles. Upon checking, there is a high number of read operations in the database, which affects the website's performance. Which of the following actions should you take to resolve the issue with minimal code change?\",\"answers\":[{\"text\":\"Set up a multi-AZ deployments configuration in RDS.\",\"isCorrect\":false},{\"text\":\"Launch a large ElastiCache Cluster as a database cache for RDS and apply the required code change.\",\"isCorrect\":false},{\"text\":\"Upgrade the EC2 instances to a higher instance type.\",\"isCorrect\":false},{\"text\":\"Create an RDS Read Replica instance and configure the application to use this for read queries.\",\"isCorrect\":true}],\"explanation\":\"$3a\"},{\"question\":\"Using AWS SAM, a developer recently deployed a serverless application consisting of Lambda functions, API Gateway, Kinesis Data stream, and a DynamoDB table. The application has worked fine for a few days, but lately, there were a lot of ProvisionedThroughputExceeded exceptions being returned by DynamoDB. The developer also noticed that there's a sudden increase in read capacity units (RCU) usage whenever this issue happens. How should the developer refactor the application to find items based on primary key values and use the LEAST amount of RCU?\",\"answers\":[{\"text\":\"Use the Scan operation with eventual consistency reads.\",\"isCorrect\":false},{\"text\":\"Use the Query operation with eventual consistency reads.\",\"isCorrect\":true},{\"text\":\"Use the Query operation with strong consistency reads.\",\"isCorrect\":false},{\"text\":\"Use the Scan operation with strong consistency reads.\",\"isCorrect\":false}],\"explanation\":\"$3b\"},{\"question\":\"A developer needs to configure the environment name, solution stack, and environment links of his application environment which will be hosted in Elastic Beanstalk. Which configuration file should the developer add in the source bundle to meet the above requirement?\",\"answers\":[{\"text\":\"env.yaml\",\"isCorrect\":true},{\"text\":\"cron.yaml\",\"isCorrect\":false},{\"text\":\"Dockerrun.aws.json\",\"isCorrect\":false},{\"text\":\"env.config\",\"isCorrect\":false}],\"explanation\":\"$3c\"},{\"question\":\"A developer has a set of EC2 instances that runs the Amazon Kinesis Client Library to process a data stream in AWS. Based on the custom metrics, it shows that the instances are maxing out their CPU Utilization, and there are insufficient Kinesis shards to handle the rate of data flowing through the stream. Which of the following is the BEST course of action that the developer should take to solve this issue and prevent this situation from re-occurring in the future?\",\"answers\":[{\"text\":\"Increase the number of instances up to the number of open shards.\",\"isCorrect\":false},{\"text\":\"Increase the instance size to a larger type.\",\"isCorrect\":false},{\"text\":\"Increase both the instance size and the number of open shards.\",\"isCorrect\":true},{\"text\":\"Increase the number of shards.\",\"isCorrect\":false}],\"explanation\":\"$3d\"},{\"question\":\"A developer is building a prototype microservices that are running as tasks in an Amazon ECS Cluster. His manager instructed him to define a task placement strategy which needs to be both cost and resource efficient. The task placement should minimize the number of instances in use which will keep the cost down since high availability is not much of a concern for this prototype. What should the developer implement to meet the above requirements?\",\"answers\":[{\"text\":\"Distribute tasks evenly across Availability Zones, and then re-distribute the tasks among EC2 instances based on the least available amount of CPU/memory within each Availability Zone.\",\"isCorrect\":false},{\"text\":\"Distribute tasks evenly across all available EC2 instances using the spread task placement strategy.\",\"isCorrect\":false},{\"text\":\"Distribute tasks among all registered EC2 instances based on the least available amount of CPU or memory using the binpack task placement strategy.\",\"isCorrect\":true},{\"text\":\"Place tasks randomly using the random task placement strategy.\",\"isCorrect\":false}],\"explanation\":\"$3e\"},{\"question\":\"A company has an AWS Amplify application, relying on Amazon Cognito for user authentication. Multi-factor authentication (MFA) is disabled for their User Pool. There has been a recent data breach in a popular website. The company is worried that attackers might exploit compromised email addresses and passwords to sign into their applications. For this reason, they want to enforce MFA only on users with suspicious login attempts. How can the company satisfy these requirements?\",\"answers\":[{\"text\":\"Enable the Time-based one-time password (TOTP) software token MFA for the User Pool\",\"isCorrect\":false},{\"text\":\"Recreate the User Pool and enable SMS text message MFA.\",\"isCorrect\":false},{\"text\":\"Create a subscription filter Lambda function that monitors for the CompromisedCredentialRisk metric from Advanced Security Metrics in CloudWatch Logs and triggers MFA when detected\",\"isCorrect\":false},{\"text\":\"Enable Adaptive Authentication for the User Pool\",\"isCorrect\":true}],\"explanation\":\"$3f\"},{\"question\":\"You are developing an application that will use a Lambda function, which will be invoked asynchronously. The application will be implemented with exponential back-off that will handle failures so that the requests will be retried twice before the event is discarded. If the retries fail with an unexpected error, you have to direct unprocessed events to another service which will analyze the failure. Which of the following is the MOST suitable component that you should implement in the application architecture to meet the above requirement?\",\"answers\":[{\"text\":\"Delay Queue\",\"isCorrect\":false},{\"text\":\"FIFO Queue\",\"isCorrect\":false},{\"text\":\"Dead Letter Queue\",\"isCorrect\":true},{\"text\":\"Amazon MQ\",\"isCorrect\":false}],\"explanation\":\"$40\"},{\"question\":\"A company is using AWS Organizations to manage its multiple AWS accounts which is being used by its various departments. To avoid security issues, it is of utmost importance to test the impact of service control policies (SCPs) on your IAM policies and resource policies before applying them. Which of the following services can you use to test and troubleshoot IAM and resource-based policies?\",\"answers\":[{\"text\":\"Systems Manager\",\"isCorrect\":false},{\"text\":\"AWS Config\",\"isCorrect\":false},{\"text\":\"Amazon Inspector\",\"isCorrect\":false},{\"text\":\"IAM Policy Simulator\",\"isCorrect\":true}],\"explanation\":\"$41\"},{\"question\":\"A company has developed a Lambda function that will send status updates to a third-party provider for analytics. You need to schedule this function to run every 30 minutes. Which of the following is the MOST manageable and cost-effective way of setting up this task?\",\"answers\":[{\"text\":\"Enable scheduling on the AWS Console of your Lambda function. Define a schedule to run it at 30-minute intervals.\",\"isCorrect\":false},{\"text\":\"Integrate Amazon EventBridge (Amazon CloudWatch Events) with Lambda, which will automatically trigger the function every 30 minutes.\",\"isCorrect\":true},{\"text\":\"Use the Task Scheduler of your Windows PC to trigger the Lambda function every 30 minutes.\",\"isCorrect\":false},{\"text\":\"Launch an EC2 instance that has a cron job that triggers the Lambda function every 30 minutes.\",\"isCorrect\":false}],\"explanation\":\"$42\"},{\"question\":\"An ECS Cluster has a running X-Ray Daemon that enables developers to easily debug and troubleshoot their application. However, the trace data being sent to AWS X-Ray is still not as detailed as your manager wants it to be. There is a new requirement that requires the application to provide more granular timing information and more details about its downstream calls to various AWS resources. What should you do to satisfy this requirement?\",\"answers\":[{\"text\":\"Use metadata\",\"isCorrect\":false},{\"text\":\"Use annotations\",\"isCorrect\":false},{\"text\":\"Use inferred segment\",\"isCorrect\":false},{\"text\":\"Use subsegments\",\"isCorrect\":true}],\"explanation\":\"$43\"},{\"question\":\"A developer wants to track the number of visitors on their website, which has a DynamoDB database. This is primarily used to give a rough idea on how many people visit the site whenever they launch a new advertisement, which means it can tolerate a slight overcounting or undercounting of website visitors. Which of the following will satisfy the requirement with MINIMAL configuration?\",\"answers\":[{\"text\":\"Use conditional writes to update the counter item in the DynamoDB table only if the item has a unique primary key and the new value is greater than the current value.\",\"isCorrect\":false},{\"text\":\"Enable DynamoDB Streams to track the number of new visitors.\",\"isCorrect\":false},{\"text\":\"Use atomic counters to increment the counter item in the DynamoDB table for every new visitor.\",\"isCorrect\":true},{\"text\":\"Use conditional writes to update the counter item in the DynamoDB table and set the ReturnConsumedCapacity parameter to TOTAL.\",\"isCorrect\":false}],\"explanation\":\"$44\"},{\"question\":\"A multinational company uses Amazon EC2 Auto Scaling to maintain a fleet of EC2 instances behind an Application Load Balancer (ALB). The Amazon EC2 instances are configured with the Amazon CloudWatch agent to collect and publish custom metrics. However, the newly launched EC2 instances within the Auto Scaling group fail to send the metrics to Amazon CloudWatch. What changes are required to ensure the custom metrics are sent from all newly launched EC2 instances?\",\"answers\":[{\"text\":\"Attach the CloudWatchAgentServerPolicy policy to the IAM role specified in the EC2 Auto Scaling launch template to ensure proper permissions for the CloudWatch agent.\",\"isCorrect\":true},{\"text\":\"Configure the IAM role for the EC2 instances with the CloudWatchAgentReadOnlyAccess policy to allow the CloudWatch agent to read default metrics and publish data.\",\"isCorrect\":false},{\"text\":\"Add a user data script in the EC2 Auto Scaling launch template to install and start the CloudWatch agent during instance initialization.\",\"isCorrect\":false},{\"text\":\"Attach the CloudWatchAgentAdminPolicy IAM policy to the IAM role specified in the EC2 Auto Scaling launch template to provide enhanced permissions for the CloudWatch agent.\",\"isCorrect\":false}],\"explanation\":\"$45\"},{\"question\":\"A developer is instructed to collect data on the number of times that web visitors click the advertisement link of a popular news website. A database entry containing the count will be incremented for every click. Given that the website has millions of readers worldwide, your database should be configured to provide optimal performance to capture all the click events. What is the BEST service that the developer should implement in this scenario?\",\"answers\":[{\"text\":\"Take advantage of Amazon Aurora's performance speed and AUTO_INCREMENT feature for item updates.\",\"isCorrect\":false},{\"text\":\"Use Amazon RDS for the database and setup SQL AUTO_INCREMENT on your tables.\",\"isCorrect\":false},{\"text\":\"Launch an Amazon Redshift for the database and apply a step count of 1 for the IDENTITY column.\",\"isCorrect\":false},{\"text\":\"Set up Amazon DynamoDB for the database and implement atomic counters for UpdateItem operation of the website counter.\",\"isCorrect\":true}],\"explanation\":\"$46\"},{\"question\":\"You are hosting a website in an Amazon S3 bucket named tutorialsdojo and your users load the website using the http://tutorialsdojo.s3-website-us-east-1.amazonaws.com endpoint. You want to use JavaScript on the webpages that are stored in this bucket to be able to make authenticated GET and PUT requests. These requests are directed to the same bucket through the website.s3.amazonaws.com S3 API endpoint. However, you noticed that your web browser blocks the HTTP requests originating from your website. What should you do to rectify this issue?\",\"answers\":[{\"text\":\"Enable cross-account access.\",\"isCorrect\":false},{\"text\":\"Enable Cross-Zone Load Balancing.\",\"isCorrect\":false},{\"text\":\"Enable Cross-Region Replication (CRR).\",\"isCorrect\":false},{\"text\":\"Enable Cross-origin resource sharing (CORS) configuration in the bucket.\",\"isCorrect\":true}],\"explanation\":\"$47\"},{\"question\":\"A company has a hybrid cloud architecture that connects its on-premises data center with AWS. The DevOps team has been tasked to set up the company's continuous integration and continuous delivery (CI/CD) systems. The application deployment to both Amazon EC2 instances and on-premises servers should also be automated. Which of the following AWS service should be used to accomplish this?\",\"answers\":[{\"text\":\"Amazon Kinesis\",\"isCorrect\":false},{\"text\":\"AWS CodeDeploy\",\"isCorrect\":true},{\"text\":\"AWS CodeBuild\",\"isCorrect\":false},{\"text\":\"AWS CloudFormation\",\"isCorrect\":false}],\"explanation\":\"$48\"},{\"question\":\"In order to quickly troubleshoot their systems, your manager instructed you to record the calls that your application makes to all AWS services and resources. You developed a custom code that will send the segment documents directly to X-Ray by using the PutTraceSegments API. What should you include in your segment document to meet the above requirement?\",\"answers\":[{\"text\":\"tracing header\",\"isCorrect\":false},{\"text\":\"metadata\",\"isCorrect\":false},{\"text\":\"subsegments\",\"isCorrect\":true},{\"text\":\"annotations\",\"isCorrect\":false}],\"explanation\":\"$49\"},{\"question\":\"A mobile game is currently being developed and needs to have an authentication service. You need to use an AWS service which provides temporary AWS credentials for users who have been authenticated via their social media logins as well as for guest users who do not require any authentication. How can you BEST achieve this using AWS?\",\"answers\":[{\"text\":\"Use AWS Cognito User Pools then enable access to unauthenticated identities.\",\"isCorrect\":false},{\"text\":\"Use AWS Cognito Identity Pools then enable access to unauthenticated identities.\",\"isCorrect\":true},{\"text\":\"Use Amazon Cognito Sync.\",\"isCorrect\":false},{\"text\":\"Use AWS IAM Identity Center.\",\"isCorrect\":false}],\"explanation\":\"$4a\"},{\"question\":\"Your development team is currently developing a financial application in AWS. One of the requirements is to create and control the encryption keys used to encrypt your data using the envelope encryption strategy to comply with the strict IT security policy of the company. Which of the following correctly describes the process of envelope encryption?\",\"answers\":[{\"text\":\"Encrypt plaintext data with a data key and then encrypt the data key with a top-level plaintext key.\",\"isCorrect\":true},{\"text\":\"Encrypt plaintext data with a KMS key and then encrypt the KMS key with a top-level plaintext data key.\",\"isCorrect\":false},{\"text\":\"Encrypt plaintext data with a data key and then encrypt the data key with a top-level encrypted key.\",\"isCorrect\":false},{\"text\":\"Encrypt plaintext data with a KMS key and then encrypt the KMS key with a top-level encrypted data key.\",\"isCorrect\":false}],\"explanation\":\"$4b\"},{\"question\":\"A multinational e-commerce company hosts its product descriptions on an Amazon RDS database. All descriptions are originally written in English. Users can request on-demand translations via a Lambda function, which pulls the description and employs Amazon Translate's TranslateText API for the task. However, during sales of popular products, the surge in translation requests is stressing the RDS, causing increased response times. How can a developer improve the Lambda function's response time cost-effectively without performing database optimizations?\",\"answers\":[{\"text\":\"Store the results of the TranslateText API in an Amazon DynamoDB Accelerator (DAX) cluster.\",\"isCorrect\":false},{\"text\":\"Update the Lambda function to use asynchronous invocation. Push the translation requests to an Amazon SQS queue and then process in batches.\",\"isCorrect\":false},{\"text\":\"Use the /tmp storage in the Lambda function to cache recently translated product descriptions\",\"isCorrect\":true},{\"text\":\"Use AWS Step Functions with a Parallel state to concurrently run multiple instances of the Lambda function for translation.\",\"isCorrect\":false}],\"explanation\":\"$4c\"},{\"question\":\"A developer is creating a real-time auction app for second-hand cars using Kinesis Data Streams to ingest bids. The auction rules are as follows: A bid must be processed only once An EC2 instance consumer must process bids in the same order they were received. Which solution will meet the requirement?\",\"answers\":[{\"text\":\"Embed a unique ID in each bid record. Use Kinesis PutRecords API to write bids. Assign a timestamp-based value for the PartitionKey parameter.\",\"isCorrect\":false},{\"text\":\"Replace the stream with an SQS FIFO queue and use the SendMessageBatch API to write bids. Provide a unique id in the MessageDeduplicationId parameter for each bid request.\",\"isCorrect\":false},{\"text\":\"Embed a unique ID in each bid record. Use Kinesis PutRecord API to write bids. Assign a timestamp-based value for the SequenceNumberForOrdering parameter.\",\"isCorrect\":true},{\"text\":\"Replace the stream with an SQS FIFO queue and use the SendMessage API to write bids. Provide a unique id in the MessageDeduplicationId parameter for each bid request.\",\"isCorrect\":false}],\"explanation\":\"$4d\"},{\"question\":\"A company has different AWS accounts, namely Account A, Account B, and Account C, which are used for their Development, Test, and Production environments respectively. A developer needs access to perform an audit whenever a new version of the application has been deployed to the Test (Account B) and production (Account C) environments. What is the MOST efficient way to provide the developer access to execute the specified task?\",\"answers\":[{\"text\":\"Set up AWS Organizations and attach a Service Control Policy to the developer to access the other accounts.\",\"isCorrect\":false},{\"text\":\"Create separate identities and passwords for the developer on both the Test and Production accounts.\",\"isCorrect\":false},{\"text\":\"Enable AWS multi-factor authentication (MFA) to the IAM User of the developer.\",\"isCorrect\":false},{\"text\":\"Grant the developer cross-account access to the resources of Accounts B and C.\",\"isCorrect\":true}],\"explanation\":\"$4e\"},{\"question\":\"A developer is building an application that will be hosted in ECS and must be configured to run tasks and services using the Fargate launch type. The application will have four different tasks, each of which will access different AWS resources than the others. Which of the following is the MOST efficient solution that can provide your application in ECS access to the required AWS resources?\",\"answers\":[{\"text\":\"Create 4 different IAM Roles with the required permissions and attach them to each of the 4 ECS tasks.\",\"isCorrect\":true},{\"text\":\"Create an IAM Group with all the required permissions and attach them to each of the 4 ECS tasks.\",\"isCorrect\":false},{\"text\":\"Create 4 different Container Instance IAM Roles with the required permissions and attach them to each of the 4 ECS tasks.\",\"isCorrect\":false},{\"text\":\"Create 4 different Service-Linked Roles with the required permissions and attach them to each of the 4 ECS tasks.\",\"isCorrect\":false}],\"explanation\":\"$4f\"}]}}]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n7:null\n"])</script><script>self.__next_f.push([1,"8:null\nc:[[\"$\",\"title\",\"0\",{\"children\":\"v0 App\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Created with v0\"}],[\"$\",\"meta\",\"2\",{\"name\":\"generator\",\"content\":\"v0.app\"}]]\n"])</script></body></html>