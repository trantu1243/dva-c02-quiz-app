<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/dva-c02-quiz-app/_next/static/css/31d717072bdb6d85.css" data-precedence="next"/><link rel="stylesheet" href="/dva-c02-quiz-app/_next/static/css/93628e14bbee05a5.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/dva-c02-quiz-app/_next/static/chunks/webpack-b20bf3550849aea8.js"/><script src="/dva-c02-quiz-app/_next/static/chunks/4bd1b696-65f2dd90969a3b23.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/684-499f1a03f1824ea2.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/main-app-01726a8d9af88025.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/app/layout-7d8a5f63f536cdcf.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/261-2d9b76ccba401937.js" async=""></script><script src="/dva-c02-quiz-app/_next/static/chunks/app/quiz/%5Bid%5D/page-d7cb26530e4508b3.js" async=""></script><title>v0 App</title><meta name="description" content="Created with v0"/><meta name="generator" content="v0.app"/><script src="/dva-c02-quiz-app/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans __variable_1f39b6 __variable_c20681"><div class="min-h-screen bg-gradient-to-br from-background via-background to-primary/5 flex items-center justify-center"><div class="text-center"><div class="w-16 h-16 border-4 border-primary border-t-transparent rounded-full animate-spin mx-auto mb-4"></div><p class="text-muted-foreground">Loading quiz...</p></div></div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><script src="/dva-c02-quiz-app/_next/static/chunks/webpack-b20bf3550849aea8.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7555,[],\"\"]\n3:I[1295,[],\"\"]\n4:I[9742,[\"177\",\"static/chunks/app/layout-7d8a5f63f536cdcf.js\"],\"Analytics\"]\n6:I[9665,[],\"OutletBoundary\"]\n9:I[9665,[],\"ViewportBoundary\"]\nb:I[9665,[],\"MetadataBoundary\"]\nd:I[6614,[],\"\"]\n:HL[\"/dva-c02-quiz-app/_next/static/css/31d717072bdb6d85.css\",\"style\"]\n:HL[\"/dva-c02-quiz-app/_next/static/css/93628e14bbee05a5.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"yCqRTNLEPBdkYbRRbuCZd\",\"p\":\"/dva-c02-quiz-app\",\"c\":[\"\",\"quiz\",\"aws-developer-4\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"quiz\",{\"children\":[[\"id\",\"aws-developer-4\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/dva-c02-quiz-app/_next/static/css/31d717072bdb6d85.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/dva-c02-quiz-app/_next/static/css/93628e14bbee05a5.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"font-sans __variable_1f39b6 __variable_c20681\",\"children\":[[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"$L4\",null,{}]]}]}]]}],{\"children\":[\"quiz\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"id\",\"aws-developer-4\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",\"$undefined\",null,[\"$\",\"$L6\",null,{\"children\":[\"$L7\",\"$L8\",null]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"-pvxI5ozd1d4o4FFcNC6G\",{\"children\":[[\"$\",\"$L9\",null,{\"children\":\"$La\"}],null]}],[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$d\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"e:I[1184,[\"261\",\"static/chunks/261-2d9b76ccba401937.js\",\"200\",\"static/chunks/app/quiz/%5Bid%5D/page-d7cb26530e4508b3.js\"],\"default\"]\nf:Tb32,"])</script><script>self.__next_f.push([1,"A stage is a named reference to a deployment, which is a snapshot of the API. You use a Stage to manage and optimize a particular deployment. For example, you can set up stage settings to enable caching, customize request throttling, configure logging, define stage variables or attach a canary release for testing. APIs are deployed to stages:Stage variables are namevalue pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates.With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints. For example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, http://example.com).In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage, and calls a different web host (for example, beta.example.com). Similarly, stage variables can be used to specify a different AWS Lambda function name for each stage in your API.Therefore, for this scenario the Developers can deploy the API versions as unique stages with unique endpoints and use stage variables to provide further context such as connections to different backend services.CORRECT: \"Deploy the API versions as unique stages with unique endpoints and use stage variables to provide further context\" is the correct answer.INCORRECT: \"Create an API Gateway resource policy to isolate versions and provide context to the Lambda functions\" is incorrect. API Gateway resource policies are JSON policy documents that you attach to an API to control whether a specified principal (typically, an IAM user or role) can invoke the API.INCORRECT: \"Create an AWS Lambda authorizer to route API clients to the correct API version\" is incorrect. A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API. This is not used for routing API clients to different versions.INCORRECT: \"Deploy an HTTP Proxy integration and configure the proxy with API versions\" is incorrect. The HTTP proxy integration allows a client to access the backend HTTP endpoints with a streamlined integration setup on a single API method. This is not used for providing multiple versions of the API, use stages and stage variables instead.References:https://docs.aws.amazon.com/apigateway/latest/developerguide/setupstages.html https://docs.aws.amazon.com/apigateway/latest/developerguide/stagevariables.html"])</script><script>self.__next_f.push([1,"10:T6b1,The AWS Serverless Application Model (AWS SAM) is an opensource framework that you can use to build serverless applications on AWS. A serverless application is a combination of Lambda functions, event sources, and other resources that work together to perform tasks.Note: A serverless application is more than just a Lambda function—it can include additional resources such as APIs, databases, and event source mappings.You can use AWS SAM to define your serverless applications. AWS SAM consists of the following components:• AWS SAM template specification. You use this specification to define your serverless application. It provides you with a simple and clean syntax to describe the functions, APIs, permissions, configurations, and events that make up aserverless application.• AWS SAM command line interface (AWS SAM CLI). You use this tool to build serverless applications that are defined by AWS SAM templates.CORRECT: \"AWS Serverless Application Model\" is the correct answer.INCORRECT: \"AWS Step Functions State Machine\" is incorrect. AWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. It does not provide a simplified syntax for expressing serverless resources in a CloudFormation template.INCORRECT: \"OpenAPI Swagger Specification\" is incorrect. Swagger is an opensource software framework backed by a large ecosystem of tools that helps developers design, build, document, and consume RESTful web services.INCORRECT: \"A CloudFormation Serverless Plugin\" is incorrect as this does not exist.References: https://docs.aws.amazon.com/serverlessapplicationmodel/latest/developerguide/whatissam.html11:T57c,With this scenario we have a table that has a large number of items quickly written to it on a recurring schedule. These items are no longer of use after they have been processed (within 2 hours) so from that point on until the next job the table is not being used. The items need to be deleted and we need to choose th"])</script><script>self.__next_f.push([1,"e most efficient (think cost as well as operations) way of doing this.Any delete operation will consume RCUs to scan/query the table and WCUs to delete the items. It will be much cheaper and simpler to just delete the table and recreate it again ahead of the next batch job. This can easily be automated through the API.CORRECT: \"Delete the entire table and recreate it each day\" is the correct answer.INCORRECT: \"Use the BatchUpdateItem API with expressions\" is incorrect as this API does not exist.INCORRECT: \"Issue an AWS CLI aws dynamodb deleteitem command with a wildcard\" is incorrect as this operation deletes data from a table one item at a time, which is highly inefficient. You also must specify the item's primary key values; you cannot use a wildcard.INCORRECT: \"Use the BatchWriteItem API with a DeleteRequest\" is incorrect as this is an inefficient way to solve this challenge.References: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchWriteItem.html https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/SQLtoNoSQL.DeleteData.html12:T4fe,To enable connectivity to an application in a private subnet and the Internet you must first allow the function to connect to the private subnet (which has already been done).Lambda needs the following VPC configuration information so that it can connect to the VPC:• Private subnet ID.• Security Group ID (with required access).Lambda uses this information to setup an Elastic Network Interface (ENI) using an available IP address from your private subnet. Next you need to add a NAT Gateway for Internet access (no public IP). The NAT Gateway should be connected to a public subnet and a route needs to be added to the private subnet.CORRECT: \"Add a NAT Gateway to the private subnet\" is the correct answer.INCORRECT: \"Connect the Lambda function to an Internet Gateway\" is incorrect. Though by using a NAT Gateway you are effectively establishing routing to an Internet Gateway, you cannot actually connect Lambda to an Internet Gateway.INCORRECT: "])</script><script>self.__next_f.push([1,"\"Connect an AWS VPN to Lambda to connect to the Internet\" is incorrect as you cannot connect an AWS VPN to a Lambda function.INCORRECT: \"Add an Elastic IP to the Lambda function\" is incorrect as you cannot add an Elastic IP to a Lambda function.References: https://docs.aws.amazon.com/lambda/latest/dg/configurationvpc.html13:T48a,A serverless application can include one or more nested applications. You can deploy a nested application as a standalone artifact or as a component of a larger application.As serverless architectures grow, common patterns emerge in which the same components are defined in multiple application templates. You can now separate out common patterns as dedicated applications, and then nest them as part of new or existing application templates. With nested applications, you can stay more focused on the business logic that's unique to yourapplication.To define a nested application in your serverless application, use the AWS::Serverless::Application resource type.CORRECT: \"AWS::Serverless::Application\" is the correct answer.INCORRECT: \"AWS::Serverless:Function\" is incorrect as this is used to define a serverless Lamdba function.INCORRECT: \"AWS::Serverless:HttpApi\" is incorrect as this is used to define an API Gateway HTTP API.INCORRECT: \"AWS::Serverless:SimpleTable\" is incorrect as this is used to define a DynamoDB table.References: https://docs.aws.amazon.com/serverlessapplicationmodel/latest/developerguide/serverlesssamtemplatenestedapplications.html14:T65c,Some applications might need to perform many kinds of queries, using a variety of different attributes as query criteria. To support these requirements, you can create one or more global secondary indexes and issue Query requests against these indexes in Amazon DynamoDB.When items from a primary table are written to the GSI they consume write capacity units. It is essential to ensure the GSI has sufficient WCUs (typically, at least as many as the primary table). If writes are throttled on the GSI, the main table will be throttled (even if th"])</script><script>self.__next_f.push([1,"ere’s enough WCUs on the main table). LSIs do not cause any special throttling considerations.In this scenario, it is likely that the Developer assumed that the GSI would need fewer WCUs as it is more readintensive and neglected to factor in the WCUs required for writing data into the GSI. Therefore, the most likely explanation is that the write capacity units on the GSI are under provisionedCORRECT: \"The write capacity units on the GSI are under provisioned\" is the correct answer.INCORRECT: \"There are insufficient read capacity units on the primary table\" is incorrect as the table is being throttled due to writes, not reads.INCORRECT: \"The Developer should have added an LSI instead of a GSI\" is incorrect as a GSI has specific advantages and there was likely good reason for adding a GSI. Also, you cannot add an LSI to an existing table.INCORRECT: \"There are insufficient write capacity units on the primary table\" is incorrect as the question states that the WCUs are underutilized.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html15:T50e,You can create crossaccount crossRegion dashboards, which summarize your CloudWatch data from multiple AWS accounts and multiple Regions into one dashboard. From this highlevel dashboard you can get a view of your entire application, and also drill down into more specific dashboards without having to log in and out of accounts or switch Regions.You can create crossaccount crossRegion dashboards in the AWS Management Console and programmatically.CORRECT: \"Create an Amazon CloudWatch crossaccount crossregion dashboard\" is the correct answer.INCORRECT: \"Create an Amazon CloudWatch dashboard in one account and region and import the data from the other accounts and regions\" is incorrect as this is more complex and unnecessary.INCORRECT: \"Create an AWS Lambda function that collects metrics from each account and region and pushes the metrics to the account where the dashboard has been created\" is incorrect as this is not a simple solution.INCORRECT: \"C"])</script><script>self.__next_f.push([1,"reate an Amazon CloudTrail trail that applies to all regions and deliver the logs to a single Amazon S3 bucket.Create a dashboard using the data in the bucket\" is incorrect as CloudTrail logs API activity, not performance metrics.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_xaxr_dashboard.html16:T9d7,"])</script><script>self.__next_f.push([1,"With AWS KMS you can encrypt files directly with a customer master key (CMK). A CMK can encrypt up to 4KB (4096 bytes) of data in a single encrypt, decrypt, or reencrypt operation. As CMKs cannot be exported from KMS this is a very safe way to encrypt small amounts of data.Customer managed CMKs are CMKs in your AWS account that you create, own, and manage. You have full control over these CMKs, including establishing and maintaining their key policies, IAM policies, and grants, enabling and disabling them, rotating their cryptographic material, adding tags, creating aliases that refer to the CMK, and scheduling the CMKs for deletion.AWS managed CMKs are CMKs in your account that are created, managed, and used on your behalf by an AWS service that is integrated with AWS KMS. Some AWS services support only an AWS managed CMK. In this example the Amazon EC2 instance is saving files on a proprietary networkattached file system and this will not have support for AWS managed CMKs.Data keys are encryption keys that you can use to encrypt data, including large amounts of data and other data encryption keys. You can use AWS KMS CMKs to generate, encrypt, and decrypt data keys. However, AWS KMS does not store, manage, or track your data keys, or perform cryptographic operations with data keys. You must use and manage data keys outside of AWS KMS – this is potentially less secure as you need to manage the security of these keys.CORRECT: \"Encrypt the data directly with a customer managed customer master key\" is the correct answer.INCORRECT: \"Create a data encryption key from a customer master key and encrypt the data with the data encryption key\" is incorrect as this is not the most secure option here as you need to secure the data encryption key outside of KMS. It is also unwarranted as you can use a CMK directly to encrypt files up to 4KB in size.INCORRECT: \"Create a data encryption key from a customer master key and encrypt the data with the customer master key\" is incorrect as the creation of the data encryption key is of no use here. It does not necessarily pose a security risk as the data key hasn’t been used (and you can use the CMK to encrypt the data), however this is not the correct process to follow.INCORRECT: \"Encrypt the data directly with an AWS managed customer master key\" is incorrect as the networkattached file system is proprietary and therefore will not be supported by AWS managed CMKs.References: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html"])</script><script>self.__next_f.push([1,"17:T84a,"])</script><script>self.__next_f.push([1,"A read capacity unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. For example, suppose that you create a table with 10 provisioned read capacity units. This allows you to perform 10 strongly consistent reads per second, or 20 eventually consistent reads per second, for items up to 4 KB.Reading an item larger than 4 KB consumes more read capacity units. For example, a strongly consistent read of an item that is 8 KB (4 KB × 2) consumes 2 read capacity units. An eventually consistent read on that same item consumes only 1 read capacity unit.Item sizes for reads are rounded up to the next 4 KB multiple. For example, reading a 3,500byte item consumes the same throughput as reading a 4 KB item. Therefore, the smaller (1 KB) items in this scenario would consume the same number of RCUs as the 4 KB items. Also, we know that eventually consistent reads consume half the RCUs of strongly consistent reads.The following bullets provide the read throughput for each configuration:• Eventually consistent, 15 RCUs, 1 KB item = 30 items read per second.• Strongly consistent, 15 RCUs, 1 KB item = 15 items read per second.• Eventually consistent, 5 RCUs, 4 KB item = 10 items read per second.• Strongly consistent, 5 RCUs, 4 KB item = 5 items read per second.Therefore, the Developer should choose the option to enable eventually consistent reads of 15 RCUs reading items that are 1 KB in size as this will result in the highest number of items read per second.CORRECT: \"Eventually consistent reads of 15 RCUs reading items that are 1 KB in size\" is the correct answer.INCORRECT: \"Eventually consistent reads of 5 RCUs reading items that are 4 KB in size\" is incorrect as described above.INCORRECT: \"Strongly consistent reads of 5 RCUs reading items that are 4 KB in size\" is incorrect as described above.INCORRECT: \"Strongly consistent reads of 15 RCUs reading items that are 1KB in size\" is incorrect as described above.References: https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/ProvisionedThroughput.html"])</script><script>self.__next_f.push([1,"18:T7dd,You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN. You can point an alias to multiple versions of your function code and then assign a weighting to direct certain amounts of traffic to each version. This enables a blue/green style of deployment and means it’s easy to roll back to the older version by simplyupdating the weighting if issues occur.CORRECT: \"Create an alias that points to both the new and previous versions of the function code and assign a weighting for sending a portion of traffic to the new version\" is the correct answer.INCORRECT: \"Create two versions of the function code. Configure the application to direct a subset of requests to the new version\" is incorrect as this would entail using application logic to direct traffic to the different versions. This is not the best way to solve this problem as Lambda aliases are a better solution.INCORRECT: \"Create an API using API Gateway and use stage variables to point to different versions of the Lambda function\" is incorrect. Stage variables are namevalue pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates. You can use stage variables to point to different Lambda ARNs and associate these with different stages of your API, however this is not a good solution for this scenario.INCORRECT: \"Create a new function using the new code and update the application to split requests between the new functions\" is incorrect as this would entail using application logic to direct traffic to the different versions. This is not the best way to solve this problem as Lambda aliases are a better solution.References:https://docs.aws.amazon.com/lambda/latest/dg/configurationaliases.html https://docs.aws.amazon.com/lambda/latest/dg/configurationversions.html19:T614,You can monitor Ama"])</script><script>self.__next_f.push([1,"zon DynamoDB using CloudWatch, which collects and processes raw data from DynamoDB into readable, near realtime metrics. These statistics are retained for a period of time, so that you can access historical information for a better perspective on how your web application or service is performing. By default, DynamoDB metric data is sent toCloudWatch automatically.To determine how much of the provisioned capacity is being used you can monitor ConsumedReadCapacityUnits or ConsumedWriteCapacityUnits over the specified time period.CORRECT: \"Monitor the ConsumedReadCapacityUnits and ConsumedWriteCapacityUnits over a specified time period\" is the correct answer.INCORRECT: \"Monitor the ReadThrottleEvents and WriteThrottleEvents metrics for the table\" is incorrect as these metrics are used to determine which requests exceed the provisioned throughput limits of a table.INCORRECT: \"Use Amazon CloudTrail and monitor the DescribeLimits API action\" is incorrect as CloudTrail records API actions, not performance metrics.INCORRECT: \"Use AWS XRay to instrument the DynamoDB table and monitor subsegments\" is incorrect. DynamoDB does not directly integrate with XRay but you can record information in subsegments for downstream requests. This is not, however, a method for monitoring provisioned capacity utilization.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/monitoringcloudwatch.html https://digitalcloud.training/certificationtraining/awsdeveloperassociate/awsmanagementandgovernance/amazoncloudwatch/1a:Tb9a,"])</script><script>self.__next_f.push([1,"By default, the DynamoDB write operations (PutItem, UpdateItem, DeleteItem) are unconditional: Each operation overwrites an existing item that has the specified primary key.DynamoDB optionally supports conditional writes for these operations. A conditional write succeeds only if the item attributes meet one or more expected conditions. Otherwise, it returns an error. Conditional writes are helpful in many situations. For example, you might want a PutItem operation to succeed only if there is not already an item with the same primary key. Or you could prevent an UpdateItem operation from modifying an item if one of its attributes has a certain value. Conditional writes can be idempotent if the conditional check is on the same attribute that is being updated. This means that DynamoDB performs a given write request only if certain attribute values in the item match what you expect them to be at the time of the request.For example, suppose that you issue an UpdateItem request to increase the Price of an item by 3, but only if the Price is currently 20. After you send the request, but before you get the results back, a network error occurs, and you don't know whether the request was successful. Because this conditional write is idempotent, you can retry the same UpdateItem request, and DynamoDB updates the item only if the Price is currently 20.The following example shows how to use the conditionexpression parameter to achieve a conditional write with idempotence:aws dynamodb updateitem \\tablename ProductCatalog \\key '{\"Id\":{\"N\":\"1\"}}' \\updateexpression \"SET Price = :newval\" \\conditionexpression \"Price = :currval\" \\expressionattributevalues file://expressionattributevalues.jsonFor this scenario, conditional writes with idempotence will mean that each writer can check the current price and update the price only if the price matches that price. If the price is updated by another writer before the write is made, it will fail as the item price has changed and will not reflect the expected price.CORRECT: \"Use conditional writes\" is the correct answer.INCORRECT: \"Use concurrent writes\" is incorrect as writing concurrently to the same items is exactly what we want to avoid.INCORRECT: \"Use atomic counters\" is incorrect. An atomic counter is a numeric attribute that is incremented, unconditionally, without interfering with other write requests. This is used for cases such as tracking visitors to a website. This does not prevent recent updated from being overwritten.INCORRECT: \"Use batch operations\" is incorrect. Batch operations can reduce the number of network round trips from your application to DynamoDB. However, this does not solve the problem of preventing recent updates from being overwritten.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ConditionExpressions.html"])</script><script>self.__next_f.push([1,"1b:T728,User pools are for authentication (identify verification). With a user pool, your app users can sign in through the user pool or federate through a thirdparty identity provider (IdP).Identity pools are for authorization (access control). You can use identity pools to create unique identities for users and give them access to other AWS services.User pool use cases:Use a user pool when you need to:• Design signup and signin webpages for your app.• Access and manage user data.• Track user device, location, and IP address, and adapt to signin requests of different risk levels.• Use a custom authentication flow for your app.Identity pool use cases:Use an identity pool when you need to:• Give your users access to AWS resources, such as an Amazon Simple Storage Service (Amazon S3) bucket or anAmazon DynamoDB table.• Generate temporary AWS credentials for unauthenticated users.Therefore, a user pool is the correct service to use as in this case we are not granting access to AWS services, just providing signup and signin capabilities for a mobile app.CORRECT: \"AWS Cognito user pool\" is the correct answer.INCORRECT: \"AWS Cognito identity pool\" is incorrect as an identity pool is used when you need to provide access to AWS resources (see explanation above).INCORRECT: \"API Gateway with a Lambda authorizer\" is incorrect as AWS Cognito is the best solution for providing signup and signin for mobile apps and also integrates with the 3rd party IdPs.INCORRECT: \"AWS IAM and STS\" is incorrect as AWS Cognito is the best solution for providing signup and signin for mobile apps and also integrates with the 3rd party IdPs.References:https://aws.amazon.com/premiumsupport/knowledgecenter/cognitouserpoolsidentitypools/ https://docs.aws.amazon.com/cognito/latest/developerguide/cognitouserpoolsidentityfederation.html1c:T446,To avoid throttling in Amazon S3 you must ensure you do not exceed certain limits on a perprefix basis. You can send 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in an A"])</script><script>self.__next_f.push([1,"mazon S3 bucket. There are no limits to the number of prefixes that you can have in your bucket.In this case the Developer would need to split the files across at least 10 prefixes in a single Amazon S3 bucket. The application should then read the files across the prefixes in parallel.CORRECT: \"Create at least 10 prefixes and split the files across the prefixes\" is the correct answer.INCORRECT: \"Create at least 10 S3 buckets and split the files across the buckets\" is incorrect. Performance is improved based on splitting reads across prefixes, not buckets.INCORRECT: \"Move the files to Amazon EFS. Index the files with S3 metadata\" is incorrect. This is not costeffective.INCORRECT: \"Move the files to Amazon DynamoDB. Index the files with S3 metadata\" is incorrect. This is not costeffective.References: https://aws.amazon.com/premiumsupport/knowledgecenter/s3requestlimitavoidthrottling/1d:Tb11,"])</script><script>self.__next_f.push([1,"The Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. You store this configuration in the notification subresource that is associated with a bucket. Currently, Amazon S3 can publish notifications for the following events: \u0026bull; New object created events \u0026mdash; Amazon S3 supports multiple APIs to create objects. You can request notification when only a specific API is used (for example, s3:ObjectCreated:Put), or you can use a wildcard (for example, s3:ObjectCreated:*) to request notification when an object is created regardless of the API used. \u0026bull; Object removal events \u0026mdash; Amazon S3 supports deletes of versioned and unversioned objects. For information about object versioning, see Object Versioning and Using Versioning. \u0026bull; Restore object events \u0026mdash; Amazon S3 supports the restoration of objects archived to the S3 Glacier storage class. You request to be notified of object restoration completion by using s3:ObjectRestore:Completed. You use s3:ObjectRestore:Post to request notification of the initiation of a restore. \u0026bull; Reduced Redundancy Storage (RRS) object lost events \u0026mdash; Amazon S3 sends a notification message when it detects that an object of the RRS storage class has been lost. \u0026bull; Replication events \u0026mdash; Amazon S3 sends event notifications for replication configurations that have S3 Replication Time Control (S3 RTC) enabled. It sends these notifications when an object fails replication, when an object exceeds the 15minute threshold, when an object is replicated after the 15minute threshold, and when an object is no longer tracked by replication metrics. It publishes a second event when that object replicates to the destination Region. Therefore, the Developer should create an event notification for all s3:ObjectCreated:* API calls as this will capture all new object creation events. CORRECT: \"Create an event notification for all s3:ObjectCreated:* API calls\" is the correct answer. INCORRECT: \"Create an event notification for all s3:ObjectCreated:Put API calls\" is incorrect as this will not capture all new object creation events (e.g. POST or COPY). The wildcard should be used instead. INCORRECT: \"Create an event notification for all s3:ObjectRemoved:Delete API calls\" is incorrect as this is used for object deletions. INCORRECT: \"Create an event notification for all s3:ObjectRestore:Post API calls\" is incorrect as this is used for restore events from Amazon S3 Glacier archives. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html https://aws.amazon.com/sns/faqs/"])</script><script>self.__next_f.push([1,"1e:T4b6,When CloudFront receives an HTTP response from the origin server, if there is an originresponse trigger associated with the cache behavior, you can modify the HTTP response to override what was returned from the origin.Some common scenarios for updating HTTP responses include the following:• Changing the status to set an HTTP 200 status code and creating static body content to return to the viewer when an origin returns an error status code (4xx or 5xx)• Changing the status to set an HTTP 301 or HTTP 302 status code, to redirect the user to another website when an origin returns an error status code (4xx or 5xx)You can also replace the HTTP responses in viewer and origin request events. However, in this case it is the error response being returned from the origin that must be modified when a 404 error is encountered for a page that has been removed.CORRECT: \"Origin response\" is the correct answer.INCORRECT: \"Origin request\" is incorrect as explained above.INCORRECT: \"Viewer response\" is incorrect as explained above.INCORRECT: \"Viewer request\" is incorrect as explained above.References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambdaupdatinghttpresponses.html1f:T54e,The /tmp directory can be used for storing temporary files within the execution context. This can be used for storing static assets that can be used by subsequent invocations of the function. If the assets must be deleted before the function is invoked again the function code should take care of deleting them.There is a limit of 512 MB storage space in the /tmp directory, but this is more than adequate for this scenario.CORRECT: \"Store the files in the /tmp directory and delete the files when the execution completes\" is the correct answer.INCORRECT: \"Store the files in Amazon S3 and use a lifecycle policy to delete the files automatically\" is incorrect. The /tmp directory within the execution context has enough space for these files and this will reduce latency, cost, and execution time.INCORRECT: \"Store the files "])</script><script>self.__next_f.push([1,"in an Amazon Instance Store and delete the files when the execution completes\" is incorrect. Instance stores are ephemeral storage attached to Ec2 instances, they cannot be used except by EC2 instances for temporary storage.INCORRECT: \"Store the files in an Amazon EFS filesystem and delete the files when the execution completes\" is incorrect. This is another option that would increase cost, complexity and latency. It is better to use the /tmp directory.References: https://docs.aws.amazon.com/lambda/latest/dg/bestpractices.html20:T5a1,The writethrough strategy adds data or updates data in the cache whenever data is written to the database. The advantages of writethrough are as follows:• Data in the cache is never stale. Because the data in the cache is updated every time it's written to the database, the data in the cache is always current.• Write penalty vs. read penalty. Every write involves two trips:1. A write to the cache2. A write to the databaseWhich adds latency to the process. That said, end users are generally more tolerant of latency when updating data than when retrieving data. There is an inherent sense that updates are more work and thus take longer.CORRECT: \"Use a writethrough caching strategy\" is the correct answer.INCORRECT: \"Use a lazyloading caching strategy\" is incorrect. Lazy loading is a caching strategy that loads data into the cache only when necessary. This will not ensure strong consistency between the database and the cache.INCORRECT: \"Add a short duration TTL value to each write\" is incorrect. A TTL specifies the number of seconds until the key expires. This will not ensure strong consistency between the database and the cache.INCORRECT: \"Invalidate the cache for each database write\" is incorrect. This will allow the cache to be updated when an item is next read but will not ensure the best performance for all items in the database.References: https://docs.aws.amazon.com/AmazonElastiCache/latest/memug/Strategies.html21:T548,You can use the /tmp directory if the function needs to dow"])</script><script>self.__next_f.push([1,"nload a large file or disk space for operations. The maximum size is 512 MB. The content is frozen within the execution context so multiple invocations can use the data.Therefore, the download will occur once, and then subsequent invocations will use the file from the /tmp directory. This requires minimal refactoring and is the best way of resolving these issues.CORRECT: \"Store the file in the /tmp directory of the execution context and reuse it on subsequent invocations\" is the correct answer.INCORRECT: \"Increase the memory allocation of the function\" is incorrect as this will not resolve the issue of needing to download the file for each invocation. Adding memory results in more CPU being allocated which can reduce processing time but the problem still remains.INCORRECT: \"Increase the timeout of the function\" is incorrect as this does not resolve the main issue. The download will still need to occur for each invocation and therefore the application will continue to be affected by poor performance.INCORRECT: \"Increase the concurrency allocation of the function\" is incorrect as concurrency is not the issue here. The issue that needs to be resolved is to remove the requirement to download the large file for each invocation.References: https://aws.amazon.com/lambda/features/22:T9ea,"])</script><script>self.__next_f.push([1,"AWS CodePipeline is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. You can quickly model and configure the different stages of a software release process. CodePipeline automates the steps required to release your software changes continuously.Specifically, you can:• Automate your release processes: CodePipeline fully automates your release process from end to end, starting from your source repository through build, test, and deployment. You can prevent changes from moving through a pipeline by including a manual approval action in any stage except a Source stage. You can release when you want, in the way you want, on the systems of your choice, across one instance or multiple instances.• Establish a consistent release process: Define a consistent set of steps for every code change. CodePipeline runs each stage of your release according to your criteria.• Speed up delivery while improving quality: You can automate your release process to allow your developers to test and release code incrementally and speed up the release of new features to your customers.• Use your favorite tools: You can incorporate your existing source, build, and deployment tools into your pipeline.• View progress at a glance: You can review realtime status of your pipelines, check the details of any alerts, retry failed actions, view details about the source revisions used in the latest pipeline execution in each stage, and manually rerun any pipeline.• View pipeline history details: You can view details about executions of a pipeline, including start and end times, run duration, and execution IDs.Therefore, AWS CodePipeline is the perfect tool for the Developer’s requirements.CORRECT: \"AWS CodePipeline\" is the correct answer.INCORRECT: \"AWS CloudFormation\" is incorrect as CloudFormation is not triggered by changes in a source code repository. You must create change sets for deploying updates.INCORRECT: \"AWS Elastic Beanstalk\" is incorrect as this is a platform service that can be used to deploy code to managed runtimes such as Nodejs. It does not update automatically based on changes to source code. You must update that environment when you need to release new code.INCORRECT: \"AWS CodeBuild\" is incorrect as CodeBuild is used for compiling code, running unit tests and creating the deployment package. It does not manage the deployment of the code.References: https://docs.aws.amazon.com/codepipeline/latest/userguide/welcomewhatcanIdo.html"])</script><script>self.__next_f.push([1,"23:T49e,In Amazon ECS, create a Docker image that runs the XRay daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster. You can use port mappings and network mode settings in your task definition file to allow your application to communicate with the daemon container.CORRECT: \"Create a Docker image that runs the XRay daemon, upload it to a Docker image repository, and then deploy it to the Amazon ECS cluster.\" is the correct answer.INCORRECT: \"Install the Amazon CloudWatch agent on the container image. Use the CloudWatch SDK to publish custom metrics from each of the microservices\" is incorrect. The CloudWatch agent does not capture trace information between Docker containers.INCORRECT: \"Install the AWS XRay daemon on each of the Amazon ECS instances\" is incorrect. The XRay daemon must be installed on the Docker containers, not the ECS hosts.INCORRECT: \"Install the AWS XRay daemon locally on an Amazon EC2 instance and instrument the Amazon ECS microservices using the XRay SDK\" is incorrect. You cannot trace Docker microservices from an Amazon EC2 instance.References: https://docs.aws.amazon.com/xray/latest/devguide/xraydaemonecs.html24:T7fd,The most secure configuration to authenticate the request is to create an IAM role with a permissions policy that only provides the minimum permissions requires (least privilege). This IAM role should have a customermanaged permissions policy applied with the PutMetricData allowed. The PutMetricData API publishes metric data points to Amazon CloudWatch. CloudWatch associates the data points with the specified metric. If the specified metric does not exist, CloudWatch creates the metric. When CloudWatch creates a metric, it can take up to fifteen minutes for the metric to appear in calls to ListMetrics.The following images shows a permissions policy being created with the PutMetricData permission:CORRECT: \"Create an IAM role with the PutMetricData permission and create a new Auto Scaling launch configuration to launch instances using that "])</script><script>self.__next_f.push([1,"role\" is the correct answerINCORRECT: \"Modify the CloudWatch metric policies to allow the PutMetricData permission to instances from the Auto Scaling group\" is incorrect as this is not possible. You should instead grant the permissions through a permissions policy and attach that to a role that the EC2 instances can assume.INCORRECT: \"Create an IAM user with the PutMetricData permission and modify the Auto Scaling launch configuration to inject the user credentials into the instance user data\" is incorrect. You cannot “inject user credentials” using a launch configuration. Instead, you can attach an IAM role which allows the instance to assume the role and take on the privileges allowed through any permissions policies that are associated with that role.INCORRECT: \"Create an IAM role with the PutMetricData permission and modify the Amazon EC2 instances to use that role\" is incorrect as you should create a new launch configuration for the Auto Scaling group rather than updating the instances manually.References:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iamrolesforamazonec2.html https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_PutMetricData.html25:T582,When you request a strongly consistent read, DynamoDB returns a response with the most uptodate data, reflecting the updates from all prior write operations that were successful. The GetItem operation returns a set of attributes for the item with the given primary key. If there is no matching item, GetItem does not return any data and there will be no Item element in the response. GetItem provides an eventually consistent read by default. If your application requires a strongly consistent read, set ConsistentRead to true. Although a strongly consistent read might take more time than an eventually consistent read, it always returns the last updated value. Therefore, the Developer should set ConsistentRead to true when calling GetItem. CORRECT: \"Set ConsistentRead to true when calling GetItem\" is the correct answer. INCORRECT: \"Create "])</script><script>self.__next_f.push([1,"a new DynamoDB Accelerator (DAX) table\" is incorrect as DAX is not used to enable strongly consistent reads. DAX is used for improving read performance as it caches data in an inmemory cache. INCORRECT: \"Set consistency to strong when calling UpdateTable\" is incorrect as you cannot use this API action to configure consistency at a table level. INCORRECT: \"Use the GetShardIterator command\" is incorrect as this is not related to DynamoDB, it is related to Amazon Kinesis. References: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_GetItem.html26:T5b1,The application specification file (AppSpec file) is a YAMLformatted or JSONformatted file used by CodeDeploy to manage a deployment.If your application uses the Amazon ECS compute platform, the AppSpec file is named appspec.yaml. It is used by CodeDeploy to determine:• Your Amazon ECS task definition file. This is specified with its ARN in the TaskDefinition instruction in the AppSpec file.• The container and port in your replacement task set where your Application Load Balancer or Network Load Balancer reroutes traffic during a deployment. This is specified with the LoadBalancerInfo instruction in the AppSpec file.• Optional information about your Amazon ECS service, such the platform version on which it runs, its subnets, and its security groups.• Optional Lambda functions to run during hooks that correspond with lifecycle events during an Amazon ECS deployment. For more information, see AppSpec 'hooks' Section for an Amazon ECS Deployment.CORRECT: \"The AppSpec file\" is the correct answer.INCORRECT: \"The BuildSpec file\" is incorrect as this is a file type that is used with AWS CodeBuild.INCORRECT: \"The Template file\" is incorrect as this is something that is used with AWS CloudFormation and AWS SAM.INCORRECT: \"The Policy file\" is incorrect as a policy is typically referring to an IAM permissions policy document.References: https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfile.html#appspecreferenceecs27:T6cb,In general"])</script><script>self.__next_f.push([1,", Scan operations are less efficient than other operations in DynamoDB. A Scan operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set.If possible, you should avoid using a Scan operation on a large table or index with a filter that removes many results. Also, as a table or index grows, the Scan operation slows. The Scan operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation. For faster response times, design your tables and indexes so that your applications can use Query instead of Scan. (For tables, you can also consider using the GetItem and BatchGetItem APIs.)Additionally, eventual consistency consumes fewer RCUs than strong consistency. Therefore, the application should be refactored to use query APIs with eventual consistency.CORRECT: \"Modify the application to issue query API calls with eventual consistency reads\" is the correct answer.INCORRECT: \"Modify the application to issue scan API calls with strong eventual reads\" is incorrect as the Scan API is less efficient as it will return all items in the table.INCORRECT: \"Modify the application to issue query API calls with strong consistency reads\" is incorrect as strong consistency reads will consume more RCUs.INCORRECT: \"Modify the application to issue scan API calls with strong consistency reads\" is incorrect as the Scan API is less efficient as it will return all items in the table and strong consistency reads will use more RCUs.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bpqueryscan.html28:T6f0,Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a highresolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period"])</script><script>self.__next_f.push([1," of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of60 seconds.User activity is not a standard CloudWatch metric and as stated above for the resolution we need in this scenario a custom CloudWatch metric is required anyway. Therefore, for this scenario the Developer should create a highresolution custom Amazon CloudWatch metric for user activity data and publish the data every 10 seconds.CORRECT: \"Create a highresolution custom Amazon CloudWatch metric for user activity data, then publish data every 10 seconds\" is the correct answer.INCORRECT: \"Create a highresolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds\" is incorrect as the resolution is higher than required which will cost more. We need the resolution to be 20 seconds so that means publishing in 10 second intervals with 2 data points. At 5 second intervals there would be 4 data points which will incur additional costs.INCORRECT: \"Create a standardresolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds\" is incorrect as standard resolution metrics have a granularity of one minute.INCORRECT: \"Create a standardresolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds\" is incorrect as standard resolution metrics have a granularity of one minute.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html29:T83a,"])</script><script>self.__next_f.push([1,"You can use Amazon S3 to host a static website. On a static website, individual webpages include static content. They might also contain clientside scripts.To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document.When you enable static website hosting for your bucket, you enter the name of the index document (for example, index.html). After you enable static website hosting for your bucket, you upload an HTML file with the index document name to your bucket. Note that an error document is optional.To provide permissions, it is necessary to disable “block public access” settings and then create a bucket policy that grants everyone the s3:GetObject permission. For example:{\"Version\": \"20121017\",\"Statement\": [{\"Sid\": \"PublicReadGetObject\",\"Effect\": \"Allow\",\"Principal\": \"*\",\"Action\": [\"s3:GetObject\"],\"Resource\": [\"arn:aws:s3:::example.com/*\"]}]}CORRECT: \"Upload an index document and enter the name of the index document when enabling static website hosting\" is a correct answer.CORRECT: \"Enable public access and grant everyone the s3:GetObject permissions\" is also a correct answer.INCORRECT: \"Upload an index and error document and enter the name of the index and error documents when enabling static website hosting\" is incorrect as the error document is optional and the question specifically asks for the steps that MUST be completed.INCORRECT: \"Create an object access control list (ACL) granting READ permissions to the AllUsers group\" is incorrect. This may be necessary if the bucket objects are not owned by the bucket owner but the question states that the Developer created thebucket and uploaded the objects and so must be the object owner.INCORRECT: \"Upload a certificate from AWS Certificate Manager\" is incorrect as this is not supported or necessary for staticwebsites on Amazon S3.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html"])</script><script>self.__next_f.push([1,"2a:T502,There are two types of API logging in CloudWatch: execution logging and access logging. In execution logging, API Gateway manages the CloudWatch Logs. The process includes creating log groups and log streams, and reporting to the log streams any caller's requests and responses.The logged data includes errors or execution traces (such as request or response parameter values or payloads), data used by Lambda authorizers, whether API keys are required, whether usage plans are enabled, and so on.In access logging, you, as an API Developer, want to log who has accessed your API and how the caller accessed the API. You can create your own log group or choose an existing log group that could be managed by API Gateway.CORRECT: \"Enable API Gateway execution logging\" is a correct answer.CORRECT: \"Enable API Gateway access logs\" is also a correct answer.INCORRECT: \"Enable detailed logging in Amazon CloudWatch\" is incorrect. Detailed logging does not provide the requested information.INCORRECT: \"Create an API Gateway usage plan\" is incorrect. This will not enable logging.INCORRECT: \"Enable server access logging\" is incorrect. This is a type of logging that applies to Amazon S3 buckets.References: https://docs.aws.amazon.com/apigateway/latest/Developerguide/setuplogging.html2b:T525,CloudFront is a web service that gives businesses and web application developers an easy and costeffective way to distribute content with low latency and high data transfer speeds. CloudFront is a good choice for distribution of frequently accessed static content that benefits from edge delivery—like popular website images, videos, media files or software downloads.CORRECT: \"Use Amazon CloudFront to cache the website content\" is the correct answer.INCORRECT: \"Use Amazon ElastiCache to cache the website content\" is incorrect as ElastiCache is used for caching the contents of databases, not S3 buckets.INCORRECT: \"Use cross region replication to replicate the bucket to several global regions\" is incorrect as though this would get the content"])</script><script>self.__next_f.push([1," closer to users it would not provide a mechanism for connecting to those copies. This could be achieved using Route 53 latency based routing however it would be easier to use CloudFront.INCORRECT: \"Use Amazon S3 Transfer Acceleration to improve the performance of the website\" is incorrect as this service is used for improving the performance of uploads to Amazon S3.References:https://aws.amazon.com/blogs/networkingandcontentdelivery/amazons3amazoncloudfrontamatchmadeinthecloud/ https://aws.amazon.com/premiumsupport/knowledgecenter/cloudfrontservestaticwebsite/2c:T56c,ElastiCache is a fully managed, low latency, inmemory data store that supports either Memcached or Redis. With ElastiCache, management tasks such as provisioning, setup, patching, configuration, monitoring, backup, and failure recovery are taken care of, so you can focus on application development.Amazon ElastiCache is a popular choice for realtime use cases like Caching, Session Stores, Gaming, Geospatial Services, RealTime Analytics, and Queuing. For this scenario, the company is currently running an inmemory database and needs to ensure similar performance, so this is an ideal use case for ElastiCache.CORRECT: \"Amazon ElastiCache\" is the correct answer.INCORRECT: \"Amazon RDS\" is incorrect as RDS is not an inmemory database so the performance may not be as good as ElastiCache.INCORRECT: \"Amazon DynamoDB\" is incorrect as this is not an inmemory database. DynamoDB does offer great performance but if you need an inmemory cache you must use DynamoDB Accelerator (DAX).INCORRECT: \"Amazon Elastic Beanstalk\" is incorrect as this is not a database service at all. You can launch databases such as RDS through Elastic Beanstalk, however EB itself is a platform service responsible for launching and managing the resources.References:https://aws.amazon.com/blogs/database/buildingarealtimegamingleaderboardwithamazonelasticacheforredis/ https://aws.amazon.com/elasticache/features/2d:T853,"])</script><script>self.__next_f.push([1,"A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API.A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity.When a client makes a request to one of your API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output.There are two types of Lambda authorizers:• A tokenbased Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token.• A request parameterbased Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of headers, query string parameters, stageVariables, and $context variables.For this scenario, a Lambda authorizer is the most secure method available. It can also be used with usage plans and AWS recommend that you don’t rely only on API keys, so a Lambda authorizer is a better solution.CORRECT: \"Create an API Gateway Lambda authorizer\" is the correct answer.INCORRECT: \"Setup usage plans and distribute API keys to the customers\" is incorrect as this is not the most secure (safest) option. AWS recommend that you don't rely on API keys as your only means of authentication and authorization for your APIs.INCORRECT: \"Create an Amazon Cognito identity pool\" is incorrect. You can create an authorizer in API Gateway that uses Cognito user pools, but not identity pools.INCORRECT: \"Use AWS Single Signon to authenticate the customers\" is incorrect. This is used to centrally access multiple AWS accounts and business applications from one place.References:https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayuselambdaauthorizer.html https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayapiusageplans.html https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayintegratewithcognito.html"])</script><script>self.__next_f.push([1,"2e:T64a,Cluster queries are expressions that enable you to group objects. For example, you can group container instances by attributes such as Availability Zone, instance type, or custom metadata.After you have defined a group of container instances, you can customize Amazon ECS to place tasks on container instances based on group. You can also apply a group filter when listing container instances.As an example, the following cluster query expressions selects instances with the specified instance type: attribute:ecs.instancetype == t2.smallThe following cluster query expressions selects instances in the useast1a or useast1b availability zones: attribute:ecs.availabilityzone in [useast1a, useast1b]The Developer should therefore use the cluster query language to generate expressions that can group the container instances by instance type and availability zone.CORRECT: \"Cluster Query Language\" is the correct answer.INCORRECT: \"Task Group\" is incorrect as this is just a set of related tasks with the same task group name.INCORRECT: \"Task Placement Constraints\" is incorrect. A task placement constraint is a rule that is considered during task placement. Task placement constraints can be specified when either running a task or creating a new service.INCORRECT: \"Task Placement Strategy\" is incorrect. A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/clusterquerylanguage.html2f:T972,"])</script><script>self.__next_f.push([1,"The template shown is an AWS SAM template for deploying a serverless application. This can be identified by the template header: Transform: 'AWS::Serverless20161031'The Developer will need to package and then deploy the template. To do this the source code must be available in the same directory or referenced using the “codeuri” parameter. Then, the Developer can use the “aws cloudformation package” or “sam package” commands to prepare the local artifacts (local paths) that your AWS CloudFormation template references.The command uploads local artifacts, such as source code for an AWS Lambda function or a Swagger file for an AWS API Gateway REST API, to an S3 bucket. The command returns a copy of your template, replacing references to local artifacts with the S3 location where the command uploaded the artifacts.Once that is complete the template can be deployed using the “aws cloudformation deploy” or “sam deploy” commands.Therefore, the next step in this scenario is for the Developer to run the “aws cloudformation” package command to upload the source code to an Amazon S3 bucket and produce a modified CloudFormation template. An example of this command is provided below:aws cloudformation package templatefile /path_to_template/template.json s3bucket bucketname outputtemplatefilepackagedtemplate.jsonCORRECT: \"Run the aws cloudformation package command to upload the source code to an Amazon S3 bucket and produce a modified CloudFormation template\" is the correct answer.INCORRECT: \"Run the aws cloudformation compile command to base64 encode and embed the source file into a modified CloudFormation template\" is incorrect as the Developer should run the “aws cloudformation package” command.INCORRECT: \"Run the aws lambda zip command to package the source file together with the CloudFormation template and deploy the resulting zip archive\" is incorrect as the Developer should run the “aws cloudformation package” command which will automatically copy the relevant files to Amazon S3.INCORRECT: \"Run the aws serverless createpackage command to embed the source file directly into the existing CloudFormation template\" is incorrect as the Developer has the choice to run either “aws cloudformation package” or “sam package”, but not “aws serverless createpackage”.References: https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html"])</script><script>self.__next_f.push([1,"30:T59b,In addition to encryption at rest, which is a serverside encryption feature, AWS provides the Amazon DynamoDB Encryption Client. This clientside encryption library enables you to protect your table data before submitting it to DynamoDB. With serverside encryption, your data is encrypted in transit over an HTTPS connection, decrypted at the DynamoDB endpoint, and then reencrypted before being stored in DynamoDB. Clientside encryption provides endtoend protection for your data from its source to storage in DynamoDB. CORRECT: \"Use the DynamoDB Encryption Client to enable endtoend protection using clientside encryption\" is the correct answer. INCORRECT: \"Use the UpdateTable operation to switch to a customer managed customer master key (CMK)\" is incorrect. This will not ensure data is encrypted before it is submitted to DynamoDB; to meet this requirement, clientside encryption must be used. INCORRECT: \"Use the UpdateTable operation to switch to an AWS managed customer master key (CMK)\" is incorrect. is will not ensure data is encrypted before it is submitted to DynamoDB; to meet this requirement, clientside encryption must be used. INCORRECT: \"Use AWS Certificate Manager (ACM) to create one certificate for each DynamoDB table\" is incorrect. ACM is used to create SSL/TLS certificates and you cannot attach these to a DynamoDB table. References: https://docs.aws.amazon.com/kms/latest/Developerguide/servicesdynamodb.html31:T563,Amazon RDS read replicas are used for offloading reads from the primary database instance. Read replicas provide a readonly copy of the database. In this scenario this represents the simplest way of achieving the required outcome. The application will need to be modified to point to the read replica for all read requests. This requires some code changes, but they are minimal.CORRECT: \"Create an RDS Read Replica and modify the application to send read requests to the replica\" is the correct answer.INCORRECT: \"Change the RDS database instance type to an instance with more CPU/RAM\" is incorrec"])</script><script>self.__next_f.push([1,"t as this is not a way of “offloading the read traffic from the database”. This is an example of scaling vertically, rather than scaling horizontally.INCORRECT: \"Create an RDS Multi AZ DB and modify the application to send read requests to the standby DB\" is incorrect as we are trying to simply offload read traffic which is a use case for a read replica. However, it is possible for some database engines (MySQL and MariaDB) to combine multiAZ and read replicas.INCORRECT: \"Create an ElastiCache Memcached cluster and modify the application to send read requests to the cluster\" is incorrect as this would require more code changes and higher cost. For this use case an RDS read replica will be simpler and cheaper.References: https://aws.amazon.com/rds/features/readreplicas/32:T97a,"])</script><script>self.__next_f.push([1,"Applications that run on an EC2 instance must include AWS credentials in their AWS API requests. You could have your developers store AWS credentials directly within the EC2 instance and allow applications in that instance to use those credentials. But developers would then have to manage the credentials and ensure that they securely pass the credentials to each instance and update each EC2 instance when it's time to rotate the credentials. That's a lot of additional work.Instead, you can and should use an IAM role to manage temporary credentials for applications that run on an EC2 instance. When you use a role, you don't have to distribute longterm credentials (such as a user name and password or access keys) to an EC2 instance. Instead, the role supplies temporary permissions that applications can use when they make calls to other AWS resources. When you launch an EC2 instance, you specify an IAM role to associate with the instance. Applications that run on the instance can then use the rolesupplied temporary credentials to sign API requests. There are two answers that would work in this scenario. In one a customermanaged policy is used and in the other an AWS managed policy is used. The customermanaged policy is more secure in this situation as it can be locked down with more granularity to ensure the EC2 instances can only read and write to the specific bucket.With an AWS managed policy, you must choose from read only or full access and full access would provide more access than is required:CORRECT: \"Create an IAM Role with a customermanaged policy attached that has the necessary permissions and attach the role to the EC2 instances\" is the correct answer.INCORRECT: \"Store an access key and secret ID that has the necessary permissions on the EC2 instances\" is incorrect as storing access keys on the EC2 instances is insecure and cumbersome to manage.INCORRECT: \"Create an IAM Role with an AWS managed policy attached that has the necessary permissions and attach the role to the EC2 instances\" is incorrect as the AWS managed policy would provide more privileges than required.INCORRECT: \"Use the AWS SDK and authenticate with a user account that has the necessary permissions on the EC2 instances \" is incorrect as you cannot authenticate through the AWS SDK using a user account on an EC2 instance.References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switchroleec2.html"])</script><script>self.__next_f.push([1,"33:T434,An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. Using an instance profile you can attach an IAM Role to an EC2 instance that the instance can then assume in order to gain access to AWS services.CORRECT: \"Use EC2 instance profiles\" is the correct answer.INCORRECT: \"Use AWS KMS to store and retrieve credentials\" is incorrect as KMS is used to manage encryption keys.INCORRECT: \"Store the credentials in AWS CloudHSM\" is incorrect as CloudHSM is also used to manage encryption keys. It is similar to KMS but uses a dedicated hardware device that is not multitenant.INCORRECT: \"Store the credentials in the ~/.aws/credentials file\" is incorrect as this is not the most secure option. The credentials file is associated with the AWS CLI and used for passing credentials in the form of an access key ID and secret access key when making programmatic requests from the command line.References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switchroleec2_instanceprofiles.html34:T496,Use environment variables to pass environmentspecific settings to your code. For example, you can have two functions with the same code but different configuration. One function connects to a test database, and the other connects to a production database. In this situation, you use environment variables to tell the function the hostname and other connection details for the database. You might also set an environment variable to configure your test environment to use more verbose logging or more detailed tracing.Therefore, using environment variables is the correct place to store the connection strings associated with the external endpoints.CORRECT: \"Environment variables\" is the correct answer.INCORRECT: \"Aliases\" is incorrect. A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN.INCORRECT: \"Tags\" is incorrect. Tags are keyvalue pairs that you attach to AWS resources"])</script><script>self.__next_f.push([1," to better organize them.INCORRECT: \"Versions\" is incorrect. You can use versions to manage the deployment of your AWS Lambda functions.References: https://docs.aws.amazon.com/lambda/latest/dg/configurationenvvars.html35:T41a,The simplest way to set up connections to AWS CodeCommit repositories is to configure Git credentials for CodeCommit in the IAM console, and then use those credentials for HTTPS connections.You can also use these same credentials with any thirdparty tool or individual development environment (IDE) that supports HTTPS authentication using a static user name and password. For examples, see For Connections from Development Tools.CORRECT: \"A set of credentials generated from IAM\" is the correct answer.INCORRECT: \"A GitHub secure authentication token\" is incorrect as this is not how you authenticated to CodeCommit.INCORRECT: \"A public and private SSH key file\" is incorrect as that is required for accessing CodeCommit using SSH.INCORRECT: \"An Amazon EC2 IAM role with CodeCommit permissions\" is incorrect as that would be used to provide access to administer CodeCommit. However, the question is asking how to authenticate a Git client to CodeCommit using HTTPS.References: https://docs.aws.amazon.com/codecommit/latest/userguide/settingupgc.html36:T48a,Amazon ElastiCache is being used to cache data from the Amazon RDS database to improve performance when performing queries. In this case the cache has stale stock volume data stored and is returning this information when customers are purchasing items.The resolution is to ensure that the cache is invalidated whenever the stock volume data is changed. This can be done in the application layer.CORRECT: \"The cache is not being invalidated when the stock volume data is changed\" is the correct answer.INCORRECT: \"The stock volume data is being retrieved using a writethrough ElastiCache cluster\" is incorrect. If this was the case the data would not be stale.INCORRECT: \"The Amazon RDS database is deployed as MultiAZ and the standby is inconsistent\" is incorrect."])</script><script>self.__next_f.push([1," MultiAZ standbys are not used for reading data and the replication is synchronous so it would not be inconsistent.INCORRECT: \"The Amazon RDS database has insufficient IOPS provisioned for its EBS volumes\" is incorrect. This is not the issue here; the stale data is being retrieved from the ElastiCache database.References: https://docs.aws.amazon.com/AmazonElastiCache/latest/redug/Strategies.html37:T985,"])</script><script>self.__next_f.push([1,"Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services as well as data stored in the AWS Cloud.In API Gateway, an API's method request can take a payload in a different format from the corresponding integration request payload, as required in the backend. Similarly, the backend may return an integration response payload different from the method response payload, as expected by the frontend.API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response.If an existing legacy service returns XMLstyle data, you can use the API Gateway to transform the output to JSON as part of your modernization effort. The API Gateway can be configured to transform the output of legacy services from XML to JSON, allowing them to make a move that is seamless and nondisruptive. The transformation is specified using JSONSchema.Therefore, the technique the Developer should use is to create a RESTful API with the API Gateway and transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates.CORRECT: \"Create a RESTful API with the API Gateway; transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates\" is the correct answer.INCORRECT: \"Create a RESTful API with the API Gateway; pass the incoming JSON to the SOAP interface through an Application Load Balancer\" is incorrect as we don’t need an ALB to do this, we can use a mapping template within the API Gateway which will be more costefficient.INCORRECT: \"Create a RESTful API with the API Gateway; pass the incoming XML to the SOAP interface through an Application Load Balancer\" is incorrect as the incoming data will be JSON, not XML as the Developer needs to publish a modern application interface. A mapping template should also be used in place of the ALB.INCORRECT: \"Create a RESTful API with the API Gateway; transform the incoming XML into a valid message for the SOAP interface using mapping templates\" is incorrect as the incoming data will be JSON, not XML as the Developer needs to publish a modern application interface.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/modelsmappings.html"])</script><script>self.__next_f.push([1,"38:T7a5,To encrypt an object at the time of upload, you need to add a header called xamzserversideencryption to the request to tell S3 to encrypt the object using SSEC, SSES3, or SSEKMS. The following code example shows a Put request using SSES3.Enabling encryption on an S3 bucket does not enforce encryption however, so it is still necessary to take extra steps to force compliance with the policy. As the message in the image below states, bucket policies are applied before encryption settings so PUT requests without encryption information can be rejected by a bucket policy: Therefore, we need to create an S3 bucket policy that denies any S3 Put request that do not include the xamzserversideencryption header. There are two possible values for the xamzserversideencryption header: AES256, which tells S3 to use S3managed keys, and aws:kms, which tells S3 to use AWS KMS–managed keys.CORRECT: \"Create an S3 bucket policy that denies any S3 Put request that does not include the xamzserversideencryption\" is the correct answer.INCORRECT: \"Create a bucket policy that denies the S3 PutObject request with the attribute xamzacl having values publicread, publicreadwrite, or authenticatedread\" is incorrect. This policy means that authenticated users cannot upload objects to the bucket if the objects have public permissions.INCORRECT: \"Enable ServerSide Encryption with Amazon S3Managed Keys (SSES3) on the Amazon S3 bucket\" is incorrect as this will enable default encryption but will not enforce encryption on the S3 bucket. You do still need to enable default encryption on the bucket, but this alone will not enforce encryption.INCORRECT: \"Create an Amazon CloudWatch alarm that notifies an administrator if unencrypted objects are uploaded to the S3 bucket\" is incorrect. This is operationally difficult to manage and only notifies, it does not prevent.References: https://aws.amazon.com/blogs/security/howtopreventuploadsofunencryptedobjectstoamazons3/39:T633,With AWS KMS you can choose to have AWS KMS automatically rotate CMKs every"])</script><script>self.__next_f.push([1," year, provided that those keys were generated within AWS KMS HSMs. Automatic key rotation is not supported for imported keys, asymmetric keys, or keys generated in an AWS CloudHSM cluster using the AWS KMS custom key store feature.If you choose to import keys to AWS KMS or asymmetric keys or use a custom key store, you can manually rotate them by creating a new CMK and mapping an existing key alias from the old CMK to the new CMK.If you choose to have AWS KMS automatically rotate keys, you don’t have to reencrypt your data. AWS KMS automatically keeps previous versions of keys to use for decryption of data encrypted under an old version of a key. All new encryption requests against a key in AWS KMS are encrypted under the newest version of the key.CORRECT: \"Use AWS KMS keys with automatic rotation enabled\" is the correct answer.INCORRECT: \"Import a custom key into AWS KMS and configure automatic rotation\" is incorrect as per the explanation above KMS will not automatically rotate imported encryption keys (it can automatically rotate imported CMKs though).INCORRECT: \"Encrypt the data within the application before writing to S3\" is incorrect as this is both an incomplete solution (where would the encryption keys come from) and would also likely require more maintenance and management overhead.INCORRECT: \"Configure automatic rotation with AWS Secrets Manager\" is incorrect as Secrets Manager is used for rotating credentials, not encryption keys.References: https://aws.amazon.com/kms/faqs/3a:T78c,Standard API Gateway parameter and response code mapping templates allow you to map parameters onetoone and map a family of integration response status codes (matched by a regular expression) to a single response status code.Mapping template overrides provides you with the flexibility to perform manytoone parameter mappings; override parameters after standard API Gateway mappings have been applied; conditionally map parameters based on body content or other parameter values; programmatically create new parameters on the fl"])</script><script>self.__next_f.push([1,"y; and override status codes returned by yourintegration endpoint.Any type of request parameter, response header, or response status code may be overridden.Following are example uses for a mapping template override:• To create a new header (or overwrite an existing header) as a concatenation of two parameters• To override the response code to a success or failure code based on the contents of the body• To conditionally remap a parameter based on its contents or the contents of some other parameter• To iterate over the contents of a json body and remap key value pairs to headers or query stringsTherefore, the Developer can convert the query string parameters by creating a mapping template.CORRECT: \"Create a mapping template\" is the correct answer.INCORRECT: \"Enable request validation\" is incorrect as this is used to configure API Gateway to perform basic validation of an API request before proceeding with the integration request.INCORRECT: \"Include the Amazon Resource Name (ARN) of the Lambda function\" is incorrect as that doesn’t assist with converting the query string parameters.INCORRECT: \"Change the integration type\" is incorrect as to perform a conversion the Lambda integration does not need to have a different integration type such as Lambda proxy.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayoverriderequestresponseparameters.html3b:T840,"])</script><script>self.__next_f.push([1,"AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments. Each deployment policy has advantages and disadvantages and it’s important to select the best policy to use for each situation.The following tables summarizes the different deployment policies:The “rolling with additional batch” policy will add an additional batch of instances, updates those instances, then move onto the next batch.Rolling with additional batch:• Like Rolling but launches new instances in a batch ensuring that there is full availability.• Application is running at capacity.• Can set the bucket size.• Application is running both versions simultaneously.• Small additional cost.• Additional batch is removed at the end of the deployment.• Longer deployment.• Good for production environments.For this scenario there can be no reduction in application performance and availability during the update. The question also asks for the most costeffective choice.Therefore, the “rolling with additional batch” is the best choice as it will ensure fully availability of the application but minimize cost as the additional batch size can be kept small.CORRECT: \"Rolling with additional batch\" is the correct answer.INCORRECT: \"Rolling\" is incorrect as this will result in a reduction in capacity as there is no additional batch of instances introduced to the environment. This is a better choice if speed is required and a reduction in capacity of a batch size is acceptable.INCORRECT: \"All at once\" is incorrect as this will take the application down and cause a complete outage of the application during the update.INCORRECT: \"Immutable\" is incorrect as this is the most expensive option as it doubles capacity with a whole new set of instances attached to a new ASG.References:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.rollingversiondeploy.html"])</script><script>self.__next_f.push([1,"3c:T421,Crossorigin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. In this scenario the S3 bucket is the requestor and is requesting access to resources served by Amazon API Gateway and AWS Lambda. Therefore, the CORS configuration must be enabled on the requested endpoint which is the method in API Gateway.CORRECT: \"Enable crossorigin resource sharing (CORS) for the method in API Gateway\" is the correct answer.INCORRECT: \"Enable crossorigin resource sharing (CORS) on the S3 bucket\" is incorrect as CORS must be enabled on the requested endpoint which is API Gateway, not S3.INCORRECT: \"Add the AccessControlRequestMethod header to the request\" is incorrect as this is a request header value that asks permission to use a specific HTTP method.INCORRECT: \"Add the AccessControlRequestHeaders header to the request\" is incorrect as this notifies a server what headers will be sent in a request.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html3d:T5a1,Amazon S3 default encryption provides a way to set the default encryption behavior for an S3 bucket. You can set default encryption on a bucket so that all new objects are encrypted when they are stored in the bucket. The objects are encrypted using serverside encryption with either Amazon S3managed keys (SSES3) or customer master keys (CMKs) stored in AWS Key Management Service (AWS KMS). CORRECT: \"Enable default encryption when creating the bucket\" is the correct answer. INCORRECT: \"Use SSL to transmit the data over the Internet\" is incorrect as this only deals with encrypting the data whilst it is being transmitted, it does not provide encryption at rest. INCORRECT: \"Ensure all requests use the xamzserversideencryptioncustomerkey header\" is incorrect as it is unnecessary to use customerprovided keys. This is used with clientside encryption which is more complex to manage and is not required in this scenario. INCORRECT: \"Ensure all requests use the xamzserver"])</script><script>self.__next_f.push([1,"sideencryption header\" is incorrect as though this has the required effect of ensuring all data is encrypted, it is not the simplest method. In this scenario there is a team migrating data from different file shares which increases the risk of human error where a team member may neglect to add the header to the API call. Using default encryption on the bucket is a simpler solution. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/bucketencryption.html3e:T79b,Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, inmemory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add inmemory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management.How it works:• DAX is a writethrough caching service – this means the data is written to the cache as well as the back end store atthe same time.• Allows you to point your DynamoDB API calls at the DAX cluster and if the item is in the cache (cache hit), DAX returnsthe result to the application.• If the item requested is not in the cache (cache miss) then DAX performs an Eventually Consistent GetItem operationsagainst DynamoDB• Retrieval of data from DAX reduces the read load on DynamoDB tables.• This may result in being able to reduce the provisioned read capacity on the table. DynamoDB DAX is the correct solution for best performance for a readheavy workload.CORRECT: \"Amazon DynamoDB Accelerator (DAX)\" is the correct answer.INCORRECT: \"Amazon CloudFront\" is incorrect. CloudFront is a content delivery network (CDN) that is used for serving static assets from a cache around the globe. It is used to get content closer to end users for better performance. However, it cannot cache DynamoDB read requests.INCORRECT: \"Amazon ElastiCache\" is incorrect. ElastiCache is an inmemory database that can be used for caching read reque"])</script><script>self.__next_f.push([1,"sts to a backend database. However, ElastiCache is not the correct choice to put in front of DynamoDB (better for RDS), you should choose DAX instead.INCORRECT: \"Amazon SQS\" is incorrect. An SQS queue is used for decoupling application components, this will not assist with improving the performance of the DynamoDB database.References: https://aws.amazon.com/dynamodb/dax/3f:T851,"])</script><script>self.__next_f.push([1,"With provisioned capacity mode, you specify the number of data reads and writes per second that you require for your application.Read capacity unit (RCU):• Each API call to read data from your table is a read request.• Read requests can be strongly consistent, eventually consistent, or transactional.• For items up to 4 KB in size, one RCU can perform one strongly consistent read request per second.• Items larger than 4 KB require additional RCUs.• For items up to 4 KB in size, one RCU can perform two eventually consistent read requests per second.• Transactional read requests require two RCUs to perform one read per second for items up to 4 KB.• For example, a strongly consistent read of an 8 KB item would require two RCUs, an eventually consistent read of an 8 KB item would require one RCU, and a transactional read of an 8 KB item would require four RCUs.Write capacity unit (WCU):• Each API call to write data to your table is a write request.• For items up to 1 KB in size, one WCU can perform one standard write request per second.• Items larger than 1 KB require additional WCUs.• Transactional write requests require two WCUs to perform one write per second for items up to 1 KB.• For example, a standard write request of a 1 KB item would require one WCU, a standard write request of a 3 KB item would require three WCUs, and a transactional write request of a 3 KB item would require six WCUs.To determine the number of RCUs required to handle 100 strongly consistent reads per/second with an average item size of 5KB, perform the following steps:1. Determine the average item size by rounding up the next multiple of 4KB (5KB rounds up to 8KB).2. Determine the RCU per item by dividing the item size by 4KB (8KB/4KB = 2).3. Multiply the value from step 2 with the number of reads required per second (2x100 = 200).CORRECT: \"200 Read Capacity Units\" is the correct answer.INCORRECT: \"50 Read Capacity Units\" is incorrect.INCORRECT: \"250 Read Capacity Units\" is incorrect.INCORRECT: \"500 Read Capacity Units\" is incorrect.References: https://aws.amazon.com/dynamodb/pricing/provisioned/"])</script><script>self.__next_f.push([1,"40:T42a,When you create a user pool in Amazon Cognito and then configure a domain for it, Amazon Cognito automatically provisions a hosted web UI to let you add signup and signin pages to your app. You can add a custom logo or customize the CSS for the hosted web UI.CORRECT: \"Customize the Amazon Cognito hosted web UI and add the company logo\" is the correct answer.INCORRECT: \"Create a REST API using Amazon API Gateway and add a Cognito authorizer. Upload the company logo to a stage in the API\" is incorrect. There is no need to add a REST API to this solution.INCORRECT: \"Upload the company logo to an Amazon S3 bucket. Specify the S3 object path in the app client settings in Amazon Cognito\" is incorrect. This is not required as the hosted web UI can be used.INCORRECT: \"Create a custom login page that includes the company logo and upload it to Amazon Cognito. Specify the login page in the app client settings\" is incorrect. This is not required as the hosted web UI can be used.References: https://aws.amazon.com/premiumsupport/knowledgecenter/cognitohostedwebui/41:T5bd,If you would like to run a Docker image that is available in Amazon ECR, you can pull it to your local environment with the docker pull command. You can do this from either your default registry or from a registry associated with another AWS account.Docker CLI does not support standard AWS authentication methods, so client authentication must be handled so that ECR knows who is requesting to push or pull an image. To do this you can issue the aws ecr getlogin or aws ecr getloginpassword (AWS CLI v2) and then use the output to login using docker login and then issue a docker pull command specifying the repository and image with the REPOSITORY URI : TAG format.CORRECT: \"Run the output of the following: aws ecr getlogin and then run: docker pull REPOSITORY URI : TAG\" is the correct answer.INCORRECT: \"Run the following: docker pull REPOSITORY URI : TAG\" is incorrect as you need to authenticate first.INCORRECT: \"Run the following: aws ecr getlogin and then "])</script><script>self.__next_f.push([1,"run: docker pull REPOSITORY URI : TAG\" is incorrect as you need to run the output of the “aws ecr getlogin” command before you can issue a “docker pull” command.INCORRECT: \"Run the output of the following: aws ecr getdownloadurlforlayer and then run: docker pull REPOSITORY URI :TAG\" is incorrect as the first command is incorrect. You need to run the output of the “aws ecr getlogin” command instead.References: https://docs.aws.amazon.com/AmazonECR/latest/userguide/dockerpullecrimage.html42:T6a7,A serverless application is a combination of Lambda functions, event sources, and other resources that work together to perform tasks. Note that a serverless application is more than just a Lambda function—it can include additional resources such as APIs, databases, and event source mappings.AWS SAM templates are an extension of AWS CloudFormation templates, with some additional components that make them easier to work with.To create a Lambda function using an AWS SAM template the Developer can use the AWS::Serverless::Function resource type.The AWS::Serverless::Function resource type can be used to Create a Lambda function, IAM execution role, and event source mappings that trigger the function.To create a DynamoDB table using an AWS SAM template the Developer can use the AWS::Serverless::SimpleTable resource type which creates a DynamoDB table with a single attribute primary key. It is useful when data only needs to be accessed via a primary key.CORRECT: \"AWS::Serverless:Function\" is a correct answer.CORRECT: \"AWS::Serverless:SimpleTable\" is also a correct answer.INCORRECT: \"AWS::Serverless::Application\" is incorrect as this embeds a serverless application from the AWS Serverless Application Repository or from an Amazon S3 bucket as a nested application.INCORRECT: \"AWS::Serverless:LayerVersion\" is incorrect as this creates a Lambda LayerVersion that contains library or runtime code needed by a Lambda Function.INCORRECT: \"AWS::Serverless:API\" is incorrect as this creates a collection of Amazon API Gateway res"])</script><script>self.__next_f.push([1,"ources and methods that can be invoked through HTTPS endpoints.References: https://docs.aws.amazon.com/serverlessapplicationmodel/latest/developerguide/whatissam.html43:T5e9,When you create an object, you specify the key name, which uniquely identifies the object in the bucket. For example, in the Amazon S3 console, when you highlight a bucket, a list of objects in your bucket appears. These names are the object keys.The name for a key is a sequence of Unicode characters whose UTF8 encoding is at most 1024 bytes long.The Amazon S3 data model is a flat structure: you create a bucket, and the bucket stores objects. There is no hierarchy of subbuckets or subfolders. However, you can infer logical hierarchy using key name prefixes and delimiters as the Amazon S3 console does. The Amazon S3 console supports a concept of folders. Suppose that your bucket (admincreated) has four objectswith the following object keys:• Development/Projects.xls• Finance/statement1.pdf• Private/taxdocument.pdf• s3dg.pdfThe console uses the key name prefixes (Development/, Finance/, and Private/) and delimiter ('/') to present a folder structure as shown.CORRECT: \"Development/Projects.xls\" is the correct answer.INCORRECT: \"s3://dctlabs/Development/Projects.xls\" is incorrect as this is the full path to a file including the bucket name and object key.INCORRECT: \"Project=Blue\" is incorrect as this is an example of an object tag. You can use object tagging to categorize storage. Each tag is a keyvalue pair.INCORRECT: \"arn:aws:s3:::dctlabs\" is incorrect as this is the Amazon Resource Name (ARN) of a bucket.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html44:T762,You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API.Caching is enabled for a stage. When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified timetolive "])</script><script>self.__next_f.push([1,"(TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint.The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled. In this scenario we are asked to choose the most costefficient solution. Therefore, the best answer is to use a single API Gateway with three stages and, as caching is enabled per stage, we can choose to save cost by only enabling the cache on DEV and TEST when we need to perform tests relating to that functionality.CORRECT: \"Create a single API Gateway with three stages and enable the cache for the DEV and TEST environments only when required\" is the correct answer.INCORRECT: \"Create three API Gateways, one for each environment and enable the cache for the DEV and TEST environments only when required\" is incorrect. It is unnecessary to create separate API Gateways. This will increase complexity. Instead we can choose to use stages for the different environments.INCORRECT: \"Create a single API Gateway with three stages and enable the cache for all environments\" is incorrect as this would not be the most costefficient option.INCORRECT: \"Create a single API Gateway with three deployments and configure a global cache of 250GB\" is incorrect. When you deploy you API, you do so to a stage. Caching is enabled at the stage level, not globally.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaycaching.html45:T628,In your WebSocket API, incoming JSON messages are directed to backend integrations based on routes that you configure. (NonJSON messages are directed to a $default route that you configure.)A route includes a route key, which is the value that is expected once a route selection expression is evaluated.The routeSelectionExpression is an attribute defined at the API level. It specifies a JSON property that is expected to be present in the message payload.For example, if your JSON messages contain an "])</script><script>self.__next_f.push([1,"action property and you want to perform different actions based on this property, your route selection expression might be ${request.body.action}. Your routing table would specify which action to perform by matching the value of the action property against the custom route key values that you have defined in the table.CORRECT: \"Set the value of the route selection expression to $request.body.action\" is the correct answer.INCORRECT: \"Create a separate stage for each possible value of the action key\" is incorrect. There is no need to create separate stages, the action key can be used for routing as described above.INCORRECT: \"Create a mapping template to map the action key to an integration request\" is incorrect. Mapping templates are not used for routing to different integrations, they are used for transforming data.INCORRECT: \"Set the value of the route selection expression to $default\" is incorrect. The $default route is used for routing nonJSON messages.References: https://docs.aws.amazon.com/apigateway/latest/Developerguide/websocketapideveloproutes.html46:Tc83,"])</script><script>self.__next_f.push([1,"Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.Concurrency is subject to a Regional limit that is shared by all functions in a Region. For an initial burst of traffic, your functions' cumulative concurrency in a Region can reach an initial level of between 500 and 3000, which varies per Region:• 3000 – US West (Oregon), US East (N. Virginia), Europe (Ireland)• 1000 – Asia Pacific (Tokyo), Europe (Frankfurt)• 500 – Other RegionsAfter the initial burst, your functions' concurrency can scale by an additional 500 instances each minute. This continues until there are enough instances to serve all requests, or until a concurrency limit is reached. When requests come in faster than your function can scale, or when your function is at maximum concurrency, additional requests fail with a throttling error (429 status code).The function continues to scale until the account's concurrency limit for the function's Region is reached. The function catches up to demand, requests subside, and unused instances of the function are stopped after being idle for some time. Unused instances are frozen while they're waiting for requests and don't incur any charges.The regional concurrency limit starts at 1,000. You can increase the limit by submitting a request in the Support Center console. Calculating concurrency requirements for this scenarioTo calculate the concurrency requirements for this scenario, simply multiply the invocation requests per second (50) with the average execution time in seconds (100). This calculation is 50 x 100 = 5,000.Therefore, 5,000 concurrent executions is over the default limit and the Developer will need to request in the AWS Support Center console.CORRECT: \"Contact AWS and request to increase the limit for concurrent executions\" is the correct answer.INCORRECT: \"No action is required as AWS Lambda can easily accommodate this requirement\" is incorrect as by default the AWS account will be limited. Lambda can easily scale to this level of demand however the account limits must first be increased.INCORRECT: \"Increase the concurrency limit for the function\" is incorrect as the default account limit of 1,000 concurrent executions will mean you can only assign up to 900 executions to the function (100 must be left unreserved). This is insufficient for this requirement to the account limit must be increased.INCORRECT: \"Implement exponential backoff in the function code\" is incorrect. Exponential backoff means configuring the application to wait longer between API calls, slowing the demand. However, this is not a good resolution to this issue as it will have negative effects on the application. The correct choice is to raise the account limits so the function can concurrently execute according to its requirements.References: https://docs.aws.amazon.com/lambda/latest/dg/invocationscaling.html"])</script><script>self.__next_f.push([1,"47:T685,In this scenario the Developer needs to maximize efficiency of RCUs. Therefore, the Developer will need to consider the item size and consistency model to determine the most efficient usage of RCUs.Item size/consistency model: we know that both 1 KB items and 4 KB items consume the same number of RCUs as a read capacity unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size.The following bullets provide the read throughput for each configuration:• Eventually consistent, 15 RCUs, 1 KB item = 30 items/s = 2 items per RCU• Strongly consistent, 15 RCUs, 1 KB item = 15 items/s = 1 item per RCU• Eventually consistent, 5 RCUs, 4 KB item = 10 items/s = 2 items per RCU• Strongly consistent, 5 RCUs, 4 KB item = 5 items/s = 1 item per RCUFrom the above we can see that 4 KB items with eventually consistent reads is the most efficient option. Therefore, the Developer should choose the option “Eventually consistent reads of 5 RCUs reading items that are 4 KB in size”. This will achieve 2x 4 KB items per RCU.CORRECT: \"Eventually consistent reads of 5 RCUs reading items that are 4 KB in size\" is the correct answer.INCORRECT: \"Eventually consistent reads of 15 RCUs reading items that are 1 KB in size\" is incorrect as described above.INCORRECT: \"Strongly consistent reads of 5 RCUs reading items that are 4 KB in size\" is incorrect as described above.INCORRECT: \"Strongly consistent reads of 15 RCUs reading items that are 1KB in size\" is incorrect as described above.References: https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/ProvisionedThroughput.html48:T741,The requirements clearly state that we cannot impact the performance of the EC2 instances at all. Therefore, we will not be able to add certificates to the EC2 instances as that would place a burden on the CPU when encrypting and decrypting data. We are therefore left with configuring SSL on the Elastic Load Balancer itself. For this we need to add an SSL certi"])</script><script>self.__next_f.push([1,"ficate to the ELB and then configure the ELB for SSL termination.You can create an HTTPS listener, which uses encrypted connections (also known as SSL offload). This feature enables traffic encryption between your load balancer and the clients that initiate SSL or TLS sessions.To use an HTTPS listener, you must deploy at least one SSL/TLS server certificate on your load balancer. The load balancer uses a server certificate to terminate the frontend connection and then decrypt requests from clients before sending them to the targets.This is the most secure solution we can created without adding any performance impact to the EC2 instances.CORRECT: \"Add an SSL certificate to the Elastic Load Balancer\" is a correct answer.CORRECT: \"Configure the Elastic Load Balancer for SSL termination\" is also a correct answer.INCORRECT: \"Configure the Elastic Load Balancer with SSL passthrough\" is incorrect as this would be used to forward encrypted packets directly to the EC2 instance for termination but we do not want to add SSL certificates to the EC2 instances due to the extra processing required.INCORRECT: \"Install SSL certificates on the EC2 instances\" is incorrect as we do not want to add SSL certificates to the EC2 instances due to the extra processing required.INCORRECT: \"Configure ServerSide Encryption with KMS managed keys\" is incorrect as this applies to Amazon S3, not ELB.References: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/createhttpslistener.html49:T629,Amazon DynamoDB is integrated with AWS Lambda so that you can create triggers—pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.If you enable DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with an AWS Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function"])</script><script>self.__next_f.push([1," synchronously when it detects new stream records.CORRECT: \"Enable a DynamoDB stream and trigger the Lambda function synchronously from the stream\" is the correct answer.INCORRECT: \"Enable a DynamoDB stream and trigger the Lambda function asynchronously from the stream\" is incorrect as the invocation should be synchronous.INCORRECT: \"Configure an Amazon CloudWatch alarm that sends an Amazon SNS notification. Trigger the Lambda function asynchronously from the SNS notification\" is incorrect as you cannot configure a CloudWatch alarm that notifies based on item lifecycle events. It is better to use DynamoDB streams and integrate Lambda.INCORRECT: \"Configure an Amazon CloudTrail API alarm that sends a message to an Amazon SQS queue. Configure the Lambda function to poll the queue and invoke the function synchronously\" is incorrect. There is no such alarm that notifies from Amazon CloudTrail relating to item lifecycle events.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html4a:T975,"])</script><script>self.__next_f.push([1,"Time to Live (TTL) for Amazon DynamoDB lets you define when items in a table expire so that they can be automatically deleted from the database. With TTL enabled on a table, you can set a timestamp for deletion on a peritem basis, allowing you to limit storage usage to only those records that are relevant.TTL is useful if you have continuously accumulating data that loses relevance after a specific time period (for example, session data, event logs, usage patterns, and other temporary data). If you have sensitive data that must be retained only for a certain amount of time according to contractual or regulatory obligations, TTL helps you ensure that it is removed promptly and as scheduled.When Time to Live (TTL) is enabled on a table in Amazon DynamoDB, a background job checks the TTL attribute of items to determine whether they are expired.DynamoDB compares the current time, in epoch time format, to the value stored in the userdefined Number attribute of an item. If the attribute’s value is in the epoch time format, is less than the current time, and is not older than 5 years, the item is deleted.Processing takes place automatically, in the background, and doesn't affect read or write traffic to the table. In addition, deletes performed via TTL are not counted towards capacity units or request units. TTL deletes are available at no additional cost.For this requirement, the Developer must add an attribute to each item with the expiration time in epoch format and then enable the Time To Live (TTL) feature based on that attribute.CORRECT: \"Add an attribute with the expiration time; enable the Time To Live feature based on that attribute\" is the correct answer.INCORRECT: \"Each day, create a new table to hold session data; delete the previous day's table\" is incorrect. This solution would delete some data that is not 24 hours old as it would have to run at a specific time.INCORRECT: \"Write a script that deletes old records; schedule the scripts as a cron job on an Amazon EC2 instance\" is incorrect. This is not an elegant solution and would also cost more as it requires RCUs/WCUs to delete the items.INCORRECT: \"Add an attribute with the expiration time; name the attribute ItemExpiration\" is incorrect as this is not a complete solution. You also need to enable the TTL feature on the table.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworksttl.html"])</script><script>self.__next_f.push([1,"5:[\"$\",\"$Le\",null,{\"quiz\":{\"id\":\"aws-developer-4\",\"title\":\"AWS Developer Associate (DVA-C02) Practice Test Latest 2025 4\",\"description\":\"Additional practice questions covering AWS development topics.\",\"questions\":[{\"question\":\"A Development team are creating a new REST API that uses Amazon API Gateway and AWS Lambda. To support testing there need to be different versions of the service. What is the BEST way to provide multiple versions of the REST API?\",\"answers\":[{\"text\":\"Create an API Gateway resource policy to isolate versions and provide context to the Lambda functions\",\"isCorrect\":false},{\"text\":\"Deploy an HTTP Proxy integration and configure the proxy with API versions\",\"isCorrect\":false},{\"text\":\"Deploy the API versions as unique stages with unique endpoints and use stage variables to provide further context\",\"isCorrect\":true},{\"text\":\"Create an AWS Lambda authorizer to route API clients to the correct API version\",\"isCorrect\":false}],\"explanation\":\"$f\"},{\"question\":\"A Developer is creating a serverless application that includes Amazon API Gateway, Amazon DynamoDB and AWS Lambda. The Developer will use AWS CloudFormation to deploy the application and is creating a template. Which tool should the Developer use to define simplified syntax for expressing serverless resources?\",\"answers\":[{\"text\":\"AWS Serverless Application Model\",\"isCorrect\":true},{\"text\":\"OpenAPI Swagger Specification\",\"isCorrect\":false},{\"text\":\"AWS Step Functions State Machine\",\"isCorrect\":false},{\"text\":\"A CloudFormation Serverless Plugin\",\"isCorrect\":false}],\"explanation\":\"$10\"},{\"question\":\"A batch job runs every 24 hours and writes around 1 million items into a DynamoDB table each day. The batch job completes quickly, and the items are processed within 2 hours and are no longer needed. What's the MOST efficient way to provide an empty table each day?\",\"answers\":[{\"text\":\"Use the BatchUpdateItem API with expressions\",\"isCorrect\":false},{\"text\":\"Issue an AWS CLI aws dynamodb deleteitem command with a wildcard\",\"isCorrect\":false},{\"text\":\"Use the BatchWriteItem API with a DeleteRequest\",\"isCorrect\":false},{\"text\":\"Delete the entire table and recreate it each day\",\"isCorrect\":true}],\"explanation\":\"$11\"},{\"question\":\"An AWS Lambda function has been connected to a VPC to access an application running a private subnet. The Lambda function also pulls data from an Internetbased service and is no longer able to connect to the Internet. How can this be rectified?\",\"answers\":[{\"text\":\"Add a NAT Gateway to a public subnet and specify a route in the private subnet\",\"isCorrect\":true},{\"text\":\"Connect the Lambda function to an Internet Gateway\",\"isCorrect\":false},{\"text\":\"Connect an AWS VPN to Lambda to connect to the Internet\",\"isCorrect\":false},{\"text\":\"Add an Elastic IP to the Lambda function\",\"isCorrect\":false}],\"explanation\":\"$12\"},{\"question\":\"A Developer is using AWS SAM to create a template for deploying a serverless application. The Developer plans to leverage an application from the AWS Serverless Application Repository in the template as a nested application. Which resource type should the Developer specify?\",\"answers\":[{\"text\":\"AWS::Serverless:SimpleTable\",\"isCorrect\":false},{\"text\":\"AWS::Serverless:HttpApi\",\"isCorrect\":false},{\"text\":\"AWS::Serverless:Function\",\"isCorrect\":false},{\"text\":\"AWS::Serverless::Application\",\"isCorrect\":true}],\"explanation\":\"$13\"},{\"question\":\"A Developer has added a Global Secondary Index (GSI) to an existing Amazon DynamoDB table. The GSI is used mainly for read operations whereas the primary table is extremely writeintensive. Recently, the Developer has noticed throttling occurring under heavy write activity on the primary table. However, the write capacity units on the primary table are not fully utilized. What is the best explanation for why the writes are being throttled on the primary table?\",\"answers\":[{\"text\":\"The Developer should have added an LSI instead of a GSI\",\"isCorrect\":false},{\"text\":\"There are insufficient read capacity units on the primary table\",\"isCorrect\":false},{\"text\":\"There are insufficient write capacity units on the primary table\",\"isCorrect\":false},{\"text\":\"The write capacity units on the GSI are under provisioned\",\"isCorrect\":true}],\"explanation\":\"$14\"},{\"question\":\"There are multiple AWS accounts across multiple regions managed by a company. The operations team require a single operational dashboard that displays some key performance metrics from these accounts and regions. What is the SIMPLEST solution?\",\"answers\":[{\"text\":\"Create an Amazon CloudTrail trail that applies to all regions and deliver the logs to a single Amazon S3 bucket. Create a dashboard using the data in the bucket\",\"isCorrect\":false},{\"text\":\"Create an AWS Lambda function that collects metrics from each account and region and pushes the metrics to the account where the dashboard has been created\",\"isCorrect\":false},{\"text\":\"Create an Amazon CloudWatch crossaccount crossregion dashboard\",\"isCorrect\":true},{\"text\":\"Create an Amazon CloudWatch dashboard in one account and region and import the data from the other accounts and regions\",\"isCorrect\":false}],\"explanation\":\"$15\"},{\"question\":\"A Developer is creating a serverless application. The application looks up information about a customer using a separate Lambda function for each item such as address and phone number. The Developer has created branches in AWS Step Functions for each lookup function. How can the Developer optimize the performance, so the lookups complete faster?\",\"answers\":[{\"text\":\"Use a Map state to iterate over all the items.\",\"isCorrect\":false},{\"text\":\"Use a Parallel state to iterate over all the branches parallel.\",\"isCorrect\":true},{\"text\":\"Use a Wait state to reduce the wait time for function execution.\",\"isCorrect\":false},{\"text\":\"Use a Choice state to lookup the specific information required.\",\"isCorrect\":false}],\"explanation\":\"The Parallel state (\\\"Type\\\": \\\"Parallel\\\") can be used to create parallel branches of execution in your AWS Step Functions state machine. This will improve the performance of the application by ensuring that all information lookups occur in parallel.CORRECT: \\\"Use a Parallel state to iterate over all the branches parallel\\\" is the correct answer.INCORRECT: \\\"Use a Choice state to lookup the specific information required\\\" is incorrect. This is used to add additional logic but is not required and is unlikely to improve performance.INCORRECT: \\\"Use a Wait state to reduce the wait time for function execution\\\" is incorrect. The Wait state delays the state machine from continuing for a specified time.INCORRECT: \\\"Use a Map state to iterate over all the items\\\" is incorrect. The Map state executes the same steps for multiple entries of an array in the state input.References: https://docs.aws.amazon.com/stepfunctions/latest/dg/amazonstateslanguageparallelstate.html\"},{\"question\":\"An application running on Amazon EC2 generates a large number of small files (1KB each) containing personally identifiable information that must be converted to ciphertext. The data will be stored on a proprietary network-attached file system. What is the SAFEST way to encrypt the data using AWS KMS?\",\"answers\":[{\"text\":\"Encrypt the data directly with a customer managed customer master key\",\"isCorrect\":true},{\"text\":\"Create a data encryption key from a customer master key and encrypt the data with the customer master key\",\"isCorrect\":false},{\"text\":\"Create a data encryption key from a customer master key and encrypt the data with the data encryption key\",\"isCorrect\":false},{\"text\":\"Encrypt the data directly with an AWS managed customer master key\",\"isCorrect\":false}],\"explanation\":\"$16\"},{\"question\":\"A Developer is creating a DynamoDB table for storing transaction logs. The table has 10 write capacity units (WCUs). The Developer needs to configure the read capacity units (RCUs) for the table in order to MAXIMIZE the number of requestsallowed per second. Which of the following configurations should the Developer use?\",\"answers\":[{\"text\":\"Strongly consistent reads of 5 RCUs reading items that are 4 KB in size\",\"isCorrect\":false},{\"text\":\"Eventually consistent reads of 5 RCUs reading items that are 4 KB in size\",\"isCorrect\":false},{\"text\":\"Strongly consistent reads of 15 RCUs reading items that are 1KB in size\",\"isCorrect\":false},{\"text\":\"Eventually consistent reads of 15 RCUs reading items that are 1 KB in size\",\"isCorrect\":true}],\"explanation\":\"$17\"},{\"question\":\"A developer is making updates to the code for a Lambda function. The developer is keen to test the code updates by directing a small amount of traffic to a new version. How can this BEST be achieved?\",\"answers\":[{\"text\":\"Create a new function using the new code and update the application to split requests between the new functions\",\"isCorrect\":false},{\"text\":\"Create an alias that points to both the new and previous versions of the function code and assign a weighting for sending a portion of traffic to the new version\",\"isCorrect\":true},{\"text\":\"Create two versions of the function code. Configure the application to direct a subset of requests to the new version\",\"isCorrect\":false},{\"text\":\"Create an API using API Gateway and use stage variables to point to different versions of the Lambda function\",\"isCorrect\":false}],\"explanation\":\"$18\"},{\"question\":\"An Amazon DynamoDB table has been created using provisioned capacity. A manager needs to understand whether the DynamoDB table is costeffective. How can the manager query how much provisioned capacity is actually being used?\",\"answers\":[{\"text\":\"Monitor the ConsumedReadCapacityUnits and ConsumedWriteCapacityUnits over a specified time period\",\"isCorrect\":true},{\"text\":\"Monitor the ReadThrottleEvents and WriteThrottleEvents metrics for the table\",\"isCorrect\":false},{\"text\":\"Use Amazon CloudTrail and monitor the DescribeLimits API action\",\"isCorrect\":false},{\"text\":\"Use AWS XRay to instrument the DynamoDB table and monitor subsegments\",\"isCorrect\":false}],\"explanation\":\"$19\"},{\"question\":\"A development team manage a hightraffic eCommerce site with dynamic pricing that is updated in realtime. There have been incidents where multiple updates occur simultaneously and cause an original editor's updates to be overwritten. How can the developers ensure that overwriting does not occur?\",\"answers\":[{\"text\":\"Use conditional writes\",\"isCorrect\":true},{\"text\":\"Use atomic counters\",\"isCorrect\":false},{\"text\":\"Use batch operations\",\"isCorrect\":false},{\"text\":\"Use concurrent writes\",\"isCorrect\":false}],\"explanation\":\"$1a\"},{\"question\":\"A developer needs to add signup and signin capabilities for a mobile app. The solution should integrate with social identity providers (IdPs) and SAML IdPs. Which service should the developer use?\",\"answers\":[{\"text\":\"AWS Cognito identity pool\",\"isCorrect\":false},{\"text\":\"AWS Cognito user pool\",\"isCorrect\":true},{\"text\":\"API Gateway with a Lambda authorizer\",\"isCorrect\":false},{\"text\":\"AWS IAM and STS\",\"isCorrect\":false}],\"explanation\":\"$1b\"},{\"question\":\"An application reads data from Amazon S3 and makes 55,000 read requests per second. A Developer must design the storage solution to ensure the performance requirements are met cost-effectively. How can the storage be optimized to meet these requirements?\",\"answers\":[{\"text\":\"Create at least 10 prefixes and split the files across the prefixes.\",\"isCorrect\":true},{\"text\":\"Move the files to Amazon EFS. Index the files with S3 metadata.\",\"isCorrect\":false},{\"text\":\"Create at least 10 S3 buckets and split the files across the buckets.\",\"isCorrect\":false},{\"text\":\"Move the files to Amazon DynamoDB. Index the files with S3 metadata.\",\"isCorrect\":false}],\"explanation\":\"$1c\"},{\"question\":\"A Developer needs to be notified by email for all new object creation events in a specific Amazon S3 bucket. Amazon SNS will be used for sending the messages. How can the Developer enable these notifications?\",\"answers\":[{\"text\":\"Create an event notification for all s3:ObjectRemoved:Delete API calls\",\"isCorrect\":false},{\"text\":\"Create an event notification for all s3:ObjectCreated:Put API calls\",\"isCorrect\":false},{\"text\":\"Create an event notification for all s3:ObjectRestore:Post API calls\",\"isCorrect\":false},{\"text\":\"Create an event notification for all s3:ObjectCreated:* API calls\",\"isCorrect\":true}],\"explanation\":\"$1d\"},{\"question\":\"A company runs a popular website behind an Amazon CloudFront distribution that uses an Application Load Balancer as the origin. The Developer wants to set up custom HTTP responses to 404 errors for content that has been removed from the origin that redirects the users to another page. The Developer wants to use an AWS Lambda@Edge function that is associated with the current CloudFront distribution to accomplish this goal. The solution must use a minimum amount of resources. Which CloudFront event type should the Developer use to invoke the Lambda@Edge function that contains the redirect logic?\",\"answers\":[{\"text\":\"Viewer response\",\"isCorrect\":false},{\"text\":\"Origin request\",\"isCorrect\":false},{\"text\":\"Origin response\",\"isCorrect\":true},{\"text\":\"Viewer request\",\"isCorrect\":false}],\"explanation\":\"$1e\"},{\"question\":\"A Developer is creating an application that will process some data and generate an image file from it. The application will use an AWS Lambda function which will require 150 MB of temporary storage while executing. The temporary files will not be needed after the function execution is completed. What is the best location for the Developer to store the files?\",\"answers\":[{\"text\":\"Store the files in Amazon S3 and use a lifecycle policy to delete the files automatically\",\"isCorrect\":false},{\"text\":\"Store the files in an Amazon Instance Store and delete the files when the execution completes\",\"isCorrect\":false},{\"text\":\"Store the files in an Amazon EFS filesystem and delete the files when the execution completes\",\"isCorrect\":false},{\"text\":\"Store the files in the /tmp directory and delete the files when the execution completes\",\"isCorrect\":true}],\"explanation\":\"$1f\"},{\"question\":\"A Developer is creating a database solution using an Amazon ElastiCache caching layer. The solution must provide strong consistency to ensure that updates to product data are consistent between the backend database and the ElastiCache cache. Low latency performance is required for all items in the database. Which cache writing policy will satisfy these requirements?\",\"answers\":[{\"text\":\"Add a short duration TTL value to each write.\",\"isCorrect\":false},{\"text\":\"Invalidate the cache for each database write.\",\"isCorrect\":false},{\"text\":\"Use a writethrough caching strategy.\",\"isCorrect\":true},{\"text\":\"Use a lazyloading caching strategy.\",\"isCorrect\":false}],\"explanation\":\"$20\"},{\"question\":\"An AWS Lambda functions downloads a 50MB from an object storage system each time it is invoked. The download delays the function completion and causes intermittent timeouts which is slowing down the application. How can the application be refactored to resolve the timeout?\",\"answers\":[{\"text\":\"Increase the timeout of the function\",\"isCorrect\":false},{\"text\":\"Increase the concurrency allocation of the function\",\"isCorrect\":false},{\"text\":\"Increase the memory allocation of the function\",\"isCorrect\":false},{\"text\":\"Store the file in the /tmp directory of the execution context and reuse it on subsequent invocations\",\"isCorrect\":true}],\"explanation\":\"$21\"},{\"question\":\"An application will be hosted on the AWS Cloud. Developers will be using an Agile software development methodology with regular updates deployed through a continuous integration and delivery (CI/CD) model. Which AWS service can assist theDevelopers with automating the build, test, and deploy phases of the release process every time there is a code change?\",\"answers\":[{\"text\":\"AWS CodePipeline\",\"isCorrect\":true},{\"text\":\"AWS CloudFormation\",\"isCorrect\":false},{\"text\":\"AWS Elastic Beanstalk\",\"isCorrect\":false},{\"text\":\"AWS CodeBuild\",\"isCorrect\":false}],\"explanation\":\"$22\"},{\"question\":\"A Developer is deploying an application using Docker containers running on the Amazon Elastic Container Service (ECS). The Developer is testing application latency and wants to capture trace information between the microservices. Which solution will meet these requirements?\",\"answers\":[{\"text\":\"Install the AWS XRay daemon locally on an Amazon EC2 instance and instrument the Amazon ECS microservices using the XRay SDK.\",\"isCorrect\":false},{\"text\":\"Create a Docker image that runs the XRay daemon, upload it to a Docker image repository, and then deploy it to the Amazon ECS cluster.\",\"isCorrect\":true},{\"text\":\"Install the AWS XRay daemon on each of the Amazon ECS instances.\",\"isCorrect\":false},{\"text\":\"Install the Amazon CloudWatch agent on the container image. Use the CloudWatch SDK to publish custom metrics from each of the microservices.\",\"isCorrect\":false}],\"explanation\":\"$23\"},{\"question\":\"A customer requires a relational database for a transactional workload. Which type of AWS database is BEST suited to this requirement?\",\"answers\":[{\"text\":\"Amazon DynamoDB\",\"isCorrect\":false},{\"text\":\"Amazon RedShift\",\"isCorrect\":false},{\"text\":\"Amazon RDS\",\"isCorrect\":true},{\"text\":\"Amazon ElastiCache\",\"isCorrect\":false}],\"explanation\":\"Amazon Relational Database Service (Amazon RDS) is a managed service that makes it easy to set up, operate, and scale a relational database in the cloud. RDS is an Online Transaction Processing (OLTP) type of database. The primary use case is a transactional database (rather than analytical) and it is best for structured, relational data store requirements.CORRECT: \\\"Amazon RDS\\\" is the correct answer.INCORRECT: \\\"Amazon RedShift\\\" is incorrect as though this is a relational database it is best suited for analytics workloads rather than transactional workloads.INCORRECT: \\\"Amazon DynamoDB\\\" is incorrect as this is nonrelational database.INCORRECT: \\\"Amazon ElastiCache\\\" is incorrect as this is a key/value database used for caching databases (including AmazonRDS).References: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html\"},{\"question\":\"A developer is creating an Auto Scaling group of Amazon EC2 instances. The developer needs to publish a custom metric to Amazon CloudWatch. Which method would be the MOST secure way to authenticate a CloudWatch PUT request?\",\"answers\":[{\"text\":\"Create an IAM user with the PutMetricData permission and modify the Auto Scaling launch configuration to inject the user credentials into the instance user data\",\"isCorrect\":false},{\"text\":\"Create an IAM role with the PutMetricData permission and modify the Amazon EC2 instances to use that role\",\"isCorrect\":false},{\"text\":\"Create an IAM role with the PutMetricData permission and create a new Auto Scaling launch configuration to launch instances using that role\",\"isCorrect\":true},{\"text\":\"Modify the CloudWatch metric policies to allow the PutMetricData permission to instances from the Auto Scaling group\",\"isCorrect\":false}],\"explanation\":\"$24\"},{\"question\":\"A Developer has deployed an application that runs on an Auto Scaling group of Amazon EC2 instances. The application data is stored in an Amazon DynamoDB table and records are constantly updated by all instances. An instance sometimes retrieves old data. The Developer wants to correct this by making sure the reads are strongly consistent.\",\"answers\":[{\"text\":\"Use the GetShardIterator command\",\"isCorrect\":false},{\"text\":\"Set consistency to strong when calling UpdateTable\",\"isCorrect\":false},{\"text\":\"Create a new DynamoDB Accelerator (DAX) table\",\"isCorrect\":false},{\"text\":\"Set ConsistentRead to true when calling GetItem\",\"isCorrect\":true}],\"explanation\":\"$25\"},{\"question\":\"An Amazon EC2 instance requires permissions to read and write data in an Amazon S3 bucket. A Developer is creating an IAM role that will be assumed by the EC2 instance. When creating the role using the AWS CLI createrole command, which policy must be added to allow the instance to assume the role?\",\"answers\":[{\"text\":\"Trust policy\",\"isCorrect\":true},{\"text\":\"Inline policy\",\"isCorrect\":false},{\"text\":\"Bucket policy\",\"isCorrect\":false},{\"text\":\"Managed policy\",\"isCorrect\":false}],\"explanation\":\"The IAM role must have a policy attached that provides the permissions necessary to read and write data in the S3 bucket. Additionally, a trust policy must be attached. This policy defines which principals can assume the role, and under which conditions. This is sometimes referred to as a resourcebased policy for the IAM role.CORRECT: \\\"Trust policy\\\" is the correct answer.INCORRECT: \\\"Bucket policy\\\" is incorrect. A bucket policy is applied to a bucket, it does not allow the EC2 instance to assume the IAM role.INCORRECT: \\\"Inline policy\\\" is incorrect. Inline policies are permissions policies, not trust policies, that are applied directly to principals.INCORRECT: \\\"Managed policy\\\" is incorrect. A managed policy is a permissions policy managed by AWS.References: https://aws.amazon.com/blogs/security/howtousetrustpolicieswithiamroles/\"},{\"question\":\"Customers who use a REST API have reported performance issues. A Developer needs to measure the time between when API Gateway receives a request from a client and when it returns a response to the client. Which metric should the Developer monitor?\",\"answers\":[{\"text\":\"5XXError\",\"isCorrect\":false},{\"text\":\"CacheHitCount\",\"isCorrect\":false},{\"text\":\"IntegrationLatency\",\"isCorrect\":false},{\"text\":\"Latency\",\"isCorrect\":true}],\"explanation\":\"The Latency metric measures the time between when API Gateway receives a request from a client and when it returns a response to the client. The latency includes the integration latency and other API Gateway overhead.igital Cloud TrainingCORRECT: \\\"Latency\\\" is the correct answer.INCORRECT: \\\"IntegrationLatency\\\" is incorrect. This measures the time between when API Gateway relays a request to the backend and when it receives a response from the backend.INCORRECT: \\\"CacheHitCount\\\" is incorrect. This measures the number of requests served from the API cache in a given period.INCORRECT: \\\"5XXError\\\" is incorrect. This measures the number of serverside errors captured in a given period.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaymetricsanddimensions.html\"},{\"question\":\"A new application will be deployed using AWS CodeDeploy to Amazon Elastic Container Service (ECS). What must be supplied to CodeDeploy to specify the ECS service to deploy?\",\"answers\":[{\"text\":\"The AppSpec file\",\"isCorrect\":true},{\"text\":\"The Template file\",\"isCorrect\":false},{\"text\":\"The Policy file\",\"isCorrect\":false},{\"text\":\"The BuildSpec file\",\"isCorrect\":false}],\"explanation\":\"$26\"},{\"question\":\"An application searches a DynamoDB table to return items based on primary key attributes. A developer noticed some ProvisionedThroughputExceeded exceptions being generated by DynamoDB. How can the application be optimized to reduce the load on DynamoDB and use the LEAST amount of RCU?\",\"answers\":[{\"text\":\"Modify the application to issue query API calls with eventual consistency reads\",\"isCorrect\":true},{\"text\":\"Modify the application to issue scan API calls with eventual consistency reads\",\"isCorrect\":false},{\"text\":\"Modify the application to issue query API calls with strong consistency reads\",\"isCorrect\":false},{\"text\":\"Modify the application to issue scan API calls with strong consistency reads\",\"isCorrect\":false}],\"explanation\":\"$27\"},{\"question\":\"A Developer is configuring an Amazon ECS Service with Auto Scaling. The tasks should scale based on user load in the previous 20 seconds. How can the Developer enable the scaling?\",\"answers\":[{\"text\":\"Create a standardresolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds\",\"isCorrect\":false},{\"text\":\"Create a highresolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds\",\"isCorrect\":false},{\"text\":\"Create a standardresolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds\",\"isCorrect\":false},{\"text\":\"Create a highresolution custom Amazon CloudWatch metric for user activity data, then publish data every 10 seconds\",\"isCorrect\":true}],\"explanation\":\"$28\"},{\"question\":\"A Developer has created an Amazon S3 bucket and uploaded some objects that will be used for a publicly available static website. What steps MUST be performed to configure the bucket as a static website? (Select TWO.)\",\"answers\":[{\"text\":\"Enable public access and grant everyone the s3:GetObject permissions\",\"isCorrect\":true},{\"text\":\"Upload an index document and enter the name of the index document when enabling static website hosting\",\"isCorrect\":true},{\"text\":\"Upload an index and error document and enter the name of the index and error documents when enabling static website hosting\",\"isCorrect\":false},{\"text\":\"Create an object access control list (ACL) granting READ permissions to the AllUsers group\",\"isCorrect\":false},{\"text\":\"Upload a certificate from AWS Certificate Manager\",\"isCorrect\":false}],\"explanation\":\"$29\"},{\"question\":\"A company has deployed a REST API using Amazon API Gateway with a Lambda authorizer. The company needs to log who has accessed the API and how the caller accessed the API. They also require logs that include errors and execution traces for the Lambda authorizer. Which combination of actions should the Developer take to meet these requirements? (Select TWO.)\",\"answers\":[{\"text\":\"Create an API Gateway usage plan.\",\"isCorrect\":false},{\"text\":\"Enable API Gateway access logs.\",\"isCorrect\":true},{\"text\":\"Enable API Gateway execution logging.\",\"isCorrect\":true},{\"text\":\"Enable detailed logging in Amazon CloudWatch.\",\"isCorrect\":false},{\"text\":\"Enable server access logging.\",\"isCorrect\":false}],\"explanation\":\"$2a\"},{\"question\":\"A static website that serves a collection of images runs from an Amazon S3 bucket in the useast1 region. The website is gaining in popularity and a is now being viewed around the world. How can a Developer improve the performance of the website for global users?\",\"answers\":[{\"text\":\"Use Amazon CloudFront to cache the website content\",\"isCorrect\":true},{\"text\":\"Use Amazon S3 Transfer Acceleration to improve the performance of the website\",\"isCorrect\":false},{\"text\":\"Use cross region replication to replicate the bucket to several global regions\",\"isCorrect\":false},{\"text\":\"Use Amazon ElastiCache to cache the website content\",\"isCorrect\":false}],\"explanation\":\"$2b\"},{\"question\":\"A company runs a popular online game on premises. The application stores players' results in an inmemory database. The application is being migrated to AWS and the company needs to ensure there is no reduction in performance. Which database would be MOST suitable?\",\"answers\":[{\"text\":\"Amazon Elastic Beanstalk\",\"isCorrect\":false},{\"text\":\"Amazon DynamoDB\",\"isCorrect\":false},{\"text\":\"Amazon RDS\",\"isCorrect\":false},{\"text\":\"Amazon ElastiCache\",\"isCorrect\":true}],\"explanation\":\"$2c\"},{\"question\":\"A company needs to provide additional security for their APIs deployed on Amazon API Gateway. They would like to be able to authenticate their customers with a token. What is the SAFEST way to do this?\",\"answers\":[{\"text\":\"Use AWS Single Signon to authenticate the customers\",\"isCorrect\":false},{\"text\":\"Create an API Gateway Lambda authorizer\",\"isCorrect\":true},{\"text\":\"Create an Amazon Cognito identity pool\",\"isCorrect\":false},{\"text\":\"Setup usage plans and distribute API keys to the customers\",\"isCorrect\":false}],\"explanation\":\"$2d\"},{\"question\":\"A Developer is creating a design for an application that will include Docker containers on Amazon ECS with the EC2 launch type. The Developer needs to control the placement of tasks onto groups of container instances organized by availability zone and instance type. Which Amazon ECS feature provides expressions that can be used to group container instances by the relevant attributes?\",\"answers\":[{\"text\":\"Task Placement Strategy\",\"isCorrect\":false},{\"text\":\"Task Placement Constraints\",\"isCorrect\":false},{\"text\":\"Cluster Query Language\",\"isCorrect\":true},{\"text\":\"Task Group\",\"isCorrect\":false}],\"explanation\":\"$2e\"},{\"question\":\"The source code for an application is stored in a file named index.js that is in a folder along with a template file that includes the following code:AWSTemplateFormatVersion: '20100909'Transform: 'AWS::Serverless20161031'Resources:LambdaFunctionWithAPI:Type: AWS::Serverless::FunctionProperties:Handler: index.handlerRuntime: nodejs12.xWhat does a Developer need to do to prepare the template so it can be deployed using an AWS CLI command?\",\"answers\":[{\"text\":\"Run the aws cloudformation compile command to base64 encode and embed the source file into a modified CloudFormation template\",\"isCorrect\":false},{\"text\":\"Run the aws serverless createpackage command to embed the source file directly into the existing CloudFormation template\",\"isCorrect\":false},{\"text\":\"Run the aws cloudformation package command to upload the source code to an Amazon S3 bucket and produce a modified CloudFormation template\",\"isCorrect\":true},{\"text\":\"Run the aws lambda zip command to package the source file together with the CloudFormation template and deploy the resulting zip archive\",\"isCorrect\":false}],\"explanation\":\"$2f\"},{\"question\":\"A company uses Amazon DynamoDB to store sensitive data that must be encrypted. The company security policy mandates that data must be encrypted before it is submitted to DynamoDB. How can a Developer meet these requirements?\",\"answers\":[{\"text\":\"Use the DynamoDB Encryption Client to enable endtoend protection using clientside encryption.\",\"isCorrect\":true},{\"text\":\"Use AWS Certificate Manager (ACM) to create one certificate for each DynamoDB table.\",\"isCorrect\":false},{\"text\":\"Use the UpdateTable operation to switch to a customer managed customer master key (CMK).\",\"isCorrect\":false},{\"text\":\"Use the UpdateTable operation to switch to an AWS managed customer master key (CMK).\",\"isCorrect\":false}],\"explanation\":\"$30\"},{\"question\":\"An Amazon RDS database that stores product information for an online eCommerce marketplace is experiencing heavy demand. An increase in read requests is causing the database performance to be impacted and is affecting database writes. What is the best way to offload the read traffic from the database with MINIMAL code changes and cost?\",\"answers\":[{\"text\":\"Change the RDS database instance type to an instance with more CPU/RAM\",\"isCorrect\":false},{\"text\":\"Create an ElastiCache Memcached cluster and modify the application to send read requests to the cluster\",\"isCorrect\":false},{\"text\":\"Create an RDS Read Replica and modify the application to send read requests to the replica\",\"isCorrect\":true},{\"text\":\"Create an RDS Multi AZ DB and modify the application to send read requests to the standby DB\",\"isCorrect\":false}],\"explanation\":\"$31\"},{\"question\":\"A developer is building a web application that will be hosted on Amazon EC2 instances. The EC2 instances will store configuration data in an Amazon S3 bucket. What is the SAFEST way to allow the EC2 instances to access the S3 bucket?\",\"answers\":[{\"text\":\"Store an access key and secret ID that has the necessary permissions on the EC2 instances\",\"isCorrect\":false},{\"text\":\"Use the AWS SDK and authenticate with a user account that has the necessary permissions on the EC2 instances\",\"isCorrect\":false},{\"text\":\"Create an IAM Role with an AWS managed policy attached that has the necessary permissions and attach the role to the EC2 instances\",\"isCorrect\":false},{\"text\":\"Create an IAM Role with a customermanaged policy attached that has the necessary permissions and attach the role to the EC2 instances\",\"isCorrect\":true}],\"explanation\":\"$32\"},{\"question\":\"An application is running on an Amazon EC2 Linux instance. The instance needs to make AWS API calls to several AWS services. What is the MOST secure way to provide access to the AWS services with MINIMAL management overhead?\",\"answers\":[{\"text\":\"Store the credentials in the ~/.aws/credentials file\",\"isCorrect\":false},{\"text\":\"Use EC2 instance profiles\",\"isCorrect\":true},{\"text\":\"Use AWS KMS to store and retrieve credentials\",\"isCorrect\":false},{\"text\":\"Store the credentials in AWS CloudHSM\",\"isCorrect\":false}],\"explanation\":\"$33\"},{\"question\":\"A developer has deployed a serverless application with AWS Lambda. The function must make remote calls to external endpoints. Which configuration element in Lambda can be used store the connection strings related to the external endpoints?\",\"answers\":[{\"text\":\"Aliases\",\"isCorrect\":false},{\"text\":\"Versions\",\"isCorrect\":false},{\"text\":\"Tags\",\"isCorrect\":false},{\"text\":\"Environment variables\",\"isCorrect\":true}],\"explanation\":\"$34\"},{\"question\":\"A Development team would like to migrate their existing application code from a GitHub repository to AWS CodeCommit. What needs to be created before they can migrate a cloned repository to CodeCommit over HTTPS?\",\"answers\":[{\"text\":\"A public and private SSH key file\",\"isCorrect\":false},{\"text\":\"A GitHub secure authentication token\",\"isCorrect\":false},{\"text\":\"An Amazon EC2 IAM role with CodeCommit permissions\",\"isCorrect\":false},{\"text\":\"A set of credentials generated from IAM\",\"isCorrect\":true}],\"explanation\":\"$35\"},{\"question\":\"An eCommerce application uses an Amazon RDS database with Amazon ElastiCache in front. Stock volume data is updated dynamically in listings as sales are made. Customers have complained that occasionally the stock volume data is incorrect, and they end up purchasing items that are out of stock. A Developer has checked the front end and indeed some items display the incorrect stock count. What could be causing this issue?\",\"answers\":[{\"text\":\"The Amazon RDS database has insufficient IOPS provisioned for its EBS volumes.\",\"isCorrect\":false},{\"text\":\"The stock volume data is being retrieved using a writethrough ElastiCache cluster.\",\"isCorrect\":false},{\"text\":\"The Amazon RDS database is deployed as MultiAZ and the standby is inconsistent.\",\"isCorrect\":false},{\"text\":\"The cache is not being invalidated when the stock volume data is changed.\",\"isCorrect\":true}],\"explanation\":\"$36\"},{\"question\":\"A legacy service has an XMLbased SOAP interface. The Developer wants to expose the functionality of the service to external clients with the Amazon API Gateway. Which technique will accomplish this?\",\"answers\":[{\"text\":\"Create a RESTful API with the API Gateway; pass the incoming JSON to the SOAP interface through an Application Load Balancer\",\"isCorrect\":false},{\"text\":\"Create a RESTful API with the API Gateway; pass the incoming XML to the SOAP interface through an Application Load Balancer\",\"isCorrect\":false},{\"text\":\"Create a RESTful API with the API Gateway; transform the incoming XML into a valid message for the SOAP interface using mapping templates\",\"isCorrect\":false},{\"text\":\"Create a RESTful API with the API Gateway; transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates\",\"isCorrect\":true}],\"explanation\":\"$37\"},{\"question\":\"A company uses an Amazon S3 bucket to store a large number of sensitive files relating to eCommerce transactions. The company has a policy that states that all data written to the S3 bucket must be encrypted. How can a Developer ensure compliance with this policy?\",\"answers\":[{\"text\":\"Create an Amazon CloudWatch alarm that notifies an administrator if unencrypted objects are uploaded to the S3 bucket\",\"isCorrect\":false},{\"text\":\"Enable ServerSide Encryption with Amazon S3Managed Keys (SSES3) on the Amazon S3 bucket\",\"isCorrect\":false},{\"text\":\"Create an S3 bucket policy that denies any S3 Put request that does not include the xamzserversideencryption\",\"isCorrect\":true},{\"text\":\"Create a bucket policy that denies the S3 PutObject request with the attribute xamzacl having values public read, publicreadwrite, or authenticatedread\",\"isCorrect\":false}],\"explanation\":\"$38\"},{\"question\":\"An application exports documents to an Amazon S3 bucket. The data must be encrypted at rest and company policy mandates that encryption keys must be rotated annually. How can this be achieved automatically and with the LEAST effort?\",\"answers\":[{\"text\":\"Encrypt the data within the application before writing to S3\",\"isCorrect\":false},{\"text\":\"Import a custom key into AWS KMS and configure automatic rotation\",\"isCorrect\":false},{\"text\":\"Use AWS KMS keys with automatic rotation enabled\",\"isCorrect\":true},{\"text\":\"Configure automatic rotation with AWS Secrets Manager\",\"isCorrect\":false}],\"explanation\":\"$39\"},{\"question\":\"A Developer is creating a REST service using Amazon API Gateway with AWS Lambda integration. The service adds data to a spreadsheet and the data is sent as query string parameters in the method request. How should the Developer convert the query string parameters to arguments for the Lambda function?\",\"answers\":[{\"text\":\"Enable request validation\",\"isCorrect\":false},{\"text\":\"Change the integration type\",\"isCorrect\":false},{\"text\":\"Create a mapping template\",\"isCorrect\":true},{\"text\":\"Include the Amazon Resource Name (ARN) of the Lambda function\",\"isCorrect\":false}],\"explanation\":\"$3a\"},{\"question\":\"A developer is designing a web application that will run on Amazon EC2 Linux instances using an Auto Scaling Group. The application should scale based on a threshold for the number of users concurrently using the application. How should the Auto Scaling Group be configured to scale out?\",\"answers\":[{\"text\":\"Create a custom Amazon CloudWatch metric for concurrent users\",\"isCorrect\":true},{\"text\":\"Create a custom Amazon CloudWatch metric for memory usage\",\"isCorrect\":false},{\"text\":\"Use a target tracking scaling policy\",\"isCorrect\":false},{\"text\":\"Use the Amazon CloudWatch metric “NetworkIn”\",\"isCorrect\":false}],\"explanation\":\"You can create a custom CloudWatch metric for your EC2 Linux instance statistics by creating a script through the AWS Command Line Interface (AWS CLI). Then, you can monitor that metric by pushing it to CloudWatch. In this scenario you could then monitor the number of users currently logged in.CORRECT: \\\"Create a custom Amazon CloudWatch metric for concurrent users\\\" is the correct answer.INCORRECT: \\\"Use the Amazon CloudWatch metric “NetworkIn”\\\" is incorrect as this will only shows statistics for the number of inbound connections, not the number of concurrent users.INCORRECT: \\\"Use a target tracking scaling policy\\\" is incorrect as this is used to maintain a certain number of instances based on a target utilization.INCORRECT: \\\"Create a custom Amazon CloudWatch metric for memory usage\\\" is incorrect as memory usage does not tell us how many users are logged in.References: https://aws.amazon.com/premiumsupport/knowledgecenter/cloudwatchcustommetrics/\"},{\"question\":\"A Developer has completed some code updates and needs to deploy the updates to an Amazon Elastic Beanstalk environment. The environment includes twelve Amazon EC2 instances and there can be no reduction in application performance and availability during the update. Which deployment policy is the most cost-effective choice to suit these requirements?\",\"answers\":[{\"text\":\"Rolling\",\"isCorrect\":false},{\"text\":\"All at once\",\"isCorrect\":false},{\"text\":\"Immutable\",\"isCorrect\":false},{\"text\":\"Rolling with additional batch\",\"isCorrect\":true}],\"explanation\":\"$3b\"},{\"question\":\"A Developer implemented a static website hosted in Amazon S3 that makes web service requests hosted in Amazon API Gateway and AWS Lambda. The site is showing an error that reads:“No 'AccessControlAllowOrigin' header is present on the requested resource. Origin 'null' is therefore not allowed access.”What should the Developer do to resolve this issue?\",\"answers\":[{\"text\":\"Add the AccessControlRequestMethod header to the request\",\"isCorrect\":false},{\"text\":\"Enable crossorigin resource sharing (CORS) for the method in API Gateway\",\"isCorrect\":true},{\"text\":\"Enable crossorigin resource sharing (CORS) on the S3 bucket\",\"isCorrect\":false},{\"text\":\"Add the AccessControlRequestHeaders header to the request\",\"isCorrect\":false}],\"explanation\":\"$3c\"},{\"question\":\"A development team is migrating data from various file shares to AWS from onpremises. The data will be migrated into a single Amazon S3 bucket. What is the SIMPLEST method to ensure the data is encrypted at rest in the S3 bucket?\",\"answers\":[{\"text\":\"Ensure all requests use the xamzserversideencryptioncustomerkey header\",\"isCorrect\":false},{\"text\":\"Use SSL to transmit the data over the Internet\",\"isCorrect\":false},{\"text\":\"Ensure all requests use the xamzserversideencryption header\",\"isCorrect\":false},{\"text\":\"Enable default encryption when creating the bucket\",\"isCorrect\":true}],\"explanation\":\"$3d\"},{\"question\":\"A company recently migrated a multitier application to AWS. The web tier runs on an Auto Scaling group of Amazon EC2 instances and the database tier uses Amazon DynamoDB. The database tier requires extremely high performance and most requests are repeated read requests. What service can be used to scale the database tier for BEST performance?\",\"answers\":[{\"text\":\"Amazon SQS\",\"isCorrect\":false},{\"text\":\"Amazon CloudFront\",\"isCorrect\":false},{\"text\":\"Amazon DynamoDB Accelerator (DAX)\",\"isCorrect\":true},{\"text\":\"Amazon ElastiCache\",\"isCorrect\":false}],\"explanation\":\"$3e\"},{\"question\":\"An application is using Amazon DynamoDB as its data store and needs to be able to read 100 items per second as strongly consistent reads. Each item is 5 KB in size. What value should be set for the table's provisioned throughput for reads?\",\"answers\":[{\"text\":\"250 Read Capacity Units\",\"isCorrect\":false},{\"text\":\"500 Read Capacity Units\",\"isCorrect\":false},{\"text\":\"50 Read Capacity Units\",\"isCorrect\":false},{\"text\":\"200 Read Capacity Units\",\"isCorrect\":true}],\"explanation\":\"$3f\"},{\"question\":\"A Developer has created an Amazon Cognito user pool and configured a domain for it. The Developer wants to add signup and sign in pages to an app with a company logo. What should the Developer do to meet these requirements?\",\"answers\":[{\"text\":\"Upload the company logo to an Amazon S3 bucket. Specify the S3 object path in the app client settings in Amazon Cognito.\",\"isCorrect\":false},{\"text\":\"Create a REST API using Amazon API Gateway and add a Cognito authorizer. Upload the company logo to a stage in the API.\",\"isCorrect\":false},{\"text\":\"Customize the Amazon Cognito hosted web UI and add the company logo.\",\"isCorrect\":true},{\"text\":\"Create a custom login page that includes the company logo and upload it to Amazon Cognito. Specify the login page in the app client settings.\",\"isCorrect\":false}],\"explanation\":\"$40\"},{\"question\":\"AWS CodeBuild builds code for an application, creates the Docker image, pushes the image to Amazon Elastic Container Registry (Amazon ECR), and tags the image with a unique identifier. If the Developers already have AWS CLI configured on their workstations, how can the Docker images be pulled to the workstations?\",\"answers\":[{\"text\":\"Run the output of the following: aws ecr getlogin and then run: docker pull REPOSITORY URI : TAG\",\"isCorrect\":true},{\"text\":\"Run the following: docker pull REPOSITORY URI : TAG\",\"isCorrect\":false},{\"text\":\"Run the following: aws ecr getlogin and then run: docker pull REPOSITORY URI : TAG\",\"isCorrect\":false},{\"text\":\"Run the output of the following: aws ecr getdownloadurlforlayer and then run: docker pull REPOSITORY URI : TAG\",\"isCorrect\":false}],\"explanation\":\"$41\"},{\"question\":\"A Developer is using AWS SAM to create a template for deploying a serverless application. The Developer plans deploy an AWS Lambda function and an Amazon DynamoDB table using the template. Which resource types should the Developer specify? (Select TWO.)\",\"answers\":[{\"text\":\"AWS::Serverless:Function\",\"isCorrect\":true},{\"text\":\"AWS::Serverless:API\",\"isCorrect\":false},{\"text\":\"AWS::Serverless::Application\",\"isCorrect\":false},{\"text\":\"AWS::Serverless:LayerVersion\",\"isCorrect\":false},{\"text\":\"AWS::Serverless::SimpleTable\",\"isCorrect\":true}],\"explanation\":\"$42\"},{\"question\":\"A developer needs use the attribute of an Amazon S3 object that uniquely identifies the object in a bucket. Which of the following represents an Object Key?\",\"answers\":[{\"text\":\"s3://dctlabs/Development/Projects.xls\",\"isCorrect\":false},{\"text\":\"Project=Blue\",\"isCorrect\":false},{\"text\":\"Development/Projects.xls\",\"isCorrect\":true},{\"text\":\"arn:aws:s3:::dctlabs\",\"isCorrect\":false}],\"explanation\":\"$43\"},{\"question\":\"The development team is working on an API that will be served from Amazon API Gateway. The API will serve three environments PROD, DEV, and TEST and requires a cache size of 250GB. What is the MOST costefficient deployment strategy?\",\"answers\":[{\"text\":\"Create a single API Gateway with three stages and enable the cache for all environments\",\"isCorrect\":false},{\"text\":\"Create a single API Gateway with three deployments and configure a global cache of 250GB\",\"isCorrect\":false},{\"text\":\"Create three API Gateways, one for each environment and enable the cache for the DEV and TEST environments only when required\",\"isCorrect\":false},{\"text\":\"Create a single API Gateway with three stages and enable the cache for the DEV and TEST environments only when required\",\"isCorrect\":true}],\"explanation\":\"$44\"},{\"question\":\"A Developer is building a WebSocket API using Amazon API Gateway. The payload sent to this API is JSON that includes an action key which can have multiple values. The Developer must integrate with different routes based on the value of the action key of the incoming JSON payload. How can the Developer accomplish this task with the LEAST amount of configuration?\",\"answers\":[{\"text\":\"Create a mapping template to map the action key to an integration request.\",\"isCorrect\":false},{\"text\":\"Set the value of the route selection expression to $request.body.action.\",\"isCorrect\":true},{\"text\":\"Create a separate stage for each possible value of the action key.\",\"isCorrect\":false},{\"text\":\"Set the value of the route selection expression to $default.\",\"isCorrect\":false}],\"explanation\":\"$45\"},{\"question\":\"A Developer is creating an AWS Lambda function that will process data from an Amazon Kinesis data stream. The function is expected to be invoked 50 times per second and take 100 seconds to complete each request. What MUST the Developer do to ensure the functions runs without errors?\",\"answers\":[{\"text\":\"No action is required as AWS Lambda can easily accommodate this requirement\",\"isCorrect\":false},{\"text\":\"Increase the concurrency limit for the function\",\"isCorrect\":false},{\"text\":\"Contact AWS and request to increase the limit for concurrent executions\",\"isCorrect\":true},{\"text\":\"Implement exponential backoff in the function code\",\"isCorrect\":false}],\"explanation\":\"$46\"},{\"question\":\"A Developer is creating a DynamoDB table for storing application logs. The table has 5 write capacity units (WCUs). The Developer needs to configure the read capacity units (RCUs) for the table. Which of the following configurations represents themost efficient use of throughput?\",\"answers\":[{\"text\":\"Strongly consistent reads of 15 RCUs reading items that are 1KB in size\",\"isCorrect\":false},{\"text\":\"Strongly consistent reads of 5 RCUs reading items that are 4 KB in size\",\"isCorrect\":false},{\"text\":\"Eventually consistent reads of 15 RCUs reading items that are 1 KB in size\",\"isCorrect\":false},{\"text\":\"Eventually consistent reads of 5 RCUs reading items that are 4 KB in size\",\"isCorrect\":true}],\"explanation\":\"$47\"},{\"question\":\"A company is running a web application on Amazon EC2 behind an Elastic Load Balancer (ELB). The company is concerned about the security of the web application and would like to secure the application with SSL certificates. The solution should not have any performance impact on the EC2 instances. What steps should be taken to secure the web application? (Select TWO.)\",\"answers\":[{\"text\":\"Add an SSL certificate to the Elastic Load Balancer\",\"isCorrect\":true},{\"text\":\"Configure ServerSide Encryption with KMS managed keys\",\"isCorrect\":false},{\"text\":\"Install SSL certificates on the EC2 instances\",\"isCorrect\":false},{\"text\":\"Configure the Elastic Load Balancer with SSL passthrough\",\"isCorrect\":false},{\"text\":\"Configure the Elastic Load Balancer for SSL termination\",\"isCorrect\":true}],\"explanation\":\"$48\"},{\"question\":\"An application uses Amazon API Gateway, an AWS Lambda function and a DynamoDB table. The developer requires that another Lambda function is triggered when an item lifecycle activity occurs in the DynamoDB table. How can this be achieved?\",\"answers\":[{\"text\":\"Enable a DynamoDB stream and trigger the Lambda function synchronously from the stream\",\"isCorrect\":true},{\"text\":\"Enable a DynamoDB stream and trigger the Lambda function asynchronously from the stream\",\"isCorrect\":false},{\"text\":\"Configure an Amazon CloudTrail API alarm that sends a message to an Amazon SQS queue. Configure the Lambda function to poll the queue and invoke the function synchronously\",\"isCorrect\":false},{\"text\":\"Configure an Amazon CloudWatch alarm that sends an Amazon SNS notification. Trigger the Lambda function asynchronously from the SNS notification\",\"isCorrect\":false}],\"explanation\":\"$49\"},{\"question\":\"A company runs an application on a fleet of web servers running on Amazon EC2 instances. The web servers are behind an Elastic Load Balancer (ELB) and use an Amazon DynamoDB table for storing session state. A Developer has been asked to implement a mechanism for automatically deleting session state data that is older than 24 hours. What is the SIMPLEST solution to this requirement?\",\"answers\":[{\"text\":\"Add an attribute with the expiration time; enable the Time To Live feature based on that attribute\",\"isCorrect\":true},{\"text\":\"Add an attribute with the expiration time; name the attribute ItemExpiration\",\"isCorrect\":false},{\"text\":\"Write a script that deletes old records; schedule the scripts as a cron job on an Amazon EC2 instance\",\"isCorrect\":false},{\"text\":\"Each day, create a new table to hold session data; delete the previous day's table\",\"isCorrect\":false}],\"explanation\":\"$4a\"}]}}]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n7:null\n"])</script><script>self.__next_f.push([1,"8:null\nc:[[\"$\",\"title\",\"0\",{\"children\":\"v0 App\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Created with v0\"}],[\"$\",\"meta\",\"2\",{\"name\":\"generator\",\"content\":\"v0.app\"}]]\n"])</script></body></html>