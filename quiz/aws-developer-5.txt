1:"$Sreact.fragment"
2:I[7555,[],""]
3:I[1295,[],""]
4:I[9742,["177","static/chunks/app/layout-220de9febe1f86f4.js"],"Analytics"]
6:I[9665,[],"OutletBoundary"]
9:I[9665,[],"ViewportBoundary"]
b:I[9665,[],"MetadataBoundary"]
d:I[6614,[],""]
:HL["/_next/static/css/6ad9841b43ad2bc9.css","style"]
:HL["/_next/static/css/4d55be17aa0cdcd5.css","style"]
0:{"P":null,"b":"hVObTwCHKjnTJqIoS5_8Q","p":"","c":["","quiz","aws-developer-5"],"i":false,"f":[[["",{"children":["quiz",{"children":[["id","aws-developer-5","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/6ad9841b43ad2bc9.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/4d55be17aa0cdcd5.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"font-sans __variable_fb8f2c __variable_f910ec","children":[["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}],["$","$L4",null,{}]]}]}]]}],{"children":["quiz",["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["id","aws-developer-5","d"],["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L5","$undefined",null,["$","$L6",null,{"children":["$L7","$L8",null]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","yYDcMfMZNnGJ3xFGVoacC",{"children":[["$","$L9",null,{"children":"$La"}],null]}],["$","$Lb",null,{"children":"$Lc"}]]}],false]],"m":"$undefined","G":["$d","$undefined"],"s":false,"S":true}
e:I[1184,["261","static/chunks/261-2d9b76ccba401937.js","200","static/chunks/app/quiz/%5Bid%5D/page-d7cb26530e4508b3.js"],"default"]
f:T9c6,Correct option:ParametersParameters enable you to input custom values to your CloudFormation template each time you create or update a stack. Please see this note to understand how to define a parameter in a template: via https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameterssectionstructure.html The optional Conditions section contains statements that define the circumstances under which entities are created or configured. For example, you can create a condition and then associate it with a resource or output so that AWS CloudFormation only creates the resource or output if the condition is true.You might use conditions when you want to reuse a template that can create resources in different contexts, such as a test environment versus a production environment. In your template, you can add an EnvironmentType input parameter, which accepts either prod or test as inputs. For the production environment, you might include Amazon EC2 instances with certain capabilities; however, for the test environment, you want to use reduced capabilities to save money.Conditions cannot be used within the Parameters section. After you define all your conditions, you can associate them with resources and resource properties only in the Resources and Outputs sections of a template.Please review this note for more details: via https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditionssectionstructure.html Please visit https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameterssectionstructure.html for more information on the parameter structure.Incorrect options:Resources Resources section describes the resources that you want to provision in your AWS CloudFormation stacks. You can associate conditions with the resources that you want to conditionally create.Conditions You actually define conditions in this section of the CloudFormation templateOutputs The optional Outputs section declares output values that you can import into other stacks (to create crossstack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name for a stack to make the bucket easier to find. You can associate conditions with the outputs that you want to conditionally create.References:https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameterssectionstructure.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditionssectionstructure.htm10:T42f,Correct option:AWS::AccountIdUsing CloudFormation, you can create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and AWS CloudFormation takes care of provisioning and configuring those resources for you.Pseudo parameters are parameters that are predefined by AWS CloudFormation. You do not declare them in your template. Use them the same way as you would a parameter, as the argument for the Ref function.AWS::AccountId returns the AWS account ID of the account in which the stack is being created.Incorrect options:AWS::NoValue This removes the corresponding resource property when specified as a return value in the Fn::If intrinsic function.AWS::Region Returns a string representing the AWS Region in which the encompassing resource is being created, such as uswest2.AWS::StackName Returns the name of the stack as specified with the aws cloudformation createstack command, such as "teststack".Reference:https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudoparameterreference.html11:T6e3,Amazon ECS tasks support Docker volumes. To use data volumes, you must specify the volume and mount point configurations in your task definition. Docker volumes are supported for the EC2 launch type only.To configure a Docker volume, in the task definition volumes section, define a data volume with name and DockerVolumeConfiguration values. In the containerDefinitions section, define multiple containers with mountPoints values that reference the name of the defined volume and the containerPath value to mount the volume at on the container.The containers should both be specified in the same task definition. Therefore, the Development team should create one task definition, specify both containers in the definition and then mount a shared volume between those two containersCORRECT: "Create one task definition. Specify both containers in the definition. Mount a shared volume between those two containers" is the correct answer.INCORRECT: "Create two pod specifications. Make one to include the application container and the other to include the other container. Link the two pods together" is incorrect as pods are a concept associated with the Elastic Kubernetes Service (EKS).INCORRECT: "Create two task definitions. Make one to include the application container and the other to include the other container. Mount a shared volume between the two tasks" is incorrect as a single task definition should be created with both containers.INCORRECT: "Create a single pod specification. Include both containers in the specification. Mount a persistent volume to both containers" is incorrect as pods are a concept associated with the Elastic Kubernetes Service (EKS).References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/dockervolumes.html12:T858,You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API.When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified timetolive (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600seconds. TTL=0 means caching is disabled.A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests.The client must send a request that contains the CacheControl: maxage=0 header.The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint.To grant permission for a client, attach a policy of the following format to an IAM execution role for the user.This policy allows the API Gateway execution service to invalidate the cache for requests on the specified resource (orresources).Therefore, as described above the solution is to get the partners to pass the HTTP header CacheControl: maxage=0.CORRECT: "They can pass the HTTP header CacheControl: maxage=0" is the correct answer.INCORRECT: "They can use the query string parameter INVALIDATE_CACHE" is incorrect. This is not a valid method of invalidating the cache with API Gateway.INCORRECT: "They must wait for the TTL to expire" is incorrect as this is not true, you do not need to wait as you can pass the HTTP header CacheControl: maxage=0 whenever you need to in order to invalidate the cache.INCORRECT: "They can invoke an AWS API endpoint which invalidates the cache" is incorrect. This is not a valid method of invalidating the cache with API Gateway.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaycaching.html13:T63e,It is common to use key/value stores for storing session state data. The two options presented in the answers are Amazon DynamoDB and Amazon ElastiCache Redis. Of these two, ElastiCache will provide the lowest latency as it is an inmemorydatabase.Therefore, the best answer is to create an Amazon ElastiCache Redis cluster, then implement session handling at the application level to leverage the cluster for session data storage.CORRECT: "Create an Amazon ElastiCache Redis cluster, then implement session handling at the application level to leverage the cluster for session data storage" is the correct answer.INCORRECT: "Create an Amazon DynamoDB table, then implement session handling at the application level to leverage the table for session data storage" is incorrect as though this is a good solution for storing session state data, the latency will not be as low as with ElastiCache.INCORRECT: "Create an Amazon RedShift instance, then implement session handling at the application level to leverage a database inside the RedShift database instance for session data storage" is incorrect. RedShift is a data warehouse that is used for OLAP use cases, not for storing session state data.INCORRECT: "Implement a shared Amazon EFS file system solution across the underlying Amazon EC2 instances, then implement session handling at the application level to leverage the EFS file system for session data storage" is incorrect. For session state data a key/value store such as DynamoDB or ElastiCache will provide better performance.References: https://aws.amazon.com/caching/sessionmanagement/14:T90f,With AWS Systems Manager Parameter Store, you can create secure string parameters, which are parameters that have a plaintext parameter name and an encrypted parameter value. Parameter Store uses AWS KMS to encrypt and decrypt the parameter values of secure string parameters.With Parameter Store you can create, store, and manage data as parameters with values. You can create a parameter in Parameter Store and use it in multiple applications and services subject to policies and permissions that you design. When you need to change a parameter value, you change one instance, rather than managing errorprone changes to numerous sources.Parameter Store supports a hierarchical structure for parameter names, so you can qualify a parameter for specific uses.To manage sensitive data, you can create secure string parameters. Parameter Store uses AWS KMS customer master keys (CMKs) to encrypt the parameter values of secure string parameters when you create or change them. It also uses CMKs to decrypt the parameter values when you access them. You can use the AWS managed CMK that Parameter Store creates for your account or specify your own customer managed CMK.Therefore, you can use a combination of AWS Systems Manager Parameter Store and AWS Key Management Store to store the credentials securely. These keys can be then be referenced in the Lambda function code or through environment variables.NOTE: Systems Manager Parameter Store does not natively perform rotation of credentials so this must be done in the application. AWS Secrets Manager does perform credential rotation however it is not an answer option for this question.CORRECT: "AWS Systems Manager Parameter Store" is a correct answer.CORRECT: "AWS Key Management Store (KMS)" is also a correct answer.INCORRECT: "AWS Certificate Manager (ACM)" is incorrect as this service is used to issue SSL/TLS certificates not encryption keys.INCORRECT: "AWS Artifact" is incorrect as this is a service to view compliance information about the AWS platformINCORRECT: "Amazon GuardDuty" is incorrect. Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads.References: https://docs.aws.amazon.com/kms/latest/developerguide/servicesparameterstore.html15:T491,If you set an alarm on a highresolution metric, you can specify a highresolution alarm with a period of 10 seconds or 30 seconds, or you can set a regular alarm with a period of any multiple of 60 seconds. There is a higher charge for highresolution alarms.Amazon SNS can then be used to send notifications based on the CloudWatch alarm.CORRECT: "Configure a highresolution CloudWatch alarm and use Amazon SNS to send the alert" is the correct answer.INCORRECT: "Specify an Amazon SNS topic for alarms when issuing the putmetricdata AWS CLI command" is incorrect. You cannot specify an SNS topic with this CLI command.INCORRECT: "Use Amazon CloudWatch Logs Insights and trigger an Amazon Eventbridge rule to send a notification" is incorrect.Logs Insights cannot be used for alarms or alerting based on custom CloudWatch metrics.INCORRECT: "Use a default CloudWatch metric, configure an alarm, and use Amazon SNS to send the alert" is incorrect. There is no default metric that would monitor the application uptime and the resolution would be lower.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html#highresolutionalarms16:T64e,Correct option:Elastic Beanstalk will replace them with instances running the application version from the most recent successful deploymentWhen processing a batch, Elastic Beanstalk detaches all instances in the batch from the load balancer, deploys the new application version, and then reattaches the instances. If you enable connection draining, Elastic Beanstalk drains existing connections from the Amazon EC2 instances in each batch before beginning the deployment.If a deployment fails after one or more batches completed successfully, the completed batches run the new version of your application while any pending batches continue to run the old version. You can identify the version running on the instances in your environment on the health page in the console. This page displays the deployment ID of the most recent deployment that was executed on each instance in your environment. If you terminate instances from the failed deployment, Elastic Beanstalk replaces them with instances running the application version from the most recent successful deployment.Incorrect options:Elastic Beanstalk will not replace the failed instancesElastic Beanstalk will replace the failed instances with instances running the application version from the oldest successful deploymentElastic Beanstalk will replace the failed instances after the application version to be installed is manually chosen from AWS ConsoleThese three options contradict the explanation provided above, so these options are incorrect.Reference:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.rollingversiondeploy.html17:T4cb,The company require a managed service therefore the Developer should choose to use Elastic Beanstalk for the compute layer and Amazon RDS with the PostgreSQL engine for the database layer.AWS Elastic Beanstalk will handle all capacity provisioning, load balancing, and autoscaling for the web frontend and Amazon RDS provides pushbutton scaling for the backend.CORRECT: "AWS Elastic Beanstalk" is a correct answer.CORRECT: "Amazon RDS with PostrgreSQL" is also a correct answer.INCORRECT: "Amazon EC2 with Auto Scaling" is incorrect as though these services will be used to provide the automatic scalability required for the solution, they still need to be managed. The questions asks for a managed solution and Elastic Beanstalk will manage this for you. Also, there is no mention of a load balancer so connections cannot be distributed toinstances.INCORRECT: "Amazon EC2 with PostgreSQL" is incorrect as the question asks for a managed service and therefore the database should be run on Amazon RDS.INCORRECT: "AWS Lambda with CloudWatch Events" is incorrect as there is no mention of refactoring application code to run on AWS Lambda.References:https://aws.amazon.com/elasticbeanstalk/ https://aws.amazon.com/rds/postgresql/18:T6fe,A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service.Amazon ECS supports the following task placement strategies:binpackPlace tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.randomPlace tasks randomly.spreadPlace tasks evenly based on the specified value. Accepted values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availabilityzone.You can specify task placement strategies with the following actions: CreateService, UpdateService, and RunTask. You can also use multiple strategies together as in the example JSON code provided with the question.CORRECT: "It distributes tasks evenly across Availability Zones and then distributes tasks evenly across the instances within each Availability Zone" is the correct answer.INCORRECT: "It distributes tasks evenly across Availability Zones and then bin packs tasks based on memory within each Availability Zone" is incorrect as it does not use the binpack strategy.INCORRECT: "It distributes tasks evenly across Availability Zones and then distributes tasks evenly across distinct instances within each Availability Zone" is incorrect as it does not spread tasks across distinct instances (use a task placement constraint).INCORRECT: "It distributes tasks evenly across Availability Zones and then distributes tasks randomly across instances within each Availability Zone" is incorrect as it does not use the random strategy.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskplacementstrategies.html19:T6b1,In AWS, a resource is an entity that you can work with. Examples include an Amazon EC2 instance, an AWS CloudFormation stack, or an Amazon S3 bucket. If you work with multiple resources, you might find it useful to manage them as a group rather than move from one AWS service to another for each task.By default, the AWS Management Console is organized by AWS service. But with Resource Groups, you can create a custom console that organizes and consolidates information based on criteria specified in tags, or the resources in an AWS CloudFormation stack. The following list describes some of the cases in which resource grouping can help organize yourresources.• An application that has different phases, such as development, staging, and production.• Projects managed by multiple departments or individuals.• A set of AWS resources that you use together for a common project or that you want to manage or monitor as a group.• A set of resources related to applications that run on a specific platform, such as Android or iOS.CORRECT: "Create a resource group" is the correct answer.INCORRECT: "Deploy all resources using a single Amazon CloudFormation stack" is incorrect as this would not be a best practice as it is better to create separate stacks to manage deployment separately.INCORRECT: "Create an AWS Elastic Beanstalk environment for each stage" is incorrect. It’s fine to create separate environments for each stage, however this won’t create a single view to view and manage all resources.INCORRECT: "Create a single AWS CodeDeploy deployment" is incorrect as each stage should be created in a separate deployment.References: https://docs.aws.amazon.com/ARG/latest/userguide/welcome.html1a:T5c6,If you need to remove a file from CloudFront edge caches before it expires, you can do one of the following:• Invalidate the file from edge caches. The next time a viewer requests the file, CloudFront returns to the origin to fetch the latest version of the file.• Use file versioning to serve a different version of the file that has a different name. For more information, see Updating Existing Files Using Versioned File Names.In this case, the best option available is to invalidate all the application objects from the edge caches. This will result in the new objects being cached next time a request is made for them.CORRECT: "Invalidate all the application objects from the edge caches" is the correct answer.INCORRECT: "Remove the origin from the CloudFront configuration and add it again" is incorrect as this is going to cause all objects to be removed and then recached which is overkill and will cost more.INCORRECT: "Disable forwarding of query strings and request headers from the CloudFront distribution configuration" is incorrect as this is not a way to invalidate objects in Amazon CloudFront.INCORRECT: "Disable the CloudFront distribution and enable it again to update all the edge locations" is incorrect as this will not cause the objects to expire, they will expire whenever their expiration date occurs and must be invalidated to make this happen sooner.References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html1b:T788,An event source mapping is an AWS Lambda resource that reads from an event source and invokes a Lambda function. You can use event source mappings to process items from a stream or queue in services that don't invoke Lambda functions directly.Lambda provides event source mappings for the following services.Services That Lambda Reads Events From• Amazon Kinesis• Amazon DynamoDB• Amazon Simple Queue ServiceAn event source mapping uses permissions in the function's execution role to read and manage items in the event source.Permissions, event structure, settings, and polling behavior vary by event source.The configuration of the event source mapping for streambased services (DynamoDB, Kinesis), and Amazon SQS, is made on the Lambda side.Note: for other services, such as Amazon S3 and SNS, the function is invoked asynchronously and the configuration is made on the source (S3/SNS) rather than Lambda.CORRECT: "An eventsource mapping must be created on the Lambda side to associate the DynamoDB stream with the Lambda function" is the correct answer.INCORRECT: "An alarm should be created in CloudWatch that sends a notification to Lambda when a new entry is added to the DynamoDB stream" is incorrect as you should use an eventsource mapping between Lambda and DynamoDB instead.INCORRECT: "An eventsource mapping must be created on the DynamoDB side to associate the DynamoDB stream with the Lambda function" is incorrect because for streambased services that don’t invoke Lambda functions directly, the configuration should be made on the Lambda side.INCORRECT: "Update the CloudFormation template to map the DynamoDB stream to the Lambda function" is incorrect as CloudFormation may not even be used in this scenario (it wasn’t mentioned) and wouldn’t continuously send events from DynamoDB streams to Lambda either.References: https://docs.aws.amazon.com/lambda/latest/dg/invocationeventsourcemapping.html1c:T598,Correct option:AWS::Serverless::UserPoolThe AWS Serverless Application Model (SAM) is an opensource framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML.SAM supports the following resource types:AWS::Serverless::ApiAWS::Serverless::ApplicationAWS::Serverless::FunctionAWS::Serverless::HttpApiAWS::Serverless::LayerVersionAWS::Serverless::SimpleTableAWS::Serverless::StateMachineUserPool applies to the Cognito service which is used for authentication for mobile app and web. There is no resource named UserPool in the Serverless Application Model.Incorrect options:AWS::Serverless::Function This resource creates a Lambda function, IAM execution role, and event source mappings that trigger the function.AWS::Serverless::Api This creates a collection of Amazon API Gateway resources and methods that can be invoked through HTTPS endpoints. It is useful for advanced use cases where you want full control and flexibility when you configure your APIs.AWS::Serverless::SimpleTable This creates a DynamoDB table with a single attribute primary key. It is useful when data only needs to be accessed via a primary key.Reference:https://docs.aws.amazon.com/serverlessapplicationmodel/latest/developerguide/samspecificationresourcesandproperties.html1d:T848,DynamoDB Streams captures a timeordered sequence of itemlevel modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in nearreal time.You can also use the CreateTable or UpdateTable API operations to enable or modify a stream.The StreamSpecification parameter determines how the stream is configured:StreamEnabled — Specifies whether a stream is enabled (true) or disabled (false) for the table.StreamViewType — Specifies the information that will be written to the stream whenever data in the table is modified:• KEYS_ONLY — Only the key attributes of the modified item.• NEW_IMAGE — The entire item, as it appears after it was modified.• OLD_IMAGE — The entire item, as it appeared before it was modified.• NEW_AND_OLD_IMAGES — Both the new and the old images of the item.In this scenario, we only need to keep a copy of the items before they were modified. Therefore, the solution is to enable DynamoDB streams and set the StreamViewType to OLD_IMAGES.CORRECT: "Enable DynamoDB Streams for the table" is the correct answer.CORRECT: "Set the StreamViewType to OLD_IMAGE" is the correct answer.INCORRECT: "Create a CloudWatch alarm that sends a notification when an item is modified" is incorrect as DynamoDB streams is the best way to capture a timeordered sequence of itemlevel modifications in a DynamoDB table.INCORRECT: "Set the StreamViewType to NEW_AND_OLD_IMAGES" is incorrect as we only need to keep a record of the items before they were modified. This setting would place a record in the stream that includes the item before and after modification.INCORRECT: "Use an AWS Lambda function to extract the item records from the notification and write to an S3 bucket" is incorrect. There is no requirement to write the updates to S3 and if you did want to do this with Lambda you would need to extract the information from the stream, not a notification.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html1e:T6ef,Correct option:ElastiCache defined in .ebextensions/ Any resources created as part of your .ebextensions is part of your Elastic Beanstalk template and will get deleted if the environment is terminated.via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.htmlRDS database defined externally and referenced through environment variables To decouple your database instance from your environment, you can run a database instance in Amazon RDS and configure your application to connect to it on launch. This enables you to connect multiple environments to a database, terminate an environment without affecting the database, and perform seamless updates with bluegreen deployments. To allow the Amazon EC2 instances in your environment to connect to an outside database, you can configure the environment's Auto Scaling group with an additional security group.Using Elastic Beanstalk with Amazon RDS: via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html Incorrect options:ElastiCache bundled with the application source code ElastiCache is an AWS service and cannot be bundled with the source code.RDS database defined in .ebextensions/ The lifetime of the RDS instance gets tied to the lifetime of the Elastic Beanstalk environment, so this option is incorrect.ElastiCache database defined externally and referenced through environment variables For the given usecase, the client is fine with losing user session data and hence defining it in .ebextensions/ is more appropriate.References:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customizeenvironmentresourceselasticache.html1f:T5bd,Correct option:"Exported Output Values in CloudFormation must have unique names within a single Region"Using CloudFormation, you can create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and AWS CloudFormation takes care of provisioning and configuring those resources for you.A CloudFormation template has an optional Outputs section which declares output values that you can import into other stacks (to create crossstack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name for a stack to make the bucket easier to find.You can use the Export Output Values to export the name of the resource output for a crossstack reference. For each AWS account, export names must be unique within a region. In this case, we would have a conflict within useast2.via https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputssectionstructure.htmlIncorrect options:"Output Values in CloudFormation must have unique names across all Regions""Exported Output Values in CloudFormation must have unique names across all Regions""Output Values in CloudFormation must have unique names within a single Region"These three options contradict the explanation provided earlier, hence these options are incorrect.Reference:https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputssectionstructure.html20:T525,AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWSManagement Console, AWS SDKs, command line tools, and other AWS services.This event history simplifies security analysis, resource change tracking, and troubleshooting. In addition, you can use CloudTrail to detect unusual activity in your AWS accounts. These capabilities help simplify operational analysis and troubleshooting.Therefore, Amazon CloudTrail is the most suitable service for the requirements in this scenario.CORRECT: "Amazon CloudTrail" is the correct answer.INCORRECT: "Amazon CloudWatch" is incorrect as this service is used for performance monitoring, not recording API actions.INCORRECT: "AWS XRay" is incorrect as this is used for tracing application activity for performance and operational statistics.INCORRECT: "AWS OpsWorks" is incorrect as this is a configuration management service that helps you build and operate highly dynamic applications, and propagate changes instantly.References: https://aws.amazon.com/cloudtrail/features/21:T75e,Please note that the question specifically asks how to enable connectivity so this is not about permissions. When using a private subnet with no Internet connectivity there are only two options available for connecting to Amazon S3 (which remember, is a service with a public endpoint, it’s not in your VPC).The first option is to enable Internet connectivity through either a NAT Gateway or a NAT Instance. However, there is no answer offering either of these as a solution. The other option is to enable a VPC endpoint for S3.The specific type of VPC endpoint to S3 is a Gateway Endpoint. EC2 instances running in private subnets of a VPC can use the endpoint to enable controlled access to S3 buckets, objects, and API functions that are in the same region as the VPC. You can then use an S3 bucket policy to indicate which VPCs and which VPC Endpoints have access to your S3 buckets.Therefore, the only answer that presents a solution to this challenge is to provision an VPC endpoint for S3.CORRECT: "A VPC endpoint should be provisioned for S3" is the correct answer.INCORRECT: "An IAM role must be added to the instance that has permissions to write to the S3 bucket" is incorrect. You do need to do this, but the question is asking about connectivity, not permissions.INCORRECT: "A bucket policy needs to be added specifying the principles that are allowed to write data to the bucket" is incorrect. You may choose to use a bucket policy to enable permissions but the question is asking about connectivity, not permissions.INCORRECT: "A VPN should be established to enable private connectivity to S3" is incorrect. You can create a VPN to establish an encrypted tunnel into a VPC from a location outside of AWS. However, you cannot create a VPN connection from a subnet within a VPC to Amazon S3.References: https://docs.aws.amazon.com/vpc/latest/userguide/vpcendpointss3.html22:T40f,Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.To calculate the concurrency requirements for the Lambda function simply multiply the number of executions per second (20) by the time it takes to complete the execution (20).Therefore, for this scenario the calculation is 20 x 20 = 400.CORRECT: "400" is the correct answer.INCORRECT: "5" is incorrect. Please use the formula above to calculate concurrency requirements.INCORRECT: "40" is incorrect. Please use the formula above to calculate concurrency requirements.INCORRECT: "20" is incorrect. Please use the formula above to calculate concurrency requirements.References: https://docs.aws.amazon.com/lambda/latest/dg/invocationscaling.html23:T6fa,Automatic scaling is the ability to increase or decrease the desired count of tasks in your Amazon ECS service automatically. Amazon ECS leverages the Application Auto Scaling service to provide this functionality.Amazon ECS publishes CloudWatch metrics with your service’s average CPU and memory usage. You can use these and other CloudWatch metrics to scale out your service (add more tasks) to deal with high demand at peak times, and to scale in your service (run fewer tasks) to reduce costs during periods of low utilization.Amazon ECS services support the Application Load Balancer, Network Load Balancer, and Classic Load Balancer load balancer types. Application Load Balancers are used to route HTTP/HTTPS (or Layer 7) traffic. Network Load Balancers and Classic Load Balancers are used to route TCP (or Layer 4) traffic.Therefore, the Developer should create an ECS Service with Auto Scaling and attach an Elastic Load Balancer.CORRECT: "Create an ECS Service with Auto Scaling and attach an Elastic Load Balancer" is the correct answer.INCORRECT: "Create an ECS Task Definition that uses Auto Scaling and Elastic Load Balancing" is incorrect as the Developer needs to configure auto scaling and load balancing in a service, not a task definition.INCORRECT: "Create a capacity provider and configure cluster auto scaling " is incorrect as this is used to scale the cluster container instances, not the number of tasks.INCORRECT: "Write statements using the Cluster Query Language to scale the Docker containers" is incorrect as cluster queries are expressions that enable you to group objects.References:https://docs.aws.amazon.com/AmazonECS/latest/developerguide/serviceautoscaling.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/serviceloadbalancing.html24:T728,Amazon DynamoDB transactions simplify the developer experience of making coordinated, allornothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, helping you to maintain data correctness in your applications.You can use the DynamoDB transactional read and write APIs to manage complex business workflows that require adding, updating, or deleting multiple items as a single, allornothing operation. For example, a video game developer can ensure that players’ profiles are updated correctly when they exchange items in a game or make ingame purchases.With the transaction write API, you can group multiple Put, Update, Delete, and ConditionCheck actions. You can then submit the actions as a single TransactWriteItems operation that either succeeds or fails as a unit. The same is true for multiple Get actions, which you can group and submit as a single TransactGetItems operation.There is no additional cost to enable transactions for your DynamoDB tables. You pay only for the reads or writes that are part of your transaction. DynamoDB performs two underlying reads or writes of every item in the transaction: one to prepare the transaction and one to commit the transaction. These two underlying read/write operations are visible in your AmazonCloudWatch metrics.CORRECT: "Transactional" is the correct answer.INCORRECT: "Standard" is incorrect as this will not provide the ACID / allor nothing transactional writes that are required for this solution.INCORRECT: "Strongly consistent" is incorrect as this applies to reads only, not writes.INCORRECT: "Eventually consistent" is incorrect as this applies to reads only, not writes.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html25:T46a,AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improvessecurity through application isolation by design.As you can see with the EC2 launch type you must manage the infrastructure layer (Amazon EC2 instances), whereas with Amazon Fargate you do not. Therefore, for this scenario the Fargate launch type should be used.CORRECT: "Amazon ECS with Fargate launch type" is the correct answer.INCORRECT: "Amazon ECS with EC2 launch type" is incorrect as the EC2 launch type requires more platform overhead as you must manage Amazon EC2 instances.INCORRECT: "Amazon Elastic Kubernetes Service (EKS)" is incorrect as this would require more management overhead (unless used with Fargate).INCORRECT: "AWS Lambda" is incorrect as this is not a service that can be used to run Docker containers.References: https://aws.amazon.com/fargate/26:T5b0,Correct option:A load balancer accepts incoming traffic from clients and routes requests to its registered targets (such as EC2 instances) in one or more Availability Zones.The nodes for a load balancer distribute requests from clients to registered targets. When crosszone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. When crosszone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone. With Application Load Balancers, crosszone load balancing is always enabled.10 When crosszone load balancing is enabled, each of the 10 targets receives 10% of the traffic. This is because each load balancer node can route its 50% of the client traffic to all 10 targets (present in both AZs).CrossZone Load Balancing Overview: via https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/howelasticloadbalancingworks.html Incorrect options:25 If crosszone load balancing is disabled, each of the two targets in AZ1 will receive 25% of the traffic. Because the load balancer is only able to send to the targets registered in AZ1 (AZ2 instances are not accessible for load balancer on AZ1)20 Invalid option, given only as a distractor.15 Invalid option, given only as a distractor.Reference:https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/howelasticloadbalancingworks.html27:T71b,AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments.Each deployment policy has advantages and disadvantages and it’s important to select the best policy to use for each situation.The “all at once” policy will deploy the update in the fastest time but will incur downtime.All at once:• Deploys the new version to all instances simultaneously.• All of your instances are out of service while the deployment takes place.• Fastest deployment.• Good for quick iterations in development environment.• You will experience an outage while the deployment is taking place – not ideal for missioncritical systems.• If the update fails, you need to roll back the changes by redeploying the original version to all of your instances.• No additional cost.For this scenario downtime is acceptable and deploying in the fastest possible time is required so the “all at once” policy is the best choice.CORRECT: "All at once" is the correct answer.INCORRECT: "Rolling" is incorrect as this takes longer than “all at once”. This is a better choice if speed is required but downtime is not acceptable.INCORRECT: "Rolling with additional batch" is incorrect if you require no reduction in capacity as it adds an additional batch of instances to the deployment.INCORRECT: "Immutable" is incorrect as this takes a long time to complete. This is good if you cannot sustain application downtime and need to be able to quickly and easily roll back if issues occur.References:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.rollingversiondeploy.html28:T691,AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments.All at once:• Deploys the new version to all instances simultaneously.Rolling:• Update a few instances at a time (bucket), and then move onto the next bucket once the first bucket is healthy(downtime for 1 bucket at a time).Rolling with additional batch:• Like Rolling but launches new instances in a batch ensuring that there is full availability.Immutable:• Launches new instances in a new ASG and deploys the version update to these instances before swapping traffic tothese instances once healthy.• Zero downtime.Blue / Green deployment:• Zero downtime and release facility.• Create a new “stage” environment and deploy updates there.The rolling with additional batch launches a new batch to ensure capacity is not reduced and then updates the existing instances. Therefore, this is the best option to use for these requirements.CORRECT: “Rolling with additional batch” is the correct answer.INCORRECT: “Rolling” is incorrect as this will only use the existing instances without introducing an extra batch and therefore this will reduce the capacity of the application while the updates are taking place.INCORRECT: “All at once” is incorrect as this will run the updates on all instances at the same time causing a total outage.INCORRECT: “Immutable” is incorrect as this installs the updates on new instances, not existing instances.References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.rollingversiondeploy.html29:T5b4,Time to Live (TTL) for Amazon DynamoDB lets you define when items in a table expire so that they can be automatically deleted from the database. With TTL enabled on a table, you can set a timestamp for deletion on a peritem basis, allowing you to limit storage usage to only those records that are relevant.TTL is useful if you have continuously accumulating data that loses relevance after a specific time period (for example, session data, event logs, usage patterns, and other temporary data). If you have sensitive data that must be retained only for a certain amount of time according to contractual or regulatory obligations, TTL helps you ensure that it is removed promptly and asscheduled.Therefore, using a TTL is the best solution as it will automatically purge items after their useful lifetime.CORRECT: "Enable a Time To Live (TTL) on the table and add a timestamp attribute on new items" is the correct answer.INCORRECT: "Use the batchwriteitem API to delete the data" is incorrect as this would use RCUs and WCUs to remove the data.INCORRECT: "Create an AWS Lambda function that purges stale items from the table daily" is incorrect as this would also require reading/writing to the table so it would require RCUs/WCUs.INCORRECT: "Use the deleteitem API to delete the data" is incorrect is incorrect as this would use RCUs and WCUs to remove the data.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworksttl.html2a:T497,Correct option:AWS Budgets lets customers set custom budgets and receive alerts if their costs or usage exceed (or are forecasted to exceed) their budgeted amount.AWS requires approximately 5 weeks of usage data to generate budget forecasts AWS requires approximately 5 weeks of usage data to generate budget forecasts. If you set a budget to alert based on a forecasted amount, this budget alert isn't triggered until you have enough historical usage information.Incorrect options:Budget forecast has been created from an account that does not have enough privileges This is an incorrect statement. If the user account does not have enough privileges, the user will not be able to create the budget at all.Amazon CloudWatch could be down and hence alerts are not being sent Amazon CloudWatch is fully managed by AWS, this option has been added as a distractor.Account has to be part of AWS Organizations to receive AWS Budget alerts This is an incorrect statement. Standalone accounts too can create budgets and being part of an Organization is not mandatory to use AWS Budgets.Reference:https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgetsbestpractices.html2b:T40f,The application specification file (AppSpec file) is a YAMLformatted or JSONformatted file used by CodeDeploy to manage a deployment. The AppSpec file defines the deployment actions you want AWS CodeDeploy to execute.The name of the AppSpec file for an EC2/OnPremises deployment must be appspec.yml. The name of the AppSpec file for an Amazon ECS or AWS Lambda deployment must be appspec.yaml.Therefore, as this is an ECS deployment the file name must be appspec.yaml.CORRECT: "appspec.yml" is the correct answer.INCORRECT: "buildspec.yml" is incorrect as this is the file name you should use for the file that defines the build instructions for AWS CodeBuild.INCORRECT: "cron.yml" is incorrect. This is a file you can use with Elastic Beanstalk if you want to deploy a worker application that processes periodic background tasks.INCORRECT: "appspec.json" is incorrect as the file extension for ECS or Lambda deployments should be .yml not .json.References: https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfile.html2c:T544,Serverside encryption is about protecting data at rest. Serverside encryption encrypts only the object data, not object metadata. Using serverside encryption with customerprovided encryption keys (SSEC) allows you to set your own encryption keys.With the encryption key you provide as part of your request, Amazon S3 manages the encryption as it writes to disks and decryption when you access your objects. Therefore, you don't need to maintain any code to perform data encryption and decryption. The only thing you do is manage the encryption keys you provide.Therefore, SSEC is the best choice as AWS will manage all encryption and decryption operations whilst the company get to supply keys that they can manage.CORRECT: "Serverside encryption with customerprovided encryption keys (SSEC)" is the correct answer.INCORRECT: "Serverside encryption with Amazon S3 managed keys (SSES3)" is incorrect as with this option AWS manage the keys in S3.INCORRECT: "Serverside encryption with AWS KMS managed keys (SSEKMS)" is incorrect as with this option the keys are managed by AWS KMS.INCORRECT: "Clientside encryption" is incorrect as with this option all encryption and decryption is handled by the company (client) which is not desired in this scenario.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html2d:T721,Correct option:Enable CodeBuild timeoutsA build represents a set of actions performed by AWS CodeBuild to create output artifacts (for example, a JAR file) based on a set of input artifacts (for example, a collection of Java class files).The following rules apply when you run multiple builds:When possible, builds run concurrently. The maximum number of concurrently running builds can vary.Builds are queued if the number of concurrently running builds reaches its limit. The maximum number of builds in a queue is five times the concurrent build limit.A build in a queue that does not start after the number of minutes specified in its time out value is removed from the queue. The default timeout value is eight hours. You can override the build queue timeout with a value between five minutes and eight hours when you run your build.By setting the timeout configuration, the build process will automatically terminate post the expiry of the configured timeout.Incorrect options:Use AWS Lambda AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Lambda cannot be used to impact the code build process.Use AWS CloudWatch Events Amazon CloudWatch allows you to monitor AWS cloud resources and the applications you run on AWS. Metrics are provided automatically for a number of AWS products and services. CloudWatch is good for monitoring and viewing logs. CloudWatch cannot be used to impact the code build process.Use VPC Flow Logs VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC but not for code compiling configuration. VPC Flow Logs cannot be used to impact the code build process.Reference:https://docs.aws.amazon.com/codebuild/latest/userguide/buildsworking.html2e:T5a9,The Developers do not want to manage the infrastructure so the best AWS service for them to use to create a website for a development environment is AWS Elastic Beanstalk. This will allow the Developers to simply upload their Node.js code to Elastic Beanstalk and it will handle the provisioning and management of the underlying infrastructure. AWS Elastic Beanstalk can be used to quickly deploy and manage applications in the AWS Cloud. Developers upload applications and Elastic Beanstalk handles the deployment details of capacity provisioning, load balancing, autoscaling, and application health monitoring. AWS Elastic Beanstalk leverages Elastic Load Balancing and Auto Scaling to automatically scale your application in and out based on your application’s specific needs.CORRECT: "Create an AWS Elastic Beanstalk environment" is the correct answer.INCORRECT: "Create an AWS CloudFormation template" is incorrect as though you can use CloudFormation to deploy the infrastructure, it will not be managed for you.INCORRECT: "Create an AWS Lambda package" is incorrect as the Developers are deploying a website and Lambda is not a website. It is possible to use a Lambda function for a website however this would require a frontend component such as REST API.INCORRECT: "Launch an Auto Scaling group of Amazon EC2 instances" is incorrect as this would not provide a managed solution.References: https://aws.amazon.com/elasticbeanstalk/details/2f:T412,Correct option:DeleteQueue Deletes the queue specified by the QueueUrl, regardless of the queue's contents. When you delete a queue, any messages in the queue are no longer available.When you delete a queue, the deletion process takes up to 60 seconds. Requests you send involving that queue during the 60 seconds might succeed. For example, a SendMessage request might succeed, but after 60 seconds the queue and the message you sent no longer exist.When you delete a queue, you must wait at least 60 seconds before creating a queue with the same name.Incorrect options:PurgeQueue Deletes the messages in a queue specified by the QueueURL parameter. When you use the PurgeQueue action, you can't retrieve any messages deleted from a queue. The queue however remains.RemoveQueue This is an invalid option, given only as a distractor.RemovePermission Revokes any permissions in the queue policy that matches the specified Label parameter.Reference:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_RemovePermission.html30:T9fc,In general, Scan operations are less efficient than other operations in DynamoDB. A Scan operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set.If possible, you should avoid using a Scan operation on a large table or index with a filter that removes many results. Also, as a table or index grows, the Scan operation slows. The Scan operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation.Instead of using a large Scan operation, you can use the following techniques to minimize the impact of a scan on a table's provisioned throughput.• Reduce page sizeBecause a Scan operation reads an entire page (by default, 1 MB), you can reduce the impact of the scan operation by setting a smaller page size. The Scan operation provides a Limit parameter that you can use to set the page size for your request.Each Query or Scan request that has a smaller page size uses fewer read operations and creates a "pause" between each request.• Isolate scan operationsDynamoDB is designed for easy scalability. As a result, an application can create tables for distinct purposes, possibly even duplicating content across several tables. You want to perform scans on a table that is not taking "missioncritical" traffic. Some applications handle this load by rotating traffic hourly between two tables—one for critical traffic, and one for bookkeeping.Other applications can do this by performing every write on two tables: a "missioncritical" table, and a "shadow" table. Therefore, the best option to reduce the impact of the scan on the table's provisioned throughput is to set a smaller page size for the scan.CORRECT: "Set a smaller page size for the scan" is the correct answer.INCORRECT: "Use parallel scans" is incorrect as this will return results faster but place more burden on the table’s provisioned throughput.INCORRECT: "Define a range key on the table" is incorrect. A range key is a composite key that includes the hash key and another attribute. This is of limited use in this scenario as the table is being scanned to analyze multiple attributes.INCORRECT: "Prewarm the table by updating all items" is incorrect as updating all items would incur significant costs in terms of provisioned throughput and would not be advantageous.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bpqueryscan.html31:T521,Sticky sessions are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information in order to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies. In this case, it is likely that the clients authenticate to the backend instance and when they are reconnecting without sticky sessions enabled they may be load balanced to a different instance and need to authenticate again. The most obvious first step in troubleshooting this issue is to enable sticky sessions on the target group. CORRECT: "Enable Sticky Sessions on the target group" is the correct answer. INCORRECT: "Enable IAM authentication on the ALBs listener" is incorrect as you cannot enable &ldquo;IAM authentication&rdquo; on a listener. INCORRECT: "Add a new SSL certificate to the ALBs listener" is incorrect as this is not related to authentication. INCORRECT: "Change the load balancing algorithm on the target group to &ldquo;least outstanding requests)" is incorrect as this does not prevent the customer from being load balanced to a different instance, which is what is most likely to resolve this issue. References: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/loadbalancertargetgroups.html#stickysessions32:T602,Amazon CloudWatch Events help you to respond to state changes in your AWS resources. When your resources change state, they automatically send events into an event stream. You can create rules that match selected events in the stream and route them to your AWS Lambda function to take action.You can create a Lambda function and direct AWS Lambda to execute it on a regular schedule. You can specify a fixed rate (for example, execute a Lambda function every hour or 15 minutes), or you can specify a Cron expression.Therefore, the Developer should create an Amazon CloudWatch Events rule that triggers on a regular schedule to invoke the Lambda function. This is a serverless and automated solution.CORRECT: "Create an Amazon CloudWatch Events rule that triggers on a regular schedule to invoke the Lambda function" is the correct answer.INCORRECT: "Deploy an Amazon EC2 instance based on Linux, and edit it’s /etc/crontab file by adding a command to periodically invoke the Lambda function" is incorrect as EC2 is not a serverless solution.INCORRECT: "Configure an environment variable named PERIOD for the Lambda function. Set the value at 600" is incorrect as you cannot cause a Lambda function to execute based on a value in an environment variable.INCORRECT: "Create an Amazon SNS topic that has a subscription to the Lambda function with a 600second timer" is incorrect as SNS does not run on a timer, CloudWatch Events should be used instead.References: https://docs.aws.amazon.com/lambda/latest/dg/servicescloudwatchevents.html33:T702,You can specify one or more deployment groups for a CodeDeploy application. Each application deployment uses one of its deployment groups. The deployment group contains settings and configurations used during the deployment.You can associate more than one deployment group with an application in CodeDeploy. This makes it possible to deploy an application revision to different sets of instances at different times. For example, you might use one deployment group to deploy an application revision to a set of instances tagged Test where you ensure the quality of the code.Next, you deploy the same application revision to a deployment group with instances tagged Staging for additional verification.Finally, when you are ready to release the latest application to customers, you deploy to a deployment group that includes instances tagged Production.Therefore, using AWS CodeDeploy to create multiple deployment groups can be used to meet the requirementCORRECT: "Use AWS CodeDeploy to create multiple deployment groups" is the correct answer.INCORRECT: "Use AWS CodeCommit to create multiple repositories to deploy the application" is incorrect as the requirement is to deploy the same code to separate environments in a staged manner. Therefore, having multiple code repositories is not useful.INCORRECT: "Use AWS CodeBuild to create, configure, and deploy multiple build application projects" is incorrect as the requirement is not to build the application, it is to deploy the application.INCORRECT: "Use AWS Data Pipeline to create multiple data pipeline provisions to deploy the application" is incorrect as Data Pipeline is a service used for data migration, not deploying updates to applications.References: https://docs.aws.amazon.com/codedeploy/latest/userguide/deploymentgroups.html34:T62c,For this scenario the key requirement is to ensure the data is not lost. Therefore, the data must be stored in a durable data store outside of the EC2 instances. Amazon DynamoDB is a suitable solution for storing session data. DynamoDB has a session handling capability for multiple languages as in the below example for PHP:“The DynamoDB Session Handler is a custom session handler for PHP that allows developers to use Amazon DynamoDB as a session store. Using DynamoDB for session storage alleviates issues that occur with session handling in a distributed web application by moving sessions off of the local file system and into a shared location. DynamoDB is fast, scalable, easy to setup, and handles replication of your data automatically.”Therefore, the best answer is to use DynamoDB to store the session data.CORRECT: "Use Amazon DynamoDB to perform scalable session handling" is the correct answer.INCORRECT: "Enable Sticky Sessions on the Elastic Load Balancer" is incorrect. Sticky sessions attempts to direct a user that has reconnected to the application to the same EC2 instance that they connected to previously. However, this does not ensure that the session data is going to be available.INCORRECT: "Use an EC2 Auto Scaling group to automatically launch new instances" is incorrect as this does not provide a solution for storing the session data.INCORRECT: "Use Amazon SQS to save session data" is incorrect as Amazon SQS is not suitable for storing session data.References: https://docs.aws.amazon.com/awssdkphp/v2/guide/featuredynamodbsessionhandler.html35:T42a,The optional Mappings section matches a key to a corresponding set of named values. For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. You use the Fn::FindInMap intrinsic function to retrieve values in a map.CORRECT: "Mappings" is the correct answer.INCORRECT: "Outputs" is incorrect. The optional Outputs section declares output values that you can import into other stacks (to create crossstack references), return in response (to describe stack calls), or view on the AWS CloudFormation console.INCORRECT: "Parameters" is incorrect. Parameters enable you to input custom values to your template each time you create or update a stack.INCORRECT: "Resources" is incorrect. The required Resources section declares the AWS resources that you want to include in the stack, such as an Amazon EC2 instance or an Amazon S3 bucket.References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappingssectionstructure.html36:T65f,By default, the DynamoDB write operations (PutItem, UpdateItem, DeleteItem) are unconditional: Each operation overwrites an existing item that has the specified primary key.DynamoDB optionally supports conditional writes for these operations. A conditional write succeeds only if the item attributes meet one or more expected conditions. Otherwise, it returns an error. Conditional writes are helpful in many situations. For example, you might want a PutItem operation to succeed only if there is not already an item with the same primary key. Or you could prevent an UpdateItem operation from modifying an item if one of its attributes has a certain value. Conditional writes are helpful in cases where multiple users attempt to modify the same item. Consider the following diagram, in which two users (Alice and Bob) are working with the same item from a DynamoDB table.Therefore, conditional writes are should be used to prevent the overwriting that has been occurring.CORRECT: "Conditional writes" is the correct answer.INCORRECT: "Concurrent writes" is incorrect is not a feature of DynamoDB. If concurrent writes occur this could lead to the very issues that conditional writes can be used to resolve.INCORRECT: "Atomic writes" is incorrect. Atomic reads and writes are something that can be performed using DynamoDB transactions using conditional writes.INCORRECT: "Batch writes" is incorrect as this is just a way of making multiple put or delete API operations in a single batch operation.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate37:T66a,You can register your Lambda functions as targets and configure a listener rule to forward requests to the target group for your Lambda function. When the load balancer forwards the request to a target group with a Lambda function as a target, it invokes your Lambda function and passes the content of the request to the Lambda function, in JSON format.You need to create a target group, which is used in request routing, and register a Lambda function to the target group. If the request content matches a listener rule with an action to forward it to this target group, the load balancer invokes the registered Lambda function.CORRECT: "Create a target group and register the Lambda function using the AWS CLI" is the correct answer.INCORRECT: "Create an Auto Scaling Group (ASG) and register the Lambda function in the launch configuration" is incorrect as launch configurations and ASGs are used for launching Amazon EC2 instances, you cannot use an ASG with a Lambda function.INCORRECT: "Setup an API in front of the ALB using API Gateway and use an integration request to map the request to the Lambda function" is incorrect as it is not a common design pattern to map an API Gateway API to a Lambda function when using an ALB. Though technically possible, typically you would choose to put API Gateway or an ALB in front of your application, not both.INCORRECT: "Configure an eventsource mapping between the ALB and the Lambda function" is incorrect as you cannot configure an eventsource mapping between and ALB and a Lambda function.References: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/lambdafunctions.html38:T4e6,Amazon ElastiCache is a fully managed implementation of two popular inmemory data stores – Redis and Memcached. The inmemory caching provided by ElastiCache can be used to significantly improve latency and throughput for many readheavy application workloads or computeintensive workloads.There are two types of engine you can choose from: Memcached, Redis:MEMCACHED• Simplest model and can run large nodes.• Can be scaled in and out and cache objects such as DBs.• Widely adopted memory object caching system.• Multithreaded.REDIS• Opensource inmemory keyvalue store.• Supports more complex data structures: sorted sets and lists.• Supports master / slave replication and multiAZ for crossAZ redundancy.• Support automatic failover and backup/restore.As the Developer requires a multithreaded cache, the best choice is Memcached.CORRECT: "Amazon ElastiCache Memcached" is the correct answer.INCORRECT: "Amazon ElastiCache Redis" is incorrect as Redis it not multithreaded.INCORRECT: "Amazon DynamoDB DAX" is incorrect as this is more suitable for use with an Amazon DynamoDB table.INCORRECT: "Amazon RedShift" is incorrect as this is not an inmemory caching engine, it is a data warehouse.References: https://aws.amazon.com/elasticache/39:T5b4,In this case the build is using environment variables that are too large for AWS CodeBuild. CodeBuild can raise errors when the length of all environment variables (all names and values added together) reach a combined maximum of around 5,500 characters.The recommended solution is to use Amazon EC2 Systems Manager Parameter Store to store large environment variables and then retrieve them from your buildspec file. Amazon EC2 Systems Manager Parameter Store can store an individual environment variable (name and value added together) that is a combined 4,096 characters or less.CORRECT: "Use AWS Systems Manager Parameter Store to store large numbers of environment variables" is the correct answer.INCORRECT: "Add the export LC_ALL=”en_US.utf8” command to the pre_build section to ensure POSIX localization" is incorrect as this is used to set the locale and will not affect the limits that have been reached.INCORRECT: "Use Amazon Cognito to store keyvalue pairs for large numbers of environment variables" is incorrect as Cognito is used for authentication and authorization and is not suitable for this purpose.INCORRECT: "Update the settings for the build project to use an Amazon S3 bucket for large numbers of environment variables" is incorrect as Systems Manager Parameter Store is designed for this purpose and is a better fit.References: https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html#troubleshootinglargeenvvars3a:T58c,The key requirement is to provide readonly access to the team for a specific DynamoDB table. Therefore, the AWS managed policy cannot be used as it will provide access to all DynamoDB tables in the account which does not follow the principal of least privilege.Therefore, a customer managed policy should be created that provides readonly access and specifies the ARN of the table. For instance, the resource element might include the following ARN:arn:aws:dynamodb:uswest1:515148227241:table/exampletableThis will lock down access to the specific DynamoDB table, following the principal of least privilege.CORRECT: "Create a customer managed policy with read only access to DynamoDB and specify the ARN of the table for the“Resource” element. Attach the policy to the group" is the correct answer.INCORRECT: "Assign the AmazonDynamoDBReadOnlyAccess AWS managed policy to the group" is incorrect as this will provide readonly access to all DynamoDB tables in the account.INCORRECT: "Assign the AWSLambdaDynamoDBExecutionRole AWS managed policy to the group" is incorrect as this is a role used with AWS Lambda.INCORRECT: "Create a customer managed policy with read/write access to DynamoDB for all resources. Attach the policy to the group" is incorrect as readonly access should be provided, not read/write.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/usingidentitybasedpolicies.html3b:T671,Port mappings allow containers to access ports on the host container instance to send or receive traffic. Port mappings are specified as part of the container definition. The container port is the port number on the container that is bound to the userspecified or automatically assigned host port. The host port is the port number on the container instance to reserve for your container.As we cannot have multiple services bound to the same host port, we need to ensure that each container port mapping uses a different host port. The easiest way to do this is to set the host port number to 0 and ECS will automatically assign an available port. We also need to assign port 80 to the container port so that the web service is able to run.CORRECT: "Specify port 80 for the container port and port 0 for the host port" is the correct answer.INCORRECT: "Specify port 80 for the container port and a unique port number for the host port" is incorrect as this is more difficult to manage as you have to manually assign the port number.INCORRECT: "Specify a unique port number for the container port and port 80 for the host port" is incorrect as the web service on the container needs to run on pot 80 and you can only bind one container to port 80 on the host so this would not allow more than one container to work.INCORRECT: "Leave both the container port and host port configuration blank" is incorrect as this would mean that ECS would dynamically assign both the container and host port. As the web service must run on port 80 this would not work correctly.References: https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PortMapping.html3c:Tb22,Correct option:Set up an AWS CloudFormation template that defines the load test resources. Leverage the AWS CLI createstackset command to create a stack set in the desired RegionsAWS CloudFormation StackSets extends the capability of stacks by enabling you to create, update, or delete stacks across multiple accounts and AWS Regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified AWS Regions.via https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/whatiscfnstacksets.html A stack set lets you create stacks in AWS accounts across regions by using a single CloudFormation template. A stack set's CloudFormation template defines all the resources in each stack. As you create the stack set, specify the template to use, in addition to any parameters and capabilities that the template requires.After you've defined a stack set, you can create, update, or delete stacks in the target accounts and AWS Regions you specify.via https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksetsconcepts.html Incorrect options:Set up an AWS CloudFormation template that defines the load test resources. Develop regionspecific Lambda functions to create a stack from the AWS CloudFormation template in each Region when the respective function is invoked If you do not use a stack set, then you need to define the CloudFormation templates in each region as well as develop lambda functions in each region to create a stack from the corresponding CloudFormation template. This is unnecessary bloat that can be avoided by simply using the CloudFormation StackSets.Set up an AWS Cloud Development Kit (CDK) ToolKit that defines the load test resources. Leverage the CDK CLI to create a stack from the template in each Region AWS CDK is a framework for defining cloud infrastructure in code and provisioning it through AWS CloudFormation. The CDK Toolkit again poses regional limitations and is not the right fit for the given use case.Set up an AWS Organizations template that defines the load test resources across the organization. Leverage the AWS CLI createstackset command to create a stack set in the desired Regions This option acts as a distractor. AWS Organizations cannot be used to create templates for provisioning AWS infrastructure. AWS Organizations is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage.References:https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksetsconcepts.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/whatiscfnstacksets.html3d:T734,Correct option:Amazon EFS volumes EFS volumes provide a simple, scalable, and persistent file storage for use with your Amazon ECS tasks. With Amazon EFS, storage capacity is elastic, growing and shrinking automatically as you add and remove files. Your applications can have the storage they need, when they need it. Amazon EFS volumes are supported for tasks hosted on Fargate or Amazon EC2 instances.You can use Amazon EFS file systems with Amazon ECS to export file system data across your fleet of container instances. That way, your tasks have access to the same persistent storage, no matter the instance on which they land. However, you must configure your container instance AMI to mount the Amazon EFS file system before the Docker daemon starts. Also, your task definitions must reference volume mounts on the container instance to use the file system.Incorrect options:Docker volumes A Dockermanaged volume that is created under /var/lib/docker/volumes on the host Amazon EC2 instance. Docker volume drivers (also referred to as plugins) are used to integrate the volumes with external storage systems, such as Amazon EBS. The builtin local volume driver or a thirdparty volume driver can be used. Docker volumes are only supported when running tasks on Amazon EC2 instances.Bind mounts A file or directory on the host, such as an Amazon EC2 instance or AWS Fargate, is mounted into a container. Bind mount host volumes are supported for tasks hosted on Fargate or Amazon EC2 instances. Bind mounts provide temporary storage, and hence these are a wrong choice for this use case.AWS Storage Gateway volumes This is an incorrect choice, given only as a distractor.Reference:https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_data_volumes.html https://aws.amazon.com/blogs/containers/amazonecsavailabilitybestpractices/3e:T920,Performing a scan on a table consumes a lot of RCUs. A Scan operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set. To reduce the amount of RCUs used by the scan so it doesn’t affect production workloads whilst minimizing the execution time, there are a couple of recommendations the Developer can follow.Firstly, the Limit parameter can be used to reduce the page size. The Scan operation provides a Limit parameter that you can use to set the page size for your request. Each Query or Scan request that has a smaller page size uses fewer read operations and creates a "pause" between each request.Secondly, the Developer can configure parallel scans. With parallel scans the Developer can maximize usage of the available throughput and have the scans distributed across the table’s partitions.A parallel scan can be the right choice if the following conditions are met:• The table size is 20 GB or larger.• The table's provisioned read throughput is not being fully used.• Sequential Scan operations are too slow.Therefore, to optimize the scan operation the Developer should use parallel scans while limiting the rate as this will ensure that the scan operation does not affect the performance of production workloads and still have it complete in the minimum time.CORRECT: "Use parallel scans while limiting the rate" is the correct answer.INCORRECT: "Use sequential scans" is incorrect as this is slower than parallel scans and the Developer needs to minimize scan execution time.INCORRECT: "Increase the RCUs during the scan operation" is incorrect as the table is only using half of the RCUs during nonpeak hours so there are RCUs available. You could increase RCUs and perform the scan faster, but this would be more expensive. The better solution is to use parallel scans with the limit parameter.INCORRECT: "Change to eventually consistent RCUs during the scan operation" is incorrect as this does not provide a solution for preventing impact to the production workloads. The limit parameter should be used to ensure the tables RCUs are not fully used.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bpqueryscan.html#QueryAndScanGuidelines.ParallelScan3f:T785,AWS Lambda is a stateless compute service and so you cannot store session data in AWS Lambda itself. You can store a limited amount of information (up to 512 MB) in the /tmp directory. This information is preserved if the function is reused (i.e. the execution context is reused). However, it is not guaranteed that the execution context will be reused so the data could bedestroyed.The /tmp should only be used for data that can be regenerated or for operations that require a local filesystem, but not as a permanent storage solution. It is ideal for setting up database connections that will be needed across invocations of the function as the connection is made once and preserved across invocations.Amazon DynamoDB is a good solution for this scenario as it is a lowlatency NoSQL database that is often used for storing session state data. Amazon S3 would also be a good fit for this scenario but is not offered as an option.With both Amazon DynamoDB and Amazon S3 you can store data longterm and it is available for multiple invocations of your function as well as being available from multiple invocations simultaneously.CORRECT: "Store the data in an Amazon DynamoDB table" is the correct answer.INCORRECT: "Store the data in an Amazon Kinesis Data Stream" is incorrect as this service is used for streaming data. It is not used for sessionstore use cases.INCORRECT: "Store the data in the /tmp directory" is incorrect as any data stored in the /tmp may not be available for subsequent calls to your function. The /tmp directory content remains when the execution context is frozen, providing transient cache that can be used for multiple invocations. However, it is not guaranteed that the execution context will bereused so the data could be lost.INCORRECT: "Store the data in an Amazon SQS queue" is incorrect as a message queue is not used for longterm storage of data.References: https://aws.amazon.com/dynamodb/40:T624,Access keys are longterm credentials for an IAM user or the AWS account root user. You can use access keys to sign programmatic requests to the AWS CLI or AWS API (directly or using the AWS SDK).Access keys are stored in one of the locations on a client that needs to make authenticated API calls to AWS services:• Linux: ~/.aws/credentials• Windows: %UserProfle%\.aws\credentialsIn this scenario the application server is running onpremises. Therefore, you cannot assign an IAM role (which would be the preferable solution for Amazon EC2 instances). In this case it is therefore better to use access keys.CORRECT: "Create an IAM user and generate access keys. Create a credentials file on the application server" is the correct answer.INCORRECT: "Create an IAM role with the necessary permissions and assign it to the application server" is incorrect. This is an onpremises server so it is not possible to use an IAM role. If it was an EC2 instance, this would be the preferred (best practice)option.INCORRECT: "Create an IAM group with the necessary permissions and add the onpremise application server to the group" is incorrect. You cannot add a server to an IAM group. You put IAM users into groups and assign permissions to them using a policy.INCORRECT: "Create an IAM user and generate a key pair. Use the key pair in API calls to AWS services" is incorrect as key pairs are used for SSH access to Amazon EC2 instances. You cannot use them in API calls to AWS services.References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_accesskeys.html41:Tab5,AWS XRay is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization. For any traced request to your application, you can see detailed information not only about the request and response, but also about calls that yourapplication makes to downstream AWS resources, microservices, databases and HTTP web APIs.You can run the XRay daemon on the following operating systems on Amazon EC2:• Amazon Linux• Ubuntu• Windows Server (2012 R2 and newer)The XRay daemon must be running on the EC2 instance in order to collect data. You can use a user data script to run the daemon automatically when you launch the instance. The XRay daemon uses the AWS SDK to upload trace data to XRay, and it needs AWS credentials with permission to do that.On Amazon EC2, the daemon uses the instance's instance profile role automatically. The IAM role or user that the daemon's credentials belong to must have permission to write data to the service on your behalf.• To use the daemon on Amazon EC2, create a new instance profile role or add the managed policy to an existing one.• To use the daemon on Elastic Beanstalk, add the managed policy to the Elastic Beanstalk default instance profile role.• To run the daemon locally, create an IAM user and save its access keys on your computer.Therefore, the most likely cause of the issues being experienced in this scenario is that the instance’s instance profile role does not have permission to upload trace data to XRay or the XRay daemon is not running on the EC2 instance.CORRECT: "The instance’s instance profile role does not have permission to upload trace data to XRay" is the correct answer.CORRECT: "The XRay daemon is not installed on the EC2 instance." is also a correct answer.INCORRECT: "The XRay API is not installed on the EC2 instance " is incorrect as you do not install the XRay API, you run the XRay daemon. The API will always be accessible using the XRay endpoint.INCORRECT: "The traces are reaching XRay, but the Developer does not have permission to view the records" is incorrect as the developer previously viewed data in XRay so clearly has permissions.INCORRECT: "The XRay segments are being queued" is incorrect. The XRay daemon is responsible for relaying trace data to XRay. However, it will not queue data for an extended period of time so this is unlikely to be a cause of this issue.References:https://docs.aws.amazon.com/xray/latest/devguide/xraydaemonec2.html https://docs.aws.amazon.com/xray/latest/devguide/xraydaemon.html https://docs.aws.amazon.com/xray/latest/devguide/xraydaemon.html#xraydaemonpermissions42:T5d1,The Kinesis Client Library (KCL) helps you consume and process data from a Kinesis data stream. This type of application is also referred to as a consumer. The KCL takes care of many of the complex tasks associated with distributed computing, such as load balancing across multiple instances, responding to instance failures, checkpointing processed records, and reacting to resharding. The KCL enables you to focus on writing recordprocessing logic.The KCL is different from the Kinesis Data Streams API that is available in the AWS SDKs. The Kinesis Data Streams API helps you manage many aspects of Kinesis Data Streams (including creating streams, resharding, and putting and getting records). The KCL provides a layer of abstraction specifically for processing data in a consumer role.Therefore, the correct answer is to use the Kinesis Client Library.CORRECT: "Amazon Kinesis Client Library (KCL)" is the correct answer.INCORRECT: "Amazon Kinesis API" is incorrect. You can work with Kinesis Data Streams directly from your consumers using the API but this is does not deploy an intermediary component as required.INCORRECT: "AWS CLI" is incorrect. The AWS CLI can be used to work directly with the Kinesis API but this does not deploy an intermediary component as required.INCORRECT: "Amazon Kinesis CLI" is incorrect as this does not exist. The AWS CLI has commands for working with Kinesis.References: https://docs.aws.amazon.com/streams/latest/dev/developingconsumerswithkcl.html43:T83d,You can use environment variables to store secrets securely and adjust your function's behavior without updating code. An environment variable is a pair of strings that are stored in a function's versionspecific configuration.Use environment variables to pass environmentspecific settings to your code. For example, you can have two functions with the same code but different configuration. One function connects to a test database, and the other connects to a production database.In this situation, you use environment variables to tell the function the hostname and other connection details for the database. You might also set an environment variable to configure your test environment to use more verbose logging or more detailed tracing.You set environment variables on the unpublished version of your function by specifying a key and value. When you publish a version, the environment variables are locked for that version along with other versionspecific configuration.It is possible to create separate versions of a function with different environment variables referencing the relevant database connection strings. Therefore, using environment variables is the best way to ensure the environmentspecific database connection strings are available in a single deployment package.CORRECT: "Use environment variables for the database connection strings" is the correct answer.INCORRECT: "Use a separate function for development and production" is incorrect as there’s a single deployment package that must contain the connection strings for multiple environments. Therefore, using environment variables is necessary.INCORRECT: "Include the resources in the function code" is incorrect. It would not be secure to include the database connection strings in the function code. With environment variables the password string can be encrypted using KMS which is much more secure.INCORRECT: "Use layers for storing the database connection strings" is incorrect. Layers are used for adding external libraries to your functions.References: https://docs.aws.amazon.com/lambda/latest/dg/configurationenvvars.html44:T580,In API Gateway, an API's method request can take a payload in a different format from the corresponding integration request payload, as required in the backend. Similarly, the backend may return an integration response payload different from the method response payload, as expected by the frontend.API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response.CORRECT: "Create a RESTful API using Amazon API Gateway. Transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates" is the correct answer.INCORRECT: "Create a RESTful API using Amazon API Gateway. Pass the incoming JSON to the SOAP interface through an Application Load Balancer" is incorrect. The API Gateway cannot process the XML SOAP data and cannot pass it through an ALB.INCORRECT: "Create a SOAP API using Amazon API Gateway. Transform the incoming JSON into a valid XML message for the SOAP interface using AWS Lambda" is incorrect. API Gateway does not support SOAP APIs.INCORRECT: "Create a SOAP API using Amazon API Gateway. Pass the incoming JSON to the SOAP interface through a Network Load Balancer" is incorrect. API Gateway does not support SOAP APIs.References: https://docs.aws.amazon.com/apigateway/latest/Developerguide/requestresponsedatamappings.html45:T69e,Correct options:Auto Scaling groups that span across multiple Regions need to be enabled for all the Regions specified This is not valid for Auto Scaling groups. Auto Scaling groups cannot span across multiple Regions.An Auto Scaling group can contain EC2 instances in only one Availability Zone of a Region This is not valid for Auto Scaling groups. An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same Region.Amazon EC2 Auto Scaling Overview: via https://docs.aws.amazon.com/autoscaling/ec2/userguide/whatisamazonec2autoscaling.html Incorrect options:An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same Region This is a valid statement. Auto Scaling groups can span across the availability Zones of a Region.Amazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group When one Availability Zone becomes unhealthy or unavailable, Auto Scaling launches new instances in an unaffected Availability Zone. When the unhealthy Availability Zone returns to a healthy state, Auto Scaling automatically redistributes the application instances evenly across all of the designated Availability Zones.For Auto Scaling groups in a VPC, the EC2 instances are launched in subnets For Auto Scaling groups in a VPC, the EC2 instances are launched in subnets. Customers can select the subnets for your EC2 instances when you create or update the Auto Scaling group.References:https://docs.aws.amazon.com/autoscaling/ec2/userguide/whatisamazonec2autoscaling.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscalingbenefits.html46:T559,DynamoDB supports eventually consistent and strongly consistent reads. When using eventually consistent reads the response might not reflect the results of a recently completed write operation. The response might include some stale data.When using strongly consistent reads DynamoDB returns a response with the most uptodate data, reflecting the updates from all prior write operations that were successful.DynamoDB uses eventually consistent reads unless you specify otherwise. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation.CORRECT: "Set the ConsistentRead parameter to true when calling GetItem" is the correct answer.INCORRECT: "Cache the database writes using Amazon DynamoDB Accelerator" is incorrect. DynamoDB DAX caches items from DynamoDB to improve read performance but will not ensure the latest data is retrieved.INCORRECT: "Use the TransactWriteItems API when issuing PutItem actions" is incorrect. This operation is used to group transactions in an allornothing update.INCORRECT: "Use the UpdateGlobalTable API to create a global secondary index" is incorrect. A GSI does not assist in any way in this solution.References: https://docs.aws.amazon.com/amazondynamodb/latest/Developerguide/HowItWorks.ReadConsistency.html47:T89f,Correct option:Elastic BeanstalkAWS Elastic Beanstalk provides an environment to easily deploy and run applications in the cloud. It is integrated with developer tools and provides a onestop experience for you to manage the lifecycle of your applications.AWS Elastic Beanstalk lets you manage all of the resources that run your application as environments where each environment runs only a single application version at a time. When an environment is being created, Elastic Beanstalk provisions all the required resources needed to run the application version. You don't need to worry about server provisioning, configuration, and deployment as that's taken care of by Beanstalk.Benefits of Elastic Beanstalk: via https://aws.amazon.com/elasticbeanstalk/ Incorrect options:CloudFormation AWS CloudFormation is a service that gives developers and businesses an easy way to create a collection of related AWS and thirdparty resources and provision them in an orderly and predictable fashion. With CloudFormation, you still need to create a template to specify the type of resources you need, hence this option is not correct.How CloudFormation Works: via https://aws.amazon.com/cloudformation/ CodeDeploy AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your onpremises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. It can deploy an application to an instance but it cannot provision the instance.Serverless Application Model The AWS Serverless Application Model (AWS SAM) is an opensource framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML. As the web application needs to be deployed on EC2 instances, so this option is ruled out.References:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html https://aws.amazon.com/cloudformation/48:T460,AWS CodeDeploy can deploy software packages using an archive that has been uploaded to an Amazon S3 bucket. The archive file will typically be a .zip file containing the code and files required to deploy the software package.CORRECT: "Upload the bundle to an Amazon S3 bucket and specify the S3 location when doing a deployment using AWS CodeDeploy" is the correct answer.INCORRECT: "Use AWS CodeDeploy and point it to the local file system to deploy the software package" is incorrect. You cannot point CodeDeploy to a local file system running onpremises.INCORRECT: "Create a repository using AWS CodeCommit to automatically trigger a deployment to the EC2 instances" is incorrect. CodeCommit is a source control system. In this case the source code has already been package using a thirdparty tool.INCORRECT: "Use AWS CodeBuild to commit the package and automatically deploy the software package" is incorrect. CodeBuild does not commit packages (CodeCommit does) or deploy the software. It is a build service. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/tutorialswindowsuploadapplication.html49:T6a2,Correct options:You can't change the queue type after you create it You can't change the queue type after you create it and you can't convert an existing standard queue into a FIFO queue. You must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue.The visibility timeout value for the queue is in seconds, which defaults to 30 seconds The visibility timeout for the queue is in seconds. Valid values are: An integer from 0 to 43,200 (12 hours), the Default value is 30.Incorrect options:The deadletter queue of a FIFO queue must also be a FIFO queue. Whereas, the deadletter queue of a standard queue can be a standard queue or a FIFO queue The deadletter queue of a FIFO queue must also be a FIFO queue. Similarly, the deadletter queue of a standard queue must also be a standard queue.The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using MessageRetentionPeriod attribute The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using DelaySeconds attribute. MessageRetentionPeriod attribute controls the length of time, in seconds, for which Amazon SQS retains a message.Queue tags are case insensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag Queue tags are casesensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag. To be able to tag a queue on creation, you must have the sqs:CreateQueue and sqs:TagQueue permissions.Reference:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_CreateQueue.htmll4a:Tbf1,The process of consuming messages from a queue depends on whether you use short or long polling. By default, Amazon SQS uses short polling, querying only a subset of its servers (based on a weighted random distribution) to determine whether any messages are available for a response. You can use long polling to reduce your costs while allowing your consumers to receive messages as soon as they arrive in the queue.When you consume messages from a queue using short polling, Amazon SQS samples a subset of its servers (based on a weighted random distribution) and returns messages from only those servers. Thus, a particular ReceiveMessage request might not return all of your messages. However, if you have fewer than 1,000 messages in your queue, a subsequent request will return your messages. If you keep consuming from your queues, Amazon SQS samples all of its servers, and you receive all of your messages.The following diagram shows the shortpolling behavior of messages returned from a standard queue after one of your system components makes a receive request. Amazon SQS samples several of its servers (in gray) and returns messages A, C, D, and B from these servers. Message E isn't returned for this request but is returned for a subsequent request.When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response).Long polling occurs when the WaitTimeSeconds parameter of a ReceiveMessage request is set to a value greater than 0 in one of two ways:• The ReceiveMessage call sets WaitTimeSeconds to a value greater than 0.• The ReceiveMessage call doesn’t set WaitTimeSeconds, but the queue attribute ReceiveMessageWaitTimeSeconds is set to a value greater than 0.Therefore, the Developer should set the ReceiveMessage API with a WaitTimeSeconds of 20.CORRECT: "Set the ReceiveMessage API with a WaitTimeSeconds of 20" is the correct answer.INCORRECT: "Set the SetQueueAttributes API with a DelaySeconds of 20" is incorrect as this would be used to configure a delay queue where the delivery of messages in the queue is delayed.INCORRECT: "Set the ReceiveMessage API with a VisibilityTimeout of 30" is incorrect as this would configure the visibility timeout which is the length of time a message that has been received is invisible.INCORRECT: "Set the SetQueueAttributes with a MessageRetentionPeriod of 60" is incorrect as this would configure how long messages are retained in the queue.References:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsshortandlongpolling.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html4b:T6cd,Correct option:Create a CNAME recordA CNAME record maps DNS queries for the name of the current record, such as acme.example.com, to another domain (example.com or example.net) or subdomain (acme.example.com or zenith.example.org).CNAME records can be used to map one domain name to another. Although you should keep in mind that the DNS protocol does not allow you to create a CNAME record for the top node of a DNS namespace, also known as the zone apex. For example, if you register the DNS name example.com, the zone apex is example.com. You cannot create a CNAME record for example.com, but you can create CNAME records for www.example.com, newproduct.example.com, and so on.Please review the major differences between CNAME and Alias Records: via https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resourcerecordsetschoosingaliasnonalias.htmlIncorrect options:Create an A record Used to point a domain or subdomain to an IP address. 'A record' cannot be used to map one domain name to another.Create a PTR record A Pointer (PTR) record resolves an IP address to a fullyqualified domain name (FQDN) as an opposite to what A record does. PTR records are also called Reverse DNS records. 'PTR record' cannot be used to map one domain name to another.Create an Alias Record Alias records let you route traffic to selected AWS resources, such as CloudFront distributions and Amazon S3 buckets. They also let you route traffic from one record in a hosted zone to another record. 3rd party websites do not qualify for these as we have no control over those. 'Alias record' cannot be used to map one domain name to another.Reference:https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resourcerecordsetschoosingaliasnonalias.htmll4c:T91b,With Amazon Cognito User Pools your app users can sign in either directly through a user pool or federate through a thirdparty identity provider (IdP). The user pool manages the overhead of handling the tokens that are returned from social signin through Facebook, Google, Amazon, and Apple, and from OpenID Connect (OIDC) and SAML IdPs.After successful authentication, Amazon Cognito returns user pool tokens to your app. You can use the tokens to grant your users access to your own serverside resources, or to the Amazon API Gateway. Or, you can exchange them for AWS credentials to access other AWS services.The ID token is a JSON Web Token (JWT) that contains claims about the identity of the authenticated user such as name, email, and phone_number. You can use this identity information inside your application. The ID token can also be used to authenticate users against your resource servers or server applications.CORRECT: "Use Amazon Cognito user pools and a custom authorizer to authenticate and authorize users based on JSON Web Tokens" is the correct answer.INCORRECT: "Use Amazon ElastiCache to store user credentials and pass them to the APIs for authentication and authorization" is incorrect. This option does not provide a solution for authenticating based on Open ID providers and is not secure as there is no mechanism mentioned for ensuring the secrecy of the credentials.INCORRECT: "Use Amazon DynamoDB to store user credentials and have the application retrieve temporary credentials from AWS STS. Make API calls by passing user credentials to the APIs for authentication and authorization" is incorrect. This option also does not solve the requirement of integrating with Open ID providers and also suffers from the same security concerns as the option above.INCORRECT: "Build an OpenID token broker with Amazon and Facebook. Users will authenticate with these identify providers and pass the JSON Web Token to the API to authenticate each API call" is incorrect. This may be a workable and secure solution however it is definitely not the simplest as it would require significant custom development.References:https://docs.aws.amazon.com/cognito/latest/developerguide/authentication.html https://docs.aws.amazon.com/cognito/latest/developerguide/amazoncognitouserpoolsusingtokenswithidentityproviders.html4d:T5de,If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency.To use latencybased routing, you create latency records for your resources in multiple AWS Regions. When Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you've created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server.CORRECT: "Create A records in AWS Route 53 and use a latencybased routing policy" is the correct answer.INCORRECT: "Create Alias records in AWS Route 53 and direct the traffic to an Elastic Load Balancer" is incorrect as an ELB is within a single region. In this case the Developer needs to direct traffic to different regions.INCORRECT: "Create A records in AWS Route 53 and use a weighted routing policy" is incorrect as weighting is used to send more traffic to one region other another, not to direct for best performance.INCORRECT: "Create CNAME records in AWS Route 53 and direct traffic to Amazon CloudFront" is incorrect as this does not direct traffic to different regions for best performance which is what the questions asks for.References: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routingpolicy.html#routingpolicylatency4e:T55b,To make the most of the table’s provisioned throughput, the Developer can use the Parallel Scan API operation so that the scan is distributed across the table’s partitions. This will help to optimize the scan to complete in the fastest possible time. However, the Developer will also need to apply rate limiting to ensure that the scan does not affect normal workloads.CORRECT: "Use the Parallel Scan API operation and limit the rate" is the correct answer.INCORRECT: "Use sequential scans and apply a FilterExpression" is incorrect. A FilterExpression is a string that contains conditions that DynamoDB applies after the Scan operation, but before the data is returned to you. This will not assist with speeding up the scan or preventing it from affecting normal workloads.INCORRECT: "Increase read capacity units during the scan operation" is incorrect. There are already more RCUs provisioned than are needed during the nonpeak hours. The key here is to use what is available for costeffectiveness whilst ensuing normal workloads are not affected.INCORRECT: "Use sequential scans and set the ConsistentRead parameter to false" is incorrect. This setting would turn off consistent reads making the scan eventually consistent. This will not satisfy the requirements of the question.References: https://aws.amazon.com/blogs/Developer/ratelimitedscansinamazondynamodb/4f:T877,Serverside encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects.As you can see in the image above, there are three options for serverside encryption:• ServerSide Encryption with Amazon S3Managed Keys (SSES3) – the data is encrypted by Amazon S3 using keys that are managed through S3• ServerSide Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSEKMS) – this options uses CMKs managed in AWS KMS. There are additional benefits such as auditing and permissions associated with the CMKs but also additional charges• ServerSide Encryption with CustomerProvided Keys (SSEC) – you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects. The most suitable option for the requirements in this scenario is to use ServerSide Encryption with Amazon S3Managed Keys (SSES3) as the company do not want to manage CMKs and require a simple solution.CORRECT: "ServerSide Encryption with Amazon S3Managed Keys (SSES3)" is the correct answer.INCORRECT: "ServerSide Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSEKMS) using customer managed CMKs" is incorrect as the company do not want to manage CMKs and they need the most costeffective option and this does add additional costs.INCORRECT: "ServerSide Encryption with CustomerProvided Keys (SSEC)" is incorrect as with this option the customer must manage the keys or use keys managed in AWS KMS (which adds cost and complexity).INCORRECT: "Clientside encryption with Amazon S3 managed keys" is incorrect as you cannot use Amazon S3 managed keys for clientside encryption and the encryption does not need to take place clientside for this solution.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/servsideencryption.html50:T465,The optional Mappings section matches a key to a corresponding set of named values. For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. You use the Fn::FindInMap intrinsic function to retrieve values in a map.CORRECT: "Mappings" is the correct answer.INCORRECT: "Metadata" is incorrect. You can use the optional Metadata section to include arbitrary JSON or YAML objects that provide details about the template.INCORRECT: "Parameters" is incorrect. Parameters enable you to input custom values to your template each time you create or update a stack.INCORRECT: "Conditions" is incorrect. The optional Conditions section contains statements that define the circumstances under which entities are created or configured. For example, you can create a condition and then associate it with a resource or output so that AWS CloudFormation only creates the resource or output if the condition is true.References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/templateanatomy.html51:T482,The Condition element (or Condition block) lets you specify conditions for when a policy is in effect. The Condition element is optional. In the Condition element, you build expressions in which you use condition operators (equal, less than, etc.) to match the condition keys and values in the policy against keys and values in the request context.For example, in this scenario the following condition statement could be used:"Condition": {"DateLessThanEquals" : {"aws:CurrentTime" : "20200418T12:00:00Z"}}CORRECT: "Condition" is the correct answer.INCORRECT: "NotResource" is incorrect. NotResource is an advanced policy element that explicitly matches every resource except those specified.INCORRECT: "Action" is incorrect. The Action element describes the specific action or actions that will be allowed or denied.INCORRECT: "Version" is incorrect. The Version policy element is used within a policy and defines the version of the policy language.References:https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition.html https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition_operators.html52:T53b,Amazon CloudWatch Events help you to respond to state changes in your AWS resources. When your resources change state, they automatically send events into an event stream. You can create rules that match selected events in the stream and route them to your AWS Lambda function to take action.AWS CodePipeline can be configured as an event source in CloudWatch Events and can then send notifications using as service such as Amazon SNS.Therefore, the best answer is to create an Amazon CloudWatch Events rule that uses CodePipeline as an event source.CORRECT: "Create an Amazon CloudWatch Events rule that uses CodePipeline as an event source" is the correct answer.INCORRECT: "Create a trigger that invokes the Lambda function from the Lambda console by selecting CodePipeline as the event source" is incorrect as CodePipeline cannot be configured as a trigger for Lambda.INCORRECT: "Create an event trigger and specify the Lambda function from the CodePipeline console" is incorrect as CodePipeline cannot be configured as a trigger for Lambda.INCORRECT: "Create an Amazon CloudWatch alarm that monitors status changes in CodePipeline and triggers the Lambda function" is incorrect as CloudWatch Events is used for monitoring state changes.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html53:T845,Amazon RDS uses the MariaDB, MySQL, Oracle, and PostgreSQL DB engines' builtin replication functionality to create a special type of DB instance called a Read Replica from a source DB instance. Updates made to the source DB instance are asynchronously copied to the Read Replica.You can reduce the load on your source DB instance by routing read queries from your applications to the Read Replica. Using Read Replicas, you can elastically scale out beyond the capacity constraints of a single DB instance for readheavy database workloads.In the image below a primary Amazon RDS database server allows reads and writes while a Read Replica can be used for running readonly workloads such as BI/reporting. This reduces the load on the primary database.It is necessary to add logic to your code to direct read traffic to the Read Replica and write traffic to the primary database.Therefore, in this scenario the Development team will need to “Add a connection string to use an Amazon RDS read replica for read queries”.CORRECT: "Add a connection string to use an Amazon RDS read replica for read queries" is the correct answer.INCORRECT: "Add database retries to the code and vertically scale the Amazon RDS database" is incorrect as this is not a good way to scale reads as you will likely hit a ceiling at some point in terms of cost or instance type. Scaling reads can be better implemented with horizontal scaling using a Read Replica.INCORRECT: "Use Amazon RDS with a multiAZ deployment" is incorrect as this creates a standby copy of the database in another AZ that can be failed over to in a failure scenario. This is used for DR not (at least not primarily) used for scaling performance. It is possible for certain RDS engines to use a multiAZ standby as a read replica however the requirements in thissolution do not warrant this configuration.INCORRECT: "Add a connection string to use a read replica on an Amazon EC2 instance" is incorrect as Read Replicas are something you create on Amazon RDS, not on an EC2 instance.References: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html54:Tad9,Correct option:Set up an Amazon DynamoDB table with a primary key that consists of the name as the partition key and the version number as the sort key. Create a global secondary index that has the category as the partition key and the name as the sort keyWhen you create a DynamoDB table, in addition to the table name, you must specify the primary key of the table. The primary key uniquely identifies each item in the table, so that no two items can have the same key. You can create one or more secondary indexes on a table. A secondary index lets you query the data in the table using an alternate key, in addition to queries against the primary key. DynamoDB doesn't require that you use indexes, but they give your applications more flexibility when querying your data.via https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html You should note that a global secondary index (GSI) contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table. The Global secondary indexes allow you to perform queries on attributes that are not part of the table's primary key.via https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html Incorrect options:Set up an Amazon DynamoDB table with a primary key that consists of the category as the partition key and the version number as the sort key. Create a global secondary index that has the name as the partition key The DynamoDB table for this option has the primary key and GSI that do not solve for the condition "For a given name and version number, get all details about the game that has that name and version number". This option does not allow for efficient querying of a specific game by its name and version number as you need multiple queries which would be less efficient than the single query allowed by the correct option.Set up an Amazon RDS MySQL instance having a games table that contains columns for name, version number, and category. Configure the name column as the primary key This option is not the right fit as it does not allow you to efficiently query on the version number and category columns.Permanently store the name, version number, and category information about the games in an Amazon Elasticache for Memcached instance You cannot use Elasticache for Memcached to permanently store values meant to be persisted in a database (relational or NoSQL). Elasticache is a caching layer. So this option is incorrect.References:https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html https://aws.amazon.com/premiumsupport/knowledgecenter/primarykeydynamodbtable/55:T5b1,There are various ways to manage user sessions including storing those sessions locally to the node responding to the HTTP request or designating a layer in your architecture which can store those sessions in a scalable and robust manner. Common approaches used include utilizing Sticky sessions or using a Distributed Cache for your session management.In this scenario, a distributed cache is suitable for storing session state data. ElastiCache can perform this role and with the Redis engine replication is also supported. Therefore, the solution is faulttolerant and natively highly scalable.CORRECT: "Store the session state in Amazon ElastiCache" is the correct answer.INCORRECT: "Store the session state in Amazon CloudFront" is incorrect as CloudFront is not suitable for storing session state data, it is used for caching content for better global performance.INCORRECT: "Store the session state in Amazon S3" is incorrect as though you can store session data in Amazon S3 and replicate the data to another bucket, this would result in a service interruption if the S3 bucket was not accessible.INCORRECT: "Enable session stickiness using elastic load balancers" is incorrect as this feature directs sessions from a specific client to a specific EC2 instances. Therefore, if the instance fails the user must be redirected to another EC2 instance and the session state data would be lost.References: https://aws.amazon.com/caching/sessionmanagement/56:T4f7,AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, and Immutable) and options that let you configure batch size and health check behavior during deployments.For this scenario we need to ensure that no downtime occurs if the update fails and there is a quick way to rollback. All policies except for Immutable and Blue/Green require manual redeployment of the previous version of the code which will take time and result in downtime. The blue/green option is not actually an Elastic Beanstalk policy but it is a method you can use, however it is not offered as an answer choiceTherefore, the best deployment policy to use for this scenario is the Immutable deployment policy.CORRECT: "Immutable" is the correct answer.INCORRECT: "All at once" is incorrect as it causes complete downtime and manual redeployment in the case of failure.INCORRECT: "Rolling" is incorrect because it requires manual redeployment in the case of failure.INCORRECT: "Rolling with Additional Batch" is incorrect because it requires manual redeployment in the case of failure.References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html57:T425,An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. This is a secure way to authorize and EC2 instance to access AWS services.Instance profiles are created automatically if you use the console to add a role to an instance. You can also create instance profiles using the AWS CLI or API and assign roles to them.CORRECT: "Use EC2 instance profiles" is the correct answer.INCORRECT: "Use AWS KMS to store and retrieve credentials" is incorrect as KMS is used for encrypting data, not storing credentials.INCORRECT: "Use AWS root user to make requests to the application " is incorrect as this is not a secure way to access services as the root user has full privileges to the AWS account.INCORRECT: "Store and retrieve credentials from AWS CodeCommit" is incorrect as this is not a suitable solution for storing this data as CodeCommit is used for storing source code.References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switchroleec2_instanceprofiles.html58:Tcec,Applications must sign their API requests with AWS credentials. Therefore, if you are an application developer, you need a strategy for managing credentials for your applications that run on EC2 instances. For example, you can securely distribute your AWS credentials to the instances, enabling the applications on those instances to use your credentials to sign requests, while protecting your credentials from other users.However, it's challenging to securely distribute credentials to each instance, especially those that AWS creates on your behalf, such as Spot Instances or instances in Auto Scaling groups. You must also be able to update the credentials on each instance when you rotate your AWS credentials.IAM roles are designed so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use.Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles as follows:1. Create an IAM role.2. Define which accounts or AWS services can assume the role.3. Define which API actions and resources the application can use after assuming the role.4. Specify the role when you launch your instance or attach the role to an existing instance.5. Have the application retrieve a set of temporary credentials and use them.For example, you can use IAM roles to grant permissions to applications running on your instances that need to use a bucket in Amazon S3. You can specify permissions for IAM roles by creating a policy in JSON format. These are similar to the policies that you create for IAM users. If you change a role, the change is propagated to all instances.Therefore, the best solution is to create an AWS IAM Role with the necessary privileges (through an IAM policy) and attach the role to the instance’s instance profile.CORRECT: "Create an AWS IAM Role, attach a policy with the necessary privileges and attach the role to the instance’s instanceprofile" is the correct answer.INCORRECT: "Run the “aws configure” AWS CLI command and specify the access key ID and secret access key" is incorrect as this in insecure as the access key ID and secret access key are stored in plaintext on the instance’s local disk.INCORRECT: "Store a users’ console login credentials in the application code so the application can call AWS STS and gain temporary security credentials" is incorrect. This is a nonsense solution that would not work for multiple reasons. Firstly, the user console login credentials and not used for API access; secondly the STS service will not accept user login credentials andreturn temporary access credentials.INCORRECT: "Store the access key ID and secret access key as encrypted AWS Lambda environment variables and invoke Lambda for each API call" is incorrect. You can encrypt Lambda variables with KMS keys; however, this is not an ideal solution as you will still need to decrypt the keys through the Lambda code and then pass them to the EC2 instance. There could be security risks in this process. This is generally a poor use case for Lambda and IAM Roles are a far superior way of providing the necessary access.References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iamrolesforamazonec2.html59:T7ee,With IAM roles for Amazon ECS tasks, you can specify an IAM role that can be used by the containers in a task. Applications must sign their AWS API requests with AWS credentials, and this feature provides a strategy for managing credentials for your applications to use, similar to the way that Amazon EC2 instance profiles provide credentials to EC2 instances.Instead of creating and distributing your AWS credentials to the containers or using the EC2 instance’s role, you can associate an IAM role with an ECS task definition or RunTask API operation. The applications in the task’s containers can then use the AWS SDK or CLI to make API requests to authorized AWS services.In this case each service requires access to different AWS services so following the principal of least privilege it is best to assign as a separate role to each task definition.CORRECT: "Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then configure each ECS task definition to reference the associated IAM role" is the correct answer.INCORRECT: "Create a new Identity and Access Management (IAM) instance profile containing the required permissions for the various ECS services, then associate that instance role with the underlying EC2 instances" is incorrect. It is a best practice to use IAM roles for tasks instead of assigning the roles to the container instances.INCORRECT: "Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then configure each ECS service to reference the associated IAM role" is incorrect as the reference should be made within the task definition.INCORRECT: "Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then, create an IAM group and configure the ECS cluster to reference that group" is incorrect as the reference should be made within the task definition.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskiamroles.html5a:Tb88,Correct option:Persist the unique identifier for each request in a DynamoDB table. Change the Lambda function to check the table for the identifier before processing the requestDynamoDB is a fully managed, serverless, keyvalue NoSQL database designed to run highperformance applications at any scale. DynamoDB offers builtin security, continuous backups, automated multiRegion replication, inmemory caching, and data import and export tools. Ondemand backup and restore allows you to create full backups of your DynamoDB. Pointintime recovery (PITR) helps protect your DynamoDB tables from accidental write or delete operations. PITR provides continuous backups of your DynamoDB table data, and you can restore that table to any point in time up to the second during the preceding 35 days.These features ensure that there is no data loss for the application, thereby meeting a key requirement for the given use case. The solution should also be able to address any duplicate requests without inconsistencies, so the Lambda function should be changed to inspect the table for the given identifier and process the request only if the identifier is unique.DynamoDB Overview: via https://aws.amazon.com/dynamodb/ DynamoDBIncorrect options:Persist the unique identifier for each request in an ElastiCache for Memcached cache. Change the Lambda function to check the cache for the identifier before processing the request Memcached is designed for simplicity and it does not offer any snapshot or replication features. This can lead to data loss for applications. Therefore, this option is not the right fit for the given use case.via https://aws.amazon.com/elasticache/redisvsmemcached/ Persist the unique identifier for each request in an RDS MySQL table. Change the Lambda function to check the table for the identifier before processing the request DynamoDB is a better fit than RDS MySQL to handle massive traffic spikes for write requests. DynamoDB is a keyvalue and document database that supports tables of virtually any size with horizontal scaling. DynamoDB scales to more than 10 trillion requests per day and with tables that have more than ten million read and write requests per second and petabytes of data storage. DynamoDB can be used to build applications that need consistent singledigit millisecond performance. MySQL RDS can be scaled vertically, however, it cannot match the performance benefits offered by DynamoDB for the given use case.Persist the unique identifier for each request in a DynamoDB table. Change the Lambda function to send a client error response when the function receives a duplicate request The solution should be able to address any duplicates without any inconsistencies. If Lambda sends a client error response upon receiving a duplicate request, it represents an inconsistent response. So this option is incorrect.References:https://aws.amazon.com/dynamodb/ https://aws.amazon.com/elasticache/redisvsmemcached/5b:T531,A segment can break down the data about the work done into subsegments. Subsegments provide more granular timing information and details about downstream calls that your application made to fulfill the original request.A subsegment can contain additional details about a call to an AWS service, an external HTTP API, or an SQL database. You can even define arbitrary subsegments to instrument specific functions or lines of code in your application.CORRECT: "Subsegments" is the correct answer.INCORRECT: "Annotations" is incorrect. Annotations are simple keyvalue pairs that are indexed for use with filter expressions.Use annotations to record data that you want to use to group traces in the console, or when calling the the GetTraceSummaries API.INCORRECT: "Metadata" is incorrect. Metadata are keyvalue pairs with values of any type, including objects and lists, but that are not indexed. Use metadata to record data you want to store in the trace but don't need to use for searching traces.INCORRECT: "Filter expressions" is incorrect. You can use filter expressions to find traces related to specific paths or users.References:https://docs.aws.amazon.com/xray/latest/devguide/xraysdkdotnetsubsegments.html https://github.com/awsdocs/awsxraydeveloperguide/blob/master/docsource/xrayconcepts.md#xrayconceptsfilterexpressions5c:T5ee,AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, and Immutable) and options that let you configure batch size and health check behavior during deployments.For this scenario we need to ensure we do not reduce the capacity of the application but we also need to minimize cost. In the table below you can see the different deployment policies available and how they impact capacity and cost:The Rolling with additional batch deployment policy does require extra cost but the extra cost is the size of a batch of instances, therefore you can reduce cost by reducing the batch size. The Immutable deployment policy requires a total deployment of new instances – i.e. if you have 4 instances this will double to 8 instances.Therefore, the best deployment policy to use for this scenario is the Rolling with additional batch.CORRECT: "Rolling with additional batch" is the correct answer.INCORRECT: "Immutable" is incorrect as this would require a higher cost as you need a total deployment of new instances.INCORRECT: "Rolling" is incorrect as this will result in a reduction in capacity which will affect performance.INCORRECT: "All at once" is incorrect as this results in a total reduction in capacity, i.e. your entire application is taken down at once while the application update is installed.References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.rollingversiondeploy.html5d:T5ce,You can grant your IAM users’ permission to switch to roles within your AWS account or to roles defined in other AWS accounts that you own. This is known as crossaccount access.In the image below a user in the Development account needs to access an S3 bucket in the Production account:The user is able to assume the role in the Production account and access the S3 bucket. This is more efficient than providing the user with multiple accounts. In this scenario the user requests to switch to the role through either the console or the API/CLI.CORRECT: "Create a role with the required permissions in the Production and Testing accounts and have the Developer assume that role" is the correct answer.INCORRECT: "Create a separate IAM user in each account and have the Developer login separately to each account" is incorrect as this is not the most efficient method of providing access. Crossaccount access is preferred .INCORRECT: "Create an IAM group in the Production and Testing accounts and add the Developer’s user from the Development account to the groups" is incorrect as you cannot add an IAM user from another AWS account to a group.INCORRECT: "Create an IAM permissions policy in the Production and Testing accounts and reference the IAM user in the Development account" is incorrect as you cannot reference an IAM user from another AWS account in a permissions policy.References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_commonscenarios_awsaccounts.html5e:T4a2,When you use the AWS Elastic Beanstalk console to deploy a new application or an application version, you'll need to upload a source bundle. Your source bundle must meet the following requirements:• Consist of a single ZIP file or WAR file (you can include multiple WAR files inside your ZIP file)• Not exceed 512 MB• Not include a parent folder or toplevel directory (subdirectories are fine)If you want to deploy a worker application that processes periodic background tasks, your application source bundle must also include a cron.yaml file, but in other cases it is not required.CORRECT: "Must not include a parent folder or toplevel directory" is a correct answer.CORRECT: "Must not exceed 512 MB" is also a correct answer.INCORRECT: "Must include the cron.yaml file" is incorrect. As mentioned above, this is not required in all cases.INCORRECT: "Must include a parent folder or toplevel directory" is incorrect. A parent folder or toplevel directory must NOT be included.INCORRECT: "Must consist of one or more ZIP files" is incorrect. You bundle into a single ZIP or WAR file.References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applicationssourcebundle.html5f:Ta3f,Correct option:AWS Elastic Beanstalk offers several deployment policies and settings. Choosing the right deployment policy for your application is a tradeoff based on a few considerations and depends on your business needs.Deploy using 'Rolling with additional batch' deployment policy With this method, Elastic Beanstalk launches an extra batch of instances, then performs a rolling deployment. Launching the extra batch takes time, and ensures that the same bandwidth is retained throughout the deployment. This policy also avoids any reduced availability, although at a cost of an even longer deployment time compared to the Rolling method. Finally, this option is suitable if you must maintain the same bandwidth throughout the deployment.Overview of Elastic Beanstalk Deployment Policies: via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html Incorrect options:Deploy using 'Immutable' deployment policy A slower deployment method, that ensures your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails. With this method, Elastic Beanstalk performs an immutable update to deploy your application. In an immutable update, a second Auto Scaling group is launched in your environment and the new version serves traffic alongside the old version until the new instances pass health checks.Deploy using 'All at once' deployment policy This is the quickest deployment method. Suitable if you can accept a short loss of service, and if quick deployments are important to you. With this method, Elastic Beanstalk deploys the new application version to each instance. Then, the web proxy or application server might need to restart. As a result, your application might be unavailable to users (or have low availability) for a short time.Deploy using 'Rolling' deployment policy With this method, your application is deployed to your environment one batch of instances at a time. Most bandwidth is retained throughout the deployment. Avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. Suitable if you can't accept any period of completely lost service. The use case states that the application has high traffic and high availability requirements, so full capacity must be maintained during deployments, hence rolling with additional batch deployment is a better fit than the rolling deployment.Reference:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html60:T53a,Amazon Cognito lets you add user signup, signin, and access control to your web and mobile apps quickly and easily. Cognito supports multiple devices types including mobile applications, desktops and tablets.You can enable device remembering for Amazon Cognito user pools. A remembered device can serve in place of the security code delivered via SMS as a second factor of authentication. This suppresses the second authentication challenge from remembered devices and thus reduces the friction users experience with multifactor authentication (MFA).Therefore, Amazon Cognito is the best answer and will support all of the requirements in the scenario.CORRECT: "Amazon Cognito" is the correct answer.INCORRECT: "AWS Directory Service" is incorrect as this service enables directoryaware workloads and AWS resources to use managed Active Directory in the AWS Cloud.INCORRECT: "AWS KMS" is incorrect as KMS is used to manage encryption keys; it does not enable authentication from mobile devices.INCORRECT: "Amazon IAM" is incorrect as IAM is not the best authentication solution for mobile users. It also does not support device remembering or any ability to suppress MFA when it is enabled.References:https://aws.amazon.com/blogs/mobile/trackingandrememberingdevicesusingamazoncognitoyouruserpools/ https://aws.amazon.com/cognito/details/61:T4cb,CodeDeploy provides two deployment type options – inplace and blue/green. Note that AWS Lambda and Amazon ECS deployments cannot use an inplace deployment type.The Blue/green deployment type on an Amazon ECS compute platform works like this:• Traffic is shifted from the task set with the original version of an application in an Amazon ECS service to a replacement task set in the same service.• You can set the traffic shifting to linear or canary through the deployment configuration.• The protocol and port of a specified load balancer listener is used to reroute production traffic.• During a deployment, a test listener can be used to serve traffic to the replacement task set while validation tests are run.CORRECT: "Blue/green" is the correct answer.INCORRECT: "Canary" is incorrect as this is a traffic shifting option, not a deployment type. Traffic is shifted in two increments.INCORRECT: "Linear" is incorrect as this is a traffic shifting option, not a deployment type. Traffic is shifted in two increments.INCORRECT: "Inplace" is incorrect as AWS Lambda and Amazon ECS deployments cannot use an inplace deployment type.References: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html62:T505,You can configure the AWS CLI on Linux, MacOS, and Windows. Computers can be located anywhere as long as they can connect to the AWS API.For this scenario, the best solution is to run aws configure and use the IAM user’s access key ID and secret access key. This will mean that commands run using the AWS CLI will use the user’s IAM permissions as required.CORRECT: "Run the aws configure command and provide the Developer’s IAM access key ID and secret access key" is the correct answer.INCORRECT: "Create an IAM Role with the required permissions and attach it to the local server’s instance profile" is incorrect as this is not an Amazon EC2 instance so you cannot attach an IAM role.INCORRECT: "Put the Developer’s IAM user account in an IAM group that has the necessary permissions" is incorrect as this does not assist with configuring the AWS CLI.INCORRECT: "Save the Developer’s IAM login credentials as environment variables and reference them when executing AWS CLI commands" is incorrect as the IAM login credentials cannot be used with the AWS CLI. You need to use an access key ID and secret access key with the AWS CLI and these are configured for use by running aws configure.References: https://docs.aws.amazon.com/cli/latest/userguide/clichapconfigure.html63:T472,Amazon Simple Notification Service (Amazon SNS) is a web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. In Amazon SNS, there are two types of clients—publishers and subscribers—also referred to as producers and consumers.Publishers communicate asynchronously with subscribers by producing and sending a message to a topic, which is a logical access point and communication channel.Subscribers (that is, web servers, email addresses, Amazon SQS queues, AWS Lambda functions) consume or receive the message or notification over one of the supported protocols (that is, Amazon SQS, HTTP/S, email, SMS, Lambda) when they are subscribed to the topic.CORRECT: "Amazon SNS" is the correct answer.INCORRECT: "Amazon SES" is incorrect as this service only sends email, not SMS text messages.INCORRECT: "Amazon SQS" is incorrect as this is a hosted message queue for decoupling application components.INCORRECT: "Amazon SWF" is incorrect as the Simple Workflow Service is used for orchestrating multistep workflows.References: https://docs.aws.amazon.com/sns/latest/dg/welcome.html64:T7c0,A usage plan specifies who can access one or more deployed API stages and methods—and also how much and how fast they can access them. The plan uses API keys to identify API clients and meters access to the associated API stages for each key. It also lets you configure throttling limits and quota limits that are enforced on individual client API keys. API keys are alphanumeric string values that you distribute to application developer customers to grant access to your API. You can use API keys together with usage plans or Lambda authorizers to control access to your APIs. API Gateway can generate API keys on your behalf, or you can import them from a CSV file. You can generate an API key in API Gateway, or import it into APIGateway from an external source.To associate the newly created key with a usage plan the CreatUsagePlanKey API can be called. This creates a usage plan key for adding an existing API key to a usage plan.CORRECT: "The createUsagePlanKey method must be called to associate the newly created API key with the correct usage plan" is the correct answer.INCORRECT: "The createDeployment method must be called so the API can be redeployed to include the newly created API key" is incorrect as you do not need to redeploy an API to a stage in order to associate an API key.INCORRECT: "The updateAuthorizer method must be called to update the API’s authorizer to include the newly created API key" is incorrect as this updates and authorizer resource, not an API key.INCORRECT: "The importApiKeys method must be called to import all newly created API keys into the current stage of the API" is incorrect as this imports API keys to API Gateway from an external source such as a CSV file which is not relevant to this scenario.References:https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayapiusageplans.html http://docs.amazonaws.cn/en_us/sdkfornet/v3/apidocs/items/APIGateway/MAPIGatewayCreateUsagePlanKeyCreateUsagePlanKeyRequest.html65:T525,Correct option:Use TTLTime To Live (TTL) for DynamoDB allows you to define when items in a table expire so that they can be automatically deleted from the database. TTL is provided at no extra cost as a way to reduce storage usage and reduce the cost of storing irrelevant data without using provisioned throughput. With TTL enabled on a table, you can set a timestamp for deletion on a peritem basis, allowing you to limit storage usage to only those records that are relevant.Incorrect options:Use DynamoDB Streams These help you get a changelog of your DynamoDB table but won't help you delete expired data. Note that data expired using a TTL will appear as an event in your DynamoDB streams.Use DAX Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, inmemory cache for DynamoDB that delivers up to a 10x performance improvement: from milliseconds to microseconds: even at millions of requests per second. This is a caching technology for your DynamoDB tables.Use a Lambda function This could work but would require setting up indexes, queries, or scans to work, as well as trigger them often enough using a CloudWatch Events. This bandaid solution would never be as good as using the TTL feature in DynamoDB.Reference: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html66:T983,Occasionally ,you may receive the 400 ThrottlingException error for PutMetricData API calls in Amazon CloudWatch with a detailed response similar the following:CloudWatch requests are throttled for each Amazon Web Services (AWS) account on a perRegion basis to help service performance. For current PutMetricData API request limits, see CloudWatch Limits.All calls to the PutMetricData API in an AWS Region count towards the maximum allowed request rate. This number includes calls from any custom or thirdparty application, such as calls from the CloudWatch Agent, the AWS Command Line Interface (AWS CLI), or the AWS Management Console.Resolutions: It's a best practice to use the following methods to reduce your call rate and avoid API throttling:• Distribute your API calls evenly over time rather than making several API calls in a short time span. If you require data to be available with a oneminute resolution, you have an entire minute to emit that metric. Use jitter (randomized delay) to send data points at various times.• Combine as many metrics as possible into a single API call. For example, a single PutMetricData call can include 20 metrics and 150 data points. You can also use preaggregated data sets, such as StatisticSet, to publish aggregated data points, thus reducing the number of PutMetricData calls per second.• Retry your call with exponential backoff and jitter.Following attempting the above resolutions AWS suggest the following: “If you still require a higher limit, you can request a limit increase. Increasing the rate limit can have a high financial impact on your AWS bill.”Therefore, the first thing the Developer should do, from the list of options presented, is to retry the call with exponential backoff.CORRECT: "Retry the call with exponential backoff" is the correct answer.INCORRECT: "Contact AWS Support for a limit increase" is incorrect. As mentioned above, there are other resolutions the Developer should attempt before contacting support to raise the limit.INCORRECT: "Use the AWS CLI to get the metrics" is incorrect as this will still make the same API calls.INCORRECT: "Analyze the applications and remove the API call" is incorrect as this is not a good resolution to the issue as this may mean that important monitoring and logging data is not recorded for the application.References: https://aws.amazon.com/premiumsupport/knowledgecenter/cloudwatch400errorthrottling/67:T59e,To relay trace data from your application to AWS XRay, you can run the XRay daemon on your Elastic Beanstalk environment's Amazon EC2 instances.Elastic Beanstalk platforms provide a configuration option that you can set to run the daemon automatically. You can enable the daemon in a configuration file in your source code or by choosing an option in the Elastic Beanstalk console. When you enable the configuration option, the daemon is installed on the instance and runs as a service.The above code will ensure the XRay daemon starts and the Developer can enable tracing for the application as required.CORRECT: "Add a .ebextensions/xraydaemon.config file to the source code to enable the XRay daemon" is the correct answer.INCORRECT: "Add a xraydaemon.config file to the root of the source code to enable the XRay deamon" is incorrect as all.config files must be stored in the .ebextensions folder in the source code.INCORRECT: "Enable active tracing in the Elastic Beanstalk console" is incorrect as you cannot enable active tracing through the console for Elastic Beanstalk. This is available for AWS Lambda and API Gateway.INCORRECT: "Enable XRay tracing using an AWS Lambda function" is incorrect as there is no need to add a Lambda function to the application to add tracing support. The developer can enable tracing by enabling the XRay daemon.References: https://docs.aws.amazon.com/xray/latest/devguide/xraydaemonbeanstalk.html68:T56d,The content in the 'hooks' section of the AppSpec file varies, depending on the compute platform for your deployment. The 'hooks' section for an EC2/OnPremises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts.The 'hooks' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment.Therefore, in this scenario a valid structure for the order of hooks that should be specified in the appspec.yml file is:BeforeAllowTraffic > AfterAllowTrafficCORRECT: "BeforeAllowTraffic > AfterAllowTraffic" is the correct answer.INCORRECT: "BeforeInstall > AfterInstall > ApplicationStart > ValidateService" is incorrect as this would be valid for Amazon EC2.INCORRECT: "BeforeInstall > AfterInstall > AfterAllowTestTraffic > BeforeAllowTraffic > AfterAllowTraffic" is incorrect as this would be valid for Amazon ECS.INCORRECT: "BeforeBlockTraffic > AfterBlockTraffic > BeforeAllowTraffic > AfterAllowTraffic" is incorrect as this is a partial listing of hooks for Amazon EC2 but is incomplete.References: https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfilestructurehooks.html69:T912,Correct option:You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. Resourcebased policies are JSON policy documents that you attach to a resource such as an Amazon S3 bucket. These policies grant the specified principal permission to perform specific actions on that resource and define under what conditions this applies.Trust policy Trust policies define which principal entities (accounts, users, roles, and federated users) can assume the role. An IAM role is both an identity and a resource that supports resourcebased policies. For this reason, you must attach both a trust policy and an identitybased policy to an IAM role. The IAM service supports only one type of resourcebased policy called a role trust policy, which is attached to an IAM role.Incorrect options:AWS Organizations Service Control Policies (SCP) If you enable all features of AWS organization, then you can apply service control policies (SCPs) to any or all of your accounts. SCPs are JSON policies that specify the maximum permissions for an organization or organizational unit (OU). The SCP limits permissions for entities in member accounts, including each AWS account root user. An explicit deny in any of these policies overrides the allow.Access control list (ACL) Access control lists (ACLs) are service policies that allow you to control which principals in another account can access a resource. ACLs cannot be used to control access for a principal within the same account. Amazon S3, AWS WAF, and Amazon VPC are examples of services that support ACLs.Permissions boundary AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identitybased policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identitybased policies and its permissions boundaries.References:https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_resourcebased https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html6a:T609,Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.In this scenario, the average execution time is 100 seconds and 50 requests are received per second. This means the concurrency requirement is 100 x 50 = 5,000. As 5,000 is well above the default allowed concurrency of 1,000 executions a second. Therefore, the Developer will need to contact AWS Support to increase the concurrent execution limits.CORRECT: “Contact AWS Support to increase the concurrent execution limits” is the correct answer.INCORRECT: “Implement deadletter queues to capture invocation errors” is incorrect as this is a method of capturing information for later analysis. The Developer first needs to ensure the Lambda can scale to its expected load.INCORRECT: “Add an event source from Amazon API Gateway to the Lambda function ” is incorrect as this is not necessary and will not ensure Lambda can scale to handle the load.INCORRECT: “Implement error handling within the application code” is incorrect as there’s little point relying on error handling when you know the function will not be able to scale to expected load.References: https://docs.aws.amazon.com/lambda/latest/dg/configurationconcurrency.html6b:T4a6,Amazon S3 static websites use the HTTP protocol only and you cannot enable HTTPS. To enable HTTPS connections to your S3 static website, use an Amazon CloudFront distribution that is configured with an SSL/TLS certificate. This will ensure that connections between clients and the CloudFront distribution are encrypted intransit as per the requirements.CORRECT: "Create an Amazon CloudFront distribution. Set the S3 bucket as an origin" is a correct answer.CORRECT: "Configure an Amazon CloudFront distribution with an SSL/TLS certificate" is also a correct answer.INCORRECT: "Create an AWS WAF WebACL with a secure listener" is incorrect. You cannot configure a secure listener on a WebACL.INCORRECT: "Configure an Amazon CloudFront distribution with an AWS WAF WebACL" is incorrect. This will not enable encrypted connections.INCORRECT: "Configure the S3 bucket with an SSL/TLS certificate" is incorrect. You cannot manually add SSL/TLS certificates to Amazon S3, and it is not possible to directly configure an S3 bucket that is configured as a static website to accept encrypted connections.References: https://aws.amazon.com/premiumsupport/knowledgecenter/cloudfrontservestaticwebsite/6c:T5f6,Amazon Cognito leverages IAM roles to generate temporary credentials for your application's users. Access to permissions is controlled by a role's trust relationships.In this example the Developer must limit access to specific identities in the SAML directory. The Developer can create a trust policy with an IAM condition key that limits access to a specific set of app users by checking the value of cognitoidentity.amazonaws.com:sub:CORRECT: "Use an Amazon Cognito identity pool, federate with the SAML provider, and use a trust policy with an IAM condition key to limit employee access" is the correct answer.INCORRECT: "Use Amazon Cognito user pools, federate with the SAML provider, and use user pool groups with an IAM policy" is incorrect. A user pool can be used to authenticate but the identity pool is used to provide authorized access to AWS services.INCORRECT: "Create the application within an Amazon VPC and use a VPC endpoint with a trust policy to grant access to the employees" is incorrect. You cannot provide access to an onpremises SAML directory using a VPC endpoint.INCORRECT: "Create a unique IAM role for each employee and have each employee assume the role to access the application so they can access their personal data only" is incorrect. This is not an integration into the SAML directory and would be very difficult to manage.References:https://docs.aws.amazon.com/cognito/latest/Developerguide/roletrustandpermissions.html https://docs.aws.amazon.com/cognito/latest/Developerguide/iamroles.html6d:T421,An API Gateway REST API is a collection of HTTP resources and methods that are integrated with backend HTTP endpoints, Lambda functions, or other AWS services. You can deploy this collection in one or more stages. Typically, API resources are organized in a resource tree according to the application logic. Each API resource can expose one or more API methods thathave unique HTTP verbs supported by API Gateway.As you can see from the image above, the Developer would need to create a resource which in this case would be /products.The Developer would then create a GET method within the resource.CORRECT: "Create a /products resource" is a correct answer.CORRECT: "Create a GET method" is a correct answer.INCORRECT: "Create a /products method" is incorrect as a resource should be created.INCORRECT: "Create a GET resource" is incorrect as a method should be created.INCORRECT: "Create a /GET method" is incorrect as a method is not preceded by a slash.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaybasicconcept.html6e:T6c9,Correct option:CodeDeployAWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, onpremises instances, or serverless Lambda functions. AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy.The blue/green deployment type uses the blue/green deployment model controlled by CodeDeploy. This deployment type enables you to verify a new deployment of service before sending production traffic to it.CodeDeploy offers lot of control over deployment steps. Please see this note for more details: via https://aws.amazon.com/aboutaws/whatsnew/2017/01/awscodedeployintroducesbluegreendeployments/ Incorrect options:CodeBuild AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. It cannot be used to deploy applications.Elastic Beanstalk AWS Elastic Beanstalk offers hooks but not as much control as CodeDeploy. Because AWS Elastic Beanstalk performs an inplace update when you update your application versions, your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly.CodePipeline CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change. CodePipeline by itself cannot deploy applications.Reference:https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/deploymentconfigurations.html6f:T42b,AWS CodeCommit is a fully managed source control service that hosts secure Gitbased repositories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem. AWS CodeCommit automatically encrypts your files in transit and at rest.AWS CodeCommit helps you collaborate on code with teammates via pull requests, branching, and merging. You can implement workflows that include code reviews and feedback by default, and control who can make changes to specific branches.CORRECT: "AWS CodeCommit" is the correct answer.INCORRECT: "AWS CodeBuild" is incorrect. AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packagesINCORRECT: "Amazon S3" is incorrect. Amazon S3 is an objectbased storage system and does not support the features required here.INCORRECT: "AWS Cloud9" is incorrect. AWS Cloud9 is a cloudbased integrated development environment (IDE) that lets you write, run, and debug your code with just a browser.References: https://aws.amazon.com/codecommit/70:T631,TransactWriteItems is a synchronous and idempotent write operation that groups up to 25 write actions in a single allornothing operation. These actions can target up to 25 distinct items in one or more DynamoDB tables within the same AWS account and in the same Region. The aggregate size of the items in the transaction cannot exceed 4 MB. The actions arecompleted atomically so that either all of them succeed or none of them succeeds.A TransactWriteItems operation differs from a BatchWriteItem operation in that all the actions it contains must be completed successfully, or no changes are made at all. With a BatchWriteItem operation, it is possible that only some of the actions in the batch succeed while the others do not.CORRECT: "Update the items in the table using the TransactWriteltems operation to group the changes" is the correct answer.INCORRECT: "Update the items in the table using the BatchWriteltem operation and configure idempotency at the table level" is incorrect. As explained above, the TransactWriteItems operation must be used.INCORRECT: "Enable DynamoDB streams and capture new images. Update the items in the table using the BatchWriteltem" is incorrect. DynamoDB streams will not assist with making idempotent write operations.INCORRECT: "Create an Amazon SQS FIFO queue and use the SendMessageBatch operation to group the changes" is incorrect. Amazon SQS should not be used as it does not assist and this solution is supposed to use a DynamoDB tableReferences: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_TransactWriteItems.html71:T5af,On a perfunction basis, you can configure Lambda to use an encryption key that you create and manage in AWS Key Management Service. These are referred to as customer managed customer master keys (CMKs) or customer managed keys. If you don't configure a customer managed key, Lambda uses an AWS managed CMK named aws/lambda, which Lambda creates in your account.The CMK can be used to generate a data encryption key that can be used for encrypting all data uploaded to Lambda or generated by Lambda.CORRECT: "Configure Lambda to use an AWS KMS customer managed customer master key (CMK). Use the CMK to generate a data key and encrypt all data prior to writing to /tmp storage" is the correct answer.INCORRECT: "Attach the Lambda function to a VPC and encrypt Amazon EBS volumes at rest using the AWS managed CMK. Mount the EBS volume to /tmp" is incorrect. You cannot attach an EBS volume to a Lambda function.INCORRECT: "Enable default encryption on an Amazon S3 bucket using an AWS KMS customer managed customer master key (CMK). Mount the S3 bucket to /tmp" is incorrect. You cannot mount an S3 bucket to a Lambda function.INCORRECT: "Enable secure connections over HTTPS for the AWS Lambda API endpoints using Transport Layer Security (TLS)" is incorrect. The Lambda API endpoints are always encrypted using TLS and this is encryption intransit not encryption atrest.References: https://docs.aws.amazon.com/lambda/latest/dg/securitydataprotection.html72:Taf0,Correct option:AWS Cloud Development Kit (CDK) The AWS Cloud Development Kit (AWS CDK) is an opensource software development framework to define your cloud application resources using familiar programming languages.Provisioning cloud applications can be a challenging process that requires you to perform manual actions, write custom scripts, maintain templates, or learn domainspecific languages. AWS CDK uses the familiarity and expressive power of programming languages such as JavaScript/TypeScript, Python, Java, and .NET for modeling your applications. It provides you with highlevel components called constructs that preconfigure cloud resources with proven defaults, so you can build cloud applications without needing to be an expert. AWS CDK provisions your resources in a safe, repeatable manner through AWS CloudFormation. It also enables you to compose and share your own custom constructs that incorporate your organization's requirements, helping you start new projects faster.Incorrect options:AWS CloudFormation When AWS CDK applications are run, they compile down to fully formed CloudFormation JSON/YAML templates that are then submitted to the CloudFormation service for provisioning. Because the AWS CDK leverages CloudFormation, you still enjoy all the benefits CloudFormation provides such as safe deployment, automatic rollback, and drift detection. But, CloudFormation by itself is not sufficient for the current use case.AWS Serverless Application Model (SAM) The AWS Serverless Application Repository is a managed repository for serverless applications. It enables teams, organizations, and individual developers to store and share reusable applications, and easily assemble and deploy serverless architectures in powerful new ways. Using the Serverless Application Repository, you don't need to clone, build, package, or publish source code to AWS before deploying it.AWS Serverless Application Model and AWS CDK both abstract AWS infrastructure as code making it easier for you to define your cloud infrastructure. If you prefer defining your serverless infrastructure in concise declarative templates, SAM is the better fit. If you want to define your AWS infrastructure in a familiar programming language, as is the requirement in the current use case, AWS CDK is the right fit.AWS CodeDeploy AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your onpremises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. CodeDeploy can be used with AWS CDK for deployments.Reference:https://aws.amazon.com/cdk/faqs/73:T4fb,You can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The request must contain the CacheControl: maxage=0 header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint.CORRECT: "Send requests with the CacheControl: maxage=0 header" is the correct answer.INCORRECT: "Modify the TTL on the cache to a lower number" is incorrect as that would expire all entries after the TTL expires.The question states that for some requests (not all requests) that latest data must be received, in this case the best way to ensure this is to use invalidate the cache entries using the header in the correct answer.INCORRECT: "The cache must be disables" is incorrect as you can achieve this requirement using invalidation as detailed in the explanation above.INCORRECT: "Send requests with the CacheDelete: maxage=0 header " is incorrect as that is the wrong header to use. The Developer should use the CacheControl: maxage=0 header instead.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaycaching.html74:T743,Correct option:Use API Gateway Usage PlansAmazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud.How API Gateway Works: via https://aws.amazon.com/apigateway/ A usage plan specifies who can access one or more deployed API stages and methods—and also how much and how fast they can access them. The plan uses API keys to identify API clients and meters access to the associated API stages for each key.You can configure usage plans and API keys to allow customers to access selected APIs at agreedupon request rates and quotas that meet their business requirements and budget constraints.Overview of API Gateway Usage Plans and API keys: via https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayapiusageplans.html Incorrect options:Use AWS Billing Usage Plans AWS Billing and Cost Management is the service that you use to pay your AWS bill, monitor your usage, and analyze and control your costs. There is no such thing as AWS Billing Usage Plans. You cannot use AWS Billing to set up public APIs for the application.Use CloudFront Usage Plans Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developerfriendly environment. There is no such thing as CloudFront Usage Plans. You cannot use CloudFront to set up public APIs for the application.Use AWS Lambda Custom Authorizers Lambda is a separate service than Gateway API, therefore, it cannot be used to determine the API usage limits.Reference:https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayapiusageplans.html75:T566,Clientside encryption is the act of encrypting data before sending it to Amazon S3. You have the following options:1. Use a customer master key (CMK) stored in AWS Key Management Service (AWS KMS).2. Use a master key you store within your application.Additionally, using HTTPS/SSL to encrypt the data as it is transmitted over the Internet adds an additional layer of protection.CORRECT: "Use clientside encryption with a KMS managed CMK and SSL" is the correct answer.INCORRECT: "Use serverside encryption with client provided keys" is incorrect as this will encrypt the data as it is written to the S3 bucket. The questions states that you need to encrypt date before it is sent to S3.INCORRECT: "Use clientside encryption and a hardware VPN to a VPC and an S3 endpoint" is incorrect. You can configure a hardware VPN to a VPC and configure an S3 endpoint to access S3 privately (rather than across the Internet). However, this is certainly not the simplest solution. Encrypting the data using clientside encryption and then using HTTPS/SSL to transmit the data is operationally easier to configure and manage and provides ample security.INCORRECT: "Use serverside encryption with S3 managed keys and SSL" is incorrect as this does not encrypt the data before it is sent to S3 which is a requirement.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html76:T871,Correct option:RollingWith Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.How Elastic BeanStalk Works: via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html The rolling deployment policy deploys the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch. The cost remains the same as the number of EC2 instances does not increase. This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time.Overview of Elastic Beanstalk Deployment Policies: via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html Incorrect options:Immutable The 'Immutable' deployment policy ensures that your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails.All at once This policy deploys the new version to all instances simultaneously. Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time.Rolling with additional batches This policy deploys the new version in batches, but first launches a new batch of instances to ensure full capacity during the deployment process. This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. These increase the costs as you're adding extra instances during the deployment.Reference:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html77:T6cd,AWS CodeCommit is a managed version control service that hosts private Git repositories in the AWS cloud. To use CodeCommit, you configure your Git client to communicate with CodeCommit repositories. As part of this configuration, you provide IAM credentials that CodeCommit can use to authenticate you. IAM supports CodeCommit with three types ofcredentials:• Git credentials, an IAM generated user name and password pair you can use to communicate with CodeCommit repositories over HTTPS.• SSH keys, a locally generated publicprivate key pair that you can associate with your IAM user to communicate with CodeCommit repositories over SSH.• AWS access keys, which you can use with the credential helper included with the AWS CLI to communicate with CodeCommit repositories over HTTPS.In this scenario the Development team need to connect to CodeCommit using HTTPS so they need either AWS access keys to use the AWS CLI or Git credentials generated by IAM. Access keys are not offered as an answer choice so the best answer is that they need to create a set of Git credentials generated with IAMCORRECT: "A set of Git credentials generated with IAM" is the correct answer.INCORRECT: "A GitHub secure authentication token" is incorrect as they need to authenticate to AWS CodeCommit, not GitHub(they have already accessed and cloned the repository).INCORRECT: "A public and private SSH key file" is incorrect as these are used to communicate with CodeCommit repositories using SSH, not HTTPS.INCORRECT: "An Amazon EC2 IAM role with CodeCommit permissions" is incorrect as you need the Git credentials generated through IAM to connect to CodeCommit.References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_sshkeys.html78:T69b,From the AWS documentation:“When an error occurs, your function may be invoked multiple times. Retry behavior varies by error type, client, event source, and invocation type. For example, if you invoke a function asynchronously and it returns an error, Lambda executes the function up to two more times. For more information, see Retry Behavior.For asynchronous invocation, Lambda adds events to a queue before sending them to your function. If your function does not have enough capacity to keep up with the queue, events may be lost. Occasionally, your function may receive the same event multiple times, even if no error occurs. To retain events that were not processed, configure your function with a deadletterqueue.”Therefore, the most likely explanation is that the function failed, and Lambda retried the invocation.CORRECT: "The Lambda function failed, and the Lambda service retried the invocation with a delay" is the correct answer.INCORRECT: "The S3 bucket name was specified incorrectly" is incorrect. If this was the case all attempts would fail but this is not the case.INCORRECT: "There was an S3 outage, which caused duplicate entries of the same log file" is incorrect. There cannot be duplicate log files in Amazon S3 as every object must be unique within a bucket. Therefore, if the same log file was uploaded twice it would just overwrite the previous version of the file. Also, if a separate request was made to Lambda it would have adifferent request ID.INCORRECT: "The application stopped intermittently and then resumed" is incorrect. The issue is duplicate entries of the same request ID.References: https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html79:T51e,Singlenode Amazon ElastiCache Redis clusters are inmemory entities with limited data protection services (AOF). If your cluster fails for any reason, you lose all the cluster's data.However, if you're running the Redis engine, you can group 2 to 6 nodes into a cluster with replicas where 1 to 5 readonly nodes contain replicate data of the group's single read/write primary node.In this scenario, if one node fails for any reason, you do not lose all your data since it is replicated in one or more other nodes.Due to replication latency, some data may be lost if it is the primary read/write node that fails.Therefore, the best solution is to use ElastiCache Redis with replicas.CORRECT: "Use Amazon ElastiCache Redis with replicas" is the correct answer.INCORRECT: "Use Amazon ElastiCache Memcached with partitions" is incorrect as partitions are not copies of data so if you lose a partition you lose the data contained within it (no high availability).INCORRECT: "Amazon RDS with a Read Replica" is incorrect as this is not an inmemory database and is readonly.INCORRECT: "Amazon Aurora with a Global Database" is incorrect as this is not an inmemory database and this configuration is for scaling a database globally.References: https://docs.aws.amazon.com/AmazonElastiCache/latest/redug/Replication.html7a:T831,Correct option:!FindInMap [ MapName, TopLevelKey, SecondLevelKey ] The intrinsic function Fn::FindInMap returns the value corresponding to keys in a twolevel map that is declared in the Mappings section. YAML Syntax for the full function name: Fn::FindInMap: [ MapName, TopLevelKey, SecondLevelKey ]Short form of the above syntax is : !FindInMap [ MapName, TopLevelKey, SecondLevelKey ]Where,MapName Is the logical name of a mapping declared in the Mappings section that contains the keys and values. TopLevelKey The toplevel key name. Its value is a list of keyvalue pairs. SecondLevelKey The secondlevel key name, which is set to one of the keys from the list assigned to TopLevelKey.Consider the following YAML template:Mappings: RegionMap: useast1: HVM64: "ami0ff8a91507f77f867" HVMG2: "ami0a584ac55a7631c0c" uswest1: HVM64: "ami0bdb828fd58c52235" HVMG2: "ami066ee5fd4a9ef77f1" euwest1: HVM64: "ami047bb4163c506cd98" HVMG2: "ami31c2f645" apsoutheast1: HVM64: "ami08569b978cc4dfa10" HVMG2: "ami0be9df32ae9f92309" apnortheast1: HVM64: "ami06cd52961ce9f0d85" HVMG2: "ami053cdd503598e4a9d"Resources: myEC2Instance: Type: "AWS::EC2::Instance" Properties: ImageId: !FindInMap RegionMap !Ref 'AWS::Region' HVM64 InstanceType: m1.smallThe example template contains an AWS::EC2::Instance resource whose ImageId property is set by the FindInMap function.MapName is set to the map of interest, "RegionMap" in this example. TopLevelKey is set to the region where the stack is created, which is determined by using the "AWS::Region" pseudo parameter. SecondLevelKey is set to the desired architecture, "HVM64" for this example.FindInMap returns the AMI assigned to FindInMap. For a HVM64 instance in useast1, FindInMap would return "ami0ff8a91507f77f867".Incorrect options:!FindInMap [ MapName, TopLevelKey ]!FindInMap [ MapName ]!FindInMap [ MapName, TopLevelKey, SecondLevelKey, ThirdLevelKey ]These three options contradict the explanation provided above, hence these options are incorrect.Reference:https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsicfunctionreferencefindinmap.html7b:T606,A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service.Amazon ECS supports the following task placement strategies:binpack Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.random Place tasks randomly.spread Place tasks evenly based on the specified value. Accepted values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availabilityzone. Service tasks are spread based on the tasks from that service. Standalone tasks are spread based on the tasks from the same task group.The binpack task placement strategy is the most suitable for this scenario as it minimizes the number of instances used which is a requirement for this solution.CORRECT: "binpack" is the correct answer.INCORRECT: "random" is incorrect as this would assign tasks randomly to EC2 instances which would not result in minimizing the number of instances used.INCORRECT: "spread" is incorrect as this would spread the tasks based on a specified value. This is not used for minimizing the number of instances used.INCORRECT: "weighted" is incorrect as this is not an ECS task placement strategy. Weighted is associated with Amazon Route 53 routing policies.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskplacementstrategies.html7c:T688,AWS XRay makes it easy for developers to analyze the behavior of their production, distributed applications with endtoend tracing capabilities. You can use XRay to identify performance bottlenecks, edge case errors, and other hard to detect issues.AWS XRay provides an endtoend, crossservice view of requests made to your application. It gives you an applicationcentric view of requests flowing through your application by aggregating the data gathered from individual services in your application into a single unit called a trace. You can use this trace to follow the path of an individual request as it passes through each service or tier in your application so that you can pinpoint where issues are occurring.AWS XRay will assist the developer with visually analyzing the endtoend view of connectivity between the application components and how they are performing using a Service Map. XRay also provides aggregated data about the application.CORRECT: "Enable XRay tracing for the Lambda function" is the correct answer.INCORRECT: "Create an Amazon CloudWatch Events rule" is incorrect as this feature of CloudWatch is used to trigger actions based on changes in the state of AWS services.INCORRECT: "Assess the application with Amazon Inspector" is incorrect. Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS.INCORRECT: "Monitor the application with AWS Trusted Advisor" is incorrect. AWS Trusted Advisor is an online tool that provides you real time guidance to help you provision your resources following AWS best practices.References: https://aws.amazon.com/xray/features/7d:T4a1,The key requirement is that the company should be charged per running task. Therefore, the best answer is to use Amazon ECS with the Fargate launch type as with this model AWS charge you for running tasks rather than running container instances.The Fargate launch type allows you to run your containerized applications without the need to provision and manage the backend infrastructure. You just register your task definition and Fargate launches the container for you. The Fargate Launch Type is a serverless infrastructure managed by AWS.CORRECT: "Amazon ECS with the Fargate launch type" is the correct answer.INCORRECT: "Amazon ECS with the EC2 launch type" is incorrect as with this launch type you pay for running container instances (EC2 instances).INCORRECT: "An Amazon ECS Service with Auto Scaling" is incorrect as this does not specify the launch type. You can run an ECS Service on the Fargate or EC2 launch types.INCORRECT: "An Amazon ECS Cluster with Auto Scaling" is incorrect as this does not specify the launch type. You can run an ECS Cluster on the Fargate or EC2 launch types.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html7e:T75f,Correct option:The deployment was either run with immutable updates or in traffic splitting mode Immutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments.Trafficsplitting deployments let you perform canary testing as part of your application deployment. In a trafficsplitting deployment, Elastic Beanstalk launches a full set of new instances just like during an immutable deployment. It then forwards a specified percentage of incoming client traffic to the new application version for a specified evaluation period.Some policies replace all instances during the deployment or update. This causes all accumulated Amazon EC2 burst balances to be lost. It happens in the following cases:Managed platform updates with instance replacement enabledImmutable updatesDeployments with immutable updates or traffic splitting enabledIncorrect options:The deployment was run as a Rolling deployment, resulting in the resetting of EC2 burst balances With rolling deployments, Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. Rolling deployments do not result in loss of EC2 burst balances.The deployment was run as a Allatonce deployment, flushing all the accumulated EC2 burst balances The traditional Allatonce deployment, wherein all the instances are updated simultaneously, does not result in loss of EC2 burst balances.When a canary deployment fails, it resets the EC2 burst balances to zero This is incorrect and given only as a distractor.Reference:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.rollingversiondeploy.html7f:T427,The putmetricdata command publishes metric data points to Amazon CloudWatch. CloudWatch associates the data points with the specified metric. If the specified metric does not exist, CloudWatch creates the metric.CORRECT: "Use the AWS CLI putmetricdata command" is the correct answer.INCORRECT: "Use the AWS CLI putmetricalarm command" is incorrect. This command creates or updates an alarm and associates it with the specified metric, metric math expression, or anomaly detection model.INCORRECT: "Use the unified CloudWatch agent to publish custom metrics" is incorrect. It is not necessary to use the unified CloudWatch agent. In this case the Developer can use the AWS CLI with the cron job.INCORRECT: "Use the CloudWatch console with detailed monitoring" is incorrect. You cannot collect custom metric data using the CloudWatch console with detailed monitoring. Detailed monitoring sends data at 1minute rather than 5minute frequencies but will not collect custom data.References: https://docs.aws.amazon.com/cli/latest/reference/cloudwatch/putmetricdata.html80:T6ac,You can configure a dead letter queue (DLQ) on AWS Lambda to give you more control over message handling for all asynchronous invocations, including those delivered via AWS events (S3, SNS, IoT, etc.).A deadletter queue saves discarded events for further processing. A deadletter queue acts the same as an onfailure destination in that it is used when an event fails all processing attempts or expires without being processed.However, a deadletter queue is part of a function's versionspecific configuration, so it is locked in when you publish a version. Onfailure destinations also support additional targets and include details about the function's response in the invocation record.You can setup a DLQ by configuring the 'DeadLetterConfig' property when creating or updating your Lambda function. You can provide an SQS queue or an SNS topic as the 'TargetArn' for your DLQ, and AWS Lambda will write the event object invoking the Lambda function to this endpoint after the standard retry policy (2 additional retries on failure) is exhausted.CORRECT: "Configure a Dead Letter Queue (DLQ)" is the correct answer.INCORRECT: "Enable CloudWatch Logs for the Lambda function" is incorrect as CloudWatch logs will record metrics about the function but will not record records of the discarded events.INCORRECT: "Enable Lambda streams" is incorrect as this is not something that exists (DynamoDB streams does exist).INCORRECT: "Enable SNS notifications for failed events" is incorrect. Sending notifications from SNS will not include the data required for troubleshooting. A DLQ is the correct solution.References: https://digitalcloud.training/certificationtraining/awsdeveloperassociate/awscompute/awslambda/81:Tb85,Correct option:Create the app from a template provided by AWS CDK > Add code to the app to create resources within stacks > Build the app (optional) > Synthesize one or more stacks in the app > Deploy stack(s) to your AWS accountThe standard AWS CDK development workflow is similar to the workflow you're already familiar as a developer. There are a few extra steps:Create the app from a template provided by AWS CDK Each AWS CDK app should be in its own directory, with its own local module dependencies. Create a new directory for your app. Now initialize the app using the cdk init command, specifying the desired template ("app") and programming language. The cdk init command creates a number of files and folders inside the created home directory to help you organize the source code for your AWS CDK app.Add code to the app to create resources within stacks Add custom code as is needed for your application.Build the app (optional) In most programming environments, after making changes to your code, you'd build (compile) it. This isn't strictly necessary with the AWS CDK—the Toolkit does it for you so you can't forget. But you can still build manually whenever you want to catch syntax and type errors.Synthesize one or more stacks in the app to create an AWS CloudFormation template Synthesize one or more stacks in the app to create an AWS CloudFormation template. The synthesis step catches logical errors in defining your AWS resources. If your app contains more than one stack, you'd need to specify which stack(s) to synthesize.Deploy one or more stacks to your AWS account It is optional (though good practice) to synthesize before deploying. The AWS CDK synthesizes your stack before each deployment. If your code has security implications, you'll see a summary of these and need to confirm them before deployment proceeds. cdk deploy is used to deploy the stack using CloudFormation templates. This command displays progress information as your stack is deployed. When it's done, the command prompt reappears.Incorrect options:Create the app from a template provided by AWS CloudFormation > Add code to the app to create resources within stacks > Build the app (optional) > Synthesize one or more stacks in the app > Deploy stack(s) to your AWS accountCreate the app from a template provided by AWS CloudFormation > Add code to the app to create resources within stacks > Synthesize one or more stacks in the app > Deploy stack(s) to your AWS account > Build the appFor both these options, you cannot use AWS CloudFormation to create the app. So these options are incorrect.Create the app from a template provided by AWS CDK > Add code to the app to create resources within stacks > Synthesize one or more stacks in the app > Deploy stack(s) to your AWS account > Build the app You cannot have the build step after deployment. So this option is incorrect.Reference:https://docs.aws.amazon.com/cdk/latest/guide/hello_world.html82:T911,DynamoDB Streams captures a timeordered sequence of itemlevel modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in nearreal time.Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the "before" and "after" images of modified items.For this scenario, we can enable a DynamoDB stream on the “orders” table and the configure the “customers” microservice to read records from the stream and then write those records, or relevant attributes of those records, to the “customers’ table.CORRECT: "Enable Amazon DynamoDB streams on the “orders” table, configure the “customers” microservice to read records from the stream" is the correct answer.INCORRECT: "Enable DynamoDB streams for the “customers” table, trigger an AWS Lambda function to read records from the stream and write them to the “orders” table" is incorrect. This could be a good solution if it wasn’t backward. We can trigger a Lambda function to then process the records from the stream. However, we should be enabling the stream on the “orders” table, not the “customers” table, and then writing the records to the “customers” table, not the “orders” table.INCORRECT: "Use Amazon CloudWatch Events to send notifications every time an item is added or modified in the “orders” table" is incorrect. CloudWatch Events is used to respond to changes in the state of specific AWS services. It does not support DynamoDB.INCORRECT: "Use Amazon Kinesis Firehose to deliver all changes in the “orders” table to the “customers” table" is incorrect. Kinesis Firehose cannot be configured to ingest data from a DynamoDB table, nor is DynamoDB a supported destination.References:https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html83:T5e4,A deployment package is a ZIP archive that contains your function code and dependencies. You need to create a deployment package if you use the Lambda API to manage functions, or if you need to include libraries and dependencies other than the AWS SDK.You can upload the package directly to Lambda, or you can use an Amazon S3 bucket, and then upload it to Lambda. If the deployment package is larger than 50 MB, you must use Amazon S3.CORRECT: "Create a ZIP file with the source code and all dependent libraries" is the correct answer.INCORRECT: "Create a ZIP file with the source code and a script that installs the dependent libraries at runtime" is incorrect as the Developer should not run a script at runtime as this will cause latency. Instead, the Developer should include the dependent libraries in the ZIP package.INCORRECT: "Create a ZIP file with the source code and an appspec.yml file. Add the libraries to the appspec.yml file and upload to Amazon S3. Deploy using CloudFormation" is incorrect. The appspec.yml file is used with CodeDeploy, you cannot add libraries into it, and it is not deployed using CloudFormation.INCORRECT: "Create a ZIP file with the source code and a buildspec.yml file that installs the dependent libraries on AWS Lambda" is incorrect as the buildspec.yml file is used with CodeBuild for compiling source code and running tests. It cannot be used to install dependent libraries within Lambda.References: https://docs.aws.amazon.com/lambda/latest/dg/pythonpackage.html84:T599,Correct option:Templates include several major sections. The Resources section is the only required section. Sample CloudFormation YAML template: via https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/templateanatomy.html 'Dependencies' section of the template As you can see, there is no section called 'Dependencies' in the template. Although dependencies can be mentioned, there is no section itself for dependencies.Incorrect options:'Conditions' section of the template This optional section includes conditions that control whether certain resources are created or whether certain resource properties are assigned a value during stack creation or update. For example, you could conditionally create a resource that depends on whether the stack is for a production or test environment.'Resources' section of the template This is the only required section and specifies the stack resources and their properties, such as an Amazon Elastic Compute Cloud instance or an Amazon Simple Storage Service bucket. You can refer to resources in the Resources and Outputs sections of the template.'Parameters' section of the template This optional section is helpful in passing Values to your template at runtime (when you create or update a stack). You can refer to parameters from the Resources and Outputs sections of the template.Reference: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/templateanatomy.html85:T666,AWS XRay helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With XRay, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. XRay provides an endtoend view of requests as they travel through your application and shows a map of your application’s underlying components.You can record additional information about requests, the environment, or your application with annotations and metadata.You can add annotations and metadata to the segments that the XRay SDK creates, or to custom subsegments that you create.Annotations are keyvalue pairs with string, number, or Boolean values. Annotations are indexed for use with filter expressions.Use annotations to record data that you want to use to group traces in the console, or when calling the GetTraceSummaries API.CORRECT: "Annotations" is the correct answer.INCORRECT: "Metadata" is incorrect. Metadata are keyvalue pairs that can have values of any type, including objects and lists, but are not indexed for use with filter expressions. Use metadata to record additional data that you want stored in the trace butdon't need to use with search.INCORRECT: "Trace ID" is incorrect. An XRay trace ID is used to group a set of data points in AWS XRay.INCORRECT: "Daemon" is incorrect as this is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS XRay API.References: https://docs.aws.amazon.com/xray/latest/devguide/xraysdkjavasegment.html86:T6f4,Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services as well as data stored in the AWS Cloud.A stage is a named reference to a deployment, which is a snapshot of the API. You use a Stage to manage and optimize a particular deployment. For example, you can set up stage settings to enable caching, customize request throttling, configure logging, define stage variables or attach a canary release for testing.You deploy your API to a stage and it is given a unique URL that contains the stage name. This URL can be used to direct customers to your URL based on the stage (or version) you’d like them to use.Therefore, the best approach is to use API Gateway to deploy a new stage named v2 to the API and provide users with its URL.CORRECT: "Use API Gateway to deploy a new stage named v2 to the API and provide users with its URL" is the correct answer.INCORRECT: "Update the underlying Lambda function and provide clients with the new Lambda invocation URL" is incorrect as the API has been updated, not the Lambda function. We deploy API updates to stages, so we need to deploy a new stage.INCORRECT: "Use API Gateway to automatically propagate the change to clients, specifying 180 days in the phased deployment parameter" is incorrect as this is not a valid method of migrating users from one stage in API Gateway to another.INCORRECT: "Update the underlying Lambda function, create an Amazon CloudFront distribution with the updated Lambda function as its origin" is incorrect as the API has been updated, not the Lambda function.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/setupstages.html5:["$","$Le",null,{"quiz":{"id":"aws-developer-5","title":"AWS Developer Associate (DVA-C02) Practice Test Latest 2025 5","description":"Additional practice questions covering AWS development topics.","questions":[{"question":"A Developer at a company is working on a CloudFormation template to set up resources. Resources will be defined using code and provisioned based on certain conditions defined in the Conditions section. Which section of a CloudFormation template cannot be associated with Condition?","answers":[{"text":"Resources","isCorrect":false},{"text":"Parameters","isCorrect":true},{"text":"Conditions","isCorrect":false},{"text":"Outputs","isCorrect":false}],"explanation":"$f"},{"question":"Your company has configured AWS Organizations to manage multiple AWS accounts. Within each AWS account, there are many CloudFormation scripts running. Your manager has requested that each script output the account number of the account the script was executed in. Which Pseudo parameter will you use to get this information?","answers":[{"text":"AWS::Region","isCorrect":false},{"text":"AWS::AccountId","isCorrect":true},{"text":"AWS::StackName","isCorrect":false},{"text":"AWS::NoValue","isCorrect":false}],"explanation":"$10"},{"question":"A Development team wants to run their container workloads on Amazon ECS. Each application container needs to share data with another container to collect logs and metrics. What should the Development team do to meet these requirements?","answers":[{"text":"Create a single pod specification. Include both containers in the specification. Mount a persistent volume to both containers","isCorrect":false},{"text":"Create two pod specifications. Make one to include the application container and the other to include the other container. Link the two pods together","isCorrect":false},{"text":"Create two task definitions. Make one to include the application container and the other to include the other container. Mount a shared volume between the two tasks","isCorrect":false},{"text":"Create one task definition. Specify both containers in the definition. Mount a shared volume between those two containers","isCorrect":true}],"explanation":"$11"},{"question":"A company has created a set of APIs using Amazon API Gateway and exposed them to partner companies. The APIs have caching enabled for all stages. The partners require a method of invalidating the cache that they can build into their applications. What can the partners use to invalidate the API cache?","answers":[{"text":"They can use the query string parameter INVALIDATE_CACHE","isCorrect":false},{"text":"They can pass the HTTP header CacheControl: maxage=0","isCorrect":true},{"text":"They can invoke an AWS API endpoint which invalidates the cache","isCorrect":false},{"text":"They must wait for the TTL to expire","isCorrect":false}],"explanation":"$12"},{"question":"A Developer is building a three-tier web application that must be able to handle a minimum of 10,000 requests per minute. The requirements state that the web tier should be completely stateless while the application maintains session state data for users. How can the session state data be maintained externally, whilst keeping latency at the LOWEST possible value?","answers":[{"text":"Create an Amazon RedShift instance, then implement session handling at the application level to leverage a database inside the RedShift database instance for session data storage","isCorrect":false},{"text":"Implement a shared Amazon EFS file system solution across the underlying Amazon EC2 instances, then implement session handling at the application level to leverage the EFS file system for session data storage","isCorrect":false},{"text":"Create an Amazon ElastiCache Redis cluster, then implement session handling at the application level to leverage the cluster for session data storage","isCorrect":true},{"text":"Create an Amazon DynamoDB table, then implement session handling at the application level to leverage the table for session data storage","isCorrect":false}],"explanation":"$13"},{"question":"An AWS Lambda function authenticates to an external website using a regularly rotated username and password. The credentials need to be stored securely and must not be stored in the function code. What combination of AWS services can be used to achieve this requirement? (Select TWO.)","answers":[{"text":"Amazon GuardDuty","isCorrect":false},{"text":"AWS Key Management Store (KMS)","isCorrect":true},{"text":"AWS Systems Manager Parameter Store","isCorrect":true},{"text":"AWS Certificate Manager (ACM)","isCorrect":false},{"text":"AWS Artifact","isCorrect":false}],"explanation":"$14"},{"question":"A critical application runs on an Amazon EC2 instance. A Developer has configured a custom Amazon CloudWatch metric that monitors application availability with a data granularity of 1 second. The Developer must be notified within 30 seconds if the application experiences any issues. What should the Developer do to meet this requirement?","answers":[{"text":"Use Amazon CloudWatch Logs Insights and trigger an Amazon Eventbridge rule to send a notification.","isCorrect":false},{"text":"Specify an Amazon SNS topic for alarms when issuing the putmetricdata AWS CLI command.","isCorrect":false},{"text":"Configure a highresolution CloudWatch alarm and use Amazon SNS to send the alert.","isCorrect":true},{"text":"Use a default CloudWatch metric, configure an alarm, and use Amazon SNS to send the alert.","isCorrect":false}],"explanation":"$15"},{"question":"When running a Rolling deployment in Elastic Beanstalk environment, only two batches completed the deployment successfully, while rest of the batches failed to deploy the updated version. Following this, the development team terminated the instances from the failed deployment. What will be the status of these failed instances post termination?","answers":[{"text":"Elastic Beanstalk will replace the failed instances with instances running the application version from the oldest successful deployment","isCorrect":false},{"text":"Elastic Beanstalk will not replace the failed instances","isCorrect":false},{"text":"Elastic Beanstalk will replace the failed instances after the application version to be installed is manually chosen from AWS Console","isCorrect":false},{"text":"Elastic Beanstalk will replace the failed instances with instances running the application version from the most recent successful deployment","isCorrect":true}],"explanation":"$16"},{"question":"A Lambda function is taking a long time to complete. The Developer has discovered that inadequate compute capacity is being allocated to the function. How can the Developer ensure that more compute capacity is allocated to the function?","answers":[{"text":"Increase the reserved concurrency","isCorrect":false},{"text":"Increase the maximum execution time","isCorrect":false},{"text":"Allocate more memory to the function","isCorrect":true},{"text":"Use an instance type with more CPU","isCorrect":false}],"explanation":"You can allocate memory between 128 MB and 3,008 MB in 64MB increments. AWS Lambda allocates CPU power linearly in proportion to the amount of memory configured. At 1,792 MB, a function has the equivalent of one full vCPU (one vCPUsecond of credits per second).Therefore, the way provide more compute capacity to this function is to allocate more memory.CORRECT: \"Allocate more memory to the function\" is the correct answer.INCORRECT: \"Use an instance type with more CPU\" is incorrect as Lambda is a serverless service and you cannot choose an instance type for your function.INCORRECT: \"Increase the maximum execution time\" is incorrect as the function is not timing out, it’s just taking longer than expected due to having insufficient compute allocated.INCORRECT: \"Increase the reserved concurrency\" is incorrect as this would enable more invocations to run in parallel but would not add more CPU to each function execution.References: https://docs.aws.amazon.com/lambda/latest/dg/configurationconsole.html"},{"question":"An application onpremises uses Linux servers and a relational database using PostgreSQL. The company will be migrating the application to AWS and require a managed service that will take care of capacity provisioning, load balancing, and autoscaling. Which combination of services should the Developer use? (Select TWO.)","answers":[{"text":"Amazon RDS with PostrgreSQL","isCorrect":true},{"text":"Amazon EC2 with Auto Scaling","isCorrect":false},{"text":"AWS Elastic Beanstalk","isCorrect":true},{"text":"AWS Lambda with CloudWatch Events","isCorrect":false},{"text":"Amazon EC2 with PostgreSQL","isCorrect":false}],"explanation":"$17"},{"question":"A Developer has created a task definition that includes the following JSON code:\"placementStrategy\": [{\"field\": \"attribute:ecs.availabilityzone\",\"type\": \"spread\"},{\"field\": \"instanceId\",\"type\": \"spread\"}]What is the effect of this task placement strategy?","answers":[{"text":"It distributes tasks evenly across Availability Zones and then distributes tasks evenly across the instances within each Availability Zone","isCorrect":true},{"text":"It distributes tasks evenly across Availability Zones and then bin packs tasks based on memory within each Availability Zone","isCorrect":false},{"text":"It distributes tasks evenly across Availability Zones and then distributes tasks randomly across instances within each Availability Zone","isCorrect":false},{"text":"It distributes tasks evenly across Availability Zones and then distributes tasks evenly across distinct instances within each Availability Zone","isCorrect":false}],"explanation":"$18"},{"question":"A Developer is developing a web application and will maintain separate sets of resources for the alpha, beta, and release stages. Each version runs on Amazon EC2 and uses an Elastic Load Balancer. How can the Developer create a single page to view and manage all of the resources?","answers":[{"text":"Create an AWS Elastic Beanstalk environment for each stage","isCorrect":false},{"text":"Create a single AWS CodeDeploy deployment","isCorrect":false},{"text":"Deploy all resources using a single Amazon CloudFormation stack","isCorrect":false},{"text":"Create a resource group","isCorrect":true}],"explanation":"$19"},{"question":"A Developer has made an update to an application. The application serves users around the world and uses Amazon CloudFront for caching content closer to users. It has been reported that after deploying the application updates, users are not able to see the latest changes. How can the Developer resolve this issue?","answers":[{"text":"Invalidate all the application objects from the edge caches","isCorrect":true},{"text":"Disable forwarding of query strings and request headers from the CloudFront distribution configuration","isCorrect":false},{"text":"Disable the CloudFront distribution and enable it again to update all the edge locations","isCorrect":false},{"text":"Remove the origin from the CloudFront configuration and add it again","isCorrect":false}],"explanation":"$1a"},{"question":"A company is setting up a Lambda function that will process events from a DynamoDB stream. The Lambda function has been created and a stream has been enabled. What else needs to be done for this solution to work?","answers":[{"text":"An eventsource mapping must be created on the DynamoDB side to associate the DynamoDB stream with the Lambda function","isCorrect":false},{"text":"An alarm should be created in CloudWatch that sends a notification to Lambda when a new entry is added to the DynamoDB stream","isCorrect":false},{"text":"An eventsource mapping must be created on the Lambda side to associate the DynamoDB stream with the Lambda function","isCorrect":true},{"text":"Update the CloudFormation template to map the DynamoDB stream to the Lambda function","isCorrect":false}],"explanation":"$1b"},{"question":"You are a developer in a manufacturing company that has several servers onsite. The company decides to move new development to the cloud using serverless technology. You decide to use the AWS Serverless Application Model (AWS SAM) and work with an AWS SAM template file to represent your serverless architecture. Which of the following is NOT a valid serverless resource type?","answers":[{"text":"AWS::Serverless::Api","isCorrect":false},{"text":"AWS::Serverless::SimpleTable","isCorrect":false},{"text":"AWS::Serverless::Function","isCorrect":false},{"text":"AWS::Serverless::UserPool","isCorrect":true}],"explanation":"$1c"},{"question":"An application component writes thousands of itemlevel changes to a DynamoDB table per day. The developer requires that a record is maintained of the items before they were modified. What MUST the developer do to retain this information? (Select TWO.)","answers":[{"text":"Use an AWS Lambda function to extract the item records from the notification and write to an S3 bucket","isCorrect":false},{"text":"Set the StreamViewType to OLD_IMAGE","isCorrect":true},{"text":"Create a CloudWatch alarm that sends a notification when an item is modified","isCorrect":false},{"text":"Set the StreamViewType to NEW_AND_OLD_IMAGES","isCorrect":false},{"text":"Enable DynamoDB Streams for the table","isCorrect":true}],"explanation":"$1d"},{"question":"You have created a Java application that uses RDS for its main data storage and ElastiCache for user session storage. The application needs to be deployed using Elastic Beanstalk and every new deployment should allow the application servers to reuse the RDS database. On the other hand, user session data stored in ElastiCache can be recreated for every deployment. Which of the following configurations will allow you to achieve this? (Select two)","answers":[{"text":"RDS database defined in .ebextensions/","isCorrect":false},{"text":"ElastiCache bundled with the application source code","isCorrect":false},{"text":"ElastiCache database defined externally and referenced through environment variables","isCorrect":false},{"text":"RDS database defined externally and referenced through environment variables","isCorrect":true},{"text":"ElastiCache defined in .ebextensions/","isCorrect":true}],"explanation":"$1e"},{"question":"Your global organization has an IT infrastructure that is deployed using CloudFormation on AWS Cloud. One employee, in useast1 Region, has created a stack 'Application1' and made an exported output with the name 'ELBDNSName'. Another employee has created a stack for a different application 'Application2' in useast2 Region and also exported an output with the name 'ELBDNSName'. The first employee wanted to deploy the CloudFormation stack 'Application1' in useast2, but it got an error. What is the cause of the error?","answers":[{"text":"Output Values in CloudFormation must have unique names across all Regions","isCorrect":false},{"text":"Exported Output Values in CloudFormation must have unique names within a single Region","isCorrect":true},{"text":"Output Values in CloudFormation must have unique names within a single Region","isCorrect":false},{"text":"Exported Output Values in CloudFormation must have unique names across all Regions","isCorrect":false}],"explanation":"$1f"},{"question":"Change management procedures at an organization require that a log is kept recording activity within AWS accounts. The activity that must be recorded includes API activity related to creating, modifying or deleting AWS resources. Which AWS service should be used to record this information?","answers":[{"text":"Amazon CloudWatch","isCorrect":false},{"text":"AWS OpsWorks","isCorrect":false},{"text":"Amazon CloudTrail","isCorrect":true},{"text":"AWS XRay","isCorrect":false}],"explanation":"$20"},{"question":"A developer has deployed an application on an Amazon EC2 instance in a private subnet within a VPC. The subnet does not have Internet connectivity. The developer would like to write application logs to an Amazon S3 bucket. What MUST be configured to enable connectivity?","answers":[{"text":"A VPC endpoint should be provisioned for S3","isCorrect":true},{"text":"A bucket policy needs to be added specifying the principles that are allowed to write data to the bucket","isCorrect":false},{"text":"An IAM role must be added to the instance that has permissions to write to the S3 bucket","isCorrect":false},{"text":"A VPN should be established to enable private connectivity to S3","isCorrect":false}],"explanation":"$21"},{"question":"A serverless application uses an AWS Lambda function to process Amazon S3 events. The Lambda function executes 20 times per second and takes 20 seconds to complete each execution. How many concurrent executions will the Lambda function require?","answers":[{"text":"40","isCorrect":false},{"text":"20","isCorrect":false},{"text":"400","isCorrect":true},{"text":"5","isCorrect":false}],"explanation":"$22"},{"question":"A Developer is launching an application on Amazon ECS. The application should scale tasks automatically based on load and incoming connections must be spread across the containers. How should the Developer configure the ECS cluster?","answers":[{"text":"Create an ECS Service with Auto Scaling and attach an Elastic Load Balancer","isCorrect":true},{"text":"Create a capacity provider and configure cluster auto scaling","isCorrect":false},{"text":"Write statements using the Cluster Query Language to scale the Docker containers","isCorrect":false},{"text":"Create an ECS Task Definition that uses Auto Scaling and Elastic Load Balancing","isCorrect":false}],"explanation":"$23"},{"question":"A Developer is building an application that will store data relating to financial transactions in multiple DynamoDB tables. The Developer needs to ensure the transactions provide atomicity, isolation, and durability (ACID) and that changes are committed following an all-or-nothing paradigm. What write API should be used for the DynamoDB table?","answers":[{"text":"Standard","isCorrect":false},{"text":"Strongly consistent","isCorrect":false},{"text":"Transactional","isCorrect":true},{"text":"Eventually consistent","isCorrect":false}],"explanation":"$24"},{"question":"A company manages a web application that is deployed on AWS Elastic Beanstalk. A Developer has been instructed to update to a new version of the application code. There is no tolerance for downtime if the update fails and rollback should be fast. What is the SAFEST deployment method to use?","answers":[{"text":"Amazon Elastic Kubernetes Service (EKS)","isCorrect":false},{"text":"AWS Lambda","isCorrect":false},{"text":"Amazon ECS with Fargate launch type","isCorrect":true},{"text":"Amazon ECS with EC2 launch type","isCorrect":false}],"explanation":"$25"},{"question":"An organization has hosted its EC2 instances in two AZs. AZ1 has two instances and AZ2 has 8 instances. The Elastic Load Balancer managing the instances in the two AZs has crosszone load balancing enabled in its configuration. What percentage traffic will each of the instances in AZ1 receive?","answers":[{"text":"20","isCorrect":false},{"text":"25","isCorrect":false},{"text":"15","isCorrect":false},{"text":"10","isCorrect":true}],"explanation":"$26"},{"question":"A Developer has completed some code updates and needs to deploy the updates to an Amazon Elastic Beanstalk environment. The update must be deployed in the fastest possible time and application downtime is acceptable. Which deployment policy should the Developer choose?","answers":[{"text":"Rolling with additional batch","isCorrect":false},{"text":"Immutable","isCorrect":false},{"text":"All at once","isCorrect":true},{"text":"Rolling","isCorrect":false}],"explanation":"$27"},{"question":"An application deployed on AWS Elastic Beanstalk experienced increased error rates during deployments of new application versions, resulting in service degradation for users. The Development team believes that this is because of the reduction in capacity during the deployment steps. The team would like to change the deployment policy configuration of the environment to an option that maintains full capacity during deployment while using the existing instances. Which deployment policy will meet these requirements while using the existing instances?","answers":[{"text":"Rolling with additional batch","isCorrect":true},{"text":"All at once","isCorrect":false},{"text":"Rolling","isCorrect":false},{"text":"Immutable","isCorrect":false}],"explanation":"$28"},{"question":"A DynamoDB table is being used to store session information for users of an online game. A developer has noticed that the table size has increased considerably and much of the data is not required after a gaming session is completed. What is the MOST cost-effective approach to reducing the size of the table?","answers":[{"text":"Create an AWS Lambda function that purges stale items from the table daily","isCorrect":false},{"text":"Use the batchwriteitem API to delete the data","isCorrect":false},{"text":"Enable a Time To Live (TTL) on the table and add a timestamp attribute on new items","isCorrect":true},{"text":"Use the deleteitem API to delete the data","isCorrect":false}],"explanation":"$29"},{"question":"A multinational company has just moved to AWS Cloud and it has configured forecastbased AWS Budgets alerts for cost management. However, no alerts have been received even though the account and the budgets have been created almost three weeks ago. What could be the issue with the AWS Budgets configuration?","answers":[{"text":"Budget forecast has been created from an account that does not have enough privileges","isCorrect":false},{"text":"AWS requires approximately 5 weeks of usage data to generate budget forecasts","isCorrect":true},{"text":"Amazon CloudWatch could be down and hence alerts are not being sent","isCorrect":false},{"text":"Account has to be part of AWS Organizations to receive AWS Budgets alerts","isCorrect":false}],"explanation":"$2a"},{"question":"A developer is preparing to deploy a Docker container to Amazon ECS using CodeDeploy. The developer has defined the deployment actions in a file. What should the developer name the file?","answers":[{"text":"buildspec.yml","isCorrect":false},{"text":"appspec.yml","isCorrect":true},{"text":"cron.yml","isCorrect":false},{"text":"appspec.json","isCorrect":false}],"explanation":"$2b"},{"question":"A utilities company needs to ensure that documents uploaded by customers through a web portal are securely stored in Amazon S3 with encryption at rest. The company does not want to manage the security infrastructure inhouse. However, the company still needs maintain control over its encryption keys due to industry regulations. Which encryption strategy should a Developer use to meet these requirements?","answers":[{"text":"Serverside encryption with customerprovided encryption keys (SSEC)","isCorrect":true},{"text":"Clientside encryption","isCorrect":false},{"text":"Serverside encryption with Amazon S3 managed keys (SSES3)","isCorrect":false},{"text":"Serverside encryption with AWS KMS managed keys (SSEKMS)","isCorrect":false}],"explanation":"$2c"},{"question":"You're a developer working on a large scale order processing application. After developing the features, you commit your code to AWS CodeCommit and begin building the project with AWS CodeBuild before it gets deployed to the server. The build is taking too long and the error points to an issue resolving dependencies from a thirdparty. You would like to prevent a build running this long in the future for similar underlying reasons. Which of the following options represents the best solution to address this usecase?","answers":[{"text":"Enable CodeBuild timeouts","isCorrect":true},{"text":"Use AWS Lambda","isCorrect":false},{"text":"Use AWS CloudWatch Events","isCorrect":false},{"text":"Use VPC Flow Logs","isCorrect":false}],"explanation":"$2d"},{"question":"A team of Developers need to deploy a website for a development environment. The team do not want to manage the infrastructure and just need to upload Node.js code to the instances. Which AWS service should Developers do?","answers":[{"text":"Create an AWS Lambda package","isCorrect":false},{"text":"Launch an Auto Scaling group of Amazon EC2 instances","isCorrect":false},{"text":"Create an AWS CloudFormation template","isCorrect":false},{"text":"Create an AWS Elastic Beanstalk environment","isCorrect":true}],"explanation":"$2e"},{"question":"A developer is testing Amazon Simple Queue Service (SQS) queues in a development environment. The queue along with all its contents has to be deleted after testing. Which SQS API should be used for this requirement?","answers":[{"text":"PurgeQueue","isCorrect":false},{"text":"DeleteQueue","isCorrect":true},{"text":"RemoveQueue","isCorrect":false},{"text":"RemovePermission","isCorrect":false}],"explanation":"$2f"},{"question":"A company has a large Amazon DynamoDB table which they scan periodically so they can analyze several attributes. The scans are consuming a lot of provisioned throughput. What technique can a Developer use to minimize the impact of the scan on the table's provisioned throughput?","answers":[{"text":"Define a range key on the table","isCorrect":false},{"text":"Set a smaller page size for the scan","isCorrect":true},{"text":"Use parallel scans","isCorrect":false},{"text":"Prewarm the table by updating all items","isCorrect":false}],"explanation":"$30"},{"question":"An application is deployed using AWS Elastic Beanstalk and uses a Classic Load Balancer (CLB). A developer is performing a blue/green migration to change to an Application Load Balancer (ALB). After deployment, the developer has noticed that customers connecting to the ALB need to reauthenticate every time they connect. Normally they would only authenticate once and then be able to reconnect without reauthenticating for several hours. How can the developer resolve this issue?","answers":[{"text":"Enable Sticky Sessions on the target group","isCorrect":true},{"text":"Add a new SSL certificate to the ALBs listener","isCorrect":false},{"text":"Enable IAM authentication on the ALBs listener","isCorrect":false},{"text":"Change the load balancing algorithm on the target group to &ldquo;least outstanding requests\"","isCorrect":false}],"explanation":"$31"},{"question":"A Developer has been tasked by a client to create an application. The client has provided the following requirements for the application:• Performance efficiency of seconds with up to a minute of latency• Data storage requirements will be up to thousands of terabytes• Permessage sizes may vary between 100 KB and 100 MB• Data can be stored as key/value stores supporting eventual consistencyWhat is the MOST cost-effective AWS service to meet these requirements?","answers":[{"text":"Amazon S3","isCorrect":true},{"text":"Amazon RDS (with a MySQL engine)","isCorrect":false},{"text":"Amazon DynamoDB","isCorrect":false},{"text":"Amazon ElastiCache","isCorrect":false}],"explanation":"The question is looking for a costeffective solution. Multiple options can support the latency and scalability requirements. Amazon RDS is not a key/value store so that rules that option out. Of the remaining options ElastiCache would be expensive and DynamoDB only supports a maximum item size of 400 KB. Therefore, the best option is Amazon S3 which delivers all of therequirements.CORRECT: \"Amazon S3\" is the correct answer.INCORRECT: \"Amazon DynamoDB\" is incorrect as it supports a maximum item size of 400 KB and the messages will be up to 100 MB.INCORRECT: \"Amazon RDS (with a MySQL engine)\" is incorrect as it is not a key/value store.INCORRECT: \"Amazon ElastiCache\" is incorrect as it is an inmemory database and would be the most expensive solution.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html"},{"question":"A Developer is writing a serverless application that will process data uploaded to a file share. The Developer has created an AWS Lambda function and requires the function to be invoked every 15 minutes to process the data. What is an automated and serverless way to trigger the function?","answers":[{"text":"Deploy an Amazon EC2 instance based on Linux, and edit it’s /etc/crontab file by adding a command to periodically invoke the Lambda function","isCorrect":false},{"text":"Create an Amazon CloudWatch Events rule that triggers on a regular schedule to invoke the Lambda function","isCorrect":true},{"text":"Configure an environment variable named PERIOD for the Lambda function. Set the value at 600","isCorrect":false},{"text":"Create an Amazon SNS topic that has a subscription to the Lambda function with a 600second timer","isCorrect":false}],"explanation":"$32"},{"question":"A company has three different environments: Development, QA, and Production. The company wants to deploy its code first in the Development environment, then QA, and then Production. Which AWS service can be used to meet this requirement?","answers":[{"text":"Use AWS Data Pipeline to create multiple data pipeline provisions to deploy the application","isCorrect":false},{"text":"Use AWS CodeCommit to create multiple repositories to deploy the application","isCorrect":false},{"text":"Use AWS CodeDeploy to create multiple deployment groups","isCorrect":true},{"text":"Use AWS CodeBuild to create, configure, and deploy multiple build application projects","isCorrect":false}],"explanation":"$33"},{"question":"A Developer is designing a faulttolerant application that will use Amazon EC2 instances and an Elastic Load Balancer. The Developer needs to ensure that if an EC2 instance fails session data is not lost. How can this be achieved?","answers":[{"text":"Use Amazon SQS to save session data","isCorrect":false},{"text":"Use Amazon DynamoDB to perform scalable session handling","isCorrect":true},{"text":"Use an EC2 Auto Scaling group to automatically launch new instances","isCorrect":false},{"text":"Enable Sticky Sessions on the Elastic Load Balancer","isCorrect":false}],"explanation":"$34"},{"question":"A CloudFormation stack needs to be deployed in several regions and requires a different Amazon Machine Image (AMI) in each region. Which AWS CloudFormation template key can be used to specify the correct AMI for each region?","answers":[{"text":"Parameters","isCorrect":false},{"text":"Mappings","isCorrect":true},{"text":"Resources","isCorrect":false},{"text":"Outputs","isCorrect":false}],"explanation":"$35"},{"question":"A company runs an ecommerce website that uses Amazon DynamoDB where pricing for items is dynamically updated in real-time. At any given time, multiple updates may occur simultaneously for pricing information on a particular product. This is causing the original editor's changes to be overwritten without a proper review process. Which DynamoDB write option should be selected to prevent this overwriting?","answers":[{"text":"Concurrent writes","isCorrect":false},{"text":"Conditional writes","isCorrect":true},{"text":"Batch writes","isCorrect":false},{"text":"Atomic writes","isCorrect":false}],"explanation":"$36"},{"question":"A developer is planning to use a Lambda function to process incoming requests from an Application Load Balancer (ALB). How can this be achieved?","answers":[{"text":"Configure an eventsource mapping between the ALB and the Lambda function","isCorrect":false},{"text":"Setup an API in front of the ALB using API Gateway and use an integration request to map the request to the Lambda function","isCorrect":false},{"text":"Create a target group and register the Lambda function using the AWS CLI","isCorrect":true},{"text":"Create an Auto Scaling Group (ASG) and register the Lambda function in the launch configuration","isCorrect":false}],"explanation":"$37"},{"question":"A Developer requires a multithreaded inmemory cache to place in front of an Amazon RDS database. Which caching solution should the Developer choose?","answers":[{"text":"Amazon ElastiCache Redis","isCorrect":false},{"text":"Amazon DynamoDB DAX","isCorrect":false},{"text":"Amazon ElastiCache Memcached","isCorrect":true},{"text":"Amazon RedShift","isCorrect":false}],"explanation":"$38"},{"question":"A Developer attempted to run an AWS CodeBuild project, and received an error. The error stated that the length of all environment variables exceeds the limit for the combined maximum of characters. What is the recommended solution?","answers":[{"text":"Use Amazon Cognito to store keyvalue pairs for large numbers of environment variables","isCorrect":false},{"text":"Update the settings for the build project to use an Amazon S3 bucket for large numbers of environment variables","isCorrect":false},{"text":"Add the export LC_ALL=”en_US.utf8” command to the pre_build section to ensure POSIX localization","isCorrect":false},{"text":"Use AWS Systems Manager Parameter Store to store large numbers of environment variables","isCorrect":true}],"explanation":"$39"},{"question":"A team of Developers require readonly access to an Amazon DynamoDB table. The Developers have been added to a group. What should an administrator do to provide the team with access whilst following the principal of least privilege?","answers":[{"text":"Create a customer managed policy with read/write access to DynamoDB for all resources. Attach the policy to the group","isCorrect":false},{"text":"Assign the AmazonDynamoDBReadOnlyAccess AWS managed policy to the group","isCorrect":false},{"text":"Assign the AWSLambdaDynamoDBExecutionRole AWS managed policy to the group","isCorrect":false},{"text":"Create a customer managed policy with read only access to DynamoDB and specify the ARN of the table for the “Resource” element. Attach the policy to the group","isCorrect":true}],"explanation":"$3a"},{"question":"A Developer will be launching several Docker containers on a new Amazon ECS cluster using the EC2 Launch Type. The containers will all run a web service on port 80. What is the EASIEST way the Developer can configure the task definition to ensure the web services run correctly and there are no port conflicts on the host instances?","answers":[{"text":"Specify port 80 for the container port and a unique port number for the host port","isCorrect":false},{"text":"Specify a unique port number for the container port and port 80 for the host port","isCorrect":false},{"text":"Leave both the container port and host port configuration blank","isCorrect":false},{"text":"Specify port 80 for the container port and port 0 for the host port","isCorrect":true}],"explanation":"$3b"},{"question":"A global ecommerce company wants to perform geographic load testing of its order processing API. The company must deploy resources to multiple AWS Regions to support the load testing of the API. How can the company address these requirements without additional application code?","answers":[{"text":"Set up an AWS CloudFormation template that defines the load test resources. Leverage the AWS CLI createstackset command to create a stack set in the desired Regions","isCorrect":true},{"text":"Set up an AWS CloudFormation template that defines the load test resources. Develop regionspecific Lambda functions to create a stack from the AWS CloudFormation template in each Region when the respective function is invoked","isCorrect":false},{"text":"Set up an AWS Organizations template that defines the load test resources across the organization. Leverage the AWS CLI createstackset command to create a stack set in the desired Regions","isCorrect":false},{"text":"Set up an AWS Cloud Development Kit (CDK) ToolKit that defines the load test resources. Leverage the CDK CLI to create a stack from the template in each Region","isCorrect":false}],"explanation":"$3c"},{"question":"ECS Fargate container tasks are usually spread across Availability Zones (AZs) and the underlying workloads need persistent crossAZ shared access to the data volumes configured for the container tasks. Which of the following solutions is the best choice for these workloads?","answers":[{"text":"Bind mounts","isCorrect":false},{"text":"AWS Gateway Storage volumes","isCorrect":false},{"text":"Docker volumes","isCorrect":false},{"text":"Amazon EFS volumes","isCorrect":true}],"explanation":"$3d"},{"question":"A Developer needs to scan a full DynamoDB 50GB table within nonpeak hours. About half of the strongly consistent RCUs are typically used during nonpeak hours and the scan duration must be minimized. How can the Developer optimize the scan execution time without impacting production workloads?","answers":[{"text":"Use parallel scans while limiting the rate","isCorrect":true},{"text":"Increase the RCUs during the scan operation","isCorrect":false},{"text":"Change to eventually consistent RCUs during the scan operation","isCorrect":false},{"text":"Use sequential scans","isCorrect":false}],"explanation":"$3e"},{"question":"A serverless application uses an Amazon API Gateway and AWS Lambda. The application processes data submitted in a form by users of the application and certain data must be stored and available to subsequent function calls. What is the BEST solution for storing this data?","answers":[{"text":"Store the data in an Amazon Kinesis Data Stream","isCorrect":false},{"text":"Store the data in the /tmp directory","isCorrect":false},{"text":"Store the data in an Amazon SQS queue","isCorrect":false},{"text":"Store the data in an Amazon DynamoDB table","isCorrect":true}],"explanation":"$3f"},{"question":"A company is deploying an onpremise application server that will connect to several AWS services. What is the BEST way to provide the application server with permissions to authenticate to AWS services?","answers":[{"text":"Create an IAM user and generate a key pair. Use the key pair in API calls to AWS services","isCorrect":false},{"text":"Create an IAM role with the necessary permissions and assign it to the application server","isCorrect":false},{"text":"Create an IAM group with the necessary permissions and add the onpremise application server to the group","isCorrect":false},{"text":"Create an IAM user and generate access keys. Create a credentials file on the application server","isCorrect":true}],"explanation":"$40"},{"question":"A Developer is creating an application and would like add AWS XRay to trace user requests endtoend through the software stack. The Developer has implemented the changes and tested the application and the traces are successfully sent to XRay. The Developer then deployed the application on an Amazon EC2 instance, and noticed that the traces are not being sent to XRay. What is the most likely cause of this issue? (Select TWO.)","answers":[{"text":"The XRay API is not installed on the EC2 instance","isCorrect":false},{"text":"The XRay segments are being queued","isCorrect":false},{"text":"The traces are reaching XRay, but the Developer does not have permission to view the records","isCorrect":false},{"text":"The instance’s instance profile role does not have permission to upload trace data to XRay","isCorrect":true},{"text":"The XRay daemon is not installed on the EC2 instance.","isCorrect":true}],"explanation":"$41"},{"question":"A company is deploying an Amazon Kinesis Data Streams application that will collect streaming data from a gaming application. Consumers will run on Amazon EC2 instances. In this architecture, what can be deployed on consumers to act as an intermediary between the record processing logic and Kinesis Data Streams and instantiate a record processor for each shard?","answers":[{"text":"AWS CLI","isCorrect":false},{"text":"Amazon Kinesis Client Library (KCL)","isCorrect":true},{"text":"Amazon Kinesis CLI","isCorrect":false},{"text":"Amazon Kinesis API","isCorrect":false}],"explanation":"$42"},{"question":"An AWS Lambda function has been packaged for deployment to multiple environments including development, test, and production. The Lambda function uses an Amazon RDS MySQL database for storing data. Each environment has a different RDS MySQL database. How can a Developer configure the Lambda function package to ensure the correct database connection string is used for each environment?","answers":[{"text":"Use layers for storing the database connection strings","isCorrect":false},{"text":"Use a separate function for development and production","isCorrect":false},{"text":"Include the resources in the function code","isCorrect":false},{"text":"Use environment variables for the database connection strings","isCorrect":true}],"explanation":"$43"},{"question":"A company runs a legacy application that uses an XMLbased SOAP interface. The company needs to expose the functionality of the service to external customers and plans to use Amazon API Gateway. How can a Developer configure the integration?","answers":[{"text":"Create a SOAP API using Amazon API Gateway. Transform the incoming JSON into a valid XML message for the SOAP interface using AWS Lambda.","isCorrect":true},{"text":"Create a RESTful API using Amazon API Gateway. Pass the incoming JSON to the SOAP interface through an Application Load Balancer.","isCorrect":false},{"text":"Create a RESTful API using Amazon API Gateway. Transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates.","isCorrect":false},{"text":"Create a SOAP API using Amazon API Gateway. Pass the incoming JSON to the SOAP interface through a Network Load Balancer.","isCorrect":false}],"explanation":"$44"},{"question":"An IT company is configuring Auto Scaling for its Amazon EC2 instances spread across different AZs and Regions. Which of the following scenarios are NOT correct about EC2 Auto Scaling? (Select two)","answers":[{"text":"An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same Region","isCorrect":false},{"text":"Auto Scaling groups that span across multiple Regions need to be enabled for all the Regions specified","isCorrect":true},{"text":"Amazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group","isCorrect":false},{"text":"An Auto Scaling group can contain EC2 instances in only one Availability Zone of a Region","isCorrect":true},{"text":"For Auto Scaling groups in a VPC, the EC2 instances are launched in subnets","isCorrect":false}],"explanation":"$45"},{"question":"An application runs on a fleet of Amazon EC2 instances in an Auto Scaling group. The application stores data in an Amazon DynamoDB table and all instances make updates to the table. When querying data, EC2 instances sometimes retrieve stale data. The Developer needs to update the application to ensure the most uptodate data is retrieved for all queries. How can the Developer accomplish this?","answers":[{"text":"Use the UpdateGlobalTable API to create a global secondary index.","isCorrect":false},{"text":"Set the ConsistentRead parameter to true when calling GetItem.","isCorrect":true},{"text":"Cache the database writes using Amazon DynamoDB Accelerator.","isCorrect":false},{"text":"Use the TransactWriteItems API when issuing PutItem actions.","isCorrect":false}],"explanation":"$46"},{"question":"A developer has been asked to create a web application to be deployed on EC2 instances. The developer just wants to focus on writing application code without worrying about server provisioning, configuration and deployment. As a Developer Associate, which AWS service would you recommend for the given use-case?","answers":[{"text":"CloudFormation","isCorrect":false},{"text":"Serverless Application Model","isCorrect":false},{"text":"CodeDeploy","isCorrect":false},{"text":"Elastic Beanstalk","isCorrect":true}],"explanation":"$47"},{"question":"A Developer has used a third-party tool to build, bundle, and package a software package on-premises. The software package is stored in a local file system and must be deployed to Amazon EC2 instances. How can the application be deployed onto the EC2 instances?","answers":[{"text":"Use AWS CodeBuild to commit the package and automatically deploy the software package.","isCorrect":false},{"text":"Use AWS CodeDeploy and point it to the local file system to deploy the software package.","isCorrect":false},{"text":"Create a repository using AWS CodeCommit to automatically trigger a deployment to the EC2 instances.","isCorrect":false},{"text":"Upload the bundle to an Amazon S3 bucket and specify the S3 location when doing a deployment using AWS CodeDeploy.","isCorrect":true}],"explanation":"$48"},{"question":"Amazon Simple Queue Service (SQS) has a set of APIs for various actions supported by the service. As a developer associate, which of the following would you identify as correct regarding the CreateQueue API? (Select two)","answers":[{"text":"Queue tags are case insensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag","isCorrect":false},{"text":"The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using MessageRetentionPeriod attribute","isCorrect":false},{"text":"You can't change the queue type after you create it","isCorrect":true},{"text":"The visibility timeout value for the queue is in seconds, which defaults to 30 seconds","isCorrect":true},{"text":"The deadletter queue of a FIFO queue must also be a FIFO queue. Whereas, the deadletter queue of a standard queue can be a standard queue or a FIFO queue","isCorrect":false}],"explanation":"$49"},{"question":"To reduce the cost of API actions performed on an Amazon SQS queue, a Developer has decided to implement long polling. Which of the following modifications should the Developer make to the API actions?","answers":[{"text":"Set the SetQueueAttributes with a MessageRetentionPeriod of 60","isCorrect":false},{"text":"Set the ReceiveMessage API with a VisibilityTimeout of 30","isCorrect":false},{"text":"Set the SetQueueAttributes API with a DelaySeconds of 20","isCorrect":false},{"text":"Set the ReceiveMessage API with a WaitTimeSeconds of 20","isCorrect":true}],"explanation":"$4a"},{"question":"An application is hosted by a 3rd party and exposed at yourapp.3rdparty.com. You would like to have your users access your application using www.mydomain.com, which you own and manage under Route 53. What Route 53 record should you create?","answers":[{"text":"Create an A record","isCorrect":false},{"text":"Create a PTR record","isCorrect":false},{"text":"Create an Alias Record","isCorrect":false},{"text":"Create a CNAME record","isCorrect":true}],"explanation":"$4b"},{"question":"An organization developed an application that uses a set of APIs that are being served through Amazon API Gateway. The API calls must be authenticated based on OpenID identity providers such as Amazon, Google, or Facebook. The APIs should allow access based on a custom authorization model. Which is the simplest and MOST secure design to use to build an authentication and authorization model for the APIs?","answers":[{"text":"Use Amazon DynamoDB to store user credentials and have the application retrieve temporary credentials from AWS STS. Make API calls by passing user credentials to the APIs for authentication and authorization","isCorrect":false},{"text":"Use Amazon ElastiCache to store user credentials and pass them to the APIs for authentication and authorization","isCorrect":false},{"text":"Use Amazon Cognito user pools and a custom authorizer to authenticate and authorize users based on JSON Web Tokens","isCorrect":true},{"text":"Build an OpenID token broker with Amazon and Facebook. Users will authenticate with these identify providers and pass the JSON Web Token to the API to authenticate each API call","isCorrect":false}],"explanation":"$4c"},{"question":"A website is deployed in several AWS regions. A Developer needs to direct global users to the website that provides the best performance. How can the Developer achieve this?","answers":[{"text":"Create Alias records in AWS Route 53 and direct the traffic to an Elastic Load Balancer","isCorrect":false},{"text":"Create CNAME records in AWS Route 53 and direct traffic to Amazon CloudFront","isCorrect":false},{"text":"Create A records in AWS Route 53 and use a latencybased routing policy","isCorrect":true},{"text":"Create A records in AWS Route 53 and use a weighted routing policy","isCorrect":false}],"explanation":"$4d"},{"question":"An application uses an Amazon DynamoDB table that is 50 GB in size and provisioned with 10,000 read capacity units (RCUs) per second. The table must be scanned during nonpeak hours when normal traffic consumes around 5,000 RCUs. The Developer must scan the whole table in the shortest possible time whilst ensuring the normal workload is not affected. How would the Developer optimize this scan cost-effectively?","answers":[{"text":"Increase read capacity units during the scan operation.","isCorrect":false},{"text":"Use sequential scans and set the ConsistentRead parameter to false.","isCorrect":false},{"text":"Use sequential scans and apply a FilterExpression.","isCorrect":false},{"text":"Use the Parallel Scan API operation and limit the rate.","isCorrect":true}],"explanation":"$4e"},{"question":"A company needs to store sensitive documents on Amazon S3. The documents should be encrypted in transit using SSL/TLS and then be encrypted for storage at the destination. The company do not want to manage any of the encryption infrastructure or customer master keys and require the most cost-effective solution. What is the MOST suitable option to encrypt the data?","answers":[{"text":"Clientside encryption with Amazon S3 managed keys","isCorrect":false},{"text":"ServerSide Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSEKMS) using customer managed CMKs","isCorrect":false},{"text":"ServerSide Encryption with Amazon S3Managed Keys (SSES3)","isCorrect":true},{"text":"ServerSide Encryption with CustomerProvided Keys (SSEC)","isCorrect":false}],"explanation":"$4f"},{"question":"A CloudFormation template is going to be used by a global team to deploy infrastructure in several regions around the world. Which section of the template file can be used to set values based on a region?","answers":[{"text":"Mappings","isCorrect":true},{"text":"Parameters","isCorrect":false},{"text":"Metadata","isCorrect":false},{"text":"Conditions","isCorrect":false}],"explanation":"$50"},{"question":"A temporary Developer needs to be provided with access to specific resources for a one week period. Which element of an IAM policy statement can be used to allow access only on or before a specific date?","answers":[{"text":"Condition","isCorrect":true},{"text":"NotResource","isCorrect":false},{"text":"Version","isCorrect":false},{"text":"Action","isCorrect":false}],"explanation":"$51"},{"question":"A company has implemented AWS CodePipeline to automate its release pipelines. The Development team is writing an AWS Lambda function that will send notifications for state changes of each of the actions in the stages. Which steps must be taken to associate the Lambda function with the event source?","answers":[{"text":"Create an Amazon CloudWatch Events rule that uses CodePipeline as an event source","isCorrect":true},{"text":"Create an Amazon CloudWatch alarm that monitors status changes in CodePipeline and triggers the Lambda function","isCorrect":false},{"text":"Create a trigger that invokes the Lambda function from the Lambda console by selecting CodePipeline as the event source","isCorrect":false},{"text":"Create an event trigger and specify the Lambda function from the CodePipeline console","isCorrect":false}],"explanation":"$52"},{"question":"A Development team is involved with migrating an on-premises MySQL database to Amazon RDS. The database usage is very read-heavy. The Development team wants refactor the application code to achieve optimum read performance for queries. How can this objective be met?","answers":[{"text":"Add a connection string to use a read replica on an Amazon EC2 instance","isCorrect":false},{"text":"Add database retries to the code and vertically scale the Amazon RDS database","isCorrect":false},{"text":"Use Amazon RDS with a multiAZ deployment","isCorrect":false},{"text":"Add a connection string to use an Amazon RDS read replica for read queries","isCorrect":true}],"explanation":"$53"},{"question":"A gaming company wants to store information about all the games that the company has released. Each game has a name, version number, and category (such as sports, puzzles, strategy, etc). The game information also can include additional properties about the supported platforms and technical specifications. This additional information is inconsistent across games. You have been hired as an AWS Certified Developer Associate to build a solution that addresses the following use cases: For a given name and version number, get all details about the game that has that name and version number. For a given name, get all details about all games that have that name. For a given category, get all details about all games in that category. What will you recommend as the most efficient solution?","answers":[{"text":"Set up an Amazon DynamoDB table with a primary key that consists of the category as the partition key and the version number as the sort key. Create a global secondary index that has the name as the partition key","isCorrect":false},{"text":"Set up an Amazon RDS MySQL instance having a games table that contains columns for name, version number, and category. Configure the name column as the primary key","isCorrect":false},{"text":"Set up an Amazon DynamoDB table with a primary key that consists of the name as the partition key and the version number as the sort key. Create a global secondary index that has the category as the partition key and the name as the sort key","isCorrect":true},{"text":"Permanently store the name, version number, and category information about the games in an Amazon Elasticache for Memcached instance","isCorrect":false}],"explanation":"$54"},{"question":"An ecommerce web application that shares session state on-premises is being migrated to AWS. The application must be fault tolerant, natively highly scalable, and any service interruption should not affect the user experience. What is the best option to store the session state?","answers":[{"text":"Enable session stickiness using elastic load balancers","isCorrect":false},{"text":"Store the session state in Amazon CloudFront","isCorrect":false},{"text":"Store the session state in Amazon ElastiCache","isCorrect":true},{"text":"Store the session state in Amazon S3","isCorrect":false}],"explanation":"$55"},{"question":"A company manages a web application that is deployed on AWS Elastic Beanstalk. A Developer has been instructed to update to a new version of the application code. There is no tolerance for downtime if the update fails and rollback should be fast. What is the SAFEST deployment method to use?","answers":[{"text":"Immutable","isCorrect":true},{"text":"All at once","isCorrect":false},{"text":"Rolling","isCorrect":false},{"text":"Rolling with Additional Batch","isCorrect":false}],"explanation":"$56"},{"question":"An application that runs on an Amazon EC2 instance needs to access and make API calls to multiple AWS services. What is the MOST secure way to provide access to the AWS services with MINIMAL management overhead?","answers":[{"text":"Use AWS root user to make requests to the application","isCorrect":false},{"text":"Use EC2 instance profiles","isCorrect":true},{"text":"Use AWS KMS to store and retrieve credentials","isCorrect":false},{"text":"Store and retrieve credentials from AWS CodeCommit","isCorrect":false}],"explanation":"$57"},{"question":"An application is being deployed on an Amazon EC2 instance running Linux. The EC2 instance will need to manage other AWS services. How can the EC2 instance be configured to make API calls to AWS service securely?","answers":[{"text":"Create an AWS IAM Role, attach a policy with the necessary privileges and attach the role to the instance’s instance profile","isCorrect":true},{"text":"Store the access key ID and secret access key as encrypted AWS Lambda environment variables and invoke Lambda for each API call","isCorrect":false},{"text":"Run the “aws configure” AWS CLI command and specify the access key ID and secret access key","isCorrect":false},{"text":"Store a users’ console login credentials in the application code so the application can call AWS STS and gain temporary security credentials","isCorrect":false}],"explanation":"$58"},{"question":"A company is developing a new online game that will run on top of Amazon ECS. Four distinct Amazon ECS services will be part of the architecture, each requiring specific permissions to various AWS services. The company wants to optimize the use of the underlying Amazon EC2 instances by bin packing the containers based on memory reservation. Which configuration would allow the Development team to meet these requirements MOST securely?","answers":[{"text":"Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then, create an IAM group and configure the ECS cluster to reference that group","isCorrect":false},{"text":"Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then configure each ECS service to reference the associated IAM role","isCorrect":false},{"text":"Create a new Identity and Access Management (IAM) instance profile containing the required permissions for the various ECS services, then associate that instance role with the underlying EC2 instances","isCorrect":false},{"text":"Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then configure each ECS task definition to reference the associated IAM role","isCorrect":true}],"explanation":"$59"},{"question":"A company is creating a gaming application that will be deployed on mobile devices. The application will send data to a Lambda functionbased RESTful API. The application will assign each API request a unique identifier. The volume of API requests from the application can randomly vary at any given time of day. During request throttling, the application might need to retry requests. The API must be able to address duplicate requests without inconsistencies or data loss. Which of the following would you recommend to handle these requirements?","answers":[{"text":"Persist the unique identifier for each request in an RDS MySQL table. Change the Lambda function to check the table for the identifier before processing the request","isCorrect":false},{"text":"Persist the unique identifier for each request in a DynamoDB table. Change the Lambda function to send a client error response when the function receives a duplicate request","isCorrect":false},{"text":"Persist the unique identifier for each request in a DynamoDB table. Change the Lambda function to check the table for the identifier before processing the request","isCorrect":true},{"text":"Persist the unique identifier for each request in an ElastiCache for Memcached cache. Change the Lambda function to check the cache for the identifier before processing the request","isCorrect":false}],"explanation":"$5a"},{"question":"An XRay daemon is being used on an Amazon ECS cluster to assist with debugging stability issues. A developer requires more detailed timing information and data related to downstream calls to AWS services. What should the developer use to obtain this extra detail?","answers":[{"text":"Annotations","isCorrect":false},{"text":"Subsegments","isCorrect":true},{"text":"Filter expressions","isCorrect":false},{"text":"Metadata","isCorrect":false}],"explanation":"$5b"},{"question":"A Development team need to push an update to an application that is running on AWS Elastic Beanstalk. The business SLA states that the application must maintain full performance capabilities during updates whilst minimizing cost. Which Elastic Beanstalk deployment policy should the development team select?","answers":[{"text":"Immutable","isCorrect":false},{"text":"Rolling with additional batch","isCorrect":true},{"text":"Rolling","isCorrect":false},{"text":"All at once","isCorrect":false}],"explanation":"$5c"},{"question":"An organization has an account for each environment: Production, Testing, Development. A Developer with an IAM user in the Development account needs to launch resources in the Production and Testing accounts. What is the MOST efficient way to provide access?","answers":[{"text":"Create an IAM group in the Production and Testing accounts and add the Developer’s user from the Development account to the groups","isCorrect":false},{"text":"Create a role with the required permissions in the Production and Testing accounts and have the Developer assume that role","isCorrect":true},{"text":"Create an IAM permissions policy in the Production and Testing accounts and reference the IAM user in the Development account","isCorrect":false},{"text":"Create a separate IAM user in each account and have the Developer login separately to each account","isCorrect":false}],"explanation":"$5d"},{"question":"An application serves customers in several different geographical regions. Information about the location users connect from is written to logs stored in Amazon CloudWatch Logs. The company needs to publish an Amazon CloudWatch custom metric that tracks connections for each location. Which approach will meet these requirements?","answers":[{"text":".Configure a CloudWatch Events rule that creates a custom metric from the CloudWatch Logs group.","isCorrect":false},{"text":"Create a CloudWatch metric filter to extract metrics from the log files with location as a dimension.","isCorrect":true},{"text":"Stream data to an Amazon Elasticsearch cluster in nearreal time and export a custom metric.","isCorrect":false},{"text":"Create a CloudWatch Logs Insights query to extract the location information from the logs and to create a custom metric with location as a dimension..","isCorrect":false}],"explanation":"Create a CloudWatch Logs Insights query to extract the location information from the logs and to create a custommetric with location as a dimension."},{"question":"A Developer is creating a new web application that will be deployed using AWS Elastic Beanstalk from the AWS Management Console. The Developer is about to create a source bundle which will be uploaded using the console. Which of the following are valid requirements for creating the source bundle? (Select TWO.)","answers":[{"text":"Must include a parent folder or toplevel directory.","isCorrect":false},{"text":"Must include the cron.yaml file.","isCorrect":false},{"text":"Must consist of one or more ZIP files.","isCorrect":false},{"text":"Must not exceed 512 MB.","isCorrect":true},{"text":"Must not include a parent folder or toplevel directory.","isCorrect":true}],"explanation":"$5e"},{"question":"As an AWS Certified Developer Associate, you have been asked to create an AWS Elastic Beanstalk environment to handle deployment for an application that has high traffic and high availability needs. You need to deploy the new version using Beanstalk while making sure that performance and availability are not affected. Which of the following is the MOST optimal way to do this while keeping the solution costeffective?","answers":[{"text":"Deploy using 'Rolling' deployment policy","isCorrect":false},{"text":"Deploy using 'Rolling with additional batch' deployment policy","isCorrect":true},{"text":"Deploy using 'Immutable' deployment policy","isCorrect":false},{"text":"Deploy using 'All at once' deployment policy","isCorrect":false}],"explanation":"$5f"},{"question":"A developer created an operational dashboard for a serverless application using Amazon API Gateway, AWS Lambda, Amazon S3, and Amazon DynamoDB. Users will connect to the dashboard from a variety of mobile applications, desktops and tablets.The developer needs an authentication mechanism that can allow users to signin and will remember the devices users sign in from and suppress the second factor of authentication for remembered devices. Which AWS service should the developer use to support this scenario?","answers":[{"text":"AWS KMS","isCorrect":false},{"text":"Amazon Cognito","isCorrect":true},{"text":"Amazon IAM","isCorrect":false},{"text":"AWS Directory Service","isCorrect":false}],"explanation":"$60"},{"question":"A Developer is setting up a code update to Amazon ECS using AWS CodeDeploy. The Developer needs to complete the code update quickly. Which of the following deployment types should the Developer use?","answers":[{"text":"Linear","isCorrect":false},{"text":"Canary","isCorrect":false},{"text":"Inplace","isCorrect":false},{"text":"Blue/green","isCorrect":true}],"explanation":"$61"},{"question":"A Developer needs to manage AWS services from a local development server using the AWS CLI. How can the Developer ensure that the CLI uses their IAM permissions?","answers":[{"text":"Run the aws configure command and provide the Developer’s IAM access key ID and secret access key","isCorrect":true},{"text":"Put the Developer’s IAM user account in an IAM group that has the necessary permissions","isCorrect":false},{"text":"Save the Developer’s IAM login credentials as environment variables and reference them when executing AWS CLI commands","isCorrect":false},{"text":"Create an IAM Role with the required permissions and attach it to the local server’s instance profile","isCorrect":false}],"explanation":"$62"},{"question":"An application needs to generate SMS text messages and emails for a large number of subscribers. Which AWS service can be used to send these messages to customers?","answers":[{"text":"Amazon SNS","isCorrect":true},{"text":"Amazon SQS","isCorrect":false},{"text":"Amazon SES","isCorrect":false},{"text":"Amazon SWF","isCorrect":false}],"explanation":"$63"},{"question":"A company maintains a REST API service using Amazon API Gateway with native API key validation. The company recently launched a new registration page, which allows users to sign up for the service. The registration page creates a new API key using CreateApiKey and sends the new key to the user. When the user attempts to call the API using this key, the user receives a 403 Forbidden error. Existing users are unaffected and can still call the API. What code updates will grant these new users' access to the API?","answers":[{"text":"The updateAuthorizer method must be called to update the API’s authorizer to include the newly created API key","isCorrect":false},{"text":"The createDeployment method must be called so the API can be redeployed to include the newly created API key","isCorrect":false},{"text":"The importApiKeys method must be called to import all newly created API keys into the current stage of the API","isCorrect":false},{"text":"The createUsagePlanKey method must be called to associate the newly created API key with the correct usage plan","isCorrect":true}],"explanation":"$64"},{"question":"A serverless application uses an IAM role to authenticate and authorize access to an Amazon DynamoDB table. A Developer is troubleshooting access issues affecting the application. The Developer has access to the IAM role that the application is using. Which of the following commands will help the Developer to test the role permissions using the AWS CLI?","answers":[{"text":"aws sts getsessiontoken","isCorrect":false},{"text":"aws iam getrolepolicy","isCorrect":false},{"text":"aws sts assumerole","isCorrect":true},{"text":"aws dynamodb describeendpoints","isCorrect":false}],"explanation":"The AWS CLI “aws sts assume role” command will enable the Developer to assume the role and gain temporary security credentials. The Developer can then use those security credentials to troubleshoot access issues that are affecting the application.CORRECT: \"aws sts assumerole\" is the correct answer.INCORRECT: \"aws sts getsessiontoken\" is incorrect. This is used to get temporary credentials for an AWS account or IAM user. It can subsequently be used to call the assumerole API.INCORRECT: \"aws iam getrolepolicy\" is incorrect. This command retrieves the specified inline policy document that is embedded with the specified IAM role.INCORRECT: \"aws dynamodb describeendpoints\" is incorrect. This command returns the regional endpoint information.References: https://docs.aws.amazon.com/cli/latest/reference/sts/assumerole.html"},{"question":"You are storing bids information on your betting application and you would like to automatically expire DynamoDB table data after one week. What should you use?","answers":[{"text":"Use TTL","isCorrect":true},{"text":"Use DAX","isCorrect":false},{"text":"Use DynamoDB Streams","isCorrect":false},{"text":"Use a Lambda function","isCorrect":false}],"explanation":"$65"},{"question":"A Developer is attempting to call the Amazon CloudWatch API and is receiving HTTP 400: ThrottlingException errors intermittently. When a call fails, no data is retrieved. What best practice should the Developer first attempt to resolve this issue?","answers":[{"text":"Contact AWS Support for a limit increase","isCorrect":false},{"text":"Use the AWS CLI to get the metrics","isCorrect":false},{"text":"Analyze the applications and remove the API call","isCorrect":false},{"text":"Retry the call with exponential backoff","isCorrect":true}],"explanation":"$66"},{"question":"An application is hosted in AWS Elastic Beanstalk and is connected to a database running on Amazon RDS MySQL. A Developer needs to instrument the application to trace database queries and calls to downstream services using AWS XRay. How can the Developer enable tracing for the application?","answers":[{"text":"Enable active tracing in the Elastic Beanstalk console","isCorrect":false},{"text":"Enable XRay tracing using an AWS Lambda function","isCorrect":false},{"text":"Add a xraydaemon.config file to the root of the source code to enable the XRay deamon","isCorrect":false},{"text":"Add a .ebextensions/xraydaemon.config file to the source code to enable the XRay daemon","isCorrect":true}],"explanation":"$67"},{"question":"A Developer is deploying an AWS Lambda update using AWS CodeDeploy. In the appspec.yaml file, which of the following is a valid structure for the order of hooks that should be specified?","answers":[{"text":"BeforeInstall > AfterInstall > AfterAllowTestTraffic > BeforeAllowTraffic > AfterAllowTraffic","isCorrect":false},{"text":"BeforeAllowTraffic > AfterAllowTraffic","isCorrect":true},{"text":"BeforeBlockTraffic > AfterBlockTraffic > BeforeAllowTraffic > AfterAllowTraffic","isCorrect":false},{"text":"BeforeInstall > AfterInstall > ApplicationStart > ValidateService","isCorrect":false}],"explanation":"$68"},{"question":"As part of his development work, an AWS Certified Developer Associate is creating policies and attaching them to IAM identities. After creating necessary Identitybased policies, he is now creating Resourcebased policies. Which is the only resourcebased policy that the IAM service supports?","answers":[{"text":"AWS Organizations Service Control Policies (SCP)","isCorrect":false},{"text":"Trust policy","isCorrect":true},{"text":"Permissions boundary","isCorrect":false},{"text":"Access control list (ACL)","isCorrect":false}],"explanation":"$69"},{"question":"Users of an application using Amazon API Gateway, AWS Lambda and Amazon DynamoDB have reported errors when using the application. Which metrics should a Developer monitor in Amazon CloudWatch to determine the number of clientside and serverside errors?","answers":[{"text":"CacheHitCount and CacheMissCount","isCorrect":false},{"text":"IntegrationLatency and Latency","isCorrect":false},{"text":"4XXError and 5XXError","isCorrect":true},{"text":"Errors","isCorrect":false}],"explanation":"To determine the number of clientside errors captured in a given period the Developer should look at the 4XXError metric. To determine the number of serverside errors captured in a given period the Developer should look at the 5XXError.CORRECT: \"4XXError and 5XXError\" is the correct answer.INCORRECT: \"CacheHitCount and CacheMissCount\" is incorrect as these count the number of requests served from the cache and the number of requests served from the backend.INCORRECT: \"IntegrationLatency and Latency\" is incorrect as these measure the amount of time between when API Gateway relays a request to the backend and when it receives a response from the backend and the time between when API Gateway receives a request from a client and when it returns a response to the client.INCORRECT: \"Errors\" is incorrect as this is not a metric related to Amazon API Gateway.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaymetricsanddimensions.html"},{"question":"A company is using Amazon RDS MySQL instances for its application database tier and apache Tomcat servers for its web tier. Most of the database queries from web applications are repeated read requests.A Developer plans to add an inmemory store to improve performance for repeated read requests. Which AWS service would BEST fit these requirements?","answers":[{"text":"Amazon SQS","isCorrect":false},{"text":"Amazon RDS MultiAZ","isCorrect":false},{"text":"Amazon RDS read replica","isCorrect":false},{"text":"Amazon ElastiCache","isCorrect":true}],"explanation":"There are two options that can assist with improving performance for read requests: Amazon RDS read replicas, and Amazon ElastiCache. Both of these solutions will provide horizontal scaling for read requests to reduce the impact on the main database.However, only Amazon ElastiCache is an inmemory database so the best solution is for the Developer to use Amazon ElastiCache to improve performance for repeated read requests.CORRECT: \"Amazon ElastiCache\" is the correct answer.INCORRECT: \"Amazon RDS MultiAZ\" is incorrect as multiAZ is used for faulttolerance and disaster recovery, not improving read performance.INCORRECT: \"Amazon SQS\" is incorrect as SQS is a hosted message queue use for decoupling.INCORRECT: \"Amazon RDS read replica\" is incorrect as this is not an inmemory store.References: https://aws.amazon.com/elasticache/"},{"question":"A Developer created a new AWS account and must create a scalable AWS Lambda function that meets the following requirements for concurrent execution:• Average execution time of 100 seconds• 50 requests per secondWhich step must be taken prior to deployment to prevent errors?","answers":[{"text":"Implement deadletter queues to capture invocation errors","isCorrect":false},{"text":"Contact AWS Support to increase the concurrent execution limits","isCorrect":true},{"text":"Add an event source from Amazon API Gateway to the Lambda function","isCorrect":false},{"text":"Implement error handling within the application code","isCorrect":false}],"explanation":"$6a"},{"question":"A company is deploying a static website hosted from an Amazon S3 bucket. The website must support encryption in transit for website visitors. Which combination of actions must the Developer take to meet this requirement? (Select TWO.)","answers":[{"text":"Configure an Amazon CloudFront distribution with an SSL/TLS certificate.","isCorrect":true},{"text":"Configure an Amazon CloudFront distribution with an AWS WAF WebACL.","isCorrect":false},{"text":"Create an Amazon CloudFront distribution. Set the S3 bucket as an origin.","isCorrect":true},{"text":"Create an AWS WAF WebACL with a secure listener.","isCorrect":false},{"text":"Configure the S3 bucket with an SSL/TLS certificate.","isCorrect":false}],"explanation":"$6b"},{"question":"A Developer is creating a web application that will be used by employees working from home. The company uses a SAML directory on-premises for storing user information. The Developer must integrate with the SAML directory and authorize each employee to access only their own data when using the application. Which approach should the Developer take?","answers":[{"text":"Create a unique IAM role for each employee and have each employee assume the role to access the application so they can access their personal data only.","isCorrect":false},{"text":"Use an Amazon Cognito identity pool, federate with the SAML provider, and use a trust policy with an IAM condition key to limit employee access.","isCorrect":true},{"text":"Create the application within an Amazon VPC and use a VPC endpoint with a trust policy to grant access to the employees.","isCorrect":false},{"text":"Use Amazon Cognito user pools, federate with the SAML provider, and use user pool groups with an IAM policy.","isCorrect":false}],"explanation":"$6c"},{"question":"A new application will be hosted on the domain name dctlabs.com using an Amazon API Gateway REST API front end. The Developer needs to configure the API with a path to dctlabs.com/products that will be accessed using the HTTP GET verb. How MUST the Developer configure the API? (Select TWO.)","answers":[{"text":"Create a GET resource","isCorrect":false},{"text":"Create a /GET method","isCorrect":false},{"text":"Create a /products method","isCorrect":false},{"text":"Create a /products resource","isCorrect":true},{"text":"Create a GET method","isCorrect":true}],"explanation":"$6d"},{"question":"A developer has been asked to create an application that can be deployed across a fleet of EC2 instances. The configuration must allow for full control over the deployment steps using the bluegreen deployment. Which service will help you achieve that?","answers":[{"text":"CodePipeline","isCorrect":false},{"text":"CodeBuild","isCorrect":false},{"text":"CodeDeploy","isCorrect":true},{"text":"Elastic Beanstalk","isCorrect":false}],"explanation":"$6e"},{"question":"A team of Developers are working on a shared project and need to be able to collaborate on code. The shared application code must be encrypted at rest, stored on a highly available and durable architecture, and support multiple versions and batch change tracking. Which AWS service should the Developer use?","answers":[{"text":"AWS CodeBuild","isCorrect":false},{"text":"AWS CodeCommit","isCorrect":true},{"text":"Amazon S3","isCorrect":false},{"text":"AWS Cloud9","isCorrect":false}],"explanation":"$6f"},{"question":"A Developer is creating a serverless application that uses an Amazon DynamoDB table. The application must make idempotent, all-or-nothing operations for multiple groups of write actions. Which solution will meet these requirements?","answers":[{"text":"Update the items in the table using the BatchWriteltem operation and configure idempotency at the table level.","isCorrect":false},{"text":"Enable DynamoDB streams and capture new images. Update the items in the table using the BatchWriteltem.","isCorrect":false},{"text":"Update the items in the table using the TransactWriteltems operation to group the changes.","isCorrect":true},{"text":"Create an Amazon SQS FIFO queue and use the SendMessageBatch operation to group the changes.","isCorrect":false}],"explanation":"$70"},{"question":"A Developer is creating a serverless application that will process sensitive data. The AWS Lambda function must encrypt all data that is written to /tmp storage at rest. How should the Developer encrypt this data?","answers":[{"text":"Attach the Lambda function to a VPC and encrypt Amazon EBS volumes at rest using the AWS managed CMK. Mount the EBS volume to /tmp.","isCorrect":false},{"text":"Enable default encryption on an Amazon S3 bucket using an AWS KMS customer managed customer master key (CMK). Mount the S3 bucket to /tmp.","isCorrect":false},{"text":"Configure Lambda to use an AWS KMS customer managed customer master key (CMK). Use the CMK to generate a data key and encrypt all data prior to writing to /tmp storage.","isCorrect":true},{"text":"Enable secure connections over HTTPS for the AWS Lambda API endpoints using Transport Layer Security (TLS).","isCorrect":false}],"explanation":"$71"},{"question":"A development team wants to build an application using serverless architecture. The team plans to use AWS Lambda functions extensively to achieve this goal. The developers of the team work on different programming languages like Python, .NET and Javascript. The team wants to model the cloud infrastructure using any of these programming languages. Which AWS service/tool should the team use for the given usecase?","answers":[{"text":"AWS Serverless Application Model (SAM)","isCorrect":false},{"text":"AWS Cloud Development Kit (CDK)","isCorrect":true},{"text":"AWS CloudFormation","isCorrect":false},{"text":"AWS CodeDeploy","isCorrect":false}],"explanation":"$72"},{"question":"A developer has created an Amazon API Gateway with caching enabled in front of AWS Lambda. For some requests, it is necessary to ensure the latest data is received from the endpoint. How can the developer ensure the data is not stale?","answers":[{"text":"Send requests with the CacheDelete: maxage=0 header","isCorrect":false},{"text":"Modify the TTL on the cache to a lower number","isCorrect":false},{"text":"The cache must be disabled","isCorrect":false},{"text":"Send requests with the CacheControl: maxage=0 header","isCorrect":true}],"explanation":"$73"},{"question":"A SaaS company runs a HealthCare web application that is used worldwide by users. There have been requests by mobile developers to expose public APIs for the applicationspecific functionality. You decide to make the APIs available to mobile developers as product offerings. Which of the following options will allow you to do that?","answers":[{"text":"Use AWS Billing Usage Plans","isCorrect":false},{"text":"Use CloudFront Usage Plans","isCorrect":false},{"text":"Use API Gateway Usage Plans","isCorrect":true},{"text":"Use AWS Lambda Custom Authorizers","isCorrect":false}],"explanation":"$74"},{"question":"A company will be uploading several terabytes of data to Amazon S3. What is the SIMPLEST solution to ensure that the data is encrypted before it is sent to S3 and whilst in transit?","answers":[{"text":"Use serverside encryption with S3 managed keys and SSL","isCorrect":false},{"text":"Use serverside encryption with client provided keys","isCorrect":false},{"text":"Use clientside encryption and a hardware VPN to a VPC and an S3 endpoint","isCorrect":false},{"text":"Use clientside encryption with a KMS managed CMK and SSL","isCorrect":true}],"explanation":"$75"},{"question":"As an AWS Certified Developer Associate, you have configured the AWS CLI on your workstation. Your default region is useast1 and your IAM user has permissions to operate commands on services such as EC2, S3 and RDS in any region. You would like to execute a command to stop an EC2 instance in the useast2 region. What of the following is the MOST optimal solution to address this usecase?","answers":[{"text":"Use the region parameter","isCorrect":true},{"text":"You need to override the default region by using aws configure","isCorrect":false},{"text":"You should create a new IAM user just for that other region","isCorrect":false},{"text":"Use boto3 dependency injection","isCorrect":false}],"explanation":"Correct option:Use the region parameter: If the region parameter is not set, then the CLI command is executed against the default AWS region.You can also review all general options for AWS CLI: via https://docs.aws.amazon.com/cli/latest/topic/configvars.html#generaloptionsIncorrect options:You need to override the default region by using aws configure This is not the most optimal way as you will have to change it again to reset the default region.You should create a new IAM user just for that other region This is not the most optimal way as you would need to manage two IAM user profiles.Use boto3 dependency injection With the CLI you do not use boto3. This option is a distractor.Reference:https://docs.aws.amazon.com/cli/latest/topic/configvars.html#generaloptions"},{"question":"You have chosen AWS Elastic Beanstalk to upload your application code and allow it to handle details such as provisioning resources and monitoring. When creating configuration files for AWS Elastic Beanstalk which naming convention should you follow?","answers":[{"text":".config/.ebextensions","isCorrect":false},{"text":".config_.ebextensions","isCorrect":false},{"text":".ebextensions/.config","isCorrect":true},{"text":".ebextensions_.config","isCorrect":false}],"explanation":"Correct option:.ebextensions/.config : You can add AWS Elastic Beanstalk configuration files (.ebextensions) to your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML or JSON formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle.via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html Incorrect options:.ebextensions_.config.config/.ebextensions.config_.ebextensionsThese three options contradict the explanation provided earlier. So these are incorrect.Reference: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html"},{"question":"CodeCommit is a managed version control service that hosts private Git repositories in the AWS cloud. Which of the following credential types is NOT supported by IAM for CodeCommit?","answers":[{"text":"SSH Keys","isCorrect":false},{"text":"AWS Access Keys","isCorrect":false},{"text":"Git credentials","isCorrect":false},{"text":"IAM username and password","isCorrect":true}],"explanation":"Correct option:IAM username and password IAM username and password credentials cannot be used to access CodeCommit.Incorrect options:Git credentials These are IAM generated user name and password pair you can use to communicate with CodeCommit repositories over HTTPS.SSH Keys Are locally generated publicprivate key pair that you can associate with your IAM user to communicate with CodeCommit repositories over SSH.AWS access keys You can use these keys with the credential helper included with the AWS CLI to communicate with CodeCommit repositories over HTTPS.Reference:https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_sshkeys.html"},{"question":"A company uses Elastic Beanstalk to manage its IT infrastructure on AWS Cloud and it would like to deploy the new application version to the EC2 instances. When the deployment is executed, some instances should serve requests with the old application version, while other instances should serve requests using the new application version until the deployment is completed. Which deployment meets this requirement without incurring additional costs?","answers":[{"text":"All at once","isCorrect":false},{"text":"Immutable","isCorrect":false},{"text":"Rolling","isCorrect":true},{"text":"Rolling with additional batches","isCorrect":false}],"explanation":"$76"},{"question":"A Development team would use a GitHub repository and would like to migrate their application code to AWS CodeCommit. What needs to be created before they can migrate a cloned repository to CodeCommit over HTTPS?","answers":[{"text":"A public and private SSH key file","isCorrect":false},{"text":"A set of Git credentials generated with IAM","isCorrect":true},{"text":"An Amazon EC2 IAM role with CodeCommit permissions","isCorrect":false},{"text":"A GitHub secure authentication token","isCorrect":false}],"explanation":"$77"},{"question":"A company has an application that logs all information to Amazon S3. Whenever there is a new log file, an AWS Lambda function is invoked to process the log files. The code works, gathering all of the necessary information. However, when checking the Lambda function logs, duplicate entries with the same request ID are found. What is the BEST explanation for the duplicate entries?","answers":[{"text":"The Lambda function failed, and the Lambda service retried the invocation with a delay","isCorrect":true},{"text":"There was an S3 outage, which caused duplicate entries of the same log file","isCorrect":false},{"text":"The application stopped intermittently and then resumed","isCorrect":false},{"text":"The S3 bucket name was specified incorrectly","isCorrect":false}],"explanation":"$78"},{"question":"An application requires an inmemory caching engine. The cache should provide high availability as repopulating data is expensive. How can this requirement be met?","answers":[{"text":"Amazon Aurora with a Global Database","isCorrect":false},{"text":"Use Amazon ElastiCache Memcached with partitions","isCorrect":false},{"text":"Amazon RDS with a Read Replica","isCorrect":false},{"text":"Use Amazon ElastiCache Redis with replicas","isCorrect":true}],"explanation":"$79"},{"question":"You are creating a Cloud Formation template to deploy your CMS application running on an EC2 instance within your AWS account. Since the application will be deployed across multiple regions, you need to create a map of all the possible values for the base AMI. How will you invoke the !FindInMap function to fulfill this use case?","answers":[{"text":"!FindInMap [ MapName, TopLevelKey, SecondLevelKey ]","isCorrect":true},{"text":"!FindInMap [ MapName ]","isCorrect":false},{"text":"!FindInMap [ MapName, TopLevelKey ]","isCorrect":false},{"text":"!FindInMap [ MapName, TopLevelKey, SecondLevelKey, ThirdLevelKey ]","isCorrect":false}],"explanation":"$7a"},{"question":"A Developer is deploying an application in a microservices architecture on Amazon ECS. The Developer needs to choose the best task placement strategy to MINIMIZE the number of instances that are used. Which task placement strategy should be used?","answers":[{"text":"weighted","isCorrect":false},{"text":"spread","isCorrect":false},{"text":"random","isCorrect":false},{"text":"binpack","isCorrect":true}],"explanation":"$7b"},{"question":"A Developer has recently created an application that uses an AWS Lambda function, an Amazon DynamoDB table, and also sends notifications using Amazon SNS. The application is not working as expected and the Developer needs to analyze what is happening across all components of the application. What is the BEST way to analyze the issue?","answers":[{"text":"Enable XRay tracing for the Lambda function","isCorrect":true},{"text":"Create an Amazon CloudWatch Events rule","isCorrect":false},{"text":"Assess the application with Amazon Inspector","isCorrect":false},{"text":"Monitor the application with AWS Trusted Advisor","isCorrect":false}],"explanation":"$7c"},{"question":"A company runs many microservices applications that use Docker containers. The company are planning to migrate the containers to Amazon ECS. The workloads are highly variable and therefore the company prefers to be charged per running task. Which solution is the BEST fit for the company's requirements?","answers":[{"text":"An Amazon ECS Service with Auto Scaling","isCorrect":false},{"text":"Amazon ECS with the Fargate launch type","isCorrect":true},{"text":"Amazon ECS with the EC2 launch type","isCorrect":false},{"text":"An Amazon ECS Cluster with Auto Scaling","isCorrect":false}],"explanation":"$7d"},{"question":"After a test deployment in ElasticBeanstalk environment, a developer noticed that all accumulated Amazon EC2 burst balances were lost. Which of the following options can lead to this behavior?","answers":[{"text":"When a canary deployment fails, it resets the EC2 burst balances to zero","isCorrect":false},{"text":"The deployment was either run with immutable updates or in traffic splitting mode","isCorrect":true},{"text":"The deployment was run as a Rolling deployment, resulting in the resetting of EC2 burst balances","isCorrect":false},{"text":"The deployment was run as a Allatonce deployment, flushing all the accumulated EC2 burst balances","isCorrect":false}],"explanation":"$7e"},{"question":"A startup with newly created AWS account is testing different EC2 instances. They have used Burstable performance instance T2.micro for 35 seconds and stopped the instance. At the end of the month, what is the instance usage duration that the company is charged for?","answers":[{"text":"0 seconds","isCorrect":true},{"text":"35 seconds","isCorrect":false},{"text":"60 seconds","isCorrect":false},{"text":"30 seconds","isCorrect":false}],"explanation":"Correct option:Burstable performance instances, which are T3, T3a, and T2 instances, are designed to provide a baseline level of CPU performance with the ability to burst to a higher level when required by your workload. Burstable performance instances are the only instance types that use credits for CPU usage.0 seconds AWS states that, if your AWS account is less than 12 months old, you can use a t2.micro instance for free within certain usage limits.Incorrect options:35 seconds60 seconds30 secondsThese three options contradict the explanation provided earlier, so these are incorrect.Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstableperformanceinstances.html"},{"question":"A Developer is writing code to run in a cron job on an Amazon EC2 instance that sends status information about the application to Amazon CloudWatch. Which method should the Developer use?","answers":[{"text":"Use the AWS CLI putmetricdata command.","isCorrect":true},{"text":"Use the AWS CLI putmetricalarm command.","isCorrect":false},{"text":"Use the CloudWatch console with detailed monitoring.","isCorrect":false},{"text":"Use the unified CloudWatch agent to publish custom metrics.","isCorrect":false}],"explanation":"$7f"},{"question":"A developer is troubleshooting problems with a Lambda function that is invoked by Amazon SNS and repeatedly fails. How can the developer save discarded events for further processing?","answers":[{"text":"Configure a Dead Letter Queue (DLQ)","isCorrect":true},{"text":"Enable CloudWatch Logs for the Lambda function","isCorrect":false},{"text":"Enable SNS notifications for failed events","isCorrect":false},{"text":"Enable Lambda streams","isCorrect":false}],"explanation":"$80"},{"question":"As a developer, you are working on creating an application using AWS Cloud Development Kit (CDK). Which of the following represents the correct order of steps to be followed for creating an app using AWS CDK?","answers":[{"text":"Create the app from a template provided by AWS CloudFormation > Add code to the app to create resources within stacks > Build the app (optional) > Synthesize one or more stacks in the app > Deploy stack(s) to your AWS account","isCorrect":false},{"text":"Create the app from a template provided by AWS CDK > Add code to the app to create resources within stacks > Synthesize one or more stacks in the app > Deploy stack(s) to your AWS account > Build the app","isCorrect":false},{"text":"Create the app from a template provided by AWS CDK > Add code to the app to create resources within stacks > Build the app (optional) > Synthesize one or more stacks in the app > Deploy stack(s) to your AWS account","isCorrect":true},{"text":"Create the app from a template provided by AWS CloudFormation > Add code to the app to create resources within stacks > Synthesize one or more stacks in the app > Deploy stack(s) to your AWS account > Build the app","isCorrect":false}],"explanation":"$81"},{"question":"A company runs multiple microservices that each use their own Amazon DynamoDB table. The “customers” microservice needs data that originates in the “orders” microservice. What approach represents the SIMPLEST method for the “customers” table to get near real-time updates from the “orders” table?","answers":[{"text":"Use Amazon Kinesis Firehose to deliver all changes in the “orders” table to the “customers” table","isCorrect":false},{"text":"Enable Amazon DynamoDB streams on the “orders” table, configure the “customers” microservice to read records from the stream","isCorrect":true},{"text":"Use Amazon CloudWatch Events to send notifications every time an item is added or modified in the “orders” table","isCorrect":false},{"text":"Enable DynamoDB streams for the “customers” table, trigger an AWS Lambda function to read records from the stream and write them to the “orders” table","isCorrect":false}],"explanation":"$82"},{"question":"A Developer is writing an imaging microservice on AWS Lambda. The service is dependent on several libraries that are not available in the Lambda runtime environment. Which strategy should the Developer follow to create the Lambda deployment package?","answers":[{"text":"Create a ZIP file with the source code and all dependent libraries","isCorrect":true},{"text":"Create a ZIP file with the source code and a script that installs the dependent libraries at runtime","isCorrect":false},{"text":"Create a ZIP file with the source code and a buildspec.yml file that installs the dependent libraries on AWS Lambda","isCorrect":false},{"text":"Create a ZIP file with the source code and an appspec.yml file. Add the libraries to the appspec.yml file and upload to Amazon S3. Deploy using CloudFormation","isCorrect":false}],"explanation":"$83"},{"question":"The Technical Lead of your team has reviewed a CloudFormation YAML template written by a new recruit and specified that an invalid section has been added to the template. Which of the following represents an invalid section of the CloudFormation template?","answers":[{"text":"Conditions' section of the template","isCorrect":false},{"text":"Dependencies' section of the template","isCorrect":true},{"text":"Parameters' section of the template","isCorrect":false},{"text":"Resources' section of the template","isCorrect":false}],"explanation":"$84"},{"question":"An application uses AWS Lambda which makes remote calls to several downstream services. A developer wishes to add data to custom subsegments in AWS XRay that can be used with filter expressions. Which type of data should be used?","answers":[{"text":"Annotations","isCorrect":true},{"text":"Trace ID","isCorrect":false},{"text":"Daemon","isCorrect":false},{"text":"Metadata","isCorrect":false}],"explanation":"$85"},{"question":"A company is using Amazon API Gateway to manage access to a set of microservices implemented as AWS Lambda functions. The company has made some minor changes to one of the APIs. The company wishes to give existing customers using the API up to 6 months to migrate from version 1 to version 2. What approach should a Developer use to implement the change?","answers":[{"text":"Use API Gateway to deploy a new stage named v2 to the API and provide users with its URL","isCorrect":true},{"text":"Use API Gateway to automatically propagate the change to clients, specifying 180 days in the phased deployment parameter","isCorrect":false},{"text":"Update the underlying Lambda function and provide clients with the new Lambda invocation URL","isCorrect":false},{"text":"Update the underlying Lambda function, create an Amazon CloudFront distribution with the updated Lambda function as its origin","isCorrect":false}],"explanation":"$86"}]}}]
a:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
7:null
8:null
c:[["$","title","0",{"children":"v0 App"}],["$","meta","1",{"name":"description","content":"Created with v0"}],["$","meta","2",{"name":"generator","content":"v0.app"}]]
