1:"$Sreact.fragment"
2:I[7555,[],""]
3:I[1295,[],""]
4:I[9742,["177","static/chunks/app/layout-7d8a5f63f536cdcf.js"],"Analytics"]
6:I[9665,[],"OutletBoundary"]
9:I[9665,[],"ViewportBoundary"]
b:I[9665,[],"MetadataBoundary"]
d:I[6614,[],""]
:HL["/dva-c02-quiz-app/_next/static/css/31d717072bdb6d85.css","style"]
:HL["/dva-c02-quiz-app/_next/static/css/93628e14bbee05a5.css","style"]
0:{"P":null,"b":"yCqRTNLEPBdkYbRRbuCZd","p":"/dva-c02-quiz-app","c":["","quiz","aws-developer-6",""],"i":false,"f":[[["",{"children":["quiz",{"children":[["id","aws-developer-6","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/dva-c02-quiz-app/_next/static/css/31d717072bdb6d85.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/dva-c02-quiz-app/_next/static/css/93628e14bbee05a5.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"font-sans __variable_1f39b6 __variable_c20681","children":[["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}],["$","$L4",null,{}]]}]}]]}],{"children":["quiz",["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["id","aws-developer-6","d"],["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L5","$undefined",null,["$","$L6",null,{"children":["$L7","$L8",null]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","dDDmRkvnq6OyK-q4JxN8o",{"children":[["$","$L9",null,{"children":"$La"}],null]}],["$","$Lb",null,{"children":"$Lc"}]]}],false]],"m":"$undefined","G":["$d","$undefined"],"s":false,"S":true}
e:I[1184,["261","static/chunks/261-2d9b76ccba401937.js","200","static/chunks/app/quiz/%5Bid%5D/page-d7cb26530e4508b3.js"],"default"]
f:T68b,Correct option:Amazon SQS is highly scalable and does not need any intervention to handle the expected high volumesAmazon SQS leverages the AWS cloud to dynamically scale, based on demand. SQS scales elastically with your application so you don't have to worry about capacity planning and preprovisioning. For most standard queues (depending on queue traffic and message backlog), there can be a maximum of approximately 120,000 inflight messages (received from a queue by a consumer, but not yet deleted from the queue).Info on Queue Quotas: via https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsquotas.html Incorrect options:Preconfigure the SQS queue to increase the capacity when messages hit a certain threshold This is an incorrect statement. Amazon SQS scales dynamically, automatically provisioning the needed capacity.Enable autoscaling in the SQS queue SQS queues are, by definition, autoscalable and do not need any configuration changes for autoscaling.Convert the queue into FIFO ordered queue, since messages to the down system will be processed faster once they are ordered This is a wrong statement. You cannot convert an existing standard queue to FIFO queue. To make the move, you must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue.Standard to FIFO queue conversion: via https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFOqueues.htmlReferences:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsquotas.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFOqueues.html10:T48b,Correct option:Use the SQS Extended Client To manage large Amazon Simple Queue Service (Amazon SQS) messages, you can use Amazon Simple Storage Service (Amazon S3) and the Amazon SQS Extended Client Library for Java. This is especially useful for storing and consuming messages up to 2 GB. Unless your application requires repeatedly creating queues and leaving them inactive or storing large amounts of data in your queues, consider using Amazon S3 for storing your data.Incorrect options:Use the MultiPart API This is an incorrect statement. There is no multipart API for Amazon Simple Queue Service.Get a service limit increase from AWS While it is possible to get service limits extended for certain AWS services, AWS already offers Extended Client to deal with queues that have larger messages.Use gzip compression You can compress the messages before sending them to the queue. The messages also need to be encoded after this to cater to SQS message standards. This adds bulk to the messages and will not be an optimal solution for the current scenario.Reference: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqss3messages.html11:T596,Correct options:The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic The network access control lists (ACLs) that are associated with the subnet must have rules to allow inbound and outbound traffic on port 80 (for HTTP traffic) and port 443 (for HTTPs traffic). This is a necessary condition for Internet Gateway connectivityThe route table in the instance’s subnet should have a route to an Internet Gateway A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. The route table in the instance’s subnet should have a route defined to the Internet Gateway.Incorrect options:The instance's subnet is not associated with any route table This is an incorrect statement. A subnet is implicitly associated with the main route table if it is not explicitly associated with a particular route table. So, a subnet is always associated with some route table.The instance's subnet is associated with multiple route tables with conflicting configurations This is an incorrect statement. A subnet can only be associated with one route table at a time.The subnet has been configured to be Public and has no access to internet This is an incorrect statement. Public subnets have access to the internet via Internet Gateway.Reference:https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.htmll12:T615,Correct option:Application Stop > Before Install > Application Start > ValidateServiceIn CodeDeploy, a deployment is a process of installing content on one or more instances. This content can consist of code, web and configuration files, executables, packages, scripts, and so on. CodeDeploy deploys content that is stored in a source repository, according to the configuration rules you specify.via https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html The content in the 'hooks' section of the AppSpec file varies, depending on the compute platform for your deployment. The 'hooks' section for an EC2/OnPremises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts. The 'hooks' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event.via https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfilestructurehooks.html Incorrect options:Before Install > Application Stop > ValidateService > Application StartApplication Stop > Before Install > ValidateService > Application StartBefore Install > Application Stop > Application Start > ValidateServiceAs explained above, these three options contradict the correct order of hooks, so these are incorrect.References:https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfilestructurehooks.html13:T7ac,Correct option:Implement a DeadLetter Queue Amazon SQS supports deadletter queues, which other queues (source queues) can target for messages that can't be processed (consumed) successfully. Deadletter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. Amazon SQS does not create the deadletter queue automatically. You must first create the queue before using it as a deadletter queue.via https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsdeadletterqueues.html Incorrect options:Increase the VisibilityTimeout When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn't automatically delete the message. Immediately after a message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. Increasing visibility timeout will not help in troubleshooting the messages running into error or isolating them from the rest. Hence this is an incorrect option for the current use case.Use DeleteMessage Deletes the specified message from the specified queue. This will not help understand the reason for error or isolate messages ending with the error.Reduce the VisibilityTimeout As explained above, VisibilityTimeout makes sure that the message is not read by any other consumer while it is being processed by one consumer. By reducing the VisibilityTimeout, more consumers will receive the same failed message. Hence, this is an incorrect option for this use case.References:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsdeadletterqueues.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsvisibilitytimeout.htmll14:T7e6,Network Load BalancerA Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration. Incoming connections remain unmodified, so application software need not support XForwardedFor.via https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html Incorrect options:Application Load Balancer An Application Load Balancer functions at the application layer, the seventh layer of the Open Systems Interconnection (OSI) model. After the load balancer receives a request, it evaluates the listener rules in priority order to determine which rule to apply and then selects a target from the target group for the rule action.One of many benefits of the Application Load Balancer is its support for pathbased routing. You can configure rules for your listener that forward requests based on the URL in the request. This enables you to structure your application as smaller services, and route requests to the correct service based on the content of the URL. For needs relating to network traffic go with Network Load Balancer.Elastic Load Balancer Elastic Load Balancing is the service itself that offers different types of load balancers.Classic Load Balancer It is a basic load balancer that distributes traffic. If your account was created before 20131204, your account supports EC2Classic instances and you will benefit in using this type of load balancer. The classic load balancer can be used regardless of when your account was created and whether you use EC2Classic or whether your instances are in a VPC but just remember its the basic load balancer AWS offers and not advanced as the others.Reference: https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html15:T62e,Correct option:Bundle the dependencies in the source code during the build stage of CodeBuildAWS CodeBuild is a fully managed build service. There are no servers to provision and scale, or software to install, configure, and operate.A typical application build process includes phases like preparing the environment, updating the configuration, downloading dependencies, running unit tests, and finally, packaging the built artifact.Downloading dependencies is a critical phase in the build process. These dependent files can range in size from a few KBs to multiple MBs. Because most of the dependent files do not change frequently between builds, you can noticeably reduce your build time by caching dependencies.This will allow the code bundle to be deployed to Elastic Beanstalk to have both the dependencies and the code, hence speeding up the deployment time to Elastic BeanstalkIncorrect options:Bundle the dependencies in the source code in CodeCommit This is not the best practice and could make the CodeCommit repository huge.Store the dependencies in S3, to be used while deploying to Beanstalk This option acts as a distractor. S3 can be used as a storage location for your source code, logs, and other artifacts that are created when you use Elastic Beanstalk. Dependencies are used during the process of building code, not while deploying to Beanstalk.Create a custom platform for Elastic Beanstalk This is a more advanced feature that requires code changes, so does not fit the usecase.Reference:https://aws.amazon.com/blogs/devops/howtoenablecachingforawscodebuild//16:T7e9,Create a custom metric in CloudWatch and make your instances send data to it using PutMetricData. Then, create an alarm based on this metric You can create a custom CloudWatch metric for your EC2 Linux instance statistics by creating a script through the AWS Command Line Interface (AWS CLI). Then, you can monitor that metric by pushing it to CloudWatch.You can publish your own metrics to CloudWatch using the AWS CLI or an API. Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a highresolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.Highresolution metrics can give you more immediate insight into your application's subminute activity. But, every PutMetricData call for a custom metric is charged, so calling PutMetricData more often on a highresolution metric can lead to higher charges.Incorrect options:Create a custom alarm for your ASG and make your instances trigger the alarm using PutAlarmData API This solution will not work, your instances must be aware of each other's RAM utilization status, to know when the average RAM would be too high to trigger the alarm.Enable detailed monitoring for EC2 and ASG to get the RAM usage data and create a CloudWatch Alarm on top of it By enabling detailed monitoring you define the frequency at which the metric data has to be sent to CloudWatch, from 5 minutes to 1minute frequency window. But, you still need to create and collect the custom metric you wish to track.Migrate your application to AWS Lambda This option has been added as a distractor. You cannot use Lambda for the given usecase.References:https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/asinstancemonitoring.html#CloudWatchAlarm17:T91f,Use AWS KMS with automatic key rotation Serverside encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. You have three mutually exclusive options, depending on how you choose to manage the encryption keys: ServerSide Encryption with Amazon S3Managed Keys (SSES3), ServerSide Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSEKMS), ServerSide Encryption with CustomerProvided Keys (SSEC).When you use serverside encryption with AWS KMS (SSEKMS), you can use the default AWS managed CMK, or you can specify a customer managed CMK that you have already created. If you don't specify a customer managed CMK, Amazon S3 automatically creates an AWS managed CMK in your AWS account the first time that you add an object encrypted with SSEKMS to a bucket. By default, Amazon S3 uses this CMK for SSEKMS.You can choose to have AWS KMS automatically rotate CMKs every year, provided that those keys were generated within AWS KMS HSMs.Incorrect options:Encrypt the data before sending it to Amazon S3 The act of encrypting data before sending it to Amazon S3 is called ClientSide encryption. You will have to handle the key generation, maintenance and rotation process. Hence, this is not the right choice here.Import a custom key into AWS KMS and automate the key rotation on an annual basis by using a Lambda function When you import a custom key, you are responsible for maintaining a copy of your imported keys in your key management infrastructure so that you can reimport them at any time. Also, automatic key rotation is not supported for imported keys. Using Lambda functions to rotate keys is a possible solution, but not an optimal one for the current use case.Use SSEC with automatic key rotation on an annual basis With ServerSide Encryption with CustomerProvided Keys (SSEC), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects. The keys are not stored anywhere in Amazon S3. There is no automatic key rotation facility for this option.Reference: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html18:T5c9,Correct option:You have a hot partitionIt's not always possible to distribute read and write activity evenly. When data access is imbalanced, a "hot" partition can receive a higher volume of read and write traffic compared to other partitions. To better accommodate uneven access patterns, DynamoDB adaptive capacity enables your application to continue reading and writing to hot partitions without being throttled, provided that traffic does not exceed your table’s total provisioned capacity or the partition maximum capacity.ProvisionedThroughputExceededException explained: via https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html Hot partition explained: via https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bppartitionkeydesign.html Incorrect options:CloudWatch monitoring is lagging The error is specific to DynamoDB itself and not to any connected service. CloudWatch is a fully managed service from AWS and does not result in throttling.Configured IAM policy is wrong The error is not associated with authorization but to exceeding something preconfigured value. So, it's clearly not a permissions issue.Writecapacity units (WCU’s) are applied across to all your DynamoDB tables and this needs reconfiguration This statement is incorrect. Read Capacity Units and Write Capacity Units are specific to one table.Reference: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html19:Tb59,Correct option:Correct the policy of the IAM user to allow the kms:GenerateDataKey action You can protect data at rest in Amazon S3 by using three different modes of serverside encryption: SSES3, SSEC, or SSEKMS. SSEKMS requires that AWS manage the data key but you manage the customer master key (CMK) in AWS KMS. You can choose a customer managed CMK or the AWS managed CMK for Amazon S3 in your account. If you choose to encrypt your data using the standard features, AWS KMS and Amazon S3 perform the following actions:Amazon S3 requests a plaintext data key and a copy of the key encrypted under the specified CMK.AWS KMS generates a data key, encrypts it under the CMK, and sends both the plaintext data key and the encrypted data key to Amazon S3.Amazon S3 encrypts the data using the data key and removes the plaintext key from memory as soon as possible after use.Amazon S3 stores the encrypted data key as metadata with the encrypted data.The error message indicates that your IAM user or role needs permission for the kms:GenerateDataKey action. This permission is required for buckets that use default encryption with a custom AWS KMS key.In the JSON policy documents, look for policies related to AWS KMS access. Review statements with "Effect": "Allow" to check if the user or role has permissions for the kms:GenerateDataKey action on the bucket's AWS KMS key. If this permission is missing, then add the permission to the appropriate policy.In the JSON policy documents, look for statements with "Effect": "Deny". Then, confirm that those statements don't deny the s3:PutObject action on the bucket. The statements must also not deny the IAM user or role access to the kms:GenerateDataKey action on the key used to encrypt the bucket. Additionally, make sure the necessary KMS and S3 permissions are not restricted using a VPC endpoint policy, service control policy, permissions boundary, or session policy.Incorrect options:Correct the policy of the IAM user to allow the s3:Encrypt action This is an invalid action given only as a distractor.Correct the bucket policy of the S3 bucket to allow the IAM user to upload encrypted objects The user already has access to the bucket. What the user lacks is access to generate a KMS key, which is mandatory when a bucket is enabled for default encryption.Correct the ACL of the S3 bucket to allow the IAM user to upload encrypted objects Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. It defines which AWS accounts or groups are granted access and the type of access. ACL is another way of giving access to S3 bucket objects. Permissions to use KMS keys will still be needed.References:https://aws.amazon.com/premiumsupport/knowledgecenter/s3accessdeniederrorkms/ https://docs.aws.amazon.com/kms/latest/developerguide/servicess3.html1a:T8b0,Keep the source code in an AWS CodeCommit repository and start AWS CodePipeline whenever a change is pushed to the CodeCommit repositoryKeep the source code in an Amazon S3 bucket and start AWS CodePipeline whenever a file in the S3 bucket is updatedAWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define.How CodePipeline Works: via https://aws.amazon.com/codepipeline/ Using change detection methods that you specify, you can make your pipeline start when a change is made to a repository. You can also make your pipeline start on a schedule.When you use the console to create a pipeline that has a CodeCommit source repository or S3 source bucket, CodePipeline creates an Amazon CloudWatch Events rule that starts your pipeline when the source changes. This is the recommended change detection method.If you use the AWS CLI to create the pipeline, the change detection method defaults to starting the pipeline by periodically checking the source (CodeCommit, Amazon S3, and GitHub source providers only). AWS recommends that you disable periodic checks and create the rule manually.via https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelinesaboutstarting.html Incorrect options:Keep the source code in Amazon EFS and start AWS CodePipeline whenever a file is updatedKeep the source code in an Amazon EBS volume and start AWS CodePipeline whenever there are updates to the source codeBoth EFS and EBS are not supported as valid source providers for CodePipeline to check for any changes to the source code, hence these two options are incorrect.Keep the source code in an Amazon S3 bucket and set up AWS CodePipeline to recur at an interval of every 15 minutes As mentioned in the explanation above, although you could have the change detection method start the pipeline by periodically checking the S3 bucket, but this method is inefficient.Reference: https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelinesaboutstarting.html1b:T708,Correct option:AWS Elastic Beanstalk AWS Elastic Beanstalk is an easytouse service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. Elastic Beanstalk uses AWS CloudFormation to launch the resources in your environment and propagate configuration changes.AWS Serverless Application Model (AWS SAM) You use the AWS SAM specification to define your serverless application. AWS SAM templates are an extension of AWS CloudFormation templates, with some additional components that make them easier to work with. AWS SAM needs CloudFormation templates as a basis for its configuration.Incorrect options:AWS Lambda AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Hence, Lamda does not need CloudFormation to run its services.AWS Autoscaling AWS Auto Scaling monitors your applications and automatically adjusts the capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto Scaling, it’s easy to setup application scaling for multiple resources across multiple services in minutes. Auto Scaling used CloudFormation but is not a mandatory requirement.AWS CodeBuild AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you don’t need to provision, manage, and scale your own build servers. AWS CodePipeline uses AWS CloudFormation as a deployment action but is not a mandatory service.References:https://docs.aws.amazon.com/serverlessapplicationmodel/latest/developerguide/samspecification.html https://aws.amazon.com/elasticbeanstalk/1c:T67d,Enable SQS KMS encryptionServerside encryption (SSE) lets you transmit sensitive data in encrypted queues. SSE protects the contents of messages in queues using keys managed in AWS Key Management Service (AWS KMS).AWS KMS combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use Amazon SQS with AWS KMS, the data keys that encrypt your message data are also encrypted and stored with the data they protect.You can choose to have SQS encrypt messages stored in both Standard and FIFO queues using an encryption key provided by AWS Key Management Service (KMS).Incorrect options:Use the SSL endpoint The given usecase needs encryption at rest. When using SSL, the data is encrypted during transit, but the data needs to be encrypted at rest as well, so this option is incorrect.Use Clientside encryption For additional security, you can build your application to encrypt messages before they are placed in a message queue but will require a code change, so this option is incorrect.*Use Secrets Manager * AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with builtin integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB. Secrets Manager cannot be used for encrypting data at rest.Reference: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsserversideencryption.html1d:Tfbb,Correct option:AWS Kinesis Data StreamsAmazon Kinesis Data Streams (KDS) is a massively scalable and durable realtime data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and locationtracking events. The data collected is available in milliseconds to enable realtime analytics use cases such as realtime dashboards, realtime anomaly detection, dynamic pricing, and more.Amazon Kinesis Data Streams enables realtime processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering). Amazon Kinesis Data Streams is recommended when you need the ability to consume records in the same order a few hours later.For example, you have a billing application and an audit application that runs a few hours behind the billing application. By default, records of a stream are accessible for up to 24 hours from the time they are added to the stream. You can raise this limit to a maximum of 365 days. For the given usecase, Amazon Kinesis Data Streams can be configured to store data for up to 7 days and you can run the audit application up to 7 days behind the billing application.KDS provides the ability to consume records in the same order a few hours later via https://aws.amazon.com/kinesis/datastreams/faqs/ Incorrect options:AWS Kinesis Data Firehose Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near realtime analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect.AWS Kinesis Data Analytics Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in realtime. You can quickly build SQL queries and sophisticated Java applications using builtin templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect.Amazon SQS Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, besteffort ordering, and atleastonce delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers in the same order a few hours later, therefore this option is incorrect.References:https://aws.amazon.com/kinesis/datastreams/faqs/ https://aws.amazon.com/kinesis/datafirehose/faqs/ https://aws.amazon.com/kinesis/dataanalytics/faqs/1e:T5c4,Correct option:Use the header XForwardedFor The XForwardedFor request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the XForwardedFor request header. Elastic Load Balancing stores the IP address of the client in the XForwardedFor request header and passes the header to your server.Incorrect options:You can get the Client IP addresses from server access logs As discussed above, Load Balancers intercept traffic between clients and servers, so server access logs will contain only the IP address of the load balancer.Use the header XForwardedFrom This is a madeup option and given as a distractor.You can get the Client IP addresses from Elastic Load Balancing logs Elastic Load Balancing logs requests sent to the load balancer, including requests that never made it to the targets. For example, if a client sends a malformed request, or there are no healthy targets to respond to the request, the request is still logged. So, this is not the right option if we wish to collect the IP addresses of the clients that have access to the instances.References:https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/xforwardedheaders.html https://docs.aws.amazon.com/elasticloadbalancing/latest/application/loadbalanceraccesslogs.html1f:Taab,Correct option:Use Drift Detection feature of CloudFormationDrift detection enables you to detect whether a stack's actual configuration differs, or has drifted, from its expected configuration. Use CloudFormation to detect drift on an entire stack, or individual resources within the stack. A resource is considered to have drifted if any of its actual property values differ from the expected property values. This includes if the property or resource has been deleted. A stack is considered to have drifted if one or more of its resources have drifted.To determine whether a resource has drifted, CloudFormation determines the expected resource property values, as defined in the stack template and any values specified as template parameters. CloudFormation then compares those expected values with the actual values of those resource properties as they currently exist in the stack. A resource is considered to have drifted if one or more of its properties have been deleted, or had their value changed.You can then take corrective action so that your stack resources are again in sync with their definitions in the stack template, such as updating the drifted resources directly so that they agree with their template definition. Resolving drift helps to ensure configuration consistency and successful stack operations.Incorrect options:Use CloudFormation in Elastic Beanstalk environment to reduce direct changes to CloudFormation resources Elastic Beanstalk environment provides full access to the resources created. So, it is possible to edit the resources and hence does not solve the issue mentioned for the given use case.Use Tag feature of CloudFormation to monitor the changes happening on specific resources Tags help you identify and categorize the resources created as part of CloudFormation template. This feature is not helpful for the given use case.Use Change Sets feature of CloudFormation When you need to update a stack, understanding how your changes will affect running resources before you implement them can help you update stacks with confidence. Change sets allow you to preview how proposed changes to a stack might impact your running resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set. Change sets are not useful for the given usecase.References:https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/detectdriftstack.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/usingcfnupdatingstackschangesets.htmll20:T464,Correct option:The Lambda function invocation is asynchronous When an asynchronous invocation event exceeds the maximum age or fails all retry attempts, Lambda discards it. Or sends it to deadletter queue if you have configured one.The event fails all processing attempt A deadletter queue acts the same as an onfailure destination in that it is used when an event fails all processing attempts or expires without being processed.Incorrect options:The Lambda function invocation is synchronous When you invoke a function synchronously, Lambda runs the function and waits for a response. Queues are generally used with asynchronous invocations since queues implement the decoupling feature of various connected systems. It does not make sense to use queues when the calling code will wait on it for a response.The event has been processed successfully A successfully processed event is not sent to the deadletter queue.The event processing failed only once but succeeded thereafter A successfully processed event is not sent to the deadletter queue.Reference: https://docs.aws.amazon.com/lambda/latest/dg/invocationasync.html21:T6f2,Correct option:MOCKThis type of integration lets API Gateway return a response without sending the request further to the backend. This is useful for API testing because it can be used to test the integration setup without incurring charges for using the backend and to enable collaborative development of an API.In collaborative development, a team can isolate their development effort by setting up simulations of API components owned by other teams by using the MOCK integrations. It is also used to return CORSrelated headers to ensure that the API method permits CORS access. In fact, the API Gateway console integrates the OPTIONS method to support CORS with a mock integration.Incorrect options:AWS_PROXY This type of integration lets an API method be integrated with the Lambda function invocation action with a flexible, versatile, and streamlined integration setup. This integration relies on direct interactions between the client and the integrated Lambda function.HTTP_PROXY The HTTP proxy integration allows a client to access the backend HTTP endpoints with a streamlined integration setup on single API method. You do not set the integration request or the integration response. API Gateway passes the incoming request from the client to the HTTP endpoint and passes the outgoing response from the HTTP endpoint to the client.HTTP This type of integration lets an API expose HTTP endpoints in the backend. With the HTTP integration, you must configure both the integration request and integration response. You must set up necessary data mappings from the method request to the integration request, and from the integration response to the method response.Reference: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayapiintegrationtypes.html22:T6b6,DynamoDBA DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB Streams captures a timeordered sequence of itemlevel modifications in any DynamoDB table, and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near realtime.Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified.DynamoDB Streams Overview: via https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html Incorrect options:RDS By itself, RDS cannot be used to stream events like DynamoDB, so this option is ruled out. However, you can use Amazon Kinesis for streaming data from RDS.Please refer to this excellent blog for more details on using Kinesis for streaming data from RDS: https://aws.amazon.com/blogs/database/streamingchangesinadatabasewithamazonkinesis/ElastiCache ElastiCache works as an inmemory data store and cache, it cannot be used to stream data like DynamoDB.Kinesis Kinesis is not a database, so this option is ruled out.Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs. You can continuously add various types of data such as clickstreams, application logs, and social media to an Amazon Kinesis data stream from hundreds of thousands of sources.How Kinesis Data Streams Work via https://aws.amazon.com/kinesis/datastreams/Reference: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html23:T111e,Correct option:Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 keyA user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Google, Facebook, Amazon, or Apple, and SAML identity providers. Whether your users sign in directly or through a third party, all members of the user pool have a directory profile that you can access through a Software Development Kit (SDK).User pools provide:Signup and signin services.A builtin, customizable web UI to sign in users.Social signin with Facebook, Google, Login with Amazon, and Sign in with Apple, as well as signin with SAML identity providers from your user pool.User directory management and user profiles.Security features such as multifactor authentication (MFA), checks for compromised credentials, account takeover protection, and phone and email verification.Customized workflows and user migration through AWS Lambda triggers.To use an Amazon Cognito user pool with your Amazon API Gateway API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header.For the given use case, you can use a Cognito user pool to manage user accounts and configure an Amazon Cognito user pool authorizer in API Gateway to control access to the API. You should use a Lambda function to store the actual images on S3 and the image metadata on DynamoDB. Finally, you can get the images using the Lambda function that leverages the metadata stored in DynamoDB.Incorrect options:Use Cognito identity pools to manage user accounts and set up an Amazon Cognito identity pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 keyUse Cognito identity pools to create an IAM user for each user of the application during the signup process. Leverage IAM authentication in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 keyAmazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limitedprivilege AWS credentials to access other AWS services. You cannot use identity pools to manage users or to create IAM users. So both of these options are incorrect.via https://aws.amazon.com/premiumsupport/knowledgecenter/cognitouserpoolsidentitypools/Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images as well as the image metadata in a DynamoDB table. Have the Lambda function retrieve previously uploaded images from DynamoDB You cannot use DynamoDB to store images as the maximum allowed item size is 400KB and the images range in size from 500KB to 5MB. You should also note that storing images on DynamoDB is an antipattern. So this option is incorrect.References:https://docs.aws.amazon.com/cognito/latest/developerguide/cognitouseridentitypools.html https://docs.aws.amazon.com/cognito/latest/developerguide/cognitoidentity.html https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayintegratewithcognito.html https://aws.amazon.com/premiumsupport/knowledgecenter/cognitouserpoolsidentitypools/24:T5ff,Use Amazon S3 and make code changes in the application so all uploads are put on S3Amazon S3 is an object storage built to store and retrieve any amount of data from anywhere on the Internet. It’s a simple storage service that offers an extremely durable, highly available, and infinitely scalable data storage infrastructure at very low costs.Amazon S3 provides a simple web service interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. Using this web service, you can easily build applications that make use of Internet storage.You can use S3 PutObject API from the application to upload the objects in a single bucket, which is then accessible from all instances.Incorrect options:Use Amazon EBS and configure the application AMI to use a snapshot of the same EBS instance while launching new instances Using EBS to share data between instances is not possible because EBS volume is tied to an instance by definition. Creating a snapshot would only manage to move the stale data into the new instances.Use Instance Store type of EC2 instances and share the files via file synchronization softwareUse Amazon EBS as the storage volume and share the files via file synchronization softwareTechnically you could use file synchronization software on EC2 instances with EBS or Instance Store type, but that involves a lot of development effort and still would not be as productionready as just using S3. So both these options are incorrect.Reference: https://aws.amazon.com/s3/faqs/25:T8b3,Store the frequently accessed data in an Amazon ElastiCache for Redis instance with encryption enabled for data in transit and at restAmazon ElastiCache for Redis is a Rediscompatible inmemory data structure service that can be used as a data store or cache. It delivers the ease of use and power of Redis along with the availability, reliability, scalability, security, and performance suitable for the most demanding applications.In addition to strings, Redis supports lists, sets, sorted sets, hashes, bit arrays, and hyperlog logs. Applications can use these more advanced data structures to support a variety of use cases. For example, you can use Redis Sorted Sets to easily implement a game leaderboard that keeps a list of players sorted by their rank.Incorrect options:Store the frequently accessed data in an Amazon ElastiCache for Memcached instance with encryption enabled for data in transit and at rest Memcached is designed for simplicity and it does not offer support for advanced data structures and operations such as sort or rank.via https://aws.amazon.com/elasticache/redisvsmemcached/ Migrate the frequently accessed data to DynamoDB Accelerator (DAX) that has encryption enabled for data in transit and at rest DAX is a DynamoDBcompatible caching service that enables you to benefit from fast inmemory performance for demanding applications. DAX cannot be used with RDS MySQL as a caching service, so this option is incorrect.Migrate the frequently accessed data to an EC2 Instance Store that has encryption enabled for data in transit and at rest This option is incorrect. EC2 instance store provides temporary blocklevel storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content. It can also be used to store temporary data that you replicate across a fleet of instances, such as a loadbalanced pool of web servers.References:https://aws.amazon.com/elasticache/redis/ https://aws.amazon.com/elasticache/redisvsmemcached/ https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html26:T71f,Correct options:Standard Workflows on AWS Step Functions are suitable for longrunning, durable, and auditable workflows that can also support any human approval steps Standard Workflows on AWS Step Functions are more suitable for longrunning, durable, and auditable workflows where repeating workflow steps is expensive (e.g., restarting a longrunning media transcode) or harmful (e.g., charging a credit card twice). Example workloads include training and deploying machine learning models, report generation, billing, credit card processing, and ordering and fulfillment processes. Step functions also support any human approval steps.You should use Express Workflows for workloads with high event rates and short duration* You should use Express Workflows for workloads with high event rates and short durations. Express Workflows support event rates of more than 100,000 per second.Incorrect options:Standard Workflows on AWS Step Functions are suitable for longrunning, durable, and auditable workflows that do not support any human approval steps As Step functions support any human approval steps, so this option is incorrect.Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of 180 days or 6 months Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of one year.Both Standard and Express Workflows support all service integrations, activities, and design patterns Standard Workflows support all service integrations, activities, and design patterns. Express Workflows do not support activities, jobrun (.sync), and Callback patterns.Reference:https://aws.amazon.com/stepfunctions/features/ https://aws.amazon.com/blogs/compute/implementingserverlessmanualapprovalstepsinawsstepfunctionsandamazonapigateway/27:T71f,Correct option:Use ElastiCache to improve latency and throughput for readheavy application workloadsUse ElastiCache to improve performance of computeintensive workloadsAmazon ElastiCache allows you to run inmemory data stores in the AWS cloud. Amazon ElastiCache is a popular choice for realtime use cases like Caching, Session Stores, Gaming, Geospatial Services, RealTime Analytics, and Queuing.via https://docs.aws.amazon.com/AmazonElastiCache/latest/memug/elasticacheusecases.html Amazon ElastiCache can be used to significantly improve latency and throughput for many readheavy application workloads (such as social networking, gaming, media sharing, and Q&A portals) or computeintensive workloads (such as a recommendation engine) by allowing you to store the objects that are often read in the cache.Overview of Amazon ElastiCache features: via https://aws.amazon.com/elasticache/features/Incorrect options:Use ElastiCache to improve latency and throughput for writeheavy application workloads As mentioned earlier in the explanation, Amazon ElastiCache can be used to significantly improve latency and throughput for many readheavy application workloads. Caching is not a good fit for writeheavy applications as the cache goes stale at a very fast rate.Use ElastiCache to improve performance of ExtractTransformLoad (ETL) workloads ETL workloads involve reading and transforming high volume data which is not a good fit for caching. You should use AWS Glue or Amazon EMR to facilitate ETL workloads.Use ElastiCache to run highly complex JOIN queries Complex JSON queries can be run on relational databases such as RDS or Aurora. ElastiCache is not a good fit for this usecase.References:https://docs.aws.amazon.com/AmazonElastiCache/latest/memug/elasticacheusecases.html https://aws.amazon.com/elasticache/features/28:T61b,Correct option:Install and run the XRay daemon on the onpremises servers to capture and relay the data to the XRay serviceThe AWS XRay daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS XRay API. The daemon works in conjunction with the AWS XRay SDKs and must be running so that data sent by the SDKs can reach the XRay service.To run the XRay daemon locally, onpremises, or on other AWS services, download it, run it, and then give it permission to upload segment documents to XRay.Incorrect options:Install and run the XRay SDK on the onpremises servers to capture and relay the data to the XRay service As mentioned above, you need to run the XRay daemon on the onpremises servers and give it the required permission to upload XRay data to the XRay service. So this option is incorrect.Install and run the CloudWatch Unified Agent on the onpremises servers to capture and relay the XRay data to the XRay service using the PutTraceSegments API call This option has been added as a distractor. CloudWatch Agent cannot relay XRay data to the XRay service using the PutTraceSegments API call.Configure a Lambda function to analyze the incoming traffic data on the onpremises servers and then relay the XRay data to the XRay service using the PutTelemetryRecords API call This option is incorrect as the Lambda function cannot process the XRay data for an onpremises instance and then relay it to the XRay service.Reference: https://docs.aws.amazon.com/xray/latest/devguide/xraydaemon.html29:T736,Correct option:Update stage variable value from the stage name of test to that of prodAfter creating your API, you must deploy it to make it callable by your users. To deploy an API, you create an API deployment and associate it with a stage. A stage is a logical reference to a lifecycle state of your API (for example, dev, prod, beta, v2). API stages are identified by the API ID and stage name. They're included in the URL that you use to invoke the API. Each stage is a named reference to a deployment of the API and is made available for client applications to call.Stages enable robust version control of your API. In our current usecase, after the updates pass the test, you can promote the test stage to the prod stage. The promotion can be done by redeploying the API to the prod stage or updating a stage variable value from the stage name of test to that of prod.Incorrect options:Deploy the API without choosing a stage. This way, the working deployment will be updated in all stages An API can only be deployed to a stage. Hence, it is not possible to deploy an API without choosing a stage.Delete the existing prod stage. Create a new stage with the same name (prod) and deploy the tested version on this stage* This is possible, but not an optimal way of deploying a change. Also, as prod refers to real production system, this option will result in downtime.API performance is optimized in a different way for prod environments. Hence, promoting test to prod is not correct. The promotion should be done by redeploying the API to the prod stage For each stage, you can optimize API performance by adjusting the default accountlevel request throttling limits and enabling API caching. And these settings can be changed/updated at any time.Reference:https://docs.aws.amazon.com/apigateway/latest/developerguide/howtodeployapi.htmll2a:T87b,Correct option:Use Cognito User Pools As an alternative to using IAM roles and policies or Lambda authorizers, you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway. To use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn't authorized to make the call because the client did not have credentials that could be authorized.Incorrect options:Use Lambda Authorizer A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API. A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity. This won't be a fully managed user management solution but it would allow you to check for access at the AWS API Gateway level.Use IAM permissions with sigv4 Signature Version 4 is the process to add authentication information to AWS requests sent by HTTP. For security, most requests to AWS must be signed with an access key, which consists of an access key ID and secret access key. These two keys are commonly referred to as your security credentials. But, we cannot possibly create an IAM user for every visitor of the site, so this is where social identity providers come in to help.Use API Gateway User Pools This is a madeup option.References:https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayintegratewithcognito.html https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayuselambdaauthorizer.html https://docs.aws.amazon.com/general/latest/gr/signatureversion4.html2b:T4b0,Use 'Export' field in the Output section of the stack's templateTo share information between stacks, export a stack's output values. Other stacks that are in the same AWS account and region can import the exported values.To export a stack's output value, use the Export field in the Output section of the stack's template. To import those values, use the Fn::ImportValue function in the template for the other stacks.Incorrect options:Use 'Expose' field in the Output section of the stack's template 'Expose' is a madeup option, and only given as a distractor.Use Fn::ImportValue To import the values exported by another stack, we use the Fn::ImportValue function in the template for the other stacks. This function is not useful for the current scenario.Use Fn::Transform The intrinsic function Fn::Transform specifies a macro to perform custom processing on part of a stack template. Macros enable you to perform custom processing on templates, from simple actions like findandreplace operations to extensive transformations of entire templates. This function is not useful for the current scenario.Reference: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/usingcfnstackexports.html2c:T974,Correct option:Configure Application Auto Scaling to manage Lambda provisioned concurrency on a scheduleConcurrency is the number of requests that a Lambda function is serving at any given time. If a Lambda function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.Due to a spike in traffic, when Lambda functions scale, this causes the portion of requests that are served by new instances to have higher latency than the rest. To enable your function to scale without fluctuations in latency, use provisioned concurrency. By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with very low latency.You can configure Application Auto Scaling to manage provisioned concurrency on a schedule or based on utilization. Use scheduled scaling to increase provisioned concurrency in anticipation of peak traffic. To increase provisioned concurrency automatically as needed, use the Application Auto Scaling API to register a target and create a scaling policy.Please see this note for more details on provisioned concurrency: via https://docs.aws.amazon.com/lambda/latest/dg/configurationconcurrency.html Incorrect options:Configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule To ensure that a function can always reach a certain level of concurrency, you can configure the function with reserved concurrency. When a function has reserved concurrency, no other function can use that concurrency. More importantly, reserved concurrency also limits the maximum concurrency for the function, and applies to the function as a whole, including versions and aliases.You cannot configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule.Add an Application Load Balancer in front of the Lambda functions This is a distractor as just adding the Application Load Balancer will not help in scaling the Lambda functions to address the surge in traffic.No need to make any special provisions as Lambda is automatically scalable because of its serverless nature It's true that Lambda is serverless, however, due to the surge in traffic the Lambda functions can still hit the concurrency limits. So this option is incorrect.Reference: https://docs.aws.amazon.com/lambda/latest/dg/configurationconcurrency.html2d:T865,Correct option:Increase the amount of memory available to the Lambda functionsAWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.In the AWS Lambda resource model, you choose the amount of memory you want for your function which allocates proportional CPU power and other resources. This means you will have access to more compute power when you choose one of the new larger settings. You can set your memory in 64MB increments from 128MB to 3008MB. You access these settings when you create a function or update its configuration. The settings are available using the AWS Management Console, AWS CLI, or SDKs.via https://docs.aws.amazon.com/lambda/latest/dg/configurationconsole.htmlTherefore, by increasing the amount of memory available to the Lambda functions, you can run the computeheavy workflows.Incorrect options:Invoke the Lambda functions asynchronously to process the computeheavy workflows When you invoke a function asynchronously, you don't wait for a response from the function code. You hand off the event to Lambda and Lambda handles the rest. You can configure how Lambda handles errors and can send invocation records to a downstream resource to chain together components of your application. The method of invocation has no bearing on the Lambda function's ability to process the computeheavy workflows.Use reserved concurrency to account for the computeheavy workflowsUse provisioned concurrency to account for the computeheavy workflowsConcurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency. The type of concurrency has no bearing on the Lambda function's ability to process the computeheavy workflows. So both these options are incorrect.Reference: https://docs.aws.amazon.com/lambda/latest/dg/configurationconsole.html2e:T498,Fix the IAM permissions for the EC2 instance roleYou should use an IAM role to manage temporary credentials for applications that run on an EC2 instance. When you use a role, you don't have to distribute longterm credentials (such as a user name and password or access keys) to an EC2 instance. Instead, the role supplies temporary permissions that applications can use when they make calls to other AWS resources. In this case, make sure your role has access to the S3 bucket.Incorrect options:Fix the IAM permissions for the CodeDeploy service role The fact that CodeDeploy deployed the application to EC2 instances tells us that there was no issue between those two. The actual issue is between the EC2 instances and S3.Make the S3 bucket public This is not a good practice, you should strive to provide least privilege access. You may have files in here that should not be allowed public access and you are opening the door to security breaches.Enable CodeDeploy Proxy This is not correct as we don't need to look into CodeDeploy settings but rather between EC2 and S3 permissions.Reference: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switchroleec2.html2f:T92c,Correct option:Use AWS Step Functions state machines to orchestrate the workflowAWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or task, allowing you to scale and change applications quickly.How Step Functions Work: via https://aws.amazon.com/stepfunctions/ The following are key features of AWS Step Functions:Step Functions are based on the concepts of tasks and state machines. You define state machines using the JSONbased Amazon States Language. A state machine is defined by the states it contains and the relationships between them. States are elements in your state machine. Individual states can make decisions based on their input, perform actions, and pass output to other states. In this way, a state machine can orchestrate workflows.Please see this note for a simple example of a State Machine: via https://docs.aws.amazon.com/stepfunctions/latest/dg/amazonstateslanguagestatemachinestructure.html Incorrect options:Use AWS Step Functions activities to orchestrate the workflow In AWS Step Functions, activities are a way to associate code running somewhere (known as an activity worker) with a specific task in a state machine. When a Step Function reaches an activity task state, the workflow waits for an activity worker to poll for a task. For example, an activity worker can be an application running on an Amazon EC2 instance or an AWS Lambda function. AWS Step Functions activities cannot orchestrate a workflow.Use AWS Glue to orchestrate the workflow AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue cannot orchestrate a workflow.Use AWS Batch to orchestrate the workflow AWS Batch runs batch computing jobs on the AWS Cloud. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. AWS Batch cannot orchestrate a workflow.Reference:https://aws.amazon.com/stepfunctions/ https://docs.aws.amazon.com/stepfunctions/latest/dg/amazonstateslanguagestatemachinestructure.html30:T99b,Add a rule to the Network ACLs to allow outbound traffic on ports 1024 65535A Network Access Control List (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.When you create a custom Network ACL and associate it with a subnet, by default, this custom Network ACL denies all inbound and outbound traffic until you add rules. A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic. Network ACLs are stateless, which means that responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa).The client that initiates the request chooses the ephemeral port range. The range varies depending on the client's operating system. Requests originating from Elastic Load Balancing use ports 102465535. List of ephemeral port ranges:Many Linux kernels (including the Amazon Linux kernel) use ports 3276861000.Requests originating from Elastic Load Balancing use ports 102465535.Windows operating systems through Windows Server 2003 use ports 10255000.Windows Server 2008 and later versions use ports 4915265535.A NAT gateway uses ports 102465535.AWS Lambda functions use ports 102465535.Incorrect options:Add a rule to the Network ACLs to allow outbound traffic on ports 1025 5000 As discussed above, Windows operating systems through Windows Server 2003 use ports 10255000. ELB uses the port range 102465535.Add a rule to the Network ACLs to allow outbound traffic on ports 32768 61000 As discussed above, Linux kernels (including the Amazon Linux kernel) use ports 10255000. ELB uses the port range 102465535.Add a rule to the Security Group allowing outbound traffic on port 80 A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. Security groups act at the instance level, not the subnet level. Security groups are stateful — if you send a request from your instance, the response traffic for that request is allowed to flow in regardless of inbound security group rules. Responses to allowed inbound traffic are allowed to flow out, regardless of outbound rules.References:https://docs.aws.amazon.com/vpc/latest/userguide/vpcnetworkacls.html https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html31:Tb18,Use SQS long polling to retrieve messages from your Amazon SQS queuesAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds.Exam Alert:Please review the differences between Short Polling vs Long Polling: via https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsshortandlongpolling.html via https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsshortandlongpolling.htmlIncorrect options:Use SQS short polling to retrieve messages from your Amazon SQS queues With short polling, Amazon SQS sends the response right away, even if the query found no messages. You end up paying more because of the increased number of empty receives.Use SQS visibility timeout to retrieve messages from your Amazon SQS queues Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to retrieve messages from your Amazon SQS queues. This option has been added as a distractor.Use SQS message timer to retrieve messages from your Amazon SQS queues You can use message timers to set an initial invisibility period for a message added to a queue. So, if you send a message with a 60second timer, the message isn't visible to consumers for its first 60 seconds in the queue. The default (minimum) delay for a message is 0 seconds. The maximum is 15 minutes. You cannot use message timer to retrieve messages from your Amazon SQS queues. This option has been added as a distractor.Reference: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsshortandlongpolling.html32:T66e,Between clients and CloudFront as well as between CloudFront and backendFor web distributions, you can configure CloudFront to require that viewers use HTTPS to request your objects, so connections are encrypted when CloudFront communicates with viewers.Requiring HTTPS for Communication Between Viewers and CloudFront: via https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/usinghttpsviewerstocloudfront.htmlYou also can configure CloudFront to use HTTPS to get objects from your origin, so connections are encrypted when CloudFront communicates with your origin.Requiring HTTPS for Communication Between CloudFront and Your Custom Origin: via https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/usinghttpscloudfronttocustomorigin.htmlIncorrect options:Between clients and CloudFront only This is incorrect as you can choose to require HTTPS between CloudFront and your origin.Between CloudFront and backend only This is incorrect as you can choose to require HTTPS between viewers and CloudFront.Neither between clients and CloudFront nor between CloudFront and backend This is incorrect as you can choose HTTPS settings both for communication between viewers and CloudFront as well as between CloudFront and your origin.References:https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/secureconnectionssupportedviewerprotocolsciphers.html#secureconnectionssupportedcipherscloudfronttoorigin https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/usinghttpsviewerstocloudfront.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/usinghttpscloudfronttocustomorigin.html33:T5fa,"Cognito Identity Pools"Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token. Identity pools provide AWS credentials to grant your users access to other AWS services.Cognito Overview: via https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.html Incorrect options:"Cognito User Pools" AWS Cognito User Pools is there to authenticate users for your applications which looks similar to Cognito Identity Pools. The difference is that Identity Pools allows a way to authorize your users to use the various AWS services and User Pools is not about authorizing to AWS services but to provide add signup and signin functionality to web and mobile applications."Cognito Sync" You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status."IAM" This is not a good solution because it would require you to have an IAM user for each mobile device which is not a good practice or manageable way of handling deployment.Exam Alert:Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools: via https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.html Reference: https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.html34:T672,Correct option:Use SSM Parameter StoreAWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. For the given usecase, as the DevOps team does not want to redeploy the application every time there are configuration changes, so they can use the SSM Parameter Store to store the configuration externally.Incorrect options:Use Environment variables Environment variables provide another way to specify configuration options and credentials, and can be useful for scripting or temporarily setting a named profile as the default. Your application is not running AWS CLI. Since the usecase requires the configuration to be stored securely, so using Environment variables is ruled out, as these are not encrypted at rest and these are visible in clear text in the AWS Console as well as in the response of some actions of the Elastic BeanStalk API.Use Stage Variables You can use stage variables for managing multiple release stages for API Gateway, this is not what you are looking for here.Use S3 S3 offers the same benefit as the SSM Parameter Store where there are no servers to manage. With S3 you have to set encryption and choose other security options and there are more chances of misconfiguring security if you share your S3 bucket with other objects. You would have to create a custom setup to come close to the parameter store. Use Parameter Store and let AWS handle the rest.Reference: https://docs.aws.amazon.com/systemsmanager/latest/userguide/systemsmanagerparamstore.html35:T85c,Correct option:Configure a parameter with the list of EC2 instance types as AllowedValues in the CloudFormation templateYou can use the Parameters section to customize your templates. Parameters enable you to input custom values to your template each time you create or update a stack.via https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameterssectionstructure.html AllowedValues refers to an array containing the list of values allowed for the parameter. When applied to a parameter of type String, the parameter value must be one of the allowed values. When applied to a parameter of type CommaDelimitedList, each value in the list must be one of the specified allowed values.Incorrect options:Configure separate parameters for each EC2 instance type in the CloudFormation template Creating separate parameters for each instance type is semantically incorrect as the underlying value will point to the same resource but have multiple inputs.Configure a mapping having a list of EC2 instance types as parameters in the CloudFormation template The Mappings section matches a key to a corresponding set of named values. For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. You use the Fn::FindInMap intrinsic function to retrieve values in a map. A mapping is not a list, rather, it consists of key value pairs. You can't include parameters, pseudo parameters, or intrinsic functions in the Mappings section. So, this option is incorrect.Configure a pseudo parameter with the list of EC2 instance types as AllowedValues in the CloudFormation template Pseudo parameters are parameters that are predefined by AWS CloudFormation. You don't declare them in your template. So, this option is incorrect.References:https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameterssectionstructure.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappingssectionstructure.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudoparameterreference.html36:T937,Correct option:Configure the data producer to retry with an exponential backoffIncrease the number of shards within your data streams to provide enough capacityAmazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs. You can continuously add various types of data such as clickstreams, application logs, and social media to an Amazon Kinesis data stream from hundreds of thousands of sources.How Kinesis Data Streams Work via https://aws.amazon.com/kinesis/datastreams/ The capacity limits of an Amazon Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a ProvisionedThroughputExceeded exception.If this is due to a temporary rise of the data stream’s input data rate, retry (with exponential backoff) by the data producer will eventually lead to the completion of the requests.If this is due to a sustained rise of the data stream’s input data rate, you should increase the number of shards within your data stream to provide enough capacity for the put data calls to consistently succeed.Incorrect options:Use Amazon Kinesis Agent instead of Kinesis Producer Library (KPL) for sending data to Kinesis Data Streams Kinesis Agent works with data producers. Using Kinesis Agent instead of KPL will not help as the constraint is the capacity limit of the Kinesis Data Stream.Use Amazon SQS instead of Kinesis Data Streams This is a distractor as using SQS will not help address the ProvisionedThroughputExceeded exception for the Kinesis Data Stream. This option does not address the issues in the usecase.Use Kinesis enhanced fanout for Kinesis Data Streams You should use enhanced fanout if you have, or expect to have, multiple consumers retrieving data from a stream in parallel. Therefore, using enhanced fanout will not help address the ProvisionedThroughputExceeded exception as the constraint is the capacity limit of the Kinesis Data Stream.Please review this note for more details on enhanced fanout for Kinesis Data Streams: via https://aws.amazon.com/kinesis/datastreams/faqs/References:https://aws.amazon.com/kinesis/datastreams/ https://aws.amazon.com/kinesis/datastreams/faqs/37:T6f3,Correct option:An outbound rule must be added to the Network ACL (NACL) to allow the response to be sent to the client on the ephemeral port rangeSecurity groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic. By default, each custom Network ACL denies all inbound and outbound traffic until you add rules.To enable the connection to a service running on an instance, the associated network ACL must allow both: 1. Inbound traffic on the port that the service is listening on 2. Outbound traffic to ephemeral portsWhen a client connects to a server, a random port from the ephemeral port range (102465535) becomes the client's source port.The designated ephemeral port becomes the destination port for return traffic from the service. Outbound traffic to the ephemeral port must be allowed in the network ACL.Incorrect options:The configuration is complete on the EC2 instance for accepting and responding to requests As explained above, this is an incorrect statement.An outbound rule on the security group has to be configured, to allow the response to be sent to the client on the HTTP port Security groups are stateful. Therefore you don't need a rule that allows responses to inbound traffic.Outbound rules need to be configured both on the security group and on the NACL for sending responses to the Internet Gateway* Security Groups are stateful. Hence, return traffic is automatically allowed, so there is no need to configure an outbound rule on the security group.References:https://aws.amazon.com/premiumsupport/knowledgecenter/resolveconnectionsgaclinbound/ https://docs.aws.amazon.com/vpc/latest/userguide/vpcnetworkacls.html#naclephemeralports38:T68d,Inplace DeploymentThe application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete.Blue/green DeploymentWith a blue/green deployment, you provision a new set of instances on which CodeDeploy installs the latest version of your application. CodeDeploy then reroutes load balancer traffic from an existing set of instances running the previous version of your application to the new set of instances running the latest version. After traffic is rerouted to the new instances, the existing instances can be terminated.CodeDeploy Deployment Types: via https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.htmlIncorrect options:Cattle Deployment This is a good option if you have cattle in a farmWarm Standby Deployment This is not a valid CodeDeploy deployment option. The term "Warm Standby" is used to describe a Disaster Recovery scenario in which a scaleddown version of a fully functional environment is always running in the cloud.Pilot Light Deployment This is not a valid CodeDeploy deployment option. "Pilot Light" is a Disaster Recovery approach where you simply replicate part of your IT structure for a limited set of core services so that the AWS cloud environment seamlessly takes over in the event of a disaster.References:https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html https://aws.amazon.com/blogs/publicsector/rapidlyrecovermissioncriticalsystemsinadisaster/39:T676,Correct option:"CodeDeploy Agent"The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The CodeDeploy agent archives revisions and log files on instances. The CodeDeploy agent cleans up these artifacts to conserve disk space. You can use the :max_revisions: option in the agent configuration file to specify the number of application revisions to the archive by entering any positive integer. CodeDeploy also archives the log files for those revisions. All others are deleted, except for the log file of the last successful deployment.More info here: via https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeployagent.html Incorrect options:AWS CloudWatch Log Agent The CloudWatch Logs agent provides an automated way to send log data to CloudWatch Logs from Amazon EC2 instances. This is an incorrect choice for the current use case.Integrate with AWS CodePipeline AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodeCommit and CodePipeline are already integrated services. CodePipeline cannot help in version control and management of archives on an EC2 instance.Have a load balancer in front of your instances Load Balancer helps balance incoming traffic across different EC2 instances. It is an incorrect choice for the current use case.References:https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeployagent.html https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html3a:T4f0,DownloadBundle => BeforeInstall => ApplicationStart => ValidateServiceAWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your onpremises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications.You can specify one or more scripts to run in a hook. Each hook for a lifecycle event is specified with a string on a separate line.Please review the correct order of lifecycle events: via https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfilestructurehooks.html#referenceappspecfilestructurehooksrunorderIncorrect options:BeforeInstall => ApplicationStart => DownloadBundle => ValidateServiceValidateService => BeforeInstall =>DownloadBundle => ApplicationStartBeforeInstall => ValidateService =>DownloadBundle => ApplicationStartThese three options contradict the details provided in the explanation above, so these options are not correct.Reference: https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfilestructurehooks.html#referenceappspecfilestructurehooksrunorder3b:T10e5,Correct option:Leverage AWS Secrets Manager with an AWS KMS customermanaged key to store the access token as a secret and configure a resourcebased policy for the secret to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Secrets Manager. Fetch the token from Secrets Manager and then use the decrypted access token to send the message to the chatAWS Secrets Manager is an AWS service that encrypts and stores your secrets, and transparently decrypts and returns them to you in plaintext. It's designed especially to store application secrets, such as login credentials, that change periodically and should not be hardcoded or stored in plaintext in the application. In place of hardcoded credentials or table lookups, your application calls Secrets Manager.Secrets Manager also supports features that periodically rotate the secrets associated with commonly used databases. It always encrypts newly rotated secrets before they are stored.Secrets Manager integrates with AWS Key Management Service (AWS KMS) to encrypt every version of every secret value with a unique data key that is protected by an AWS KMS key. This integration protects your secrets under encryption keys that never leave AWS KMS unencrypted. It also enables you to set custom permissions on the KMS key and audit the operations that generate, encrypt, and decrypt the data keys that protect your secrets.To grant permission to retrieve secret values, you can attach policies to secrets or identities.via https://docs.aws.amazon.com/secretsmanager/latest/userguide/authandaccess_examples.html For the given use case, you can use the resourcebased policy to the secret to allow access from other accounts. Then you need to update the IAM role of the EC2 instances with permissions to access Secrets Manager which will retrieve the token from Secrets Manager and use the decrypted access token to send the message to the support team via the chat API.Incorrect options:Leverage AWS Systems Manager Parameter Store with an AWS KMS customermanaged key to store the access token as a SecureString parameter and configure a resourcebased policy for the parameter to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Parameter Store. Fetch the token from Parameter Store using the with decryption flag and then use the decrypted access token to send the message to the chat You cannot use a resourcebased policy with a parameter in the Parameter Store. Parameter Store supports parameter policies that are available for parameters that use the advanced parameters tier. Parameter policies help you manage a growing set of parameters by allowing you to assign specific criteria to a parameter such as an expiration date or time to live. Parameter policies are especially helpful in forcing you to update or delete passwords and configuration data stored in Parameter Store, a capability of AWS Systems Manager. So this option is incorrect.Store AWS KMS encrypted access token in a DynamoDB table and configure a resourcebased policy for the DynamoDB table to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the DynamoDB table. Fetch the token from the Dynamodb table and then use the decrypted access token to send the message to the chat You should note that DynamoDB does not support resourcebased policies. Moreover, it's a security bad practice to keep sensitive access credentials in code, database or a flat file on a file system or object storage. Therefore, this option is incorrect.Leverage SSEKMS to store the access token as an encrypted object on S3 and configure a resourcebased policy for the S3 bucket to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the S3 object. Fetch the token from S3 and then use the decrypted access token to send the message to the chat It is considered a security bad practice to keep sensitive access credentials in code, database, or a flat file on a file system or object storage. Therefore, this option is incorrect.References:https://docs.aws.amazon.com/secretsmanager/latest/userguide/authandaccess_examples.html https://docs.aws.amazon.com/systemsmanager/latest/userguide/parameterstorepolicies.html3c:T8ac,The Lambda function does not have IAM permissions to write to DynamoDBYou need to use an identitybased policy that allows read and write access to a specific Amazon DynamoDB table. To use this policy, attach the policy to a Lambda service role. A service role is a role that you create in your account to allow a service to perform actions on your behalf. That service role must include AWS Lambda as the principal in the trust policy. The role is then used to grant a Lambda function access to a DynamoDB table. By using an IAM policy and role to control access, you don’t need to embed credentials in code and can tightly control which services the Lambda function can access.Incorrect options:The Lambda function's provisioned concurrency limit has been exceededThe Lambda function's reserved concurrency limit has been exceededReserved concurrency – Reserved concurrency guarantees the maximum number of concurrent instances for the function. When a function has reserved concurrency, no other function can use that concurrency. There is no charge for configuring reserved concurrency for a function.Provisioned concurrency – Provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond immediately to your function's invocations. Note that configuring provisioned concurrency incurs charges to your AWS account.Neither reserved concurrency nor provisioned concurrency has any relevance to the given use case. Both options have been added as distractors.DynamoDB table does not have a Gateway VPC Endpoint, which is required by the Lambda function for a successful write Gateway endpoints provide reliable connectivity to Amazon S3 and DynamoDB without requiring an internet gateway or a NAT device for your VPC. Gateway endpoints do not enable AWS PrivateLink. This option acts as a distractor since the Lambda function is not provisioned within a VPC by default, so there is no need of a Gateway VPC Endpoint to access DynamoDB.References:https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_lambdaaccessdynamodb.html https://aws.amazon.com/blogs/security/howtocreateanawsiampolicytograntawslambdaaccesstoanamazondynamodbtable/3d:T72b,You create a CloudWatch custom metric and build an alarm to scale your ASGHere we need to scale on the metric "number of requests per minute", which is a custom metric we need to create, as it's not readily available in CloudWatch.Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a highresolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.Incorrect options:Attach an Elastic Load Balancer This is not what you need for autoscaling. An Elastic Load Balancer distributes workloads across multiple compute resources and checks your instances' health status to name a few, but it does not automatically increase and decrease the number of instances based on the application requirement.Attach additional Elastic File Storage This is a file storage service designed for performance. Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and onpremises resources. It is built to scale ondemand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth. This cannot be used to facilitate autoscaling.How EFS Works: via https://aws.amazon.com/efs/You enable detailed monitoring and use that to scale your ASG The detailed monitoring metrics won't provide information about database /applicationlevel requests per minute, so this option is not correct.Reference: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html3e:T743,AWS XRay helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With XRay, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. XRay provides an endtoend view of requests as they travel through your application, and shows a map of your application’s underlying components.How XRay Works: via https://aws.amazon.com/xray/ Enable XRay samplingTo ensure efficient tracing and provide a representative sample of the requests that your application serves, the XRay SDK applies a sampling algorithm to determine which requests get traced. By default, the XRay SDK records the first request each second, and five percent of any additional requests. XRay sampling is enabled directly from the AWS console, hence your application code does not need to change.You can also customize the XRay sampling rules: via https://docs.aws.amazon.com/xray/latest/devguide/xrayconsolesampling.htmlIncorrect options:Use Filter Expressions in the XRay console When you choose a time period of traces to view in the XRay console, you might get more results than the console can display. You can narrow the results to just the traces that you want to find by using a filter expression. This option is not correct because it does not reduce the volume of data sent into the XRay console.via https://docs.aws.amazon.com/xray/latest/devguide/xrayconsolefilters.htmlCustom configuration for the XRay agents You cannot do a custom configuration, instead you can do custom sampling rules. So this option is incorrect.Implement a network sampling rule This option has been added as a distractor.References:https://aws.amazon.com/xray/ https://docs.aws.amazon.com/xray/latest/devguide/xrayconsolesampling.html3f:T405,Using the Header CacheControl: maxage=0A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the CacheControl: maxage=0 header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint.via https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaycaching.html#invalidatemethodcaching Incorrect options:Use the Request parameter: ?bypass_cache=1 Method parameters take query string but this is not one of them.Using the Header BypassCache=1 This is a madeup option.Using the request parameter ?cachecontrolmaxage=0 To invalidate cache it requires a header and not a request parameter.Reference: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaycaching.html#invalidatemethodcaching40:Taea,Correct option:Use IAM roles and resourcebased policies delegate access across accounts within different partitions via programmatic access only This statement is incorrect and hence the right choice for this question. IAM roles and resourcebased policies delegate access across accounts only within a single partition. For example, assume that you have an account in US West (N. California) in the standard aws partition. You also have an account in China (Beijing) in the awscn partition. You can't use an Amazon S3 resourcebased policy in your account in China (Beijing) to allow access for users in your standard AWS account.Incorrect options:Use Resourcebased policies and AWS Identity and Access Management (IAM) policies for programmaticonly access to S3 bucket objects Use bucket policies to manage crossaccount control and audit the S3 object's permissions. If you apply a bucket policy at the bucket level, you can define who can access (Principal element), which objects they can access (Resource element), and how they can access (Action element). Applying a bucket policy at the bucket level allows you to define granular access to different objects inside the bucket by using multiple policies to control access. You can also review the bucket policy to see who can access objects in an S3 bucket.Use Access Control List (ACL) and IAM policies for programmaticonly access to S3 bucket objects Use object ACLs to manage permissions only for specific scenarios and only if ACLs meet your needs better than IAM and S3 bucket policies. Amazon S3 ACLs allow users to define only the following permissions sets: READ, WRITE, READ_ACP, WRITE_ACP, and FULL_CONTROL. You can use only an AWS account or one of the predefined Amazon S3 groups as a grantee for the Amazon S3 ACL.Use Crossaccount IAM roles for programmatic and console access to S3 bucket objects Not all AWS services support resourcebased policies. This means that you can use crossaccount IAM roles to centralize permission management when providing crossaccount access to multiple services. Using crossaccount IAM roles simplifies provisioning crossaccount access to S3 objects that are stored in multiple S3 buckets, removing the need to manage multiple policies for S3 buckets. This method allows crossaccount access to objects that are owned or uploaded by another AWS account or AWS services. If you don't use crossaccount IAM roles, the object ACL must be modified.References:https://docs.aws.amazon.com/AmazonS3/latest/dev/examplewalkthroughsmanagingaccessexample3.html https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_crossaccountwithroles.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compareresourcepolicies.html https://aws.amazon.com/premiumsupport/knowledgecenter/crossaccountaccesss3/41:T7e6,Correct option:Restrict access by using CORS Crossorigin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. When your API's resources receive requests from a domain other than the API's own domain and you want to restrict servicing these requests, you must disable crossorigin resource sharing (CORS) for selected methods on the resource.Incorrect options:Use Accountlevel throttling To prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API. By default, API Gateway limits the steadystate request rate to 10,000 requests per second (rps). It limits the burst (that is, the maximum bucket size) to 5,000 requests across all APIs within an AWS account. This is Accountlevel throttling. As you see, this is about limit on the number of requests and is not a suitable answer for the current scenario.Use Mapping Templates A mapping template is a script expressed in Velocity Template Language (VTL) and applied to the payload using JSONPath expressions. Mapping templates help format/structure the data in a way that it is easily readable, unlike a server response that might always be easy to ready. Mapping Templates have nothing to do with access and are not useful for the current scenario.Assign a Security Group to your API Gateway API Gateway does not use security groups but uses resource policies, which are JSON policy documents that you attach to an API to control whether a specified principal (typically an IAM user or role) can invoke the API. You can restrict IP address using this, the downside being, an IP address can be changed by the accessing user. So, this is not an optimal solution for the current use case.References:https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html https://docs.aws.amazon.com/apigateway/latest/developerguide/httpapiprotect.html https://docs.aws.amazon.com/apigateway/latest/developerguide/restapidatatransformations.html42:T91f,Correct option:Implement Amazon ElastiCache Redis in ClusterMode One can leverage ElastiCache for Redis with cluster mode enabled to enhance reliability and availability with little change to your existing workload. Cluster mode comes with the primary benefit of horizontal scaling of your Redis cluster, with almost zero impact on the performance of the cluster.When building production workloads, you should consider using a configuration with replication, unless you can easily recreate your data. Enabling ClusterMode provides a number of additional benefits in scaling your cluster. In short, it allows you to scale in or out the number of shards (horizontal scaling) versus scaling up or down the node type (vertical scaling). This means that ClusterMode can scale to very large amounts of storage (potentially 100s of terabytes) across up to 90 shards, whereas a single node can only store as much data in memory as the instance type has capacity for.Redis Cluster config: via https://aws.amazon.com/blogs/database/workwithclustermodeonamazonelasticacheforredis/ Incorrect options:Install Redis on an Amazon EC2 instance It is possible to install Redis directly onto Amazon EC2 instance. But, unlike ElastiCache for Redis, which is a managed service, you will need to maintain and manage your Redis installation.Implement Amazon ElastiCache Memcached Redis and Memcached are popular, opensource, inmemory data stores. Although they are both easy to use and offer high performance, there are important differences to consider when choosing an engine. Memcached is designed for simplicity while Redis offers a rich set of features that make it effective for a wide range of use cases. Redis offers snapshots facility, replication, and supports transactions, which Memcached cannot and hence ElastiCache Redis is the right choice for our use case.Migrate the database to Amazon Redshift Amazon Redshift belongs to "Big Data as a Service" cloud facility, while Redis can be primarily classified under "InMemory Databases". "Data Warehousing" is the primary reason why developers consider Amazon Redshift over the competitors, whereas "Performance" is the key factor in picking Redis.References:https://aws.amazon.com/blogs/database/workwithclustermodeonamazonelasticacheforredis/ https://aws.amazon.com/elasticache/redisvsmemcached/43:T926,Use DelaySeconds parameterAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, besteffort ordering, and atleastonce delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.Delay queues let you postpone the delivery of new messages to a queue for several seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.via https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsdelayqueues.html Incorrect options:Implement applicationside delay You can customize your application to delay sending messages but it is not a robust solution. You can run into a scenario where your application crashes before sending a message, then that message would be lost.Use visibility timeout parameter Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to postpone the delivery of new messages to the queue for a few seconds.Enable LongPolling Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds. You cannot use LongPolling to postpone the delivery of new messages to the queue for a few seconds.Reference: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsdelayqueues.html44:T7ed,All at onceThis is the quickest deployment method. Suitable if you can accept a short loss of service, and if quick deployments are important to you. With this method, Elastic Beanstalk deploys the new application version to each instance. Then, the web proxy or application server might need to restart. As a result, your application might be unavailable to users (or have low availability) for a short time.Overview of Elastic Beanstalk Deployment Policies: via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html Incorrect options:Rolling With this method, your application is deployed to your environment one batch of instances at a time. Most bandwidth is retained throughout the deployment. Avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. Suitable if you can't accept any period of completely lost service.Rolling with additional batches With this method, Elastic Beanstalk launches an extra batch of instances, then performs a rolling deployment. Launching the extra batch takes time, and ensures that the same bandwidth is retained throughout the deployment. This policy also avoids any reduced availability, although at a cost of an even longer deployment time compared to the Rolling method. Finally, this option is suitable if you must maintain the same bandwidth throughout the deployment.Immutable A slower deployment method, that ensures your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails. With this method, Elastic Beanstalk performs an immutable update to deploy your application. In an immutable update, a second Auto Scaling group is launched in your environment and the new version serves traffic alongside the old version until the new instances pass health checks. Reference: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html45:T695,Correct option:Configure CORS on the bucket B that is hosting the web fonts to allow Bucket A origin to make the requestsCrossorigin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain.To configure your bucket to allow crossorigin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support for each origin, and other operationspecific information.For the given usecase, you would create a in for bucket B to allow access from the S3 website origin hosted on bucket A.Please see this note for more details: via https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html Incorrect options:Enable versioning on both the buckets to facilitate the correct functioning of the website This option is a distractor and versioning will not help to address the web fonts loading issue on the website.Update bucket policies on both bucket A and bucket B to allow successful loading of the web fonts on the website It's not the bucket policies but the CORS configuration on bucket B that needs to be updated to allow web fonts to be loaded on the website. Updating bucket policies will not help to address the web fonts loading issue on the website.Configure CORS on the bucket A that is hosting the website to allow any origin to respond to requests The CORS configuration needs to be updated on bucket B to allow web fonts to be loaded on the website hosted on bucket A. So this option in incorrect.Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html46:T431,Correct option:VPC Flow Logs VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. After you've created a flow log, you can retrieve and view its data in the chosen destination.You can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored.Flow log data for a monitored network interface is recorded as flow log records, which are log events consisting of fields that describe the traffic flow.To create a flow log, you specify:The resource for which to create the flow logThe type of traffic to capture (accepted traffic, rejected traffic, or all traffic)The destinations to which you want to publish the flow log dataIncorrect options:VPN logsSubnet logsBGP logsThese three options are incorrect and have been added as distractors.Reference: https://docs.aws.amazon.com/vpc/latest/userguide/flowlogs.html47:Ta8b,Use a Lambda function alias that can point to the different versionsYou can use versions to manage the deployment of your functions. For example, you can publish a new version of a function for beta testing without affecting users of the stable production version. Lambda creates a new version of your function each time that you publish the function. The new version is a copy of the unpublished version of the function.By publishing a version of your function, you can store your code and configuration as a separate resource that cannot be changed.A Lambda alias is like a pointer to a specific function version. Users can access the function version using the alias Amazon Resource Name (ARN). Each alias has a unique ARN. An alias can point only to a function version, not to another alias. You can update an alias to point to the different versions of the Lambda function.Incorrect options:Use a Route 53 weighted policy that can point to the different Lambda function versions This option is a distractor, as Route 53 cannot be used for the given use case. Route 53 weighted policy lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software.Use CodeDeploy to configure blue/green deployments for the different Lambda function versions A deployment to the AWS Lambda compute platform is always a blue/green deployment. You do not specify a deployment type option. When you deploy to an AWS Lambda compute platform, the deployment configuration specifies the way traffic is shifted to the new Lambda function versions in your application. You can shift traffic using a canary, linear, or allatonce deployment configuration. Once deployed, you cannot go back to the previous versions of your Lambda function. So this option is incorrect.Use Lambda function layers that can point to the different versions Lambda layers provide a convenient way to package libraries and other dependencies that you can use with your Lambda functions. Using layers reduces the size of uploaded deployment archives and makes it faster to deploy your code. You cannot use the Lambda function layers to point to the different versions of the Lambda function.References:https://docs.aws.amazon.com/lambda/latest/dg/configurationaliases.html https://docs.aws.amazon.com/lambda/latest/dg/configurationversions.html https://docs.aws.amazon.com/codedeploy/latest/userguide/deploymentconfigurations.html#deploymentconfigurationlambda https://docs.aws.amazon.com/lambda/latest/dg/configurationlayers.html48:T7a1,Define a dev environment with a single instance and a 'load test' environment that has settings close to production environmentAWS Elastic Beanstalk makes it easy to create new environments for your application. You can create and manage separate environments for development, testing, and production use, and you can deploy any version of your application to any environment. Environments can be longrunning or temporary. When you terminate an environment, you can save its configuration to recreate it later.It is common practice to have many environments for the same application. You can deploy multiple environments when you need to run multiple versions of an application. So for the given usecase, you can set up 'dev' and 'load test' environment.via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.managing.html You cannot have multiple development environments in Elastic Beanstalk, just one development, and one production environment Incorrect, use the Create New Environment wizard in the AWS Management Console for BeanStalk to guide you on this.Use only one Beanstalk environment and perform configuration changes using an Ansible script Ansible is an opensource deployment tool that integrates with AWS. It allows us to deploy the infrastructure. Elastic Beanstalk provisions the servers that you need for hosting the application and it also handles multiple environments, so Beanstalk is a better option.Create an Application Load Balancer to route based on hostname so you can pass on parameters to the development Elastic Beanstalk environment. Create a file in .ebextensions/ to know how to handle the traffic coming from the ALB This is not a good design if you need to load test because you will have two versions on the same instances and may not be able to access resources in the system due to the load testing.Reference: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.environments.html49:T6fc,A new deployment of the last known working version of the application is deployed with a new deployment IDAWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running onpremises. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during deployment, and handles the complexity of updating your applications.CodeDeploy rolls back deployments by redeploying a previously deployed revision of an application as a new deployment. These rolledback deployments are technically new deployments, with new deployment IDs, rather than restored versions of a previous deployment.To roll back an application to a previous revision, you just need to deploy that revision. AWS CodeDeploy keeps track of the files that were copied for the current revision and removes them before starting a new deployment, so there is no difference between redeploy and rollback. However, you need to make sure that the previous revisions are available for rollback.Incorrect options:The last known working deployment is automatically restored using the snapshot stored in Amazon S3 CodeDeploy deployment does not have a snapshot stored on S3, so this option is incorrect.AWS CodePipeline promotes the most recent working deployment with a SUCCEEDED status to production The usecase does not talk about using CodePipeline, so this option just acts as a distractor.CodeDeploy switches the Route 53 alias records back to the known good green deployment and terminates the failed blue deployment The usecase does not talk about the blue/green deployment, so this option has just been added as a distractor.Reference: https://docs.aws.amazon.com/codedeploy/latest/userguide/deploymentsrollbackandredeploy.html4a:T4dd,Correct option:AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and thirdparty resources and provision them in an orderly and predictable fashion.How CloudFormation Works: via https://aws.amazon.com/cloudformation/Parameter types enable CloudFormation to validate inputs earlier in the stack creation process.CloudFormation currently supports the following parameter types:String – A literal stringNumber – An integer or floatList – An array of integers or floatsCommaDelimitedList – An array of literal strings that are separated by commasAWS::EC2::KeyPair::KeyName – An Amazon EC2 key pair nameAWS::EC2::SecurityGroup::Id – A security group IDAWS::EC2::Subnet::Id – A subnet IDAWS::EC2::VPC::Id – A VPC IDList – An array of VPC IDsList – An array of security group IDsList – An array of subnet IDsDependentParameterIn CloudFormation, parameters are all independent and cannot depend on each other. Therefore, this is an invalid parameter type.Incorrect options:StringCommaDelimitedListAWS::EC2::KeyPair::KeyNameAs mentioned in the explanation above, these are valid parameter types.Reference:https://aws.amazon.com/blogs/devops/usingthenewcloudformationparametertypes//4b:T906,Correct options:Use Amazon Cognito for usermanagement and facilitating the login/signup processUse Amazon Cognito to enable MultiFactor Authentication (MFA) when users loginAmazon Cognito lets you add user signup, signin, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports signin with social identity providers, such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0.A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a thirdparty identity provider (IdP). Whether your users signin directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK.Cognito user pools provide support for signup and signin services as well as security features such as multifactor authentication (MFA).via https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.htmlExam Alert:Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools: via https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.html Incorrect options:Use Lambda functions and DynamoDB to create a custom solution for user managementUse Lambda functions and RDS to create a custom solution for user managementAs the problem statement mentions that the solution should require the least amount of development effort, so you cannot use Lambda functions with DynamoDB or RDS to create a custom solution. So both these options are incorrect.Use Amazon SNS to send MultiFactor Authentication (MFA) code via SMS to mobile app users Amazon SNS cannot be used to send MFA codes via SMS to the user's mobile devices as this functionality is only meant to be used for IAM users. An SMS (short message service) MFA device can be any mobile device with a phone number that can receive standard SMS text messages. AWS will soon end support for SMS multifactor authentication (MFA).Please see this for more details: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_sms.html Reference: https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.html4c:T525,The cluster name Parameter has not been updated in the file /etc/ecs/ecs.config during bootstrap In the ecs.config file you have to configure the parameter ECS_CLUSTER='your_cluster_name' to register the container instance with a cluster named 'your_cluster_name'.Sample config for ECS Container Agent: via https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.htmlIncorrect options:The EC2 instance is missing IAM permissions to join the other clusters EC2 instances are getting registered to the first cluster, so permissions are not an issue here and hence this statement is an incorrect choice for the current use case.The ECS agent Docker image must be rebuilt to connect to the other clusters Since the first set of instances got created from the template without any issues, there is no issue with the ECS agent here.The security groups on the EC2 instance are pointing to the wrong ECS cluster Security groups govern the rules about the incoming network traffic to your ECS containers. The issue here is not about user access and hence is a wrong choice for the current use case.References:https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_container_instance.html4d:T595,You terminated the container instance while it was in STOPPED state, that lead to this synchronization issues If you terminate a container instance while it is in the STOPPED state, that container instance isn't automatically removed from the cluster. You will need to deregister your container instance in the STOPPED state by using the Amazon ECS console or AWS Command Line Interface. Once deregistered, the container instance will no longer appear as a resource in your Amazon ECS cluster.Incorrect options:You terminated the container instance while it was in RUNNING state, that lead to this synchronization issues This is an incorrect statement. If you terminate a container instance in the RUNNING state, that container instance is automatically removed, or deregistered, from the cluster.The container instance has been terminated with AWS CLI, whereas, for ECS instances, Amazon ECS CLI should be used to avoid any synchronization issues This is incorrect and has been added as a distractor.A custom software on the container instance could have failed and resulted in the container hanging in an unhealthy state till restarted again This is an incorrect statement. It is already mentioned in the question that the developer has terminated the instance.References:https://aws.amazon.com/premiumsupport/knowledgecenter/deregisterecsinstance/ https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html4e:T468,Correct option:Use the AWS CLI dryrun option: The dryrun option checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is DryRunOperation, otherwise, it is UnauthorizedOperation.Incorrect options:Use the AWS CLI test option This is a madeup option and has been added as a distractor.Retrieve the policy using the EC2 metadata service and use the IAM policy simulator EC2 metadata service is used to retrieve dynamic information such as instanceid, localhostname, publichostname. This cannot be used to check whether you have the required permissions for the action.Using the CLI, create a dummy EC2 and delete it using another CLI call That would not work as the current EC2 may have permissions that the dummy instance does not have. If permissions were the same it can work but it's not as elegant as using the dryrun option.References:https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html https://docs.aws.amazon.com/cli/latest/reference/ec2/terminateinstances.htmll4f:T637,Correct option:Opt for Blue/Green deployment A Blue/Green deployment is used to update your applications while minimizing interruptions caused by the changes of a new application version. CodeDeploy provisions your new application version alongside the old version before rerouting your production traffic. The behavior of your deployment depends on which compute platform you use:AWS Lambda: Traffic is shifted from one version of a Lambda function to a new version of the same Lambda function.Amazon ECS: Traffic is shifted from a task set in your Amazon ECS service to an updated, replacement task set in the same Amazon ECS service.EC2/OnPremises: Traffic is shifted from one set of instances in the original environment to a replacement set of instances.Incorrect options:Opt for Rolling deployment This deployment type is present for AWS Elastic Beanstalk and not for EC2 instances directly.Opt for Immutable deployment This deployment type is present for AWS Elastic Beanstalk and not for EC2 instances directly.Opt for Inplace deployment Under this deployment type, the application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete.References:https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcomedeploymentoverviewbluegreen https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html50:T4d3,Correct options:You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token. An authentication token is a unique string of characters that Amazon RDS generates on request. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM.RDS MySQL IAM database authentication works with MySQL and PostgreSQL engines for Aurora as well as MySQL, MariaDB and RDS PostgreSQL engines for RDS.RDS PostGreSQL IAM database authentication works with MySQL and PostgreSQL engines for Aurora as well as MySQL, MariaDB and RDS PostgreSQL engines for RDS.Incorrect options:RDS OracleRDS SQL ServerThese two options contradict the details in the explanation above, so these are incorrect.RDS Db2 This option has been added as a distractor. Db2 is a family of data management products, including database servers, developed by IBM. RDS does not support Db2 database engine.Reference: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html51:T5a2,Correct option:AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your onpremises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications.An EC2/OnPremises deployment hook is executed once per deployment to an instance. You can specify one or more scripts to run in a hook.via https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfilestructurehooks.html#referenceappspecfilestructurehooksrunorder ValidateService: ValidateService is the last deployment lifecycle event. It is used to verify the deployment was completed successfully.Incorrect options:AfterInstall You can use this deployment lifecycle event for tasks such as configuring your application or changing file permissionsApplicationStart You typically use this deployment lifecycle event to restart services that were stopped during ApplicationStopAllowTraffic During this deployment lifecycle event, internet traffic is allowed to access instances after a deployment. This event is reserved for the AWS CodeDeploy agent and cannot be used to run scriptsReference:https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfilestructurehooks.html#referenceappspecfilestructurehooksrunorderr52:Tc30,Correct option:Amazon Kinesis Data StreamsAmazon Kinesis Data Streams (KDS) is a massively scalable and durable realtime data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and locationtracking events. The data collected is available in milliseconds to enable realtime analytics use cases such as realtime dashboards, realtime anomaly detection, dynamic pricing, and more.Kinesis Data Streams enables realtime processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).Incorrect options:Amazon Simple Queue Service (SQS) Amazon Simple Queue Service (Amazon SQS) offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with messagelevel ack/fail semantics), such as automated workflows. AWS recommends using Amazon SQS for cases where individual message fail/success are important, message delays are needed and there is only one consumer for the messages received (if more than one consumers need to consume the message, then AWS suggests configuring more queues).Amazon Kinesis Firehose Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near realtime analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration.Kinesis data streams is highly customizable and best suited for developers building custom applications or streaming data for specialized needs. Data Streams also provide greater flexibility in integrating downstream applications than Firehose. Data Streams is also a costeffective option compared to Firehose. Therefore, KDS is the right solution.AWS Glue AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. AWS Glue provides all of the capabilities needed for data integration so that you can start analyzing your data and putting it to use in minutes instead of months. Glue is not best suited to handle realtime data.References:https://aws.amazon.com/kinesis/datastreams/ https://aws.amazon.com/sqs/ https://aws.amazon.com/kinesis/datafirehose/53:T70d,Correct options:SameRegion Replication (SRR) and CrossRegion Replication (CRR) can be configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags Amazon S3 Replication (CRR and SRR) is configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags. You add a replication configuration on your source bucket by specifying a destination bucket in the same or different AWS region for replication.S3 lifecycle actions are not replicated with S3 replication With S3 Replication (CRR and SRR), you can establish replication rules to make copies of your objects into another storage class, in the same or a different region. Lifecycle actions are not replicated, and if you want the same lifecycle configuration applied to both source and destination buckets, enable the same lifecycle configuration on both.Incorrect options:Object tags cannot be replicated across AWS Regions using CrossRegion Replication Object tags can be replicated across AWS Regions using CrossRegion Replication. For customers with CrossRegion Replication already enabled, new permissions are required for tags to replicate.Once replication is enabled on a bucket, all old and new objects will be replicated Replication only replicates the objects added to the bucket after replication is enabled on the bucket. Any objects present in the bucket before enabling replication are not replicated.Replicated objects do not retain metadata You can use replication to make copies of your objects that retain all metadata, such as the original object creation time and version IDs. This capability is important if you need to ensure that your replica is identical to the source object.Reference: https://docs.amazonaws.cn/en_us/AmazonS3/latest/userguide/replication.html54:T91e,Correct option:ServerSide Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSEKMS)You have the following options for protecting data at rest in Amazon S3:ServerSide Encryption – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.ClientSide Encryption – Encrypt data clientside and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.When you use serverside encryption with AWS KMS (SSEKMS), you can use the default AWS managed CMK, or you can specify a customermanaged CMK that you have already created.Creating your own customermanaged CMK gives you more flexibility and control over the CMK. For example, you can create, rotate, and disable customermanaged CMKs. You can also define access controls and audit the customermanaged CMKs that you use to protect your data.Please see this note for more details: via https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html Incorrect options:ServerSide Encryption with Amazon S3Managed Keys (SSES3) When you use ServerSide Encryption with Amazon S3Managed Keys (SSES3), each object is encrypted with a unique key. As an additional safeguard, AWS encrypts the key itself with a master key that it regularly rotates. So this option is incorrect for the given usecase.ServerSide Encryption with CustomerProvided Keys (SSEC) With ServerSide Encryption with CustomerProvided Keys (SSEC), you will need to create the encryption keys as well as manage the corresponding process to rotate and remove the encryption keys. Amazon S3 manages the data encryption, as it writes to disks, as well as the data decryption when you access your objects. So this option is incorrect for the given usecase.ServerSide Encryption with Secrets Manager AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. You cannot combine ServerSide Encryption with Secrets Manager to create, rotate, or disable the encryption keys.Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html55:T79b,The account A administrator creates an IAM role and attaches a permissions policyThe account A administrator attaches a trust policy to the role that identifies account B as the principal who can assume the roleThe account B administrator delegates the permission to assume the role to any users in account BTo grant crossaccount permissions, you need to attach an identitybased permissions policy to an IAM role. For example, the AWS account A administrator can create a role to grant crossaccount permissions to AWS account B as follows:The account A administrator creates an IAM role and attaches a permissions policy—that grants permissions on resources in account A—to the role.The account A administrator attaches a trust policy to the role that identifies account B as the principal who can assume the role.The account B administrator delegates the permission to assume the role to any users in account B. This allows users in account B to create or access queues in account A.Incorrect options:The account B administrator creates an IAM role and attaches a trust policy to the role with account B as the principal As mentioned above, the account A administrator needs to create an IAM role and then attach a permissions policy. So, this option is incorrect.The account A administrator delegates the permission to assume the role to any users in account A This is irrelevant, as users in account B need to be given access.The account A administrator attaches a trust policy to the role that identifies account B as the AWS service principal who can assume the role AWS service principal is given as principal in the trust policy when you need to grant the permission to assume the role to an AWS service. The given use case talks about giving permission to another account. So, service principal is not an option here.Reference: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsoverviewofmanagingaccess.html56:T566,Correct option:The total size of all environment variables shouldn't exceed 4 KB. There is no limit on the number of variablesAn environment variable is a pair of strings that are stored in a function's versionspecific configuration. The Lambda runtime makes environment variables available to your code and sets additional environment variables that contain information about the function and invocation request. The total size of all environment variables doesn't exceed 4 KB. There is no limit defined on the number of variables that can be used.Incorrect options:The total size of all environment variables shouldn't exceed 8 KB. The maximum number of variables that can be created is 50 Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables.The total size of all environment variables shouldn't exceed 8 KB. There is no limit on the number of variables Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables.The total size of all environment variables shouldn't exceed 4 KB. The maximum number of variables that can be created is 35 Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables.Reference: https://docs.aws.amazon.com/lambda/latest/dg/configurationenvvars.html57:T6f3,Move the Amazon S3 client initialization, out of your function handler AWS best practices for Lambda suggest taking advantage of execution context reuse to improve the performance of your functions. Initialize SDK clients and database connections outside of the function handler, and cache static assets locally in the /tmp directory. Subsequent invocations processed by the same instance of your function can reuse these resources. This saves execution time and cost. To avoid potential data leaks across invocations, don’t use the execution context to store user data, events, or other information with security implications.Incorrect options:Use environment variables to pass operational parameters This is one of the suggested best practices for Lambda. By using environment variables to pass operational parameters you can avoid hardcoding useful information. But, this is not the right answer for the current usecase, since it talks about reusing context.Assign more RAM to the function Increasing RAM will help speed up the process. But, in the current question, the reviewer has specifically mentioned about reusing context. Hence, this is not the right answer.Enable XRay integration You can use AWS XRay to visualize the components of your application, identify performance bottlenecks, and troubleshoot requests that resulted in an error. Your Lambda functions send trace data to XRay, and XRay processes the data to generate a service map and searchable trace summaries. This is a useful tool for troubleshooting. But, for the current usecase, we already know the bottleneck that needs to be fixed and that is the context reuse.References:https://docs.aws.amazon.com/lambda/latest/dg/bestpractices.html https://docs.aws.amazon.com/lambda/latest/dg/servicesxray.html58:T91f,Use the AWS::Region pseudo parameterPseudo parameters are parameters that are predefined by AWS CloudFormation. You don't declare them in your template. Use them the same way as you would a parameter, as the argument for the Ref function.You can access pseudo parameters in a CloudFormation template like so:Outputs: MyStacksRegion: Value: !Ref "AWS::Region"The AWS::Region pseudo parameter returns a string representing the Region in which the encompassing resource is being created, such as uswest2.Incorrect options:Create a CloudFormation parameter for Region and let the desired value be populated at the time of deployment Although it is certainly possible to use a CloudFormation parameter to populate the desired value of the Region at the time of deployment, however, this is not operationally efficient, as you can directly use the AWS::Region pseudo parameter for this.Set up a mapping containing the key and the named values for all AWS Regions and then have the CloudFormation template autoselect the desired value The Mappings section matches a key to a corresponding set of named values. For example, if you want to set values based on a Region, you can create a mapping that uses the Region name as a key and contains the values you want to specify for each specific Region. You use the Fn::FindInMap intrinsic function to retrieve values in a map. This option is incorrect as the CloudFormation template cannot autoselect the desired value of the Region from a mapping.Create an AWS Lambdabacked custom resource for Region and let the desired value be populated at the time of deployment by the Lambda Custom resources enable you to write custom provisioning logic in templates that AWS CloudFormation runs anytime you create, update (if you changed the custom resource), or delete stacks. When you associate a Lambda function with a custom resource, the function is invoked whenever the custom resource is created, updated, or deleted. This option is a distractor, as Region is not a custom resource that needs to be provisioned.References:https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudoparameterreference.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/templatecustomresources.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappingssectionstructure.html59:T82d,Create an S3 event to invoke a Lambda function that inserts records into DynamoDBThe Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. You store this configuration in the notification subresource that is associated with a bucket.Amazon S3 APIs such as PUT, POST, and COPY can create an object. Using these event types, you can enable notification when an object is created using a specific API, or you can use the s3:ObjectCreated:* event type to request notification regardless of the API that was used to create an object.For the given usecase, you would create an S3 event notification that triggers a Lambda function whenever we have a PUT object operation in the S3 bucket. The Lambda function in turn would execute custom code to inserts records into DynamoDB.via https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html Incorrect options:Write a cron job that will execute a Lambda function at a scheduled time and insert the records into DynamoDB This is not efficient because there may not be any unprocessed file in the S3 bucket when the cron triggers the Lambda on schedule. So this is not the correct option.Set up an event with Amazon CloudWatch Events that will monitor the S3 bucket and then insert the records into DynamoDB The CloudWatch event cannot directly insert records into DynamoDB as it's not a supported target type. The CloudWatch event needs to use something like a Lambda function to insert the records into DynamoDB.Develop a Lambda function that will poll the S3 bucket and then insert the records into DynamoDB This is not efficient because there may not be any unprocessed file in the S3 bucket when the Lambda function polls the S3 bucket at a given time interval. So this is not the correct option.Reference:https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.htmll5a:T875,Correct option:Use AWS Lambda aliases A Lambda alias is like a pointer to a specific Lambda function version. You can create one or more aliases for your AWS Lambda function. Users can access the function version using the alias ARN. An alias can only point to a function version, not to another alias. You can update an alias to point to a new version of the function. Event sources such as Amazon S3 invoke your Lambda function. These event sources maintain a mapping that identifies the function to invoke when events occur. If you specify a Lambda function alias in the mapping configuration, you don't need to update the mapping when the function version changes. This is the right choice for the current requirement.Incorrect options:Use Tags to distinguish the different versions You can tag Lambda functions to organize them by owner, project or department. Tags are freeform keyvalue pairs that are supported across AWS services for use in filtering resources and adding detail to billing reports. This does not address the given usecase.Use environment variables You can use environment variables to store secrets securely and adjust your function's behavior without updating code. An environment variable is a pair of strings that are stored in a function's versionspecific configuration. The Lambda runtime makes environment variables available to your code and sets additional environment variables that contain information about the function and invocation request. For example, you can use environment variables to point to test, development or production databases by passing it as an environment variable during runtime. This option does not address the given usecase.Deploy your Lambda in a VPC Amazon Virtual Private Cloud (Amazon VPC) enables you to launch AWS resources into a virtual network that you've defined. This adds another layer of security for your entire architecture. Not the right choice for the given scenario.References:https://docs.aws.amazon.com/lambda/latest/dg/configurationaliases.html https://docs.aws.amazon.com/lambda/latest/dg/configurationtags.html https://docs.aws.amazon.com/lambda/latest/dg/configurationenvvars.html5b:T9fc,WebSocket APIsIn a WebSocket API, the client and the server can both send messages to each other at any time. Backend servers can easily push data to connected users and devices, avoiding the need to implement complex polling mechanisms.For example, you could build a serverless application using an API Gateway WebSocket API and AWS Lambda to send and receive messages to and from individual users or groups of users in a chat room. Or you could invoke backend services such as AWS Lambda, Amazon Kinesis, or an HTTP endpoint based on message content.You can use API Gateway WebSocket APIs to build secure, realtime communication applications without having to provision or manage any servers to manage connections or largescale data exchanges. Targeted use cases include realtime applications such as the following:Chat applicationsRealtime dashboards such as stock tickersRealtime alerts and notificationsAPI Gateway provides WebSocket API management functionality such as the following:Monitoring and throttling of connections and messagesUsing AWS XRay to trace messages as they travel through the APIs to backend servicesEasy integration with HTTP/HTTPS endpointsIncorrect options:REST or HTTP APIsREST APIs An API Gateway REST API is made up of resources and methods. A resource is a logical entity that an app can access through a resource path. A method corresponds to a REST API request that is submitted by the user of your API and the response returned to the user.For example, /incomes could be the path of a resource representing the income of the app user. A resource can have one or more operations that are defined by appropriate HTTP verbs such as GET, POST, PUT, PATCH, and DELETE. A combination of a resource path and an operation identifies a method of the API. For example, a POST /incomes method could add an income earned by the caller, and a GET /expenses method could query the reported expenses incurred by the caller.HTTP APIs HTTP APIs enable you to create RESTful APIs with lower latency and lower cost than REST APIs. You can use HTTP APIs to send requests to AWS Lambda functions or to any publicly routable HTTP endpoint.For example, you can create an HTTP API that integrates with a Lambda function on the backend. When a client calls your API, API Gateway sends the request to the Lambda function and returns the function's response to the client.Server push mechanism is not possible in REST and HTTP APIs.Reference: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayoverviewdeveloperexperience.html5c:T74f,Correct option:Enable API Gateway Caching You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified timetolive (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled.Incorrect options:Use Mapping Templates A mapping template is a script expressed in Velocity Template Language (VTL) and applied to the payload using JSONPath expressions. Mapping templates help format/structure the data in a way that it is easily readable, unlike a server response that might always be easy to ready. Mapping Templates do not help in latency issues of the APIs.Use Stage Variables Stage variables act like environment variables and can be used to change the behavior of your API Gateway methods for each deployment stage; for example, making it possible to reach a different back end depending on which stage the API is running on. Stage variables do not help in latency issues.Use Amazon Kinesis Data Streams to stream incoming data and reduce the burden on Gateway APIs Amazon Kinesis Data Streams (KDS) is a massively scalable and durable realtime data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and locationtracking events.Reference: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaycaching.html5d:Td60,Correct option:Sticky sessions are enabled for the load balancer This can be the reason for potential unequal traffic routing by the load balancer. Sticky sessions are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information in order to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies.When a load balancer first receives a request from a client, it routes the request to a target, generates a cookie named AWSALB that encodes information about the selected target, encrypts the cookie, and includes the cookie in the response to the client. The client should include the cookie that it receives in subsequent requests to the load balancer. When the load balancer receives a request from a client that contains the cookie, if sticky sessions are enabled for the target group and the request goes to the same target group, the load balancer detects the cookie and routes the request to the same target.If you use durationbased session stickiness, configure an appropriate cookie expiration time for your specific use case. If you set session stickiness from individual applications, use session cookies instead of persistent cookies where possible.Instances of a specific capacity type aren’t equally distributed across Availability Zones A Classic Load Balancer with HTTP or HTTPS listeners might route more traffic to highercapacity instance types. This distribution aims to prevent lowercapacity instance types from having too many outstanding requests. It’s a best practice to use similar instance types and configurations to reduce the likelihood of capacity gaps and traffic imbalances.A traffic imbalance might also occur if you have instances of similar capacities running on different Amazon Machine Images (AMIs). In this scenario, the imbalance of the traffic in favor of highercapacity instance types is desirable.Incorrect options:There could be shortlived TCP connections between clients and instances This is an incorrect statement. Longlived TCP connections between clients and instances can potentially lead to unequal distribution of traffic by the load balancer. Longlived TCP connections between clients and instances cause uneven traffic load distribution by design. As a result, new instances take longer to reach connection equilibrium. Be sure to check your metrics for longlived TCP connections that might be causing routing issues in the load balancer.For Application Load Balancers, crosszone load balancing is disabled by default This is an incorrect statement. With Application Load Balancers, crosszone load balancing is always enabled.After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer, thereby receiving random bursts of traffic This is an incorrect statement. After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer. However, even though they remain registered, the load balancer does not route traffic to them.References:https://docs.aws.amazon.com/elasticloadbalancing/latest/application/loadbalancertargetgroups.html#stickysessions https://aws.amazon.com/premiumsupport/knowledgecenter/elbfixunequaltrafficrouting/ https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/howelasticloadbalancingworks.html#availabilityzones5e:T794,Correct option:"Save the S3 key for each user's profile photo in a DynamoDB table and use a lambda function to dynamically generate a presigned URL. Reference this URL for display via the web application"On Amazon S3, all objects by default are private. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a presigned URL, using their own security credentials, to grant timelimited permission to download the objects.You can also use an IAM instance profile to create a presigned URL. When you create a presigned URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object), and expiration date and time. The presigned URLs are valid only for the specified duration. So for the given usecase, the object key can be retrieved from the DynamoDB table, and then the application can generate the presigned URL using the IAM instance profile.Please see this note for more details: via https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html Incorrect options:"Make the S3 bucket public so that the application can reference the image URL for display" Making the S3 bucket public would violate the security and privacy requirements for the usecase, so this option is incorrect."Keep each user's profile image encoded in base64 format in a DynamoDB table and reference it from the application for display""Keep each user's profile image encoded in base64 format in an RDS table and reference it from the application for display"It's a bad practice to keep the raw image data in the database itself. Also, it would not be possible to create a secure access URL for the image without a significant development effort. Hence both these options are incorrect.Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html5f:T46d,The user will be denied access because the policy has an explicit deny on it User will be denied access because any explicit deny overrides the allow.Policy Evaluation explained: via https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluationlogic.html Incorrect options:The IAM user stands in an invalid state, because of conflicting policies This is an incorrect statement. Access policies can have allow and deny permissions on them and based on policy rules they are evaluated. A user account does not get invalid because of policies.The user will get access because it has an explicit allow As discussed above, explicit deny overrides all other permissions and hence the user will be denied access.The order of the policy matters. If policy 1 is before 2, then the user is denied access. If policy 2 is before 1, then the user is allowed access If policies that apply to a request include an Allow statement and a Deny statement, the Deny statement trumps the Allow statement. The request is explicitly denied.Reference: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluationlogic.html60:Tb1b,Correct option:Make a GenerateDataKey API call that returns a plaintext key and an encrypted copy of a data key. Use a plaintext key to encrypt the data GenerateDataKey API, generates a unique symmetric data key for clientside encryption. This operation returns a plaintext copy of the data key and a copy that is encrypted under a customer master key (CMK) that you specify. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data.GenerateDataKey returns a unique data key for each request. The bytes in the plaintext key are not related to the caller or the CMK.To encrypt data outside of AWS KMS:Use the GenerateDataKey operation to get a data key.Use the plaintext data key (in the Plaintext field of the response) to encrypt your data outside of AWS KMS. Then erase the plaintext data key from memory.Store the encrypted data key (in the CiphertextBlob field of the response) with the encrypted data.To decrypt data outside of AWS KMS:Use the Decrypt operation to decrypt the encrypted data key. The operation returns a plaintext copy of the data key.Use the plaintext data key to decrypt data outside of AWS KMS, then erase the plaintext data key from memory.Incorrect options:Make a GenerateDataKeyWithPlaintext API call that returns an encrypted copy of a data key. Use a plaintext key to encrypt the data This is a madeup option, given only as a distractor.Make an Encrypt API call to encrypt the plaintext data as ciphertext using a customer master key (CMK) with imported key material Encrypt API is used to encrypt plaintext into ciphertext by using a customer master key (CMK). The Encrypt operation has two primary use cases:To encrypt small amounts of arbitrary data, such as a personal identifier or database password, or other sensitive information.To move encrypted data from one AWS Region to another.Neither of the two is useful for the given scenario.Make a GenerateDataKeyWithoutPlaintext API call that returns an encrypted copy of a data key. Use an encrypted key to encrypt the data GenerateDataKeyWithoutPlaintext API, generates a unique symmetric data key. This operation returns a data key that is encrypted under a customer master key (CMK) that you specify.GenerateDataKeyWithoutPlaintext is identical to the GenerateDataKey operation except that returns only the encrypted copy of the data key. This operation is useful for systems that need to encrypt data at some point, but not immediately. When you need to encrypt the data, you call the Decrypt operation on the encrypted copy of the key.References:https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html https://docs.aws.amazon.com/kms/latest/APIReference/API_Encrypt.html https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKeyWithoutPlaintext.html61:T7ce,Path basedYou can create a listener with rules to forward requests based on the URL path. This is known as pathbased routing. If you are running microservices, you can route traffic to multiple backend services using pathbased routing. For example, you can route general requests to one target group and request to render images to another target group.This pathbased routing allows you to route requests to, for example, /api to one set of servers (also known as target groups) and /mobile to another set. Segmenting your traffic in this way gives you the ability to control the processing environment for each category of requests. Perhaps /api requests are best processed on Compute Optimized instances, while /mobile requests are best handled by Memory Optimized instances.Host basedYou can create Application Load Balancer rules that route incoming traffic based on the domain name specified in the Host header. Requests to api.example.com can be sent to one target group, requests to mobile.example.com to another, and all others (by way of a default rule) can be sent to a third. You can also create rules that combine hostbased routing and pathbased routing. This would allow you to route requests to api.example.com/production and api.example.com/sandbox to distinct target groups.via https://docs.aws.amazon.com/elasticloadbalancing/latest/application/loadbalancerlisteners.html#ruleconditiontypes Incorrect options:Client IP This option has been added as a distractor. Routing is not based on the client's IP address.Web browser version Routing has nothing to do with the client's web browser, if it was then there is something sneaky going on.Cookie value Application Load Balancers support load balancergenerated cookies only and you cannot modify them. When routing sticky sessions to route requests to the same target then cookies are needed to be supported by the client's browser.Reference: https://aws.amazon.com/blogs/aws/newhostbasedroutingsupportforawsapplicationloadbalancers/62:T8a4,Correct option:Use S3 Object Ownership to default bucket owner to be the owner of all objects in the bucketS3 Object Ownership is an Amazon S3 bucket setting that you can use to control ownership of new objects that are uploaded to your buckets. By default, when other AWS accounts upload objects to your bucket, the objects remain owned by the uploading account. With S3 Object Ownership, any new objects that are written by other accounts with the bucketownerfullcontrol canned access control list (ACL) automatically become owned by the bucket owner, who then has full control of the objects.S3 Object Ownership has two settings: 1. Object writer – The uploading account will own the object. 2. Bucket owner preferred – The bucket owner will own the object if the object is uploaded with the bucketownerfullcontrol canned ACL. Without this setting and canned ACL, the object is uploaded and remains owned by the uploading account.Incorrect options:Use S3 CORS to make the S3 bucket owner, the owner of all objects in the bucket Crossorigin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain.Use S3 Access Analyzer to identify the owners of all objects and change the ownership to the bucket owner Access Analyzer for S3 helps review all buckets that have bucket access control lists (ACLs), bucket policies, or access point policies that grant public or shared access. Access Analyzer for S3 alerts you to buckets that are configured to allow access to anyone on the internet or other AWS accounts, including AWS accounts outside of your organization.Use Bucket Access Control Lists (ACLs) to control access on S3 bucket and then define its owner Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. A bucket ACLs allow you to control access at bucket level.None of the above features are useful for the current scenario and hence are incorrect options.References:https://docs.aws.amazon.com/AmazonS3/latest/userguide/aboutobjectownership.html https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html63:Tb86,Correct option:Use Exponential Backoff technique to introduce delay in time before attempting to execute the operation again A “Throttling – Maximum sending rate exceeded” error is retriable. This error is different than other errors returned by Amazon SES. A request rejected with a “Throttling” error can be retried at a later time and is likely to succeed.Retries are “selfish.” In other words, when a client retries, it spends more of the server's time to get a higher chance of success. Where failures are rare or transient, that's not a problem. This is because the overall number of retried requests is small, and the tradeoff of increasing apparent availability works well. When failures are caused by overload, retries that increase load can make matters significantly worse. They can even delay recovery by keeping the load high long after the original issue is resolved.The preferred solution is to use a backoff. Instead of retrying immediately and aggressively, the client waits some amount of time between tries. The most common pattern is an exponential backoff, where the wait time is increased exponentially after every attempt.A variety of factors can affect your send rate, e.g. message size, network performance or Amazon SES availability. The advantage of the exponential backoff approach is that your application will selftune and it will call Amazon SES at close to the maximum allowed rate.Incorrect options:Configure Timeout mechanism for each request made to the SES service Requests are configured to timeout if they do not complete successfully in a given time. This helps free up the database, application and any other resource that could potentially keep on waiting to eventually succeed. But, if errors are caused by load, retries can be ineffective if all clients retry at the same time. Throttling error signifies that load is high on SES and it does not make sense to keep retrying.Raise a service request with Amazon to increase the throttling limit for the SES API If throttling error is persistent, then it indicates a high load on the system consistently and increasing the throttling limit will be the right solution for the problem. But, the error is only intermittent here, signifying that decreasing the rate of requests will handle the error.Implement retry mechanism for all 4xx errors to avoid throttling error 4xx status codes indicate that there was a problem with the client request. Common client request errors include providing invalid credentials and omitting required parameters. When you get a 4xx error, you need to correct the problem and resubmit a properly formed client request. Throttling is a server error and not a client error, hence retry on 4xx errors does not make sense here.References:https://aws.amazon.com/builderslibrary/timeoutsretriesandbackoffwithjitter/ https://aws.amazon.com/blogs/messagingandtargeting/howtohandleathrottlingmaximumsendingrateexceedederror//64:T127d,Correct option:Configure and push highresolution custom metrics to CloudWatch that record the failures of the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rateYou can publish your own metrics, known as custom metrics, to CloudWatch using the AWS CLI or an API.Each metric is one of the following:Standard resolution, with data having a oneminute granularityHigh resolution, with data at a granularity of one secondMetrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a highresolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.Highresolution metrics can give you more immediate insight into your application's subminute activity. Keep in mind that every PutMetricData call for a custom metric is charged, so calling PutMetricData more often on a highresolution metric can lead to higher charges.You can create metric and composite alarms in Amazon CloudWatch. For the given use case, you can set up a CloudWatch metric alarm that watches the custom metric that captures the API errors and then triggers the alarm when the API error rate exceeds the 5% threshold. The alarm then sends a notification via the existing SNS topic.Incorrect options:Configure CloudWatch metrics with detailed monitoring for the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rate CloudWatch provides two categories of monitoring: basic monitoring and detailed monitoring. Detailed monitoring options differ based on the services that offer it. For example, Amazon EC2 detailed monitoring provides more frequent metrics, published at oneminute intervals, instead of the fiveminute intervals used in Amazon EC2 basic monitoring. Detailed monitoring is offered by only some services. As explained above, you need to use custom metrics to capture data for the external payment processing API calls since detailed monitoring for the standard CloudWatch metrics cannot be used for this scenario.Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Logs Insights to query the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Logs Insights on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate CloudWatch Logs Insights enables you to interactively search and analyze your log data in Amazon CloudWatch Logs. You can perform queries to help you more efficiently and effectively respond to operational issues. This option is not the right fit for the given use case since Lambda cannot monitor the output of the CloudWatch Logs Insights on a realtime basis since it is being invoked on a schedule. Also, it is not an efficient solution since Lambda will need significant custom code to parse and compute the external API error rate from the CloudWatch Logs Insights data.Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Metric Filter to look at the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Metric Filter on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate You can search and filter the log data coming into CloudWatch Logs by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on. This option is not the best fit for the given use case since Lambda cannot monitor the output of the CloudWatch Metric Filter on a realtime basis since it is being invoked on a schedule. Also, it is not an efficient solution since Lambda will need significant custom code to parse and compute the external API error rate from the CloudWatch Metric Filter data.References:https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html https://aws.amazon.com/premiumsupport/knowledgecenter/cloudwatchpushcustommetrics/ https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html65:T5e0,Correct option:Amazon EBS works with AWS KMS to encrypt and decrypt your EBS volume. You can encrypt both the boot and data volumes of an EC2 instance. When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted:Data at rest inside the volumeAll data moving between the volume and the instanceAll snapshots created from the volumeAll volumes created from those snapshotsEBS volumes support both inflight encryption and encryption at rest using KMS This is a correct statement. Encryption operations occur on the servers that host EC2 instances, ensuring the security of both dataatrest and dataintransit between an instance and its attached EBS storage.Incorrect options:EBS volumes support inflight encryption but do not support encryption at rest This is an incorrect statement. As discussed above, all data moving between the volume and the instance is encrypted.EBS volumes do not support inflight encryption but do support encryption at rest using KMS This is an incorrect statement. As discussed above, data at rest is also encrypted.EBS volumes don't support any encryption This is an incorrect statement. Amazon EBS encryption offers a straightforward encryption solution for your EBS resources associated with your EC2 instances. With Amazon EBS encryption, you aren't required to build, maintain, and secure your own key management infrastructure.Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html66:T53a,Correct option:Create a cron event in CloudWatch, which triggers an AWS Lambda function that triggers the database snapshot There are multiple ways to run periodic jobs in AWS. CloudWatch Events with Lambda is the simplest of all solutions. To do this, create a CloudWatch Rule and select “Schedule” as the Event Source. You can either use a cron expression or provide a fixed rate (such as every 5 minutes). Next, select “Lambda Function” as the Target. Your Lambda will have the necessary code for snapshot functionality.Incorrect options:Enable RDS automatic backups You can enable automatic backups but as of 2020, the retention period is 0 to 35 days.Enable RDS Read replicas Amazon RDS server's builtin replication functionality to create a special type of DB instance called a read replica from a source DB instance. Updates made to the source DB instance are asynchronously copied to the read replica. Read replicas are useful for heavy readonly data workloads. These are not suitable for the given usecase.Enable RDS MultiAZ MultiAZ allows you to create a highly available application with RDS. It does not directly help in database backups or retention periods.Reference:https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html https://docs.aws.amazon.com/lambda/latest/dg/withscheduledevents.html67:T9e5,Correct option:Leverage an IAM policy with the Amazon Cognito identity prefix to restrict users to use their own folders in Amazon S3Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limitedprivilege AWS credentials to access other AWS services. Amazon Cognito identity pools support the following identity providers:Public providers: Login with Amazon (identity pools), Facebook (identity pools), Google (identity pools), Sign in with Apple (identity pools).Amazon Cognito user poolsOpenID Connect providers (identity pools)SAML identity providers (identity pools)Developer authenticated identities (identity pools)You can create an identitybased policy that allows Amazon Cognito users to access objects in a specific S3 bucket. This policy allows access only to objects with a name that includes Cognito, the name of the application, and the federated user's ID, represented by the ${cognitoidentity.amazonaws.com:sub} variable.via https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_cognitobucket.html Incorrect options:Use S3 Event Notifications to trigger a Lambda function that validates that the given file is uploaded and downloaded only by the authorized user While it is certainly possible to build this solution, however, it is not the most optimal solution as it does not prevent an invalid upload of a file into another user's designated folder. So this option is incorrect.Integrate Amazon API Gateway with a Lambda function that validates that the given file is uploaded to S3 and downloaded from S3 only by the authorized user Again, it is certainly possible to build this solution, however, it is not the most optimal solution as it does not prevent an invalid upload of a file into another user's designated folder. So this option is incorrect.Use CloudFront Lambda@Edge to validate that the given file is uploaded to S3 and downloaded from S3 only by the authorized user This option assumes that the solution comprises a CloudFront distribution. This introduces inefficiency in the solution, as one needs to pay for CloudFront/Lambda@Edge and adds unnecessary hops in the data flow for both uploads and downloads.References:https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_cognitobucket.html https://docs.aws.amazon.com/cognito/latest/developerguide/amazoncognitointegratinguserpoolswithidentitypools.html68:T4c0,Correct option:The ASG will terminate the EC2 InstanceTo maintain the same number of instances, Amazon EC2 Auto Scaling performs a periodic health check on running instances within an Auto Scaling group. When it finds that an instance is unhealthy, it terminates that instance and launches a new one. Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance.Incorrect options:The ASG will detach the EC2 instance from the group, and leave it running The goal of the autoscaling group is to get rid of the bad instance and replace itThe ASG will keep the instance running and restart the application The ASG does not have control of your applicationThe ASG will format the root EBS drive on the EC2 instance and run the User Data again This will not happen, the ASG cannot assume the format of your EBS drive, and User Data only runs once at instance first boot.References:https://aws.amazon.com/premiumsupport/knowledgecenter/autoscalingterminateinstance https://docs.aws.amazon.com/autoscaling/ec2/userguide/asmaintaininstancelevels.html#replaceunhealthyinstancee69:T4f6,Correct option:AWS Step FunctionsAWS Step Functions is a visual workflow service that helps developers use AWS services to build distributed applications, automate processes, orchestrate microservices, and create data and machine learning (ML) pipelines.via https://aws.amazon.com/stepfunctions/ Incorrect options:Amazon Kinesis Data Streams Amazon Kinesis Data Streams is a serverless streaming data service that makes it easy to capture, process, and store data streams at any scale.AWS Glue AWS Glue is a serverless data integration service that makes it easier to discover, prepare, move, and integrate data from multiple sources for analytics, machine learning (ML), and application development.AWS Batch AWS Batch is a set of batch management capabilities that enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized compute resources) based on the volume and specific resource requirements of the batch jobs submitted.References:https://aws.amazon.com/stepfunctions/ https://aws.amazon.com/kinesis/datastreams/ https://aws.amazon.com/glue/ https://aws.amazon.com/batch/faqs/6a:T8ec,Use crossRegion Read ReplicasIn addition to using Read Replicas to reduce the load on your source DB instance, you can also use Read Replicas to implement a DR solution for your production DB environment. If the source DB instance fails, you can promote your Read Replica to a standalone source server. Read Replicas can also be created in a different Region than the source database. Using a crossRegion Read Replica can help ensure that you get back up and running if you experience a regional availability issue.Enable the automated backup feature of Amazon RDS in a multiAZ deployment that creates backups in a single AWS RegionAmazon RDS provides high availability and failover support for DB instances using MultiAZ deployments. Amazon RDS uses several different technologies to provide failover support. MultiAZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology.The automated backup feature of Amazon RDS enables pointintime recovery for your database instance. Amazon RDS will backup your database and transaction logs and store both for a userspecified retention period. If it’s a MultiAZ configuration, backups occur on the standby to reduce I/O impact on the primary. Automated backups are limited to a single AWS Region while manual snapshots and Read Replicas are supported across multiple Regions.Incorrect options:Enable the automated backup feature of Amazon RDS in a multiAZ deployment that creates backups across multiple Regions This is an incorrect statement. Automated backups are limited to a single AWS Region while manual snapshots and Read Replicas are supported across multiple Regions.Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage Amazon RDS Provisioned IOPS Storage is an SSDbacked storage option designed to deliver fast, predictable, and consistent I/O performance. This storage type enhances the performance of the RDS database, but this isn't a disaster recovery option.Use database cloning feature of the RDS DB cluster This option has been added as a distractor. Database cloning is only available for Aurora and not for RDS.References:https://aws.amazon.com/rds/features/ https://aws.amazon.com/blogs/database/implementingadisasterrecoverystrategywithamazonrds/6b:T9e9,Correct options:Bucket policies, Identity and Access Management (IAM) policiesQuery String Authentication, Access Control Lists (ACLs)Customers may use four mechanisms for controlling access to Amazon S3 resources: Identity and Access Management (IAM) policies, bucket policies, Access Control Lists (ACLs), and Query String Authentication.IAM enables organizations with multiple employees to create and manage multiple users under a single AWS account. With IAM policies, customers can grant IAM users finegrained control to their Amazon S3 bucket or objects while also retaining full control over everything the users do.With bucket policies, customers can define rules which apply broadly across all requests to their Amazon S3 resources, such as granting write privileges to a subset of Amazon S3 resources. Customers can also restrict access based on an aspect of the request, such as HTTP referrer and IP address.With ACLs, customers can grant specific permissions (i.e. READ, WRITE, FULL_CONTROL) to specific users for an individual bucket or object.With Query String Authentication, customers can create a URL to an Amazon S3 object which is only valid for a limited time. Using query parameters to authenticate requests is useful when you want to express a request entirely in a URL. This method is also referred as presigning a URL.Incorrect options:Permissions boundaries, Identity and Access Management (IAM) policiesQuery String Authentication, Permissions boundariesIAM database authentication, Bucket policiesPermissions boundary A Permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identitybased policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identitybased policies and its permissions boundaries. When you use a policy to set the permissions boundary for a user, it limits the user's permissions but does not provide permissions on its own.IAM database authentication IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token. It is a database authentication technique and cannot be used to authenticate for S3.Therefore, all three options are incorrect.References:https://docs.aws.amazon.com/AmazonS3/latest/userguide/accesscontroloverview.html https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html6c:T5f4,Correct option:ApproximateNumberOfMessagesVisible This is a CloudWatch Amazon SQS queue metric. The number of messages in a queue might not change proportionally to the size of the Auto Scaling group that processes messages from the queue. Hence, this metric does not work for target tracking.Incorrect options:With target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value.It is important to note that a target tracking scaling policy assumes that it should scale out your Auto Scaling group when the specified metric is above the target value. You cannot use a target tracking scaling policy to scale out your Auto Scaling group when the specified metric is below the target value.ASGAverageCPUUtilization This is a predefined metric for target tracking scaling policy. This represents the Average CPU utilization of the Auto Scaling group.ASGAverageNetworkOut This is a predefined metric for target tracking scaling policy. This represents the Average number of bytes sent out on all network interfaces by the Auto Scaling group.ALBRequestCountPerTarget This is a predefined metric for target tracking scaling policy. This represents the Number of requests completed per target in an Application Load Balancer target group.Reference:https://docs.aws.amazon.com/autoscaling/ec2/userguide/asscalingtargettracking.htmll6d:T6a8,Correct option:Use Envelope Encryption and reference the data as file within the codeWhile AWS KMS does support sending data up to 4 KB to be encrypted directly, envelope encryption can offer significant performance benefits. When you encrypt data directly with AWS KMS it must be transferred over the network. Envelope encryption reduces the network load since only the request and delivery of the much smaller data key go over the network. The data key is used locally in your application or encrypting AWS service, avoiding the need to send the entire block of data to AWS KMS and suffer network latency.AWS Lambda environment variables can have a maximum size of 4 KB. Additionally, the direct 'Encrypt' API of KMS also has an upper limit of 4 KB for the data payload. To encrypt 1 MB, you need to use the Encryption SDK and pack the encrypted file with the lambda function.Incorrect options:Use KMS direct encryption and store as file You can only encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information, so this option is not correct for the given usecase.Use Envelope Encryption and store as an environment variable Environment variables must not exceed 4 KB, so this option is not correct for the given usecase.Use KMS Encryption and store as an environment variable You can encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information. Lambda Environment variables must not exceed 4 KB. So this option is not correct for the given usecase.References:https://docs.aws.amazon.com/lambda/latest/dg/gettingstartedlimits.html https://aws.amazon.com/kms/faqs//6e:T9fa,SNS + SQSAmazon SNS enables message filtering and fanout to a large number of subscribers, including serverless functions, queues, and distributed systems. Additionally, Amazon SNS fans out notifications to end users via mobile push messages, SMS, and email.How SNS Works: via https://aws.amazon.com/sns/ Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, besteffort ordering, and atleastonce delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.Because each buffered request can be processed independently, Amazon SQS can scale transparently to handle the load without any provisioning instructions from you.SNS and SQS can be used to create a fanout messaging scenario in which messages are "pushed" to multiple subscribers, which eliminates the need to periodically check or poll for updates and enables parallel asynchronous processing of the message by the subscribers. SQS can allow for later reprocessing and dead letter queues. This is called the fanout pattern.Incorrect options:SNS + Kinesis You can use Amazon Kinesis Data Streams to collect and process large streams of data records in realtime. Kinesis Data Streams stores records from 24 hours (by default) to 8760 hours (365 days). However, you need to manually provision shards in case the load increases or you need to use CloudWatch alarms to set up auto scaling for the shards. Since Kinesis only supports transparent scaling in the ondemand mode, however, it is not cost efficient for the given use case, so this option is not the right fit for the given use case.SNS + Lambda Amazon SNS and AWS Lambda are integrated so you can invoke Lambda functions with Amazon SNS notifications. The Lambda function receives the message payload as an input parameter and can manipulate the information in the message, publish the message to other SNS topics, or send the message to other AWS services. However, your EC2 instances cannot "poll" from Lambda functions and as such, this would not work.SQS + SES This will not work as the messages need to be processed twice (once for sending the notification and later for order fulfillment) and SQS only allows for one consuming application.References:https://aws.amazon.com/sns/ https://aws.amazon.com/gettingstarted/tutorials/sendfanouteventnotifications/6f:Tf7d,Use CloudFront signed URL feature to control access to the fileA signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content.Here's an overview of how you configure CloudFront for signed URLs and how CloudFront responds when a user uses a signed URL to request a file:In your CloudFront distribution, specify one or more trusted key groups, which contain the public keys that CloudFront can use to verify the URL signature. You use the corresponding private keys to sign the URLs.Develop your application to determine whether a user should have access to your content and to create signed URLs for the files or parts of your application that you want to restrict access to.A user requests a file for which you want to require signed URLs. Your application verifies that the user is entitled to access the file: they've signed in, they've paid for access to the content, or they've met some other requirement for access.Your application creates and returns a signed URL to the user. The signed URL allows the user to download or stream the content.This step is automatic; the user usually doesn't have to do anything additional to access the content. For example, if a user is accessing your content in a web browser, your application returns the signed URL to the browser. The browser immediately uses the signed URL to access the file in the CloudFront edge cache without any intervention from the user.CloudFront uses the public key to validate the signature and confirm that the URL hasn't been tampered with. If the signature is invalid, the request is rejected. If the request meets the requirements in the policy statement, CloudFront does the standard operations: determines whether the file is already in the edge cache, forwards the request to the origin if necessary, and returns the file to the user.Incorrect options:Use CloudFront signed cookies feature to control access to the file CloudFront signed cookies allow you to control who can access your content when you don't want to change your current URLs or when you want to provide access to multiple restricted files, for example, all of the files in the subscribers' area of a website. Our requirement has only one file that needs to be shared and hence signed URL is the optimal solution.Signed URLs take precedence over signed cookies. If you use both signed URLs and signed cookies to control access to the same files and a viewer uses a signed URL to request a file, CloudFront determines whether to return the file to the viewer based only on the signed URL.Configure AWS Web Application Firewall (WAF) to monitor and control the HTTP and HTTPS requests that are forwarded to CloudFront AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to CloudFront, and lets you control access to your content. Based on conditions that you specify, such as the values of query strings or the IP addresses that requests originate from, CloudFront responds to requests either with the requested content or with an HTTP status code 403 (Forbidden). A firewall is optimal for broader use cases than restricted access to a single file.Using CloudFront's FieldLevel Encryption to help protect sensitive data CloudFront's fieldlevel encryption further encrypts sensitive data in an HTTPS form using fieldspecific encryption keys (which you supply) before a POST request is forwarded to your origin. This ensures that sensitive data can only be decrypted and viewed by certain components or services in your application stack. This feature is not useful for the given use case.References:https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/privatecontentsignedurls.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distributionwebawswaf.html https://aws.amazon.com/aboutaws/whatsnew/2017/12/introducingfieldlevelencryptiononamazoncloudfront/70:T60a,An ALB has three possible target types: Instance, IP and LambdaWhen you create a target group, you specify its target type, which determines the type of target you specify when registering targets with this target group. After you create a target group, you cannot change its target type. The following are the possible target types:Instance The targets are specified by instance IDIP The targets are IP addressesLambda The target is a Lambda functionYou can not specify publicly routable IP addresses to an ALBWhen the target type is IP, you can specify IP addresses from specific CIDR blocks only. You can't specify publicly routable IP addresses.Incorrect options:If you specify targets using an instance ID, traffic is routed to instances using any private IP address from one or more network interfaces If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance.If you specify targets using IP addresses, traffic is routed to instances using the primary private IP address If you specify targets using IP addresses, you can route traffic to an instance using any private IP address from one or more network interfaces. This enables multiple applications on an instance to use the same port.An ALB has three possible target types: Hostname, IP and Lambda This is incorrect, as described in the correct explanation above.Reference: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/loadbalancertargetgroups.html71:T718,Increase the minimum instance capacity of the Auto Scaling Group to 2 You configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. The minimum and maximum capacity are required to create an Auto Scaling group, while the desired capacity is optional. If you do not define your desired capacity upfront, it defaults to your minimum capacity.Since a minimum capacity of 1 was defined, an instance was launched in only one AZ. This AZ went down, taking the application with it. If the minimum capacity is set to 2. As per Auto Scale AZ configuration, it would have launched 2 instances one in each AZ, making the architecture disasterproof and hence highly available.via https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscalingbenefits.html Incorrect options:Change the scaling metric of autoscaling policy to network bytes With target tracking scaling policies, you select a scaling metric and set a target value. You can use predefined customized metrics. Setting the metric to network bytes will not help in this context since the instances have to be spread across different AZs for high availability. The optimized way of doing it, is by defining minimum and maximum instance capacities, as discussed above.Configure ASG fast failover This is a madeup option, given as a distractor.Enable RDS MultiAZ This configuration will make your database highly available. But for the current scenario, you will need to have more than 1 instance in separate availability zones to keep the application highly available.References:https://docs.aws.amazon.com/autoscaling/ec2/userguide/asgcapacitylimits.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/asmaintaininstancelevels.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/asscalingtargettracking.htmll72:T57d,Create a highresolution custom metric and push the data using a script triggered every 10 secondsUsing highresolution custom metric, your applications can publish metrics to CloudWatch with 1second resolution. You can watch the metrics scroll across your screen seconds after they are published and you can set up highresolution CloudWatch Alarms that evaluate as frequently as every 10 seconds. You can alert with HighResolution Alarms, as frequently as 10second periods. HighResolution Alarms allow you to react and take actions faster and support the same actions available today with standard 1minute alarms.via https://aws.amazon.com/blogs/aws/newhighresolutioncustommetricsandalarmsforamazoncloudwatch/ Incorrect options:Enable EC2 detailed monitoring As part of basic monitoring, Amazon EC2 sends metric data to CloudWatch in 5minute periods. To send metric data for your instance to CloudWatch in 1minute periods, you can enable detailed monitoring on the instance, however, this comes at an additional cost.Simply get it from the CloudWatch Metrics You can get data from metrics. The basic monitoring data is available automatically in a 5minute interval and detailed monitoring data is available in a 1minute interval.Open a support ticket with AWS This option has been added as a distractor.Reference: https://aws.amazon.com/blogs/aws/newhighresolutioncustommetricsandalarmsforamazoncloudwatch/73:T4da,Correct option:CodeBuild scales automatically, the organization does not have to do anything for scaling or for parallel builds AWS CodeBuild is a fully managed build service in the cloud. CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. It provides prepackaged build environments for popular programming languages and build tools such as Apache Maven, Gradle, and more. You can also customize build environments in CodeBuild to use your own build tools. CodeBuild scales automatically to meet peak build requests.Incorrect options:Choose a highperformance instance type for your CodeBuild instances For the current requirement, this is will not make any difference.Run CodeBuild in an Auto Scaling Group AWS CodeBuild is a managed service and scales automatically, does not need Auto Scaling Group to scale it up.Enable CodeBuild Auto Scaling This has been added as a distractor. CodeBuild scales automatically to meet peak build requests.References:https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html https://docs.aws.amazon.com/codebuild/latest/userguide/buildenvrefcomputetypes.html74:T580,Correct option:Use an IAM roleIAM roles have been incorporated so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles.Amazon EC2 uses an instance profile as a container for an IAM role. When you create an IAM role using the IAM console, the console creates an instance profile automatically and gives it the same name as the role to which it corresponds.This is the most secure option as the role assigned to EC2 can be used to access S3 without storing any credentials onto the EC2 instance.Incorrect options:Create an IAM programmatic user and store the access key and secret access key on the EC2 ~/.aws/credentials file. While this would work, this is highly insecure as anyone gaining access to the EC2 instance would be able to steal the credentials stored in that file.Use EC2 User Data EC2 User Data is used to run bootstrap scripts at an instance's first launch. This option is not the right fit for the given usecase.Create an S3 bucket policy that authorizes public access While this would work, it would allow anyone to access your S3 bucket files. So this option is ruled out.Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iamrolesforamazonec2.html75:T88a,Correct option:Configure Lambda to connect to VPC with private subnet and Security Group needed to access RDS You can configure a Lambda function to connect to private subnets in a virtual private cloud (VPC) in your account. Use Amazon Virtual Private Cloud (Amazon VPC) to create a private network for resources such as databases, cache instances, or internal services. Connect your lambda function to the VPC to access private resources during execution. When you connect a function to a VPC, Lambda creates an elastic network interface for each combination of the security group and subnet in your function's VPC configuration. This is the right way of giving RDS access to Lambda.Lambda VPC Config: via https://docs.aws.amazon.com/lambda/latest/dg/configurationvpc.html Incorrect options:Use Lambda layers to connect to the internet and RDS separately You can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. Layers will not help in configuring access to RDS instance and hence is an incorrect choice.Configure lambda to connect to the public subnet that will give internet access and use the Security Group to access RDS inside the private subnet This is an incorrect statement. Connecting a Lambda function to a public subnet does not give it internet access or a public IP address. To grant internet access to your function, its associated VPC must have a NAT gateway (or NAT instance) in a public subnet.Use Environment variables to pass in the RDS connection string You can use environment variables to store secrets securely and adjust your function's behavior without updating code. You can use environment variables to exchange data with RDS, but you will still need access to RDS, which is not possible with just environment variables.References:https://docs.aws.amazon.com/lambda/latest/dg/configurationvpc.html https://docs.aws.amazon.com/lambda/latest/dg/configurationlayers.html https://docs.aws.amazon.com/lambda/latest/dg/configurationenvvars.html https://aws.amazon.com/premiumsupport/knowledgecenter/internetaccesslambdafunction/76:T5ac,Correct option:API Gateway exposing Lambda FunctionalityAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door" for applications to access data, business logic, or functionality from your backend services.How API Gateway Works: via https://aws.amazon.com/apigateway/ AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.How Lambda function works: via https://aws.amazon.com/lambda/ API Gateway can expose Lambda functionality through RESTful APIs. Both are serverless options offered by AWS and hence the right choice for this scenario, considering all the functionality they offer.Incorrect options:Fargate with Lambda at the front Lambda cannot directly handle RESTful API requests. You can invoke a Lambda function over HTTPS by defining a custom RESTful API using Amazon API Gateway. So, Fargate with Lambda as the frontfacing service is a wrong combination, though both Fargate and Lambda are serverless.Publicfacing Application Load Balancer with ECS on Amazon EC2 ECS on Amazon EC2 does not come under serverless and hence cannot be considered for this use case.Route 53 with EC2 as backend Amazon EC2 is not a serverless service and hence cannot be considered for this use case.References:https://aws.amazon.com/serverless/ https://aws.amazon.com/apigateway/77:T609,Correct option:Share presigned URLs with resources that need access All objects by default are private, with the object owner having permission to access the objects. However, the object owner can optionally share objects with others by creating a presigned URL, using their own security credentials, to grant timelimited permission to download the objects. When you create a presigned URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object), and expiration date and time. The presigned URLs are valid only for the specified duration.Incorrect options:Use Bucket policy to block the unintended access A bucket policy is a resourcebased AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. Bucket policy can be used to block off unintended access, but it's not possible to provide timebased access, as is the case in the current use case.Use Routing policies to reroute unintended access There is no such facility directly available with Amazon S3.It is not possible to implement time constraints on Amazon S3 Bucket access This is an incorrect statement. As explained above, it is possible to give timebound access permissions on S3 buckets and objects.References:https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html https://docs.aws.amazon.com/AmazonS3/latest/userguide/addbucketpolicy.html78:T7e3,"The developers should insert logging statements in the Lambda function code which are then available via CloudWatch logs"When you invoke a Lambda function, two types of error can occur. Invocation errors occur when the invocation request is rejected before your function receives it. Function errors occur when your function's code or runtime returns an error. Depending on the type of error, the type of invocation, and the client or service that invokes the function, the retry behavior, and the strategy for managing errors varies.Lambda function failures are commonly caused by:Permissions issues Code issues Network issues Throttling Invoke API 500 and 502 errorsYou can insert logging statements into your code to help you validate that your code is working as expected. Lambda automatically integrates with CloudWatch Logs and pushes all logs from your code to a CloudWatch Logs group associated with a Lambda function, which is named /aws/lambda/.Please see this note for more details: via https://docs.aws.amazon.com/lambda/latest/dg/monitoringcloudwatchlogs.html Incorrect options:"Use CloudWatch Events to identify and notify any failures in the Lambda code" Typically Lambda functions are triggered as a response to a CloudWatch Event. CloudWatch Events cannot identify and notify failures in the Lambda code."Use CodeCommit to identify and notify any failures in the Lambda code""Use CodeDeploy to identify and notify any failures in the Lambda code"AWS CodeCommit is a fullymanaged source control service that hosts secure Gitbased repositories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem.AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your onpremises servers.Neither CodeCommit nor CodeDeploy can identify and notify failures in the Lambda code.Reference: https://docs.aws.amazon.com/lambda/latest/dg/monitoringcloudwatchlogs.html79:T733,Correct option:Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud.How API Gateway Works: via https://aws.amazon.com/apigateway/AWS Security Token Service (STS) AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limitedprivilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). However, it is not supported by API Gateway.API Gateway supports the following mechanisms for authentication and authorization: via https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaycontrolaccesstoapi.htmlIncorrect options:Standard AWS IAM roles and policies Standard AWS IAM roles and policies offer flexible and robust access controls that can be applied to an entire API or individual methods. IAM roles and policies can be used for controlling who can create and manage your APIs, as well as who can invoke them.Lambda Authorizer Lambda authorizers are Lambda functions that control access to REST API methods using bearer token authentication—as well as information described by headers, paths, query strings, stage variables, or context variables request parameters. Lambda authorizers are used to control who can invoke REST API methods.Cognito User Pools Amazon Cognito user pools let you create customizable authentication and authorization solutions for your REST APIs. Amazon Cognito user pools are used to control who can invoke REST API methods.References:https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaycontrolaccesstoapi.htmlhttps://docs.aws.amazon.com/STS/latest/APIReference/welcome.htmll7a:T6cc,All the nodes in a Redis cluster must reside in the same regionAll the nodes in a Redis cluster (cluster mode enabled or cluster mode disabled) must reside in the same region.While using Redis with cluster mode enabled, you cannot manually promote any of the replica nodes to primaryWhile using Redis with cluster mode enabled, there are some limitations:You cannot manually promote any of the replica nodes to primary.MultiAZ is required.You can only change the structure of a cluster, the node type, and the number of nodes by restoring from a backup.Incorrect options:While using Redis with cluster mode enabled, asynchronous replication mechanisms are used to keep the read replicas synchronized with the primary. If cluster mode is disabled, the replication mechanism is done synchronously When you add a read replica to a cluster, all of the data from the primary is copied to the new node. From that point on, whenever data is written to the primary, the changes are asynchronously propagated to all the read replicas, for both the Redis offerings (cluster mode enabled or cluster mode disabled).If you have no replicas and a node fails, you experience no loss of data when using Redis with cluster mode enabled If you have no replicas and a node fails, you experience loss of all data in that node's shard, when using Redis with cluster mode enabled. If you have no replicas and the node fails, you experience total data loss in Redis with cluster mode disabled.You can scale write capacity for Redis by adding replica nodes This increases only the read capacity of the Redis cluster, write capacity is not enhanced by read replicas.Reference: https://docs.aws.amazon.com/AmazonElastiCache/latest/redug/Replication.Redis.Groups.html7b:T40b,Create an LSILSI stands for Local Secondary Index. Some applications only need to query data using the base table's primary key; however, there may be situations where an alternate sort key would be helpful. To give your application a choice of sort keys, you can create one or more local secondary indexes on a table and issue Query or Scan requests against these indexes.Differences between GSI and LSI: via https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html Incorrect options:Call Scan Scan is an operation on the data. Once you create your local secondary indexes on a table you can then issue Scan requests again.Create a GSI GSI (Global Secondary Index) is an index with a partition key and a sort key that can be different from those on the base table.Migrate away from DynamoDB Migrating to another database that is not NoSQL may cause you to make changes that require substantial code changes.Reference: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html7c:Ta2d,Correct option:The Load Balancer does not have stickiness enabled Sticky sessions are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies.When a load balancer first receives a request from a client, it routes the request to a target, generates a cookie named AWSALB that encodes information about the selected target, encrypts the cookie, and includes the cookie in the response to the client. The client should include the cookie that it receives in subsequent requests to the load balancer. When the load balancer receives a request from a client that contains the cookie, if sticky sessions are enabled for the target group and the request goes to the same target group, the load balancer detects the cookie and routes the request to the same target.More info here: via https://docs.aws.amazon.com/elasticloadbalancing/latest/application/loadbalancertargetgroups.html#stickysessions Incorrect options:Application Load Balancer is in slowstart mode, which gives ALB a little more time to read and write session data This is an invalid statement. The load balancer serves as a single point of contact for clients and distributes incoming traffic across its healthy registered targets. By default, a target starts to receive its full share of requests as soon as it is registered with a target group and passes an initial health check. Using slow start mode gives targets time to warm up before the load balancer sends them a full share of requests. This does not help in session management.The EC2 instances are logging out the users because the instances never have access to the client IPs because of the Load Balancer This is an incorrect statement. Elastic Load Balancing stores the IP address of the client in the XForwardedFor request header and passes the header to the server. If needed, the server can read IP addresses from this data.The Load Balancer does not have TLS enabled To use an HTTPS listener, you must deploy at least one SSL/TLS server certificate on your load balancer. The load balancer uses a server certificate to terminate the frontend connection and then decrypt requests from clients before sending them to the targets. This does not help in session management.References:https://docs.aws.amazon.com/elasticloadbalancing/latest/application/loadbalancertargetgroups.html#stickysessions https://docs.aws.amazon.com/elasticloadbalancing/latest/application/loadbalancertargetgroups.html#slowstartmode7d:T995,Correct option:Keep the API credentials in AWS Secrets Manager and use the credentials to make the API call by fetching the API credentials at runtime by using the AWS SDKSecrets Manager enables you to replace hardcoded credentials in your code, including passwords, with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure the secret can't be compromised by someone examining your code, because the secret no longer exists in the code. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a specified schedule. This enables you to replace longterm secrets with shortterm ones, significantly reducing the risk of compromise.via https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.htmlIn the past, when you created a custom application to retrieve information from a database, you typically embedded the credentials, the secret, for accessing the database directly in the application. When the time came to rotate the credentials, you had to do more than just create new credentials. You had to invest time to update the application to use the new credentials. Then you distributed the updated application. If you had multiple applications with shared credentials and you missed updating one of them, the application failed. Because of this risk, many customers choose not to regularly rotate credentials, which effectively substitutes one risk for another. You can also use caching with Secrets Manager to significantly improve the availability and latency of applications.Incorrect options:Keep the API credentials in an encrypted table in MySQL RDS and use the credentials to make the API call by fetching the API credentials from RDS at runtime by using the AWS SDKKeep the API credentials in an encrypted file in S3 and use the credentials to make the API call by fetching the API credentials from S3 at runtime by using the AWS SDKKeep the API credentials in a local code variable and use the local code variable at runtime to make the API callIt is considered a security bad practice to keep sensitive access credentials in code, database, or a flat file on a file system or object storage. Therefore, all three options are incorrect.References:https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html https://aws.amazon.com/blogs/security/improveavailabilityandlatencyofapplicationsbyusingawssecretmanagerspythonclientsidecachinglibrary/7e:T9d1,Correct option:Use Amazon CloudFrontStoring your static content with S3 provides a lot of advantages. But to help optimize your application’s performance and security while effectively managing cost, AWS recommends that you also set up Amazon CloudFront to work with your S3 bucket to serve and protect the content. CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more costeffective than delivering it from S3 directly to your users.By caching your content in Edge Locations, CloudFront reduces the load on your S3 bucket and helps ensure a faster response for your users when they request content. In addition, data transfer out for content by using CloudFront is often more costeffective than serving files directly from S3, and there is no data transfer fee from S3 to CloudFront.A security feature of CloudFront is Origin Access Identity (OAI), which restricts access to an S3 bucket and its content to only CloudFront and operations it performs.CloudFront Overview: via https://aws.amazon.com/blogs/networkingandcontentdelivery/amazons3amazoncloudfrontamatchmadeinthecloud/ Incorrect options:Use Amazon ElastiCache for Redis Amazon ElastiCache can be used to significantly improve latency and throughput for many readheavy application workloads (such as social networking, gaming, media sharing, and Q&A portals). ElastiCache is often used with databases that need millisecond latency. For the current scenario, we do not need a caching layer since the data load is not that heavy.Use Amazon S3 Caching This is a madeup option, given as a distractor.Use Amazon S3 Transfer Acceleration Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3 bucket. However, S3 Transfer Acceleration leverages Amazon CloudFront’s globally distributed AWS Edge Locations. Each time S3 Transfer Acceleration is used to upload an object, AWS checks whether S3 Transfer Acceleration is likely to be faster than a regular Amazon S3 transfer. If it finds that S3 Transfer Acceleration might not be significantly faster, AWS shifts back to normal Amazon S3 transfer mode. So, this is not the right option for our use case.References:https://aws.amazon.com/blogs/networkingandcontentdelivery/amazons3amazoncloudfrontamatchmadeinthecloud/ https://aws.amazon.com/elasticache/7f:T757,The partition key that you have selected isn't distributed enoughAmazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs.A Kinesis data stream is a set of shards. A shard is a uniquely identified sequence of data records in a stream. A stream is composed of one or more shards, each of which provides a fixed unit of capacity.The partition key is used by Kinesis Data Streams to distribute data across shards. Kinesis Data Streams segregates the data records that belong to a stream into multiple shards, using the partition key associated with each data record to determine the shard to which a given data record belongs.Kinesis Data Streams Overview: via https://docs.aws.amazon.com/streams/latest/dev/keyconcepts.htmlFor the given usecase, as the partition key is not distributed enough, all the data is getting skewed at a few specific shards and not leveraging the entire cluster of shards.You can also use metrics to determine which are your "hot" or "cold" shards, that is, shards that are receiving much more data, or much less data, than expected. You could then selectively split the hot shards to increase capacity for the hash keys that target those shards. Similarly, you could merge cold shards to make better use of their unused capacity.Incorrect options:Metrics are slow to update Metrics are a CloudWatch concept. This option has been added as a distractor.You have too many shards Too many shards is not the issue as you would see a LimitExceededException in that case.The data retention period is too long Your streaming data is retained for up to 365 days. The data retention period is not an issue causing this error.References:https://docs.aws.amazon.com/streams/latest/dev/keyconcepts.html https://docs.aws.amazon.com/streams/latest/dev/kinesisusingsdkjavareshardingstrategies.html80:T9c3,Correct option:Amazon Elastic Container Service (Amazon ECS) on Fargate Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type.AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.ECS Fargate Overview: via https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.htmlIncorrect options:Amazon Elastic Container Service (Amazon ECS) on EC2 Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. ECS uses EC2 instances and hence cannot be called a serverless solution.Amazon Elastic Kubernetes Service (Amazon EKS) on Fargate Amazon Elastic Kubernetes Service (Amazon EKS) is a fully managed Kubernetes service. You can choose to run your EKS clusters using AWS Fargate, which is a serverless compute for containers. Since the usecase talks about the simplest and the least effort way to deploy Docker containers, EKS is not the best fit as you can use ECS Fargate to build a much easier solution. EKS is better suited to run Kubernetes on AWS without needing to install and operate your own Kubernetes control plane or worker nodes.AWS Elastic Beanstalk AWS Elastic Beanstalk is an easytouse service for deploying and scaling web applications and services. You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, autoscaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. Beanstalk uses EC2 instances for its deployment, hence cannot be called a serverless architecture.References:https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html https://aws.amazon.com/eks/ https://aws.amazon.com/elasticbeanstalk/81:T446,Use CloudWatch integration feature with S3You can export log data from your CloudWatch log groups to an Amazon S3 bucket and use this data in custom processing and analysis, or to load onto other systems.Exporting CloudWatch Log Data to Amazon S3: via https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html Incorrect options:Use CloudWatch integration feature with Kinesis You can use both to do custom processing or analysis but with S3 you don't have to process anything. Instead, you configure the CloudWatch settings to send logs to S3.Use CloudWatch integration feature with Lambda You can use both to do custom processing or analysis but with S3 you don't have to process anything. Instead, you configure the CloudWatch settings to send logs to S3.Use CloudWatch integration feature with Glue AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. Glue is not the right fit for the given usecase.Reference: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html82:T439,Correct option:cloudformation package and cloudformation deployAWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and thirdparty resources and provision them in an orderly and predictable fashion.How CloudFormation Works: via https://aws.amazon.com/cloudformation/ The cloudformation package command packages the local artifacts (local paths) that your AWS CloudFormation template references. The command will upload local artifacts, such as your source code for your AWS Lambda function.The cloudformation deploy command deploys the specified AWS CloudFormation template by creating and then executing a changeset.Incorrect options:cloudformation package and cloudformation upload The cloudformation upload command does not exist.cloudformation zip and cloudformation upload Both commands do not exist, this is a madeup option.cloudformation zip and cloudformation deploy The cloudformation zip command does not exist, this is a madeup option.Reference: https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html83:T491,xamzserversideencryption': 'aws:kms'Serverside encryption is the encryption of data at its destination by the application or service that receives it. AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. Amazon S3 uses AWS KMS customer master keys (CMKs) to encrypt your Amazon S3 objects. AWS KMS encrypts only the object data. Any object metadata is not encrypted.If the request does not include the xamzserversideencryption header, then the request is denied.via https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html Incorrect options:'xamzserversideencryption': 'SSES3' This is an invalid header value. The correct value is 'xamzserversideencryption': 'AES256'. This refers to ServerSide Encryption with Amazon S3Managed Encryption Keys (SSES3).'xamzserversideencryption': 'SSEKMS' Invalid header value. SSEKMS is an encryption option.'xamzserversideencryption': 'AES256' This is the correct header value if you are using SSES3 serverside encryption.Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html84:Tb05,With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.Elastic BeanStalk Key Concepts: via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.htmlDedicated worker environment If your AWS Elastic Beanstalk application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker environment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your application stays responsive under load.A longrunning task is anything that substantially increases the time it takes to complete a request, such as processing images or videos, sending emails, or generating a ZIP archive. These operations can take only a second or two to complete, but a delay of a few seconds is a lot for a web request that would otherwise complete in less than 500 ms.via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeaturesmanagingenvtiers.html Incorrect options:Single Instance Worker node Worker machines in Kubernetes are called nodes. Amazon EKS worker nodes are standard Amazon EC2 instances. EKS worker nodes are not to be confused with the Elastic Beanstalk worker environment. Since we are talking about the Elastic Beanstalk environment, this is not the correct choice.Loadbalancing, Autoscaling environment A loadbalancing and autoscaling environment uses the Elastic Load Balancing and Amazon EC2 Auto Scaling services to provision the Amazon EC2 instances that are required for your deployed application. Amazon EC2 Auto Scaling automatically starts additional instances to accommodate increasing load on your application. If your application requires scalability with the option of running in multiple Availability Zones, use a loadbalancing, autoscaling environment. This is not the right environment for the given usecase since it will add costs to the overall solution.Single Instance with Elastic IP A singleinstance environment contains one Amazon EC2 instance with an Elastic IP address. A singleinstance environment doesn't have a load balancer, which can help you reduce costs compared to a loadbalancing, autoscaling environment. This is not a highly available architecture, because if that one instance goes down then your application is down. This is not recommended for production environments.Reference: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeaturesmanagingenvtypes.html85:T64e,Correct option:Create one CodePipeline for your entire flow and add a manual approval step You can add an approval action to a stage in a CodePipeline pipeline at the point where you want the pipeline to stop so someone can manually approve or reject the action. Approval actions can't be added to Source stages. Source stages can contain only source actions.Incorrect options:Create multiple CodePipelines for each environment and link them using AWS Lambda You can create Lambda functions and add them as actions in your pipelines but the approval step is confined to a workflow process and cannot be outsourced to any other AWS service.Create deeply integrated AWS CodePipelines for each environment You can use an AWS CloudFormation template in conjunction with AWS CodePipeline and AWS CodeCommit to create a test environment that deploys to your production environment when the changes to your application are approved, helping you automate a continuous delivery workflow. This is a possible answer but not an optimized way of achieving what the client needs.Use CodePipeline with Amazon Virtual Private Cloud AWS CodePipeline supports Amazon Virtual Private Cloud (Amazon VPC) endpoints powered by AWS PrivateLink. This means you can connect directly to CodePipeline through a private endpoint in your VPC, keeping all traffic inside your VPC and the AWS network. This is a robust security feature but is of no value for our current usecase.References:https://docs.aws.amazon.com/codepipeline/latest/userguide/approvalsactionadd.html https://docs.aws.amazon.com/codepipeline/latest/userguide/vpcsupport.html86:T821,Correct option:Use Cognito User pools to facilitate sign up and user management for the mobile appAmazon Cognito provides authentication, authorization, and user management for your web and mobile apps. Your users can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, Google or Apple.A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a thirdparty identity provider (IdP). Whether your users signin directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK.Cognito is fully managed by AWS and works out of the box so it meets the requirements for the given usecase.via https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.html Incorrect options:Use Cognito Identity pools to facilitate sign up and user management for the mobile app You can use Identity pools to grant your users access to other AWS services. With an identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users, as well as the specific identity providers that you can use to authenticate users for identity pools.Exam Alert:Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools: via https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.htmlCreate a custom solution with EC2 and DynamoDB to facilitate sign up and user management for the mobile appCreate a custom solution with Lambda and DynamoDB to facilitate sign up and user management for the mobile appAs the problem statement mentions that the solution needs to be fully managed and should require the least amount of development effort, so you cannot use EC2 or Lambda functions with DynamoDB to create a custom solution.Reference:https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.htmll87:Ta58,Correct option:Configure VPC endpoints for DynamoDB that will provide required internal access without using public internetWhen you create a VPC endpoint for DynamoDB, any requests to a DynamoDB endpoint within the Region (for example, dynamodb.uswest2.amazonaws.com) are routed to a private DynamoDB endpoint within the Amazon network. You don't need to modify your applications running on EC2 instances in your VPC. The endpoint name remains the same, but the route to DynamoDB stays entirely within the Amazon network, and does not access the public internet. You use endpoint policies to control access to DynamoDB. Traffic between your VPC and the AWS service does not leave the Amazon network.Using Amazon VPC Endpoints to Access DynamoDB: via https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpcendpointsdynamodb.html Incorrect options:The firm can use a virtual private network (VPN) to route all DynamoDB network traffic through their own corporate network infrastructure You can address the requested security concerns by using a virtual private network (VPN) to route all DynamoDB network traffic through your own corporate network infrastructure. However, this approach can introduce bandwidth and availability challenges and hence is not an optimal solution here.Create a NAT Gateway to provide the necessary communication channel between EC2 instances and DynamoDB You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. NAT Gateway is not useful here since the instance and DynamoDB are present in AWS network and do not need NAT Gateway for communicating with each other.Create an Internet Gateway to provide the necessary communication channel between EC2 instances and DynamoDB An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An internet gateway serves two purposes: to provide a target in your VPC route tables for internetroutable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. Using an Internet Gateway would imply that the EC2 instances are connecting to DynamoDB using the public internet. Therefore, this option is incorrect.References:https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpcendpointsdynamodb.html https://docs.aws.amazon.com/vpc/latest/userguide/vpcnatgateway.html https://docs.aws.amazon.com/vpc/latest/userguide/Carrier_Gateway.html88:Te7b,Configure the application to fetch the variables and credentials from AWS Systems Manager Parameter Store by leveraging hierarchical unique paths in Parameter Store for each variable in each environmentParameter Stores is a capability of AWS Systems Manager that provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. You can reference Systems Manager parameters in your scripts, commands, SSM documents, and configuration and automation workflows by using the unique name that you specified when you created the parameter.Managing dozens or hundreds of parameters as a flat list is timeconsuming and prone to errors. It can also be difficult to identify the correct parameter for a task. This means you might accidentally use the wrong parameter, or you might create multiple parameters that use the same configuration data.You can use parameter hierarchies to help you organize and manage parameters. A hierarchy is a parameter name that includes a path that you define by using forward slashes (/).via https://docs.aws.amazon.com/systemsmanager/latest/userguide/sysmanparamstorehierarchies.html Incorrect options:Configure the application to fetch the variables from AWS KMS by storing the API URL and credentials as unique keys in KMS for each environment AWS KMS lets you create, manage, and control cryptographic keys across your applications and AWS services. KMS is not a keyvalue service that can be used for the given use case.Configure the application to fetch the variables from an encrypted file that is stored with the application by storing the API URL and credentials in unique files for each environment It is not considered a security best practice to store sensitive data and credentials in an encrypted file with the application. So this option is incorrect.Configure the application to fetch the variables from each of the deployed environments by defining the authentication information and API URL in the ECS task definition as unique names during the deployment process ECS task definition can be thought of as a blueprint for your application. Task definitions specify various parameters for your application. Examples of task definition parameters are which containers to use, which launch type to use, which ports should be opened for your application, and what data volumes should be used with the containers in the task. The specific parameters available for the task definition depend on which launch type you are using. The task definition is a text file, in JSON format, that describes one or more containers, up to a maximum of ten, that form your application. A task is the instantiation of a task definition within a cluster. After you create a task definition for your application within Amazon ECS, you can specify the number of tasks to run on your cluster.AWS recommends storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters. Environment variables specified in the task definition are readable by all users and roles that are allowed the DescribeTaskDefinition action for the task definition. So this option is incorrect.via https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdefenvfiles.html References:https://docs.aws.amazon.com/systemsmanager/latest/userguide/sysmanparamstorehierarchies.html https://aws.amazon.com/kms/ https://ecsworkshop.com/introduction/ecs_basics/task_definition/ https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdefenvfiles.htmll89:T684,Correct option:S3 access logging is pointing to the same bucket and is responsible for the substantial growth of bucket size When your source bucket and target bucket are the same bucket, additional logs are created for the logs that are written to the bucket. The extra logs about logs might make it harder to find the log that you are looking for. This configuration would drastically increase the size of the S3 bucket.via https://aws.amazon.com/premiumsupport/knowledgecenter/s3serveraccesslogssamebucket/ Incorrect options: Erroneous Bucket policies for batch uploads can sometimes be responsible for the exponential growth of S3 Bucket size This is an incorrect statement. A bucket policy is a resourcebased AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. A bucket policy, for batch processes or normal processes, will not increase the size of the bucket or the objects in it.A DDOS attack on your S3 bucket can potentially blow up the size of data in the bucket if the bucket security is compromised during the attack This is an incorrect statement. AWS handles DDoS attacks on all of its managed services. However, a DDoS attack will not increase the size of the bucket.Object Encryption has been enabled and each object is stored twice as part of this configuration Encryption does not increase a bucket's size, that too, on daily basis, as if the case in the current scenarioReferences:https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html https://docs.aws.amazon.com/AmazonS3/latest/userguide/setpermissions.html8a:T89e,Correct option:AWS XRay helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With XRay, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. XRay provides an endtoend view of requests as they travel through your application, and shows a map of your application’s underlying components.How XRay Works: via https://aws.amazon.com/xray/Fix the IAM RoleCreate an IAM role with write permissions and assign it to the resources running your application. You can use AWS Identity and Access Management (IAM) to grant XRay permissions to users and compute resources in your account. This should be one of the first places you start by checking that your permissions are properly configured before exploring other troubleshooting options.Here is an example of XRay ReadOnly permissions via an IAM policy:{ "Version": "20121017", "Statement": [ { "Effect": "Allow", "Action": [ "xray:GetSamplingRules", "xray:GetSamplingTargets", "xray:GetSamplingStatisticSummaries", "xray:BatchGetTraces", "xray:GetServiceGraph", "xray:GetTraceGraph", "xray:GetTraceSummaries", "xray:GetGroups", "xray:GetGroup" ], "Resource": [ "*" ] } ]}Another example of write permissions for using XRay via an IAM policy:{ "Version": "20121017", "Statement": [ { "Effect": "Allow", "Action": [ "xray:PutTraceSegments", "xray:PutTelemetryRecords", "xray:GetSamplingRules", "xray:GetSamplingTargets", "xray:GetSamplingStatisticSummaries" ], "Resource": [ "*" ] } ]}Incorrect options:Enable XRay sampling If permissions are not configured correctly sampling will not work, so this option is not correct.XRay only works with AWS Lambda aliases This is not true, aliases are pointers to specific Lambda function versions. To use the XRay SDK on Lambda, bundle it with your function code each time you create a new version.Change the security group rules You grant permissions to your Lambda function to access other resources using an IAM role and not via security groups.Reference: https://docs.aws.amazon.com/xray/latest/devguide/security_iam_troubleshoot.html5:["$","$Le",null,{"quiz":{"id":"aws-developer-6","title":"AWS Developer Associate (DVA-C02) Practice Test Latest 2025 6","description":"Additional practice questions covering AWS development topics.","questions":[{"question":"A company's ecommerce website is expecting hundreds of thousands of visitors on Black Friday. The marketing department is concerned that high volumes of orders might stress SQS leading to message failures. The company has approached you for the steps to be taken as a precautionary measure against the high volumes. What step will you suggest as a Developer Associate?","answers":[{"text":"Preconfigure the SQS queue to increase the capacity when messages hit a certain threshold","isCorrect":false},{"text":"Convert the queue into FIFO ordered queue, since messages to the down system will be processed faster once they are ordered","isCorrect":false},{"text":"Enable autoscaling in the SQS queue","isCorrect":false},{"text":"Amazon SQS is highly scalable and does not need any intervention to handle the expected high volumes","isCorrect":true}],"explanation":"$f"},{"question":"You have been hired at a company that needs an experienced developer to help with a continuous integration/continuous delivery (CI/CD) workflow on AWS. You configure the company's workflow to run an AWS CodePipeline pipeline whenever the application's source code changes in a repository hosted in AWS Code Commit and compiles source code with AWS Code Build. You are configuring ProjectArtifacts in your build stage. Which of the following should you do?","answers":[{"text":"Contact AWS Support to allow AWS CodePipeline to manage build outputs","isCorrect":false},{"text":"Give AWS CodeBuild permissions to upload the build output to your Amazon S3 bucket","isCorrect":true},{"text":"Give AWS CodeCommit permissions to upload the build output to your Amazon S3 bucket","isCorrect":false},{"text":"Configure AWS CodeBuild to store output artifacts on EC2 servers","isCorrect":false}],"explanation":"Correct option:Give AWS CodeBuild permissions to upload the build output to your Amazon S3 bucketIf you choose ProjectArtifacts and your value type is S3 then the build project stores build output in Amazon Simple Storage Service (Amazon S3). For that, you will need to give AWS CodeBuild permissions to upload.Incorrect options:Configure AWS CodeBuild to store output artifacts on EC2 servers EC2 servers are not a valid output location, so this option is ruled out.Give AWS CodeCommit permissions to upload the build output to your Amazon S3 bucket AWS CodeCommit is the repository that holds source code and has no control over compiling the source code, so this option is incorrect.Contact AWS Support to allow AWS CodePipeline to manage build outputs You can set AWS CodePipeline to manage its build output locations instead of AWS CodeBuild. There is no need to contact AWS Support.Reference: https://docs.aws.amazon.com/codebuild/latest/userguide/createproject.html#createprojectcli"},{"question":"A media company uses Amazon Simple Queue Service (SQS) queue to manage their transactions. With changing business needs, the payload size of the messages is increasing. The Team Lead of the project is worried about the 256 KB message size limit that SQS has. What can be done to make the queue accept messages of a larger size?","answers":[{"text":"Use the MultiPart API","isCorrect":false},{"text":"Use gzip compression","isCorrect":false},{"text":"Get a service limit increase from AWS","isCorrect":false},{"text":"Use the SQS Extended Client","isCorrect":true}],"explanation":"$10"},{"question":"While troubleshooting, a developer realized that the Amazon EC2 instance is unable to connect to the Internet using the Internet Gateway. Which conditions should be met for Internet connectivity to be established? (Select two)","answers":[{"text":"The route table in the instance’s subnet should have a route to an Internet Gateway","isCorrect":true},{"text":"The instance's subnet is associated with multiple route tables with conflicting configurations","isCorrect":false},{"text":"The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic","isCorrect":true},{"text":"The subnet has been configured to be Public and has no access to the internet","isCorrect":false},{"text":"The instance's subnet is not associated with any route table","isCorrect":false}],"explanation":"$11"},{"question":"What is the run order of the hooks for inplace deployments using CodeDeploy?","answers":[{"text":"Application Stop > Before Install > Application Start > ValidateService","isCorrect":true},{"text":"Application Stop > Before Install > ValidateService > Application Start","isCorrect":false},{"text":"Before Install > Application Stop > ValidateService > Application Start","isCorrect":false},{"text":"Before Install > Application Stop > Application Start > ValidateService","isCorrect":false}],"explanation":"$12"},{"question":"An application running on EC2 instances processes messages from an SQS queue. However, sometimes the messages are not processed and they end up in errors. These messages need to be isolated for further processing and troubleshooting. Which of the following options will help achieve this?","answers":[{"text":"Use DeleteMessage","isCorrect":false},{"text":"Reduce the VisibilityTimeout","isCorrect":false},{"text":"Implement a DeadLetter Queue","isCorrect":true},{"text":"Increase the VisibilityTimeout","isCorrect":false}],"explanation":"$13"},{"question":"You are designing a highperformance application that requires millions of connections. You have several EC2 instances running Apache2 web servers and the application will require capturing the user's source IP address and source port without the use of XForwardedFor. Which of the following options will meet your needs?","answers":[{"text":"Elastic Load Balancer","isCorrect":false},{"text":"Application Load Balancer","isCorrect":false},{"text":"Classic Load Balancer","isCorrect":false},{"text":"Network Load Balancer","isCorrect":true}],"explanation":"$14"},{"question":"You have created a continuous delivery service model with automated steps using AWS CodePipeline. Your pipeline uses your code, maintained in a CodeCommit repository, AWS CodeBuild, and AWS Elastic Beanstalk to automatically deploy your code every time there is a code change. However, the deployment to Elastic Beanstalk is taking a very long time due to resolving dependencies on all of your 100 target EC2 instances. Which of the following actions should you take to improve performance with limited code changes?","answers":[{"text":"Create a custom platform for Elastic Beanstalk","isCorrect":false},{"text":"Bundle the dependencies in the source code in CodeCommit","isCorrect":false},{"text":"Store the dependencies in S3, to be used while deploying to Beanstalk","isCorrect":false},{"text":"Bundle the dependencies in the source code during the build stage of CodeBuild","isCorrect":true}],"explanation":"$15"},{"question":"As a developer, you are looking at creating a custom configuration for Amazon EC2 instances running in an Auto Scaling group. The solution should allow the group to autoscale based on the metric of 'average RAM usage' for your Amazon EC2 instances. Which option provides the best solution?","answers":[{"text":"Migrate your application to AWS Lambda","isCorrect":false},{"text":"Enable detailed monitoring for EC2 and ASG to get the RAM usage data and create a CloudWatch Alarm on top of it","isCorrect":false},{"text":"Create a custom alarm for your ASG and make your instances trigger the alarm using PutAlarmData API","isCorrect":false},{"text":"Create a custom metric in CloudWatch and make your instances send data to it using PutMetricData. Then, create an alarm based on this metric","isCorrect":true}],"explanation":"$16"},{"question":"A development team is storing sensitive customer data in S3 that will require encryption at rest. The encryption keys must be rotated at least annually. What is the easiest way to implement a solution for this requirement?","answers":[{"text":"Use SSEC with automatic key rotation on an annual basis","isCorrect":false},{"text":"Use AWS KMS with automatic key rotation","isCorrect":true},{"text":"Encrypt the data before sending it to Amazon S3","isCorrect":false},{"text":"Import a custom key into AWS KMS and automate the key rotation on an annual basis by using a Lambda function","isCorrect":false}],"explanation":"$17"},{"question":"Your web application reads and writes data to your DynamoDB table. The table is provisioned with 400 Write Capacity Units (WCU's) shared across 4 partitions. One of the partitions receives 250 WCU/second while others receive much less. You receive the error 'ProvisionedThroughputExceededException'. What is the likely cause of this error?","answers":[{"text":"You have a hot partition","isCorrect":true},{"text":"Configured IAM policy is wrong","isCorrect":false},{"text":"CloudWatch monitoring is lagging","isCorrect":false},{"text":"Write Capacity Units (WCU’s) are applied across to all your DynamoDB tables and this needs reconfiguration","isCorrect":false}],"explanation":"$18"},{"question":"A development team has created a new IAM user that has s3:putObject permission to write to an S3 bucket. This S3 bucket uses serverside encryption with AWS KMS managed keys (SSEKMS) as the default encryption. Using the access key ID and the secret access key of the IAM user, the application received an access denied error when calling the PutObject API. As a Developer Associate, how would you resolve this issue?","answers":[{"text":"Correct the ACL of the S3 bucket to allow the IAM user to upload encrypted objects","isCorrect":false},{"text":"Correct the policy of the IAM user to allow the kms:GenerateDataKey action","isCorrect":true},{"text":"Correct the policy of the IAM user to allow the s3:Encrypt action","isCorrect":false},{"text":"Correct the bucket policy of the S3 bucket to allow the IAM user to upload encrypted objects","isCorrect":false}],"explanation":"$19"},{"question":"A data analytics company with its IT infrastructure on the AWS Cloud wants to build and deploy its flagship application as soon as there are any changes to the source code. As a Developer Associate, which of the following options would you suggest to trigger the deployment? (Select two)","answers":[{"text":"Keep the source code in Amazon EFS and start AWS CodePipeline whenever a file is updated","isCorrect":false},{"text":"Keep the source code in an AWS CodeCommit repository and start AWS CodePipeline whenever a change is pushed to the CodeCommit repository","isCorrect":true},{"text":"Keep the source code in an Amazon EBS volume and start AWS CodePipeline whenever there are updates to the source code","isCorrect":false},{"text":"Keep the source code in an Amazon S3 bucket and start AWS CodePipeline whenever a file in the S3 bucket is updated","isCorrect":true},{"text":"Keep the source code in an Amazon S3 bucket and set up AWS CodePipeline to recur at an interval of every 15 minutes","isCorrect":false}],"explanation":"$1a"},{"question":"AWS CloudFormation helps model and provision all the cloud infrastructure resources needed for your business. Which of the following services rely on CloudFormation to provision resources (Select two)?","answers":[{"text":"AWS Serverless Application Model (AWS SAM)","isCorrect":true},{"text":"AWS Elastic Beanstalk","isCorrect":true},{"text":"AWS CodeBuild","isCorrect":false},{"text":"AWS Lambda","isCorrect":false},{"text":"AWS Autoscaling","isCorrect":false}],"explanation":"$1b"},{"question":"You work as a developer doing contract work for the government on AWS gov cloud. Your applications use Amazon Simple Queue Service (SQS) for its message queue service. Due to recent hacking attempts, security measures have become stricter and require you to store data in encrypted queues. Which of the following steps can you take to meet your requirements without making changes to the existing code?","answers":[{"text":"Use Client side encryption","isCorrect":false},{"text":"Enable SQS KMS encryption","isCorrect":true},{"text":"Use the SSL endpoint","isCorrect":false},{"text":"Use Secrets Manager","isCorrect":false}],"explanation":"$1c"},{"question":"A leading financial services company offers data aggregation services for Wall Street trading firms. The company bills its clients based on per unit of clickstream data provided to the clients. As the company operates in a regulated industry, it needs to have the same ordered clickstream data available for auditing within a window of 7 days. As a Developer Associate, which of the following AWS services do you think provides the ability to run the billing process and auditing process on the given clickstream data in the same order?","answers":[{"text":"AWS Kinesis Data Firehose","isCorrect":false},{"text":"AWS Kinesis Data Streams","isCorrect":true},{"text":"Amazon SQS","isCorrect":false},{"text":"AWS Kinesis Data Analytics","isCorrect":false}],"explanation":"$1d"},{"question":"Your company uses an Application Load Balancer to route incoming enduser traffic to applications hosted on Amazon EC2 instances. The applications capture incoming request information and store it in the Amazon Relational Database Service (RDS) running on Microsoft SQL Server DB engines. As part of new compliance rules, you need to capture the client's IP address. How will you achieve this?","answers":[{"text":"You can get the Client IP addresses from Elastic Load Balancing logs","isCorrect":false},{"text":"Use the header XForwardedFrom","isCorrect":false},{"text":"Use the header XForwardedFor","isCorrect":true},{"text":"You can get the Client IP addresses from server access logs","isCorrect":false}],"explanation":"$1e"},{"question":"An ecommerce company uses AWS CloudFormation to implement Infrastructure as Code for the entire organization. Maintaining resources as stacks with CloudFormation has greatly reduced the management effort needed to manage and maintain the resources. However, a few teams have been complaining of failing stack updates owing to outofband fixes running on the stack resources. Which of the following is the best solution that can help in keeping the CloudFormation stack and its resources in sync with each other?","answers":[{"text":"Use Tag feature of CloudFormation to monitor the changes happening on specific resources","isCorrect":false},{"text":"Use CloudFormation in Elastic Beanstalk environment to reduce direct changes to CloudFormation resources","isCorrect":false},{"text":"Use Drift Detection feature of CloudFormation","isCorrect":true},{"text":"Use Change Sets feature of CloudFormation","isCorrect":false}],"explanation":"$1f"},{"question":"A new member of your team is working on creating Dead Letter Queue (DLQ) for AWS Lambda functions. As a Developer Associate, can you help him identify the use cases, wherein AWS Lambda will add a message into a DLQ after being processed? (Select two)","answers":[{"text":"The event has been processed successfully","isCorrect":false},{"text":"The Lambda function invocation is synchronous","isCorrect":false},{"text":"The event fails all processing attempts","isCorrect":true},{"text":"The Lambda function invocation is asynchronous","isCorrect":true},{"text":"The Lambda function invocation failed only once but succeeded thereafter","isCorrect":false}],"explanation":"$20"},{"question":"A company follows collaborative development practices. The engineering manager wants to isolate the development effort by setting up simulations of API components owned by various development teams. Which API integration type is best suited for this requirement?","answers":[{"text":"HTTP","isCorrect":false},{"text":"AWS_PROXY","isCorrect":false},{"text":"HTTP_PROXY","isCorrect":false},{"text":"MOCK","isCorrect":true}],"explanation":"$21"},{"question":"You are assigned as the new project lead for a web application that processes orders for customers. You want to integrate eventdriven processing anytime data is modified or deleted and use a serverless approach using AWS Lambda for processing stream events. Which of the following databases should you choose from?","answers":[{"text":"RDS","isCorrect":false},{"text":"Kinesis","isCorrect":false},{"text":"ElastiCache","isCorrect":false},{"text":"DynamoDB","isCorrect":true}],"explanation":"$22"},{"question":"Consider an application that enables users to store their mobile phone images in the cloud and supports tens of thousands of users. The application should utilize an Amazon API Gateway REST API that leverages AWS Lambda functions for photo processing while storing photo details in Amazon DynamoDB. The application should allow users to create an account, upload images, and retrieve previously uploaded images, with images ranging in size from 500 KB to 5 MB. How will you design the application with the least operational overhead?","answers":[{"text":"Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images as well as the image metadata in a DynamoDB table. Have the Lambda function retrieve previously uploaded images from DynamoDB","isCorrect":false},{"text":"Use Cognito identity pools to create an IAM user for each user of the application during the signup process. Leverage IAM authentication in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key","isCorrect":false},{"text":"Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key","isCorrect":true},{"text":"Use Cognito identity pools to manage user accounts and set up an Amazon Cognito identity pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key","isCorrect":false}],"explanation":"$23"},{"question":"A developer is migrating an onpremises application to AWS Cloud. The application currently processes user uploads and uploads them to a local directory on the server. All such file uploads must be saved and then made available to all instances in an Auto Scaling group. As a Developer Associate, which of the following options would you recommend for this usecase?","answers":[{"text":"Use Amazon EBS as the storage volume and share the files via file synchronization software","isCorrect":false},{"text":"Use Amazon S3 and make code changes in the application so all uploads are put on S3","isCorrect":true},{"text":"Use Amazon EBS and configure the application AMI to use a snapshot of the same EBS instance while launching new instances","isCorrect":false},{"text":"Use Instance Store type of EC2 instances and share the files via file synchronization software","isCorrect":false}],"explanation":"$24"},{"question":"For an application that stores personal health information (PHI) in an encrypted Amazon RDS for MySQL DB instance, a developer wants to improve its performance by caching frequently accessed data and adding the ability to sort or rank the cached datasets. What is the best approach to meet these requirements subject to the constraint that the PHI stays encrypted at all times?","answers":[{"text":"Migrate the frequently accessed data to an EC2 Instance Store that has encryption enabled for data in transit and at rest","isCorrect":false},{"text":"Migrate the frequently accessed data to DynamoDB Accelerator (DAX) that has encryption enabled for data in transit and at rest","isCorrect":false},{"text":"Store the frequently accessed data in an Amazon ElastiCache for Memcached instance with encryption enabled for data in transit and at rest","isCorrect":false},{"text":"Store the frequently accessed data in an Amazon ElastiCache for Redis instance with encryption enabled for data in transit and at rest","isCorrect":true}],"explanation":"$25"},{"question":"A team is checking the viability of using AWS Step Functions for creating a banking workflow for loan approvals. The web application will also have human approval as one of the steps in the workflow. As a developer associate, which of the following would you identify as the key characteristics for AWS Step Function? (Select two)","answers":[{"text":"Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of 180 days or 6 months","isCorrect":false},{"text":"Standard Workflows on AWS Step Functions are suitable for longrunning, durable, and auditable workflows that can also support any human approval steps","isCorrect":true},{"text":"Both Standard and Express Workflows support all service integrations, activities, and design patterns","isCorrect":false},{"text":"Standard Workflows on AWS Step Functions are suitable for longrunning, durable, and auditable workflows that do not support any human approval steps","isCorrect":false},{"text":"You should use Express Workflows for workloads with high event rates and short duration","isCorrect":true}],"explanation":"$26"},{"question":"The development team at a social media company is considering using Amazon ElastiCache to boost the performance of their existing databases. As a Developer Associate, which of the following usecases would you recommend as the BEST fit for ElastiCache? (Select two)","answers":[{"text":"Use ElastiCache to run highly complex JOIN queries","isCorrect":false},{"text":"Use ElastiCache to improve latency and throughput for writeheavy application workloads","isCorrect":false},{"text":"Use ElastiCache to improve performance of ExtractTransformLoad (ETL) workloads","isCorrect":false},{"text":"Use ElastiCache to improve performance of computeintensive workloads","isCorrect":true},{"text":"Use ElastiCache to improve latency and throughput for readheavy application workloads","isCorrect":true}],"explanation":"$27"},{"question":"A developer wants to enable XRay tracing on an onpremises Linux server running a custom application that is accessed through Amazon API Gateway. What is the most efficient solution that requires minimal configuration?","answers":[{"text":"Install and run the CloudWatch Unified Agent on the onpremises servers to capture and relay the XRay data to the XRay service using the PutTraceSegments API call","isCorrect":false},{"text":"Install and run the XRay daemon on the onpremises servers to capture and relay the data to the XRay service","isCorrect":true},{"text":"Configure a Lambda function to analyze the incoming traffic data on the onpremises servers and then relay the XRay data to the XRay service using the PutTelemetryRecords API call","isCorrect":false},{"text":"Install and run the XRay SDK on the onpremises servers to capture and relay the data to the XRay service","isCorrect":false}],"explanation":"$28"},{"question":"A development team has deployed a REST API in Amazon API Gateway to two different stages a test stage and a prod stage. The test stage is used as a test build and the prod stage as a stable build. After the updates have passed the test, the team wishes to promote the test stage to the prod stage. Which of the following represents the optimal solution for this usecase?","answers":[{"text":"Update stage variable value from the stage name of test to that of prod","isCorrect":true},{"text":"Delete the existing prod stage. Create a new stage with the same name (prod) and deploy the tested version on this stage","isCorrect":false},{"text":"API performance is optimized in a different way for prod environments. Hence, promoting test to prod is not correct. The promotion should be done by redeploying the API to the prod stage","isCorrect":false},{"text":"Deploy the API without choosing a stage. This way, the working deployment will be updated in all stages","isCorrect":false}],"explanation":"$29"},{"question":"You are creating a mobile application that needs access to the AWS API Gateway. Users will need to register first before they can access your API and you would like the user management to be fully managed. Which authentication option should you use for your API Gateway layer?","answers":[{"text":"Use IAM permissions with sigv4","isCorrect":false},{"text":"Use API Gateway User Pools","isCorrect":false},{"text":"Use Lambda Authorizer","isCorrect":false},{"text":"Use Cognito User Pools","isCorrect":true}],"explanation":"$2a"},{"question":"An IT company is using AWS CloudFormation to manage its IT infrastructure. It has created a template to provision a stack with a VPC and a subnet. The output value of this subnet has to be used in another stack. As a Developer Associate, which of the following options would you suggest to provide this information to another stack?","answers":[{"text":"Use Fn::ImportValue","isCorrect":false},{"text":"Use 'Expose' field in the Output section of the stack's template","isCorrect":false},{"text":"Use Fn::Transform","isCorrect":false},{"text":"Use 'Export' field in the Output section of the stack's template","isCorrect":true}],"explanation":"$2b"},{"question":"The development team at a retail company is gearing up for the upcoming Thanksgiving sale and wants to make sure that the application's serverless backend running via Lambda functions does not hit latency bottlenecks as a result of the traffic spike. As a Developer Associate, which of the following solutions would you recommend to address this usecase?","answers":[{"text":"Configure Application Auto Scaling to manage Lambda provisioned concurrency on a schedule","isCorrect":true},{"text":"Add an Application Load Balancer in front of the Lambda functions","isCorrect":false},{"text":"Configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule","isCorrect":false},{"text":"No need to make any special provisions as Lambda is automatically scalable because of its serverless nature","isCorrect":false}],"explanation":"$2c"},{"question":"A cybersecurity company is running a serverless backend with several computeheavy workflows running on Lambda functions. The development team has noticed a performance lag after analyzing the performance metrics for the Lambda functions. As a Developer Associate, which of the following options would you suggest as the BEST solution to address the computeheavy workloads?","answers":[{"text":"Use provisioned concurrency to account for the computeheavy workflows","isCorrect":false},{"text":"Increase the amount of memory available to the Lambda functions","isCorrect":true},{"text":"Use reserved concurrency to account for the computeheavy workflows","isCorrect":false},{"text":"Invoke the Lambda functions asynchronously to process the computeheavy workflows","isCorrect":false}],"explanation":"$2d"},{"question":"Your AWS CodeDeploy deployment to T2 instances succeed. The new application revision makes API calls to Amazon S3 however the application is not working as expected due to authorization exceptions and you were assigned to troubleshoot the issue. Which of the following should you do?","answers":[{"text":"Fix the IAM permissions for the EC2 instance role","isCorrect":true},{"text":"Fix the IAM permissions for the CodeDeploy service role","isCorrect":false},{"text":"Make the S3 bucket public","isCorrect":false},{"text":"Enable CodeDeploy Proxy","isCorrect":false}],"explanation":"$2e"},{"question":"An ecommerce company has an order processing workflow with several tasks to be done in parallel as well as decision steps to be evaluated for successful processing of the order. All the tasks are implemented via Lambda functions. Which of the following is the BEST solution to meet these business requirements?","answers":[{"text":"Use AWS Batch to orchestrate the workflow","isCorrect":false},{"text":"Use AWS Step Functions activities to orchestrate the workflow","isCorrect":false},{"text":"Use AWS Glue to orchestrate the workflow","isCorrect":false},{"text":"Use AWS Step Functions state machines to orchestrate the workflow","isCorrect":true}],"explanation":"$2f"},{"question":"You have been asked by your Team Lead to enable detailed monitoring of the Amazon EC2 instances your team uses. As a Developer working on AWS CLI, which of the below command will you run?","answers":[{"text":"aws ec2 runinstances imageid ami09092360 monitoring Enabled=true","isCorrect":false},{"text":"aws ec2 monitorinstances instanceid i1234567890abcdef0","isCorrect":false},{"text":"aws ec2 monitorinstances instanceids i1234567890abcdef0","isCorrect":true},{"text":"aws ec2 runinstances imageid ami09092360 monitoring State=enabled","isCorrect":false}],"explanation":"Correct option:aws ec2 monitorinstances instanceids i1234567890abcdef0 This enables detailed monitoring for a running instance.EC2 detailed monitoring: via https://docs.aws.amazon.com/cli/latest/reference/ec2/monitorinstances.html Incorrect options:aws ec2 runinstances imageid ami09092360 monitoring Enabled=true This syntax is used to enable detailed monitoring when launching an instance from AWS CLI.aws ec2 runinstances imageid ami09092360 monitoring State=enabled This is an invalid syntaxaws ec2 monitorinstances instanceid i1234567890abcdef0 This is an invalid syntaxReferences:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/usingcloudwatchnew.html https://docs.aws.amazon.com/cli/latest/reference/ec2/runinstances.html"},{"question":"You have configured a Network ACL and a Security Group for the load balancer and Amazon EC2 instances to allow inbound traffic on port 80. However, users are still unable to connect to your website after launch. Which additional configuration is required to make the website accessible to all users over the internet?","answers":[{"text":"Add a rule to the Network ACLs to allow outbound traffic on ports 32768 61000","isCorrect":false},{"text":"Add a rule to the Network ACLs to allow outbound traffic on ports 1024 65535","isCorrect":true},{"text":"Add a rule to the Network ACLs to allow outbound traffic on ports 1025 5000","isCorrect":false},{"text":"Add a rule to the Security Group allowing outbound traffic on port 80","isCorrect":false}],"explanation":"$30"},{"question":"A highfrequency stock trading firm is migrating their messaging queues from selfmanaged messageoriented middleware systems to Amazon SQS. The development team at the company wants to minimize the costs of using SQS. As a Developer Associate, which of the following options would you recommend to address the given usecase?","answers":[{"text":"Use SQS visibility timeout to retrieve messages from your Amazon SQS queues","isCorrect":false},{"text":"Use SQS message timer to retrieve messages from your Amazon SQS queues","isCorrect":false},{"text":"Use SQS long polling to retrieve messages from your Amazon SQS queues","isCorrect":true},{"text":"Use SQS short polling to retrieve messages from your Amazon SQS queues","isCorrect":false}],"explanation":"$31"},{"question":"Your company leverages Amazon CloudFront to provide content via the internet to customers with low latency. Aside from latency, security is another concern and you are looking for help in enforcing endtoend connections using HTTPS so that content is protected. Which of the following options is available for HTTPS in AWS CloudFront?","answers":[{"text":"Between clients and CloudFront only","isCorrect":false},{"text":"Between CloudFront and backend only","isCorrect":false},{"text":"Between clients and CloudFront as well as between CloudFront and backend","isCorrect":true},{"text":"Neither between clients and CloudFront nor between CloudFront and backend","isCorrect":false}],"explanation":"$32"},{"question":"Your mobile application needs to perform API calls to DynamoDB. You do not want to store AWS secret and access keys onto the mobile devices and need all the calls to DynamoDB made with a different identity per mobile device. Which of the following services allows you to achieve this?","answers":[{"text":"Cognito Sync","isCorrect":false},{"text":"IAM","isCorrect":false},{"text":"Cognito User Pools","isCorrect":false},{"text":"Cognito Identity Pools","isCorrect":true}],"explanation":"$33"},{"question":"Your application is deployed automatically using AWS Elastic Beanstalk. Your YAML configuration files are stored in the folder .ebextensions and new files are added or updated often. The DevOps team does not want to redeploy the application every time there are configuration changes, instead, they would rather manage configuration externally, securely, and have it load dynamically into the application at runtime. What option allows you to do this?","answers":[{"text":"Use SSM Parameter Store","isCorrect":true},{"text":"Use Environment variables","isCorrect":false},{"text":"Use Stage Variables","isCorrect":false},{"text":"Use S3","isCorrect":false}],"explanation":"$34"},{"question":"A developer is designing an AWS CloudFormation template for deploying Amazon EC2 instances in numerous AWS accounts. The developer needs to select EC2 instances from a list of preapproved instance types. What measures could the developer take to integrate the list of authorized instance types into the CloudFormation template?","answers":[{"text":"Configure a pseudo parameter with the list of EC2 instance types as AllowedValues in the CloudFormation template","isCorrect":false},{"text":"Configure a mapping having a list of EC2 instance types as parameters in the CloudFormation template","isCorrect":false},{"text":"Configure a parameter with the list of EC2 instance types as AllowedValues in the CloudFormation template","isCorrect":true},{"text":"Configure separate parameters for each EC2 instance type in the CloudFormation template","isCorrect":false}],"explanation":"$35"},{"question":"A data analytics company is processing realtime InternetofThings (IoT) data via Kinesis Producer Library (KPL) and sending the data to a Kinesis Data Streams driven application. The application has halted data processing because of a ProvisionedThroughputExceeded exception. Which of the following actions would help in addressing this issue? (Select two)","answers":[{"text":"Use Kinesis enhanced fanout for Kinesis Data Streams","isCorrect":false},{"text":"Increase the number of shards within your data streams to provide enough capacity","isCorrect":true},{"text":"Use Amazon SQS instead of Kinesis Data Streams","isCorrect":false},{"text":"Configure the data producer to retry with an exponential backoff","isCorrect":true},{"text":"Use Amazon Kinesis Agent instead of Kinesis Producer Library (KPL) for sending data to Kinesis Data Streams","isCorrect":false}],"explanation":"$36"},{"question":"A junior developer has been asked to configure access to an Amazon EC2 instance hosting a web application. The developer has configured a new security group to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules. A custom Network Access Control List (NACL) connected with the instance's subnet is configured to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules. Which of the following solutions would you suggest if the EC2 instance needs to accept and respond to requests from the internet?","answers":[{"text":"An outbound rule on the security group has to be configured, to allow the response to be sent to the client on the HTTP port","isCorrect":false},{"text":"An outbound rule must be added to the Network ACL (NACL) to allow the response to be sent to the client on the ephemeral port range","isCorrect":true},{"text":"The configuration is complete on the EC2 instance for accepting and responding to requests","isCorrect":false},{"text":"Outbound rules need to be configured both on the security group and on the NACL for sending responses to the Internet Gateway","isCorrect":false}],"explanation":"$37"},{"question":"You have a Javabased application running on EC2 instances loaded with AWS CodeDeploy agents. You are considering different options for deployment, one is the flexibility that allows for incremental deployment of your new application versions and replaces existing versions in the EC2 instances. The other option is a strategy in which an Auto Scaling group is used to perform a deployment. Which of the following options will allow you to deploy in this manner? (Select two)","answers":[{"text":"Pilot Light Deployment","isCorrect":false},{"text":"Cattle Deployment","isCorrect":false},{"text":"Warm Standby Deployment","isCorrect":false},{"text":"Inplace Deployment","isCorrect":true},{"text":"Blue/green Deployment","isCorrect":true}],"explanation":"$38"},{"question":"You have a workflow process that pulls code from AWS CodeCommit and deploys to EC2 instances associated with tag group ProdBuilders. You would like to configure the instances to archive no more than two application revisions to conserve disk space. Which of the following will allow you to implement this?","answers":[{"text":"AWS CloudWatch Log Agent","isCorrect":false},{"text":"CodeDeploy Agent","isCorrect":true},{"text":"Integrate with AWS CodePipeline","isCorrect":false},{"text":"Have a load balancer in front of your instances","isCorrect":false}],"explanation":"$39"},{"question":"A communication platform serves millions of customers and deploys features in a production environment on AWS via CodeDeploy. You are reviewing scripts for the deployment process located in the AppSpec file. Which of the following options lists the correct order of lifecycle events?","answers":[{"text":"ValidateService => BeforeInstall =>DownloadBundle => ApplicationStart","isCorrect":false},{"text":"BeforeInstall => ApplicationStart => DownloadBundle => ValidateService","isCorrect":false},{"text":"BeforeInstall => ValidateService =>DownloadBundle => ApplicationStart","isCorrect":false},{"text":"DownloadBundle => BeforeInstall => ApplicationStart => ValidateService","isCorrect":true}],"explanation":"$3a"},{"question":"A developer wants to securely store an access token that allows a transactionprocessing application running on Amazon EC2 instances to authenticate and send a chat message (via the chat API) to the company's support team when an invalid transaction is detected. While minimizing management overhead, the chat API access token must be encrypted both at rest and in transit, and also be accessible from other AWS accounts. What is the most efficient solution to address this scenario?","answers":[{"text":"Store AWS KMS encrypted access token in a DynamoDB table and configure a resourcebased policy for the DynamoDB table to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the DynamoDB table. Fetch the token from the Dynamodb table and then use the decrypted access token to send the message to the chat","isCorrect":false},{"text":"Leverage SSEKMS to store the access token as an encrypted object on S3 and configure a resourcebased policy for the S3 bucket to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the S3 object. Fetch the token from S3 and then use the decrypted access token to send the message to the chat","isCorrect":false},{"text":"Leverage AWS Secrets Manager with an AWS KMS customermanaged key to store the access token as a secret and configure a resourcebased policy for the secret to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Secrets Manager. Fetch the token from Secrets Manager and then use the decrypted access token to send the message to the chat","isCorrect":true},{"text":"Leverage AWS Systems Manager Parameter Store with an AWS KMS customermanaged key to store the access token as a SecureString parameter and configure a resourcebased policy for the parameter to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Parameter Store. Fetch the token from Parameter Store using the with decryption flag and then use the decrypted access token to send the message to the chat","isCorrect":false}],"explanation":"$3b"},{"question":"A developer is working on an AWS Lambda function that reads data from Amazon S3 objects and writes the data to an Amazon DynamoDB table. Although the function triggers successfully from an S3 event notification upon object creation, it encounters a failure while attempting to write data to the DynamoDB table. What is the probable reason for the failure?","answers":[{"text":"The Lambda function's provisioned concurrency limit has been exceeded","isCorrect":false},{"text":"DynamoDB table does not have a Gateway VPC Endpoint, which is required by the Lambda function for a successful write","isCorrect":false},{"text":"The Lambda function's reserved concurrency limit has been exceeded","isCorrect":false},{"text":"The Lambda function does not have IAM permissions to write to DynamoDB","isCorrect":true}],"explanation":"$3c"},{"question":"Your company manages MySQL databases on EC2 instances to have full control. Applications on other EC2 instances managed by an ASG make requests to these databases to get information that displays data on dashboards viewed on mobile phones, tablets, and web browsers. Your manager would like to scale your Auto Scaling group based on the number of requests per minute. How can you achieve this?","answers":[{"text":"You enable detailed monitoring and use that to scale your ASG","isCorrect":false},{"text":"Attach an Elastic Load Balancer","isCorrect":false},{"text":"Attach additional Elastic File Storage","isCorrect":false},{"text":"You create a CloudWatch custom metric and build an alarm to scale your ASG","isCorrect":true}],"explanation":"$3d"},{"question":"An IT company has its serverless stack integrated with AWS XRay. The developer at the company has noticed a high volume of data going into XRay and the AWS monthly usage charges have skyrocketed as a result. The developer has requested changes to mitigate the issue. As a Developer Associate, which of the following solutions would you recommend to obtain tracing trends while reducing costs with minimal disruption?","answers":[{"text":"Custom configuration for the XRay agents","isCorrect":false},{"text":"Implement a network sampling rule","isCorrect":false},{"text":"Enable XRay sampling","isCorrect":true},{"text":"Use Filter Expressions in the XRay console","isCorrect":false}],"explanation":"$3e"},{"question":"A financial services company with over 10,000 employees has hired you as the new Senior Developer. Initially caching was enabled to reduce the number of calls made to all API endpoints and improve the latency of requests to the company's API Gateway. For testing purposes, you would like to invalidate caching for the API clients to get the most recent responses. Which of the following should you do?","answers":[{"text":"Using the request parameter ?cachecontrolmaxage=0","isCorrect":false},{"text":"Use the Request parameter: ?bypass_cache=1","isCorrect":false},{"text":"Using the Header CacheControl: maxage=0","isCorrect":true},{"text":"Using the Header BypassCache=1","isCorrect":false}],"explanation":"$3f"},{"question":"A large firm stores its static data assets on Amazon S3 buckets. Each service line of the firm has its own AWS account. For a business use case, the Finance department needs to give access to their S3 bucket's data to the Human Resources department. Which of the below options is NOT feasible for crossaccount access of S3 bucket objects?","answers":[{"text":"Use Resourcebased policies and AWS Identity and Access Management (IAM) policies for programmaticonly access to S3 bucket objects","isCorrect":false},{"text":"Use Crossaccount IAM roles for programmatic and console access to S3 bucket objects","isCorrect":false},{"text":"Use Access Control List (ACL) and IAM policies for programmaticonly access to S3 bucket objects","isCorrect":false},{"text":"Use IAM roles and resourcebased policies delegate access across accounts within different partitions via programmatic access only","isCorrect":true}],"explanation":"$40"},{"question":"You team maintains a public API Gateway that is accessed by clients from another domain. Usage has been consistent for the last few months but recently it has more than doubled. As a result, your costs have gone up and would like to prevent other unauthorized domains from accessing your API. Which of the following actions should you take?","answers":[{"text":"Restrict access by using CORS","isCorrect":true},{"text":"Use Accountlevel throttling","isCorrect":false},{"text":"Assign a Security Group to your API Gateway","isCorrect":false},{"text":"Use Mapping Templates","isCorrect":false}],"explanation":"$41"},{"question":"A company uses Amazon RDS as its database. For improved user experience, it has been decided that a highly reliable fullymanaged caching layer has to be configured in front of RDS. Which of the following is the right choice, keeping in mind that cache content regeneration is a costly activity?","answers":[{"text":"Implement Amazon ElastiCache Memcached","isCorrect":false},{"text":"Implement Amazon ElastiCache Redis in Cluster Mode","isCorrect":true},{"text":"Migrate the database to Amazon Redshift","isCorrect":false},{"text":"Install Redis on an Amazon EC2 instance","isCorrect":false}],"explanation":"$42"},{"question":"A senior cloud engineer designs and deploys online fraud detection solutions for credit card companies processing millions of transactions daily. The Elastic Beanstalk application sends files to Amazon S3 and then sends a message to an Amazon SQS queue containing the path of the uploaded file in S3. The engineer wants to postpone the delivery of any new messages to the queue for at least 10 seconds. Which SQS feature should the engineer leverage?","answers":[{"text":"Use visibility timeout parameter","isCorrect":false},{"text":"Implement applicationside delay","isCorrect":false},{"text":"Use DelaySeconds parameter","isCorrect":true},{"text":"Enable LongPolling","isCorrect":false}],"explanation":"$43"},{"question":"You have moved your onpremise infrastructure to AWS and are in the process of configuring an AWS Elastic Beanstalk deployment environment for production, development, and testing. You have configured your production environment to use a rolling deployment to prevent your application from becoming unavailable to users. For the development and testing environment, you would like to deploy quickly and are not concerned about downtime. Which of the following deployment policies meet your needs?","answers":[{"text":"All at once","isCorrect":true},{"text":"Rolling with additional batches","isCorrect":false},{"text":"Immutable","isCorrect":false},{"text":"Rolling","isCorrect":false}],"explanation":"$44"},{"question":"A digital marketing company has its website hosted on an Amazon S3 bucket A. The development team notices that the web fonts that are hosted on another S3 bucket B are not loading on the website. Which of the following solutions can be used to address this issue?","answers":[{"text":"Configure CORS on the bucket B that is hosting the web fonts to allow Bucket A origin to make the requests","isCorrect":true},{"text":"Enable versioning on both the buckets to facilitate correct functioning of the website","isCorrect":false},{"text":"Update bucket policies on both bucket A and bucket B to allow successful loading of the web fonts on the website","isCorrect":false},{"text":"Configure CORS on the bucket A that is hosting the website to allow any origin to respond to requests","isCorrect":false}],"explanation":"$45"},{"question":"A company is using a Border Gateway Protocol (BGP) based AWS VPN connection to connect from its onpremises data center to Amazon EC2 instances in the company's account. The development team can access an EC2 instance in subnet A but is unable to access an EC2 instance in subnet B in the same VPC. Which logs can be used to verify whether the traffic is reaching subnet B?","answers":[{"text":"Subnet logs","isCorrect":false},{"text":"BGP logs","isCorrect":false},{"text":"VPN logs","isCorrect":false},{"text":"VPC Flow Logs","isCorrect":true}],"explanation":"$46"},{"question":"A developer wants a seamless ability to return to older versions of a Lambda function that is being deployed. Which of the following solutions offers the LEAST operational overhead?","answers":[{"text":"Use CodeDeploy to configure blue/green deployments for the different Lambda function versions","isCorrect":false},{"text":"Use a Lambda function alias that can point to the different versions","isCorrect":true},{"text":"Use Lambda function layers that can point to the different versions","isCorrect":false},{"text":"Use a Route 53 weighted policy that can point to the different Lambda function versions","isCorrect":false}],"explanation":"$47"},{"question":"A company has a cloud system in AWS with components that send and receive messages using SQS queues. While reviewing the system you see that it processes a lot of information and would like to be aware of any limits of the system. Which of the following represents the maximum number of messages that can be stored in an SQS queue?","answers":[{"text":"10000000","isCorrect":false},{"text":"100000","isCorrect":false},{"text":"no limit","isCorrect":true},{"text":"10000","isCorrect":false}],"explanation":"Correct option:\"no limit\": There are no message limits for storing in SQS, but 'inflight messages' do have limits. Make sure to delete messages after you have processed them. There can be a maximum of approximately 120,000 inflight messages (received from a queue by a consumer, but not yet deleted from the queue).Incorrect options:\"10000\"\"100000\"\"10000000\"These three options contradict the details provided in the explanation above, so these are incorrect.Reference:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqslimits.htmll"},{"question":"A .NET developer team works with many ASP.NET web applications that use EC2 instances to host them on IIS. The deployment process needs to be configured so that multiple versions of the application can run in AWS Elastic Beanstalk. One version would be used for development, testing, and another version for load testing. Which of the following methods do you recommend?","answers":[{"text":"Create an Application Load Balancer to route based on hostname so you can pass on parameters to the development Elastic Beanstalk environment. Create a file in .ebextensions/ to know how to handle the traffic coming from the ALB","isCorrect":false},{"text":"Define a dev environment with a single instance and a 'load test' environment that has settings close to production environment","isCorrect":true},{"text":"Use only one Beanstalk environment and perform configuration changes using an Ansible script","isCorrect":false},{"text":"You cannot have multiple development environments in Elastic Beanstalk, just one development and one production environment","isCorrect":false}],"explanation":"$48"},{"question":"An ecommerce company has implemented AWS CodeDeploy as part of its AWS cloud CI/CD strategy. The company has configured automatic rollbacks while deploying a new version of its flagship application to Amazon EC2. What occurs if the deployment of the new version fails?","answers":[{"text":"The last known working deployment is automatically restored using the snapshot stored in Amazon S3","isCorrect":false},{"text":"AWS CodePipeline promotes the most recent working deployment with a SUCCEEDED status to production","isCorrect":false},{"text":"CodeDeploy switches the Route 53 alias records back to the known good green deployment and terminates the failed blue deployment","isCorrect":false},{"text":"A new deployment of the last known working version of the application is deployed with a new deployment ID","isCorrect":true}],"explanation":"$49"},{"question":"As a Fullstack Web Developer, you are involved with every aspect of a company's platform from development with PHP and JavaScript to the configuration of NoSQL databases with Amazon DynamoDB. You are not concerned about your response receiving stale data from your database and need to perform 16 eventually consistent reads per second of 12 KB in size each. How many read capacity units (RCUs) do you need?","answers":[{"text":"12","isCorrect":false},{"text":"48","isCorrect":false},{"text":"192","isCorrect":false},{"text":"24","isCorrect":true}],"explanation":"via https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html 24One read capacity unit represents two eventually consistent reads per second, for an item up to 4 KB in size. So that means that for an item of 12KB in size, we need 3 RCU (12 KB / 4 KB) for two eventually consistent reads per second. As we need 16 eventually consistent reads per second, we need 3 * (16 / 2) = 24 RCU.Incorrect options:1219248These three options contradict the details provided in the explanation above, so these are incorrect.Reference: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html"},{"question":"Your team lead has asked you to learn AWS CloudFormation to create a collection of related AWS resources and provision them in an orderly fashion. You decide to provide AWSspecific parameter types to catch invalid values. When specifying parameters which of the following is not a valid Parameter type?","answers":[{"text":"CommaDelimitedList","isCorrect":false},{"text":"AWS::EC2::KeyPair::KeyName","isCorrect":false},{"text":"String","isCorrect":false},{"text":"DependentParameter","isCorrect":true}],"explanation":"$4a"},{"question":"A HealthCare mobile app uses proprietary Machine Learning algorithms to provide early diagnosis using patient health metrics. To protect this sensitive data, the development team wants to transition to a scalable user management system with login/signup functionality that also supports MultiFactor Authentication (MFA). Which of the following options can be used to implement a solution with the LEAST amount of development effort? (Select two)","answers":[{"text":"Use Lambda functions and DynamoDB to create a custom solution for user management","isCorrect":false},{"text":"Use Amazon Cognito for usermanagement and facilitating the login/signup process","isCorrect":true},{"text":"Use Amazon Cognito to enable MultiFactor Authentication (MFA) when users login","isCorrect":true},{"text":"Use Lambda functions and RDS to create a custom solution for user management","isCorrect":false},{"text":"Use Amazon SNS to send MultiFactor Authentication (MFA) code via SMS to mobile app users","isCorrect":false}],"explanation":"$4b"},{"question":"You are working for a shipping company that is automating the creation of ECS clusters with an Auto Scaling Group using an AWS CloudFormation template that accepts cluster name as its parameters. Initially, you launch the template with input value 'MainCluster', which deployed five instances across two availability zones. The second time, you launch the template with an input value 'SecondCluster'. However, the instances created in the second run were also launched in 'MainCluster' even after specifying a different cluster name. What is the root cause of this issue?","answers":[{"text":"The cluster name Parameter has not been updated in the file /etc/ecs/ecs.config during bootstrap","isCorrect":true},{"text":"The EC2 instance is missing IAM permissions to join the other clusters","isCorrect":false},{"text":"The security groups on the EC2 instance are pointing to the wrong ECS cluster","isCorrect":false},{"text":"The ECS agent Docker image must be rebuilt to connect to the other clusters","isCorrect":false}],"explanation":"$4c"},{"question":"A junior developer working on ECS instances terminated a container instance in Amazon Elastic Container Service (Amazon ECS) as per instructions from the team lead. But the container instance continues to appear as a resource in the ECS cluster. As a Developer Associate, which of the following solutions would you recommend to fix this behavior?","answers":[{"text":"A custom software on the container instance could have failed and resulted in the container hanging in an unhealthy state till restarted again","isCorrect":false},{"text":"You terminated the container instance while it was in RUNNING state, that lead to this synchronization issues","isCorrect":false},{"text":"The container instance has been terminated with AWS CLI, whereas, for ECS instances, Amazon ECS CLI should be used to avoid any synchronization issues","isCorrect":false},{"text":"You terminated the container instance while it was in STOPPED state, that lead to this synchronization issues","isCorrect":true}],"explanation":"$4d"},{"question":"You are a development team lead setting permissions for other IAM users with limited permissions. On the AWS Management Console, you created a dev group where new developers will be added, and on your workstation, you configured a developer profile. You would like to test that this user cannot terminate instances. Which of the following options would you execute?","answers":[{"text":"Using the CLI, create a dummy EC2 and delete it using another CLI call","isCorrect":false},{"text":"Retrieve the policy using the EC2 metadata service and use the IAM policy simulator","isCorrect":false},{"text":"Use the AWS CLI dryrun option","isCorrect":true},{"text":"Use the AWS CLI test option","isCorrect":false}],"explanation":"$4e"},{"question":"Your web application architecture consists of multiple Amazon EC2 instances running behind an Elastic Load Balancer with an Auto Scaling group having the desired capacity of 5 EC2 instances. You would like to integrate AWS CodeDeploy for automating application deployment. The deployment should reroute traffic from your application's original environment to the new environment. Which of the following options will meet your deployment criteria?","answers":[{"text":"Opt for Rolling deployment","isCorrect":false},{"text":"Opt for Immutable deployment","isCorrect":false},{"text":"Opt for Inplace deployment","isCorrect":false},{"text":"Opt for Blue/Green deployment","isCorrect":true}],"explanation":"$4f"},{"question":"The Development team at a media company is working on securing their databases. Which of the following AWS database engines can be configured with IAM Database Authentication? (Select two)","answers":[{"text":"RDS Oracle","isCorrect":false},{"text":"RDS MySQL","isCorrect":true},{"text":"RDS PostGreSQL","isCorrect":true},{"text":"RDS SQL Server","isCorrect":false},{"text":"RDS Db2","isCorrect":false}],"explanation":"$50"},{"question":"A company uses AWS CodeDeploy to deploy applications from GitHub to EC2 instances running Amazon Linux. The deployment process uses a file called appspec.yml for specifying deployment hooks. A final lifecycle event should be specified to verify the deployment success. Which of the following hook events should be used to verify the success of the deployment?","answers":[{"text":"ApplicationStart","isCorrect":false},{"text":"ValidateService","isCorrect":true},{"text":"AfterInstall","isCorrect":false},{"text":"AllowTraffic","isCorrect":false}],"explanation":"$51"},{"question":"A company has more than 100 million members worldwide enjoying 125 million hours of TV shows and movies each day. The company uses AWS for nearly all its computing and storage needs, which use more than 10,000 server instances on AWS. This results in an extremely complex and dynamic networking environment where applications are constantly communicating inside AWS and across the Internet. Monitoring and optimizing its network is critical for the company. The company needs a solution for ingesting and analyzing the multiple terabytes of realtime data its network generates daily in the form of flow logs. Which technology/service should the company use to ingest this data economically and has the flexibility to direct this data to other downstream systems?","answers":[{"text":"AWS Glue","isCorrect":false},{"text":"Amazon Kinesis Data Streams","isCorrect":true},{"text":"Amazon Kinesis Firehose","isCorrect":false},{"text":"Amazon Simple Queue Service (SQS)","isCorrect":false}],"explanation":"$52"},{"question":"To meet compliance guidelines, a company needs to ensure replication of any data stored in its S3 buckets. Which of the following characteristics are correct while configuring an S3 bucket for replication? (Select two)","answers":[{"text":"Object tags cannot be replicated across AWS Regions using CrossRegion Replication","isCorrect":false},{"text":"SameRegion Replication (SRR) and CrossRegion Replication (CRR) can be configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags","isCorrect":true},{"text":"S3 lifecycle actions are not replicated with S3 replication","isCorrect":true},{"text":"Once replication is enabled on a bucket, all old and new objects will be replicated","isCorrect":false},{"text":"Replicated objects do not retain metadata","isCorrect":false}],"explanation":"$53"},{"question":"A financial services company wants to ensure that the customer data is always kept encrypted on Amazon S3 but wants an AWS managed solution that allows full control to create, rotate and remove the encryption keys. As a Developer Associate, which of the following would you recommend to address the given usecase?","answers":[{"text":"ServerSide Encryption with Amazon S3Managed Keys (SSES3)","isCorrect":false},{"text":"ServerSide Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSEKMS)","isCorrect":true},{"text":"ServerSide Encryption with Secrets Manager","isCorrect":false},{"text":"ServerSide Encryption with CustomerProvided Keys (SSEC)","isCorrect":false}],"explanation":"$54"},{"question":"An Amazon Simple Queue Service (SQS) has to be configured between two AWS accounts for shared access to the queue. AWS account A has the SQS queue in its account and AWS account B has to be given access to this queue. Which of the following options need to be combined to allow this crossaccount access? (Select three)","answers":[{"text":"The account A administrator attaches a trust policy to the role that identifies account B as the principal who can assume the role.","isCorrect":false},{"text":"The account B administrator creates an IAM role and attaches a trust policy to the role with account B as the principal.","isCorrect":true},{"text":"The account A administrator delegates the permission to assume the role to any users in account A.","isCorrect":false},{"text":"The account A administrator creates an IAM role and attaches a permissions policy.","isCorrect":false},{"text":"The account B administrator delegates the permission to assume the role to any users in account B.","isCorrect":true},{"text":"The account A administrator attaches a trust policy to the role that identifies account B as the AWS service principal who can assume the role.","isCorrect":false}],"explanation":"$55"},{"question":"You are a developer working with the AWS CLI to create Lambda functions that contain environment variables. Your functions will require over 50 environment variables consisting of sensitive information of database table names. What is the total set size/number of environment variables you can create for AWS Lambda?","answers":[{"text":"The total size of all environment variables shouldn't exceed 8 KB. The maximum number of variables that can be created is 50","isCorrect":false},{"text":"The total size of all environment variables shouldn't exceed 4 KB. There is no limit on the number of variables","isCorrect":true},{"text":"The total size of all environment variables shouldn't exceed 4 KB. The maximum number of variables that can be created is 35","isCorrect":false},{"text":"The total size of all environment variables shouldn't exceed 8 KB. There is no limit on the number of variables","isCorrect":false}],"explanation":"$56"},{"question":"Your team lead has requested code review of your code for Lambda functions. Your code is written in Python and makes use of the Amazon Simple Storage Service (S3) to upload logs to an S3 bucket. After the review, your team lead has recommended reuse of execution context to improve the Lambda performance. Which of the following actions will help you implement the recommendation?","answers":[{"text":"Use environment variables to pass operational parameters","isCorrect":false},{"text":"Enable XRay integration","isCorrect":false},{"text":"Assign more RAM to the function","isCorrect":false},{"text":"Move the Amazon S3 client initialization, out of your function handler","isCorrect":true}],"explanation":"$57"},{"question":"The development team at a company is looking at building an AWS CloudFormation template that selfpopulates the AWS Region variable while deploying the CloudFormation template. What is the MOST operationally efficient way to determine the Region in which the template is being deployed?","answers":[{"text":"Use the AWS::Region pseudo parameter","isCorrect":true},{"text":"Set up a mapping containing the key and the named values for all AWS Regions and then have the CloudFormation template autoselect the desired value","isCorrect":false},{"text":"Create an AWS Lambdabacked custom resource for Region and let the desired value be populated at the time of deployment by the Lambda","isCorrect":false},{"text":"Create a CloudFormation parameter for Region and let the desired value be populated at the time of deployment","isCorrect":false}],"explanation":"$58"},{"question":"The development team at a company wants to insert vendor records into an Amazon DynamoDB table as soon as the vendor uploads a new file into an Amazon S3 bucket. As a Developer Associate, which set of steps would you recommend to achieve this?","answers":[{"text":"Set up an event with Amazon CloudWatch Events that will monitor the S3 bucket and then insert the records into DynamoDB","isCorrect":false},{"text":"Develop a Lambda function that will poll the S3 bucket and then insert the records into DynamoDB","isCorrect":false},{"text":"Create an S3 event to invoke a Lambda function that inserts records into DynamoDB","isCorrect":true},{"text":"Write a cron job that will execute a Lambda function at a scheduled time and insert the records into DynamoDB","isCorrect":false}],"explanation":"$59"},{"question":"Recently, you started an online learning platform using AWS Lambda and AWS Gateway API. Your first version was successful, and you began developing new features for the second version. You would like to gradually introduce the second version by routing only 10% of the incoming traffic to the new Lambda version. Which solution should you opt for?","answers":[{"text":"Deploy your Lambda in a VPC","isCorrect":false},{"text":"Use environment variables","isCorrect":false},{"text":"Use AWS Lambda aliases","isCorrect":true},{"text":"Use Tags to distinguish the different versions","isCorrect":false}],"explanation":"$5a"},{"question":"A banking application needs to send realtime alerts and notifications based on any updates from the backend services. The company wants to avoid implementing complex polling mechanisms for these notifications. Which of the following types of APIs supported by the Amazon API Gateway is the right fit?","answers":[{"text":"REST APIs","isCorrect":false},{"text":"HTTP APIs","isCorrect":false},{"text":"WebSocket APIs","isCorrect":true},{"text":"REST or HTTP APIs","isCorrect":false}],"explanation":"$5b"},{"question":"You have a threetier web application consisting of a web layer using AngularJS, an application layer using an AWS API Gateway and a data layer in an Amazon Relational Database Service (RDS) database. Your web application allows visitors to look up popular movies from the past. The company is looking at reducing the number of calls made to endpoint and improve latency to the API. What can you do to improve performance?","answers":[{"text":"Enable API Gateway Caching","isCorrect":true},{"text":"Use Mapping Templates","isCorrect":false},{"text":"Use Stage Variables","isCorrect":false},{"text":"Use Amazon Kinesis Data Streams to stream incoming data and reduce the burden on Gateway APIs","isCorrect":false}],"explanation":"$5c"},{"question":"A developer from your team has configured the load balancer to route traffic equally between instances or across Availability Zones. However, Elastic Load Balancing (ELB) routes more traffic to one instance or Availability Zone than the others. Why is this happening and how can it be fixed? (Select two)","answers":[{"text":"For Application Load Balancers, crosszone load balancing is disabled by default","isCorrect":false},{"text":"There could be shortlived TCP connections between clients and instances","isCorrect":false},{"text":"After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer, thereby receiving random bursts of traffic","isCorrect":false},{"text":"Instances of a specific capacity type aren’t equally distributed across Availability Zones","isCorrect":true},{"text":"Sticky sessions are enabled for the load balancer","isCorrect":false}],"explanation":"$5d"},{"question":"As part of their onboarding, the employees at an IT company need to upload their profile photos in a private S3 bucket. The company wants to build an inhouse web application hosted on an EC2 instance that should display the profile photos in a secure way when the employees mark their attendance. As a Developer Associate, which of the following solutions would you suggest to address this usecase?","answers":[{"text":"Make the S3 bucket public so that the application can reference the image URL for display","isCorrect":false},{"text":"Save the S3 key for each user's profile photo in a DynamoDB table and use a lambda function to dynamically generate a presigned URL. Reference this URL for display via the web application","isCorrect":true},{"text":"Keep each user's profile image encoded in base64 format in a DynamoDB table and reference it from the application for display","isCorrect":false},{"text":"Keep each user's profile image encoded in base64 format in an RDS table and reference it from the application for display","isCorrect":false}],"explanation":"$5e"},{"question":"Two policies are attached to an IAM user. The first policy states that the user has explicitly been denied all access to EC2 instances. The second policy states that the user has been allowed permission for EC2:Describe action. When the user tries to use 'Describe' action on an EC2 instance using the CLI, what will be the output?","answers":[{"text":"The user will get access because it has an explicit allow","isCorrect":false},{"text":"The user will be denied access because one of the policies has an explicit deny on it","isCorrect":true},{"text":"The order of the policy matters. If policy 1 is before 2, then the user is denied access. If policy 2 is before 1, then the user is allowed access","isCorrect":false},{"text":"The IAM user stands in an invalid state, because of conflicting policies","isCorrect":false}],"explanation":"$5f"},{"question":"The development team at a company wants to encrypt a 111 GB object using AWS KMS. Which of the following represents the best solution?","answers":[{"text":"Make a GenerateDataKey API call that returns a plaintext key and an encrypted copy of a data key. Use a plaintext key to encrypt the data","isCorrect":true},{"text":"Make a GenerateDataKeyWithoutPlaintext API call that returns an encrypted copy of a data key. Use an encrypted key to encrypt the data","isCorrect":false},{"text":"Make a GenerateDataKeyWithPlaintext API call that returns an encrypted copy of a data key. Use a plaintext key to encrypt the data","isCorrect":false},{"text":"Make an Encrypt API call to encrypt the plaintext data as ciphertext using a customer master key (CMK) with imported key material","isCorrect":false}],"explanation":"$60"},{"question":"A firm maintains a highly available application that receives HTTPS traffic from mobile devices and web browsers. The main Developer would like to set up the Load Balancer routing to route traffic from web servers to smart.com/api and from mobile devices to smart.com/mobile. A developer advises that the previous recommendation is not needed and that requests should be sent to api.smart.com and mobile.smart.com instead. Which of the following routing options were discussed in the given usecase? (select two)","answers":[{"text":"Path based","isCorrect":true},{"text":"Host based","isCorrect":true},{"text":"Web browser version","isCorrect":false},{"text":"Cookie value","isCorrect":false},{"text":"Client IP","isCorrect":false}],"explanation":"$61"},{"question":"A development team uses shared Amazon S3 buckets to upload files. Due to this shared access, objects in S3 buckets have different owners making it difficult to manage the objects. As a developer associate, which of the following would you suggest to automatically make the S3 bucket owner, also the owner of all objects in the bucket, irrespective of the AWS account used for uploading the objects?","answers":[{"text":"Use S3 CORS to make the S3 bucket owner, the owner of all objects in the bucket","isCorrect":false},{"text":"Use Bucket Access Control Lists (ACLs) to control access on S3 bucket and then define its owner","isCorrect":false},{"text":"Use S3 Access Analyzer to identify the owners of all objects and change the ownership to the bucket owner","isCorrect":false},{"text":"Use S3 Object Ownership to default bucket owner to be the owner of all objects in the bucket","isCorrect":true}],"explanation":"$62"},{"question":"A company uses Amazon Simple Email Service (SES) to costeffectively send susbscription emails to the customers. Intermittently, the SES service throws the error: Throttling – Maximum sending rate exceeded. As a developer associate, which of the following would you recommend to fix this issue?","answers":[{"text":"Configure Timeout mechanism for each request made to the SES service","isCorrect":false},{"text":"Raise a service request with Amazon to increase the throttling limit for the SES API","isCorrect":false},{"text":"Implement retry mechanism for all 4xx errors to avoid throttling error","isCorrect":false},{"text":"Use Exponential Backoff technique to introduce delay in time before attempting to execute the operation again","isCorrect":true}],"explanation":"$63"},{"question":"A serverless application built on AWS processes customer orders 24/7 using an AWS Lambda function and communicates with an external vendor's HTTP API for payment processing. The development team wants to notify the support team in near realtime using an existing Amazon Simple Notification Service (Amazon SNS) topic, but only when the external API error rate exceeds 5% of the total transactions processed in an hour. As an AWS Certified Developer Associate, which option will you suggest as the most efficient solution?","answers":[{"text":"Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Metric Filter to look at the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Metric Filter on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate","isCorrect":false},{"text":"Configure and push highresolution custom metrics to CloudWatch that record the failures of the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rate","isCorrect":true},{"text":"Configure CloudWatch metrics with detailed monitoring for the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rate","isCorrect":false},{"text":"Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Logs Insights to query the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Logs Insights on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate","isCorrect":false}],"explanation":"$64"},{"question":"A company has a workload that requires 14,000 consistent IOPS for data that must be durable and secure. The compliance standards of the company state that the data should be secure at every stage of its lifecycle on all of the EBS volumes they use. Which of the following statements are true regarding data security on EBS?","answers":[{"text":"EBS volumes support inflight encryption but does not support encryption at rest","isCorrect":false},{"text":"EBS volumes don't support any encryption","isCorrect":false},{"text":"EBS volumes support both inflight encryption and encryption at rest using KMS","isCorrect":true},{"text":"EBS volumes do not support inflight encryption but do support encryption at rest using KMS","isCorrect":false}],"explanation":"$65"},{"question":"Your company has a threeyear contract with a healthcare provider. The contract states that monthly database backups must be retained for the duration of the contract for compliance purposes. Currently, the limit on backup retention for automated backups, on Amazon Relational Database Service (RDS), does not meet your requirements. Which of the following solutions can help you meet your requirements?","answers":[{"text":"Enable RDS automatic backups","isCorrect":false},{"text":"Enable RDS MultiAZ","isCorrect":false},{"text":"Enable RDS Read replicas","isCorrect":false},{"text":"Create a cron event in CloudWatch, which triggers an AWS Lambda function that triggers the database snapshot","isCorrect":true}],"explanation":"$66"},{"question":"A developer wants to integrate userspecific file upload and download features in an application that uses both Amazon Cognito user pools and Cognito identity pools for secure access with Amazon S3. The developer also wants to ensure that only authorized users can access their own files and that the files are securely saved and retrieved. The files are 5 KB to 500 MB in size. What do you recommend as the most efficient solution?","answers":[{"text":"Integrate Amazon API Gateway with a Lambda function that validates that the given file is uploaded to S3 and downloaded from S3 only by the authorized user","isCorrect":false},{"text":"Use S3 Event Notifications to trigger a Lambda function that validates that the given file is uploaded and downloaded only by the authorized user","isCorrect":false},{"text":"Leverage an IAM policy with the Amazon Cognito identity prefix to restrict users to use their own folders in Amazon S3","isCorrect":true},{"text":"Use CloudFront Lambda@Edge to validate that the given file is uploaded to S3 and downloaded from S3 only by the authorized user","isCorrect":false}],"explanation":"$67"},{"question":"You create an Auto Scaling group to work with an Application Load Balancer. The scaling group is configured with a minimum size value of 5, a maximum value of 20, and the desired capacity value of 10. One of the 10 EC2 instances has been reported as unhealthy. Which of the following actions will take place?","answers":[{"text":"The ASG will format the root EBS drive on the EC2 instance and run the User Data again","isCorrect":false},{"text":"The ASG will keep the instance running and restart the application","isCorrect":false},{"text":"The ASG will terminate the EC2 Instance","isCorrect":true},{"text":"The ASG will detach the EC2 instance from the group, and leave it running","isCorrect":false}],"explanation":"$68"},{"question":"You have uploaded a zip file to AWS Lambda that contains code files written in Node.Js. When your function is executed you receive the following output, 'Error: Memory Size: 10,240 MB Max Memory Used'. Which of the following explains the problem?","answers":[{"text":"Your zip file is corrupt","isCorrect":false},{"text":"The uncompressed zip file exceeds AWS Lambda limits","isCorrect":false},{"text":"You have uploaded a zip file larger than 50 MB to AWS Lambda","isCorrect":false},{"text":"Your Lambda function ran out of RAM","isCorrect":true}],"explanation":"Your Lambda function ran out of RAMAWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.How Lambda function works: via https://aws.amazon.com/lambda/ The maximum amount of memory available to the Lambda function at runtime is 10,240 MB. Your Lambda function was deployed with 10,240 MB of RAM, but it seems your code requested or used more than that, so the Lambda function failed.via https://docs.aws.amazon.com/lambda/latest/dg/configurationconsole.html Incorrect options:Your zip file is corrupt A memory size error states that Lambda was able to extract so the file is not corruptThe uncompressed zip file exceeds AWS Lambda limits This is not correct as your function was able to execute.You have uploaded a zip file larger than 50 MB to AWS Lambda This is not correct as your lambda function was able to executeReference:https://docs.aws.amazon.com/lambda/latest/dg/configurationconsole.htmll"},{"question":"A company wants to automate and orchestrate a multisource highvolume flow of data in a scalable data management solution built using AWS services. The solution must ensure that the business rules and transformations run in sequence, handle reprocessing of data in case of errors, and require minimal maintenance. Which AWS service should the company use to manage and automate the orchestration of the data flows?","answers":[{"text":"AWS Step Functions","isCorrect":true},{"text":"AWS Batch","isCorrect":false},{"text":"AWS Glue","isCorrect":false},{"text":"Amazon Kinesis Data Streams","isCorrect":false}],"explanation":"$69"},{"question":"As a Senior Developer, you manage 10 Amazon EC2 instances that make readheavy database requests to the Amazon RDS for PostgreSQL. You need to make this architecture resilient for disaster recovery. Which of the following features will help you prepare for database disaster recovery? (Select two)","answers":[{"text":"Use database cloning feature of the RDS DB cluster","isCorrect":false},{"text":"Enable the automated backup feature of Amazon RDS in a multiAZ deployment that creates backups in a single AWS Region","isCorrect":true},{"text":"Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage","isCorrect":false},{"text":"Enable the automated backup feature of Amazon RDS in a multiAZ deployment that creates backups across multiple Regions","isCorrect":false},{"text":"Use crossRegion Read Replicas","isCorrect":true}],"explanation":"$6a"},{"question":"A telecom service provider stores its critical customer data on Amazon Simple Storage Service (Amazon S3). Which of the following options can be used to control access to data stored on Amazon S3? (Select two)","answers":[{"text":"Bucket policies, Identity and Access Management (IAM) policies","isCorrect":true},{"text":"Permissions boundaries, Identity and Access Management (IAM) policies","isCorrect":false},{"text":"Query String Authentication, Permissions boundaries","isCorrect":false},{"text":"IAM database authentication, Bucket policies","isCorrect":false},{"text":"Query String Authentication, Access Control Lists (ACLs)","isCorrect":true}],"explanation":"$6b"},{"question":"A Developer is configuring Amazon EC2 Auto Scaling group to scale dynamically. Which metric below is NOT part of Target Tracking Scaling Policy?","answers":[{"text":"ALBRequestCountPerTarget","isCorrect":false},{"text":"ASGAverageCPUUtilization","isCorrect":false},{"text":"ApproximateNumberOfMessagesVisible","isCorrect":true},{"text":"ASGAverageNetworkOut","isCorrect":false}],"explanation":"$6c"},{"question":"You have launched several AWS Lambda functions written in Java. A new requirement was given that over 1MB of data should be passed to the functions and should be encrypted and decrypted at runtime. Which of the following methods is suitable to address the given usecase?","answers":[{"text":"Use Envelope Encryption and store as environment variable","isCorrect":false},{"text":"Use KMS direct encryption and store as file","isCorrect":false},{"text":"Use KMS Encryption and store as environment variable","isCorrect":false},{"text":"Use Envelope Encryption and reference the data as file within the code","isCorrect":true}],"explanation":"$6d"},{"question":"DevOps engineers are developing an order processing system where notifications are sent to a department whenever an order is placed for a product. The system also pushes identical notifications of the new order to a processing module that would allow EC2 instances to handle the fulfillment of the order. In the case of processing errors, the messages should be allowed to be reprocessed at a later stage. The order processing system should be able to scale transparently without the need for any manual or programmatic provisioning of resources. Which of the following solutions can be used to address this usecase in the most costefficient way?","answers":[{"text":"SNS + Lambda","isCorrect":false},{"text":"SNS + Kinesis","isCorrect":false},{"text":"SQS + SES","isCorrect":false},{"text":"SNS + SQS","isCorrect":true}],"explanation":"$6e"},{"question":"A pharmaceutical company uses Amazon EC2 instances for application hosting and Amazon CloudFront for content delivery. A new research paper with critical findings has to be shared with a research team that is spread across the world. Which of the following represents the most optimal solution to address this requirement without compromising the security of the content?","answers":[{"text":"Using CloudFront's FieldLevel Encryption to help protect sensitive data","isCorrect":false},{"text":"Use CloudFront signed URL feature to control access to the file","isCorrect":true},{"text":"Use CloudFront signed cookies feature to control access to the file","isCorrect":false},{"text":"Configure AWS Web Application Firewall (WAF) to monitor and control the HTTP and HTTPS requests that are forwarded to CloudFront","isCorrect":false}],"explanation":"$6f"},{"question":"A developer is configuring an Application Load Balancer (ALB) to direct traffic to the application's EC2 instances and Lambda functions. Which of the following characteristics of the ALB can be identified as correct? (Select two)","answers":[{"text":"An ALB has three possible target types: Instance, IP and Lambda","isCorrect":true},{"text":"An ALB has three possible target types: Hostname, IP and Lambda","isCorrect":false},{"text":"If you specify targets using an instance ID, traffic is routed to instances using any private IP address from one or more network interfaces","isCorrect":false},{"text":"You can not specify publicly routable IP addresses to an ALB","isCorrect":true},{"text":"If you specify targets using IP addresses, traffic is routed to instances using the primary private IP address","isCorrect":false}],"explanation":"$70"},{"question":"You have an Auto Scaling group configured to a minimum capacity of 1 and a maximum capacity of 5, designed to launch EC2 instances across 3 Availability Zones. During a low utilization period, an entire Availability Zone went down and your application experienced downtime. What can you do to ensure that your application remains highly available?","answers":[{"text":"Configure ASG fast failover","isCorrect":false},{"text":"Change the scaling metric of autoscaling policy to network bytes","isCorrect":false},{"text":"Increase the minimum instance capacity of the Auto Scaling Group to 2","isCorrect":true},{"text":"Enable RDS MultiAZ","isCorrect":false}],"explanation":"$71"},{"question":"A telecommunications company that provides internet service for mobile device users maintains over 100 c4.large instances in the useast1 region. The EC2 instances run complex algorithms. The manager would like to track CPU utilization of the EC2 instances as frequently as every 10 seconds. Which of the following represents the BEST solution for the given usecase?","answers":[{"text":"Enable EC2 detailed monitoring","isCorrect":false},{"text":"Simply get it from the CloudWatch Metrics","isCorrect":false},{"text":"Create a highresolution custom metric and push the data using a script triggered every 10 seconds","isCorrect":true},{"text":"Open a support ticket with AWS","isCorrect":false}],"explanation":"$72"},{"question":"An organization is moving its onpremises resources to the cloud. Source code will be moved to AWS CodeCommit and AWS CodeBuild will be used for compiling the source code using Apache Maven as a build tool. The organization wants the build environment should allow for scaling and running builds in parallel. Which of the following options should the organization choose for their requirement?","answers":[{"text":"Choose a highperformance instance type for your CodeBuild instances","isCorrect":false},{"text":"Enable CodeBuild Auto Scaling","isCorrect":false},{"text":"CodeBuild scales automatically, the organization does not have to do anything for scaling or for parallel builds","isCorrect":true},{"text":"Run CodeBuild in an Auto Scaling group","isCorrect":false}],"explanation":"$73"},{"question":"An application runs on an EC2 instance and processes orders on a nightly basis. This EC2 instance needs to access the orders that are stored in S3. How would you recommend the EC2 instance access the orders securely?","answers":[{"text":"Use an IAM role","isCorrect":true},{"text":"Use EC2 User Data","isCorrect":false},{"text":"Create an IAM programmatic user and store the access key and secret access key on the EC2 ~/.aws/credentials file.","isCorrect":false},{"text":"Create an S3 bucket policy that authorises public access","isCorrect":false}],"explanation":"$74"},{"question":"You have migrated an onpremise SQL Server database to an Amazon Relational Database Service (RDS) database attached to a VPC inside a private subnet. Also, the related Java application, hosted onpremise, has been moved to an Amazon Lambda function. Which of the following should you implement to connect AWS Lambda function to its RDS instance?","answers":[{"text":"Configure lambda to connect to the public subnet that will give internet access and use Security Group to access RDS inside the private subnet","isCorrect":false},{"text":"Use Environment variables to pass in the RDS connection string","isCorrect":false},{"text":"Use Lambda layers to connect to the internet and RDS separately","isCorrect":false},{"text":"Configure Lambda to connect to VPC with private subnet and Security Group needed to access RDS","isCorrect":true}],"explanation":"$75"},{"question":"As an AWS Certified Developer Associate, you have been hired to work with the development team at a company to create a REST API using the serverless architecture. Which of the following solutions will you choose to move the company to the serverless architecture paradigm?","answers":[{"text":"API Gateway exposing Lambda Functionality","isCorrect":true},{"text":"Fargate with Lambda at the front","isCorrect":false},{"text":"Route 53 with EC2 as backend","isCorrect":false},{"text":"Publicfacing Application Load Balancer with ECS on Amazon EC2","isCorrect":false}],"explanation":"$76"},{"question":"After a code review, a developer has been asked to make his publicly accessible S3 buckets private, and enable access to objects with a timebound constraint. Which of the following options will address the given usecase?","answers":[{"text":"It is not possible to implement time constraints on Amazon S3 Bucket access","isCorrect":false},{"text":"Use Bucket policy to block the unintended access","isCorrect":false},{"text":"Use Routing policies to reroute unintended access","isCorrect":false},{"text":"Share presigned URLs with resources that need access","isCorrect":true}],"explanation":"$77"},{"question":"An IT company has migrated to a serverless application stack on the AWS Cloud with the compute layer being implemented via Lambda functions. The engineering managers would like to actively troubleshoot any failures in the Lambda functions. As a Developer Associate, which of the following solutions would you suggest for this usecase?","answers":[{"text":"Use CodeCommit to identify and notify any failures in the Lambda code","isCorrect":false},{"text":"The developers should insert logging statements in the Lambda function code which are then available via CloudWatch logs","isCorrect":true},{"text":"Use CodeDeploy to identify and notify any failures in the Lambda code","isCorrect":false},{"text":"Use CloudWatch Events to identify and notify any failures in the Lambda code","isCorrect":false}],"explanation":"$78"},{"question":"A developer is looking at establishing access control for an API that connects to a Lambda function downstream. Which of the following represents a mechanism that CANNOT be used for authenticating with the API Gateway?","answers":[{"text":"Standard AWS IAM roles and policies","isCorrect":false},{"text":"Cognito User Pools","isCorrect":false},{"text":"AWS Security Token Service (STS)","isCorrect":true},{"text":"Lambda Authorizer","isCorrect":false}],"explanation":"$79"},{"question":"A development team is considering Amazon ElastiCache for Redis as its inmemory caching solution for its relational database. Which of the following options are correct while configuring ElastiCache? (Select two)","answers":[{"text":"You can scale write capacity for Redis by adding replica nodes","isCorrect":false},{"text":"All the nodes in a Redis cluster must reside in the same region","isCorrect":true},{"text":"If you have no replicas and a node fails, you experience no loss of data when using Redis with cluster mode enabled","isCorrect":false},{"text":"While using Redis with cluster mode enabled, you cannot manually promote any of the replica nodes to primary","isCorrect":true},{"text":"While using Redis with cluster mode enabled, asynchronous replication mechanisms are used to keep the read replicas synchronized with the primary. If cluster mode is disabled, the replication mechanism is done synchronously","isCorrect":false}],"explanation":"$7a"},{"question":"You are a system administrator whose company recently moved its production application to AWS and migrated data from MySQL to AWS DynamoDB. You are adding new tables to AWS DynamoDB and need to allow your application to query your data by the primary key and an alternate key. This option must be added when first creating tables otherwise changes cannot be made afterward. Which of the following actions should you take?","answers":[{"text":"Create a GSI","isCorrect":false},{"text":"Migrate away from DynamoDB","isCorrect":false},{"text":"Create a LSI","isCorrect":true},{"text":"Call Scan","isCorrect":false}],"explanation":"$7b"},{"question":"You are running a cloud file storage website with an Internetfacing Application Load Balancer, which routes requests from users over the internet to 10 registered Amazon EC2 instances. Users are complaining that your website always asks them to reauthenticate when they switch pages. You are puzzled because this behavior is not seen in your local machine or dev environment. What could be the reason?","answers":[{"text":"The Load Balancer does not have stickiness enabled","isCorrect":true},{"text":"The Load Balancer does not have TLS enabled","isCorrect":false},{"text":"The EC2 instances are logging out the users because the instances never have access to the client IPs because of the Load Balancer","isCorrect":false},{"text":"Application Load Balancer is in slowstart mode, which gives ALB a little more time to read and write session data","isCorrect":false}],"explanation":"$7c"},{"question":"A company wants to share information with a third party via an HTTP API endpoint managed by the third party. The company has the necessary API key to access the endpoint and the integration of the API key with the company's application code must not impact the application's performance. What is the most secure approach?","answers":[{"text":"Keep the API credentials in an encrypted file in S3 and use the credentials to make the API call by fetching the API credentials from S3 at runtime by using the AWS SDK","isCorrect":false},{"text":"Keep the API credentials in a local code variable and use the local code variable at runtime to make the API call","isCorrect":false},{"text":"Keep the API credentials in an encrypted table in MySQL RDS and use the credentials to make the API call by fetching the API credentials from RDS at runtime by using the AWS SDK","isCorrect":false},{"text":"Keep the API credentials in AWS Secrets Manager and use the credentials to make the API call by fetching the API credentials at runtime by using the AWS SDK","isCorrect":true}],"explanation":"$7d"},{"question":"Your company hosts a static website on Amazon Simple Storage Service (S3) written in HTML5. The website targets aviation enthusiasts and it has grown a worldwide audience with hundreds of thousands of visitors accessing the website now on a monthly basis. While users in the United States have a great user experience, users from other parts of the world are experiencing slow responses and lag. Which service can mitigate this issue?","answers":[{"text":"Use Amazon S3 Caching","isCorrect":false},{"text":"Use Amazon S3 Transfer Acceleration","isCorrect":false},{"text":"Use Amazon ElastiCache for Redis","isCorrect":false},{"text":"Use Amazon CloudFront","isCorrect":true}],"explanation":"$7e"},{"question":"You have an Amazon Kinesis Data Stream with 10 shards, and from the metrics, you are well below the throughput utilization of 10 MB per second to send data. You send 3 MB per second of data and yet you are receiving ProvisionedThroughputExceededException errors frequently. What is the likely cause of this?","answers":[{"text":"The partition key that you have selected isn't distributed enough","isCorrect":true},{"text":"The data retention period is too long","isCorrect":false},{"text":"Metrics are slow to update","isCorrect":false},{"text":"You have too many shards","isCorrect":false}],"explanation":"$7f"},{"question":"Your company is planning to move away from reserving EC2 instances and would like to adopt a more agile form of serverless architecture. Which of the following is the simplest and the least effort way of deploying the Docker containers on this serverless architecture?","answers":[{"text":"Amazon Elastic Kubernetes Service (Amazon EKS) on Fargate","isCorrect":false},{"text":"Amazon Elastic Container Service (Amazon ECS) on Fargate","isCorrect":true},{"text":"Amazon Elastic Container Service (Amazon ECS) on EC2","isCorrect":false},{"text":"AWS Elastic Beanstalk","isCorrect":false}],"explanation":"$80"},{"question":"You are getting ready for an event to show off your Alexa skill written in JavaScript. As you are testing your voice activation commands you find that some intents are not invoking as they should and you are struggling to figure out what is happening. You included the following code console.log(JSON.stringify(this.event)) in hopes of getting more details about the request to your Alexa skill. You would like the logs stored in an Amazon Simple Storage Service (S3) bucket named MyAlexaLog. How do you achieve this?","answers":[{"text":"Use CloudWatch integration feature with Kinesis","isCorrect":false},{"text":"Use CloudWatch integration feature with S3","isCorrect":true},{"text":"Use CloudWatch integration feature with Glue","isCorrect":false},{"text":"Use CloudWatch integration feature with Lambda","isCorrect":false}],"explanation":"$81"},{"question":"Your company is in the process of building a DevOps culture and is moving all of its onpremise resources to the cloud using serverless architectures and automated deployments. You have created a CloudFormation template in YAML that uses an AWS Lambda function to pull HTML files from GitHub and place them into an Amazon Simple Storage Service (S3) bucket that you specify. Which of the following AWS CLI commands can you use to upload AWS Lambda functions and AWS CloudFormation templates to AWS?","answers":[{"text":"cloudformation zip and cloudformation upload","isCorrect":false},{"text":"cloudformation zip and cloudformation deploy","isCorrect":false},{"text":"cloudformation package and cloudformation upload","isCorrect":false},{"text":"cloudformation package and cloudformation deploy","isCorrect":true}],"explanation":"$82"},{"question":"Your development team uses the AWS SDK for Java on a web application that uploads files to several Amazon Simple Storage Service (S3) buckets using the SSEKMS encryption mechanism. Developers are reporting that they are receiving permission errors when trying to push their objects over HTTP. Which of the following headers should they include in their request?","answers":[{"text":"xamzserversideencryption': 'aws:kms'","isCorrect":true},{"text":"xamzserversideencryption': 'SSES3'","isCorrect":false},{"text":"xamzserversideencryption': 'AES256'","isCorrect":false},{"text":"xamzserversideencryption': 'SSEKMS'","isCorrect":false}],"explanation":"$83"},{"question":"You company runs business logic on smaller software components that perform various functions. Some functions process information in a few seconds while others seem to take a long time to complete. Your manager asked you to decouple components that take a long time to ensure software applications stay responsive under load. You decide to configure Amazon Simple Queue Service (SQS) to work with your Elastic Beanstalk configuration. Which of the following Elastic Beanstalk environment should you choose to meet this requirement?","answers":[{"text":"Single Instance with Elastic IP","isCorrect":false},{"text":"Single Instance Worker node","isCorrect":false},{"text":"Loadbalancing, Autoscaling environment","isCorrect":false},{"text":"Dedicated worker environment","isCorrect":true}],"explanation":"$84"},{"question":"As part of employee skills upgrade, the developers of your team have been delegated few responsibilities of DevOps engineers. Developers now have full control over modeling the entire software delivery process, from coding to deployment. As the team lead, you are now responsible for any manual approvals needed in the process. Which of the following approaches supports the given workflow?","answers":[{"text":"Create one CodePipeline for your entire flow and add a manual approval step","isCorrect":true},{"text":"Create multiple CodePipelines for each environment and link them using AWS Lambda","isCorrect":false},{"text":"Create deeply integrated AWS CodePipelines for each environment","isCorrect":false},{"text":"Use CodePipeline with Amazon Virtual Private Cloud","isCorrect":false}],"explanation":"$85"},{"question":"The app development team at a social gaming mobile app wants to simplify the user sign up process for the app. The team is looking for a fully managed scalable solution for user management in anticipation of the rapid growth that the app foresees. As a Developer Associate, which of the following solutions would you suggest so that it requires the LEAST amount of development effort?","answers":[{"text":"Create a custom solution using Lambda and DynamoDB to facilitate sign up and user management for the mobile app","isCorrect":false},{"text":"Create a custom solution using EC2 and DynamoDB to facilitate sign up and user management for the mobile app","isCorrect":false},{"text":"Use Cognito User pools to facilitate sign up and user management for the mobile app","isCorrect":true},{"text":"Use Cognito Identity pools to facilitate sign up and user management for the mobile app","isCorrect":false}],"explanation":"$86"},{"question":"A CRM application is hosted on Amazon EC2 instances with the database tier using DynamoDB. The customers have raised privacy and security concerns regarding sending and receiving data across the public internet. As a developer associate, which of the following would you suggest as an optimal solution for providing communication between EC2 instances and DynamoDB without using the public internet?","answers":[{"text":"Create an Internet Gateway to provide the necessary communication channel between EC2 instances and DynamoDB","isCorrect":false},{"text":"Configure VPC endpoints for DynamoDB that will provide required internal access without using public internet","isCorrect":true},{"text":"The firm can use a virtual private network (VPN) to route all DynamoDB network traffic through their own corporate network infrastructure","isCorrect":false},{"text":"Create a NAT Gateway to provide the necessary communication channel between EC2 instances and DynamoDB","isCorrect":false}],"explanation":"$87"},{"question":"A developer wants to securely store and retrieve various types of variables, such as remote API authentication information, API URL, and related credentials across different environments of an application deployed on Amazon Elastic Container Service (Amazon ECS). What would be the best approach that needs minimal modifications in the application code?","answers":[{"text":"Configure the application to fetch the variables from each of the deployed environments by defining the authentication information and API URL in the ECS task definition as unique names during the deployment process","isCorrect":false},{"text":"Configure the application to fetch the variables from AWS KMS by storing the API URL and credentials as unique keys in KMS for each environment","isCorrect":false},{"text":"Configure the application to fetch the variables from an encrypted file that is stored with the application by storing the API URL and credentials in unique files for each environment","isCorrect":false},{"text":"Configure the application to fetch the variables and credentials from AWS Systems Manager Parameter Store by leveraging hierarchical unique paths in Parameter Store for each variable in each environment","isCorrect":true}],"explanation":"$88"},{"question":"A company has created an Amazon S3 bucket that holds customer data. The team lead has just enabled access logging to this bucket. The bucket size has grown substantially after starting access logging. Since no new files have been added to the bucket, the perplexed team lead is looking for an answer. Which of the following reasons explains this behavior?","answers":[{"text":"A DDoS attack on your S3 bucket can potentially blow up the size of data in the bucket if the bucket security is compromised during the attack","isCorrect":false},{"text":"Erroneous Bucket policies for batch uploads can sometimes be responsible for the exponential growth of S3 Bucket size","isCorrect":false},{"text":"S3 access logging is pointing to the same bucket and is responsible for the substantial growth of bucket size","isCorrect":true},{"text":"Object Encryption has been enabled and each object is stored twice as part of this configuration","isCorrect":false}],"explanation":"$89"},{"question":"Your company has been hired to build a resilient mobile voting app for an upcoming music award show that expects to have 5 to 20 million viewers. The mobile voting app will be marketed heavily months in advance so you are expected to handle millions of messages in the system. You are configuring Amazon Simple Queue Service (SQS) queues for your architecture that should receive messages from 20 KB to 200 KB. Is it possible to send these messages to SQS?","answers":[{"text":"No, the max message size is 128KB","isCorrect":false},{"text":"Yes, the max message size is 256KB","isCorrect":true},{"text":"Yes, the max message size is 512KB","isCorrect":false},{"text":"No, the max message size is 64KB","isCorrect":false}],"explanation":"Yes, the max message size is 256KBThe minimum message size is 1 byte (1 character). The maximum is 262,144 bytes (256 KB).via https://aws.amazon.com/sqs/faqs/ Incorrect options:Yes, the max message size is 512KB The max size is 256KBNo, the max message size is 128KB The max size is 256KBNo, the max message size is 64KB The max size is 256KBReference: https://aws.amazon.com/sqs/faqs/"},{"question":"Recently in your organization, the AWS XRay SDK was bundled into each Lambda function to record outgoing calls for tracing purposes. When your team leader goes to the XRay service in the AWS Management Console to get an overview of the information collected, they discover that no data is available. What is the most likely reason for this issue?","answers":[{"text":"Enable XRay sampling","isCorrect":false},{"text":"Change the security group rules","isCorrect":false},{"text":"XRay only works with AWS Lambda aliases","isCorrect":false},{"text":"Fix the IAM Role","isCorrect":true}],"explanation":"$8a"}]}}]
a:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
7:null
8:null
c:[["$","title","0",{"children":"v0 App"}],["$","meta","1",{"name":"description","content":"Created with v0"}],["$","meta","2",{"name":"generator","content":"v0.app"}]]
