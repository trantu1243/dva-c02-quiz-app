<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/6ad9841b43ad2bc9.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/4d55be17aa0cdcd5.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-deb4a51aecb498f7.js"/><script src="/_next/static/chunks/4bd1b696-05e7186cde350658.js" async=""></script><script src="/_next/static/chunks/684-0bbb5245fe3538c5.js" async=""></script><script src="/_next/static/chunks/main-app-01726a8d9af88025.js" async=""></script><script src="/_next/static/chunks/app/layout-220de9febe1f86f4.js" async=""></script><script src="/_next/static/chunks/261-2d9b76ccba401937.js" async=""></script><script src="/_next/static/chunks/app/quiz/%5Bid%5D/page-d7cb26530e4508b3.js" async=""></script><title>v0 App</title><meta name="description" content="Created with v0"/><meta name="generator" content="v0.app"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans __variable_fb8f2c __variable_f910ec"><div class="min-h-screen bg-gradient-to-br from-background via-background to-primary/5 flex items-center justify-center"><div class="text-center"><div class="w-16 h-16 border-4 border-primary border-t-transparent rounded-full animate-spin mx-auto mb-4"></div><p class="text-muted-foreground">Loading quiz...</p></div></div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><script src="/_next/static/chunks/webpack-deb4a51aecb498f7.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7555,[],\"\"]\n3:I[1295,[],\"\"]\n4:I[9742,[\"177\",\"static/chunks/app/layout-220de9febe1f86f4.js\"],\"Analytics\"]\n6:I[9665,[],\"OutletBoundary\"]\n9:I[9665,[],\"ViewportBoundary\"]\nb:I[9665,[],\"MetadataBoundary\"]\nd:I[6614,[],\"\"]\n:HL[\"/_next/static/css/6ad9841b43ad2bc9.css\",\"style\"]\n:HL[\"/_next/static/css/4d55be17aa0cdcd5.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"hVObTwCHKjnTJqIoS5_8Q\",\"p\":\"\",\"c\":[\"\",\"quiz\",\"aws-developer-10\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"quiz\",{\"children\":[[\"id\",\"aws-developer-10\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/6ad9841b43ad2bc9.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/4d55be17aa0cdcd5.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"font-sans __variable_fb8f2c __variable_f910ec\",\"children\":[[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"$L4\",null,{}]]}]}]]}],{\"children\":[\"quiz\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"id\",\"aws-developer-10\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",\"$undefined\",null,[\"$\",\"$L6\",null,{\"children\":[\"$L7\",\"$L8\",null]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"N5FzPi46zrk2pF-UrtDrx\",{\"children\":[[\"$\",\"$L9\",null,{\"children\":\"$La\"}],null]}],[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$d\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"e:I[1184,[\"261\",\"static/chunks/261-2d9b76ccba401937.js\",\"200\",\"static/chunks/app/quiz/%5Bid%5D/page-d7cb26530e4508b3.js\"],\"default\"]\nf:Tb81,"])</script><script>self.__next_f.push([1,"For applications that need to read or write multiple items, DynamoDB provides the BatchGetItem and BatchWriteItem operations. Using these operations can reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the individual read or write operations in parallel. Your applications benefit from this parallelism without having to manage concurrency or threading. The batch operations are essentially wrappers around multiple read or write requests. For example, if a BatchGetItem request contains five items, DynamoDB performs five GetItem operations on your behalf. Similarly, if a BatchWriteItem request contains two put requests and four delete requests, DynamoDB performs two PutItem and four DeleteItem requests. In general, a batch operation does not fail unless all of the requests in the batch fail. For example, suppose you perform a BatchGetItemoperation but one of the individual GetItem requests in the batch fails. In this case, BatchGetItem returns the keys and data from the GetItemrequest that failed. The other GetItem requests in the batch are not affected. Hence, the correct answer is to use DynamoDB Batch Operations API for GET, PUT, and DELETE operations in this scenario. Upgrading the EC2 instances to a higher instance type is incorrect because the network overhead is the one that affects application performance and not the compute capacity. This is due to multiple read and write requests performed as single operations on DynamoDB, instead of a Batch operation. Enabling DynamoDB Streams is incorrect because a DynamoDB stream is just an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Apparently, this feature does not solve the application issue where there is a large volume of data being processed one by one, and not by batch. Refactoring the application to use DynamoDB transactional read and write APIs is incorrect because the Amazon DynamoDB transactions feature just simplifies the developer experience of making coordinated, all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, enabling you to maintain data correctness in your applications easily. Take note that every transactional read and write API call consumes high RCU and WCUs, unlike eventual or strong consistency requests. Hence, this entails a significant increase in costs which contradicts the requirements of the scenario. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"10:T944,"])</script><script>self.__next_f.push([1,"Lambda counts a request each time it starts executing in response to an event notification or invocation call, including test invokes from the console. You are charged based on the total number of requests processed across all of your Lambda functions. Duration is calculated from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 1ms. The price depends on the amount of memory you allocate to your function. In the AWS Lambda resource model, you choose the amount of memory you want for your function and are allocated proportional CPU power and other resources. An increase in memory size triggers an equivalent increase in CPU available to your function. Hence, the correct answer is to: Increase the memory configuration of your function. The option that says: Configuring the concurrency limit of your function to be the same as the account level concurrency limit is incorrect because this configuration is primarily used for managing the number of simultaneous executions of your function as well as the capacity reservations for that concurrency level. This will just limit the number of simultaneous executions of your function and not increase the allocated CPU. In addition, AWS Lambda will keep the unreserved concurrency pool at a minimum of 100 concurrent executions, so that functions that do not have specific limits set can still process requests. This means that you cannot configure the limit to be exactly the same as the account level limit. The option that says: Manually configuring the CPU settings of the Lambda function to the maximum value is incorrect because, in the first place, you cannot manually configure the CPU settings of your function. You have to increase the memory configuration of your function instead. The option that says: Use Lambda layers to optimize the performance of the Lambda function is incorrect. Lambda Layers is just an archive for any external dependencies or custom runtimes that you want to share between Lambda functions. You can speed up the deployment of your codes by moving external dependencies to a layer, however, this has a negligible impact on performance. References: https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html https://aws.amazon.com/lambda/pricing/ Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"])</script><script>self.__next_f.push([1,"11:T899,"])</script><script>self.__next_f.push([1,"AWS CloudHSM provides hardware security modules in AWS Cloud. A hardware security module (HSM) is a computing device that processes cryptographic operations and provides secure storage for cryptographic keys. When you use an HSM from AWS CloudHSM, you can perform a variety of cryptographic tasks: - Generate, store, import, export, and manage cryptographic keys, including symmetric keys and asymmetric key pairs. - Use symmetric and asymmetric algorithms to encrypt and decrypt data. - Use cryptographic hash functions to compute message digests and hash-based message authentication codes (HMACs). - Cryptographically sign data (including code signing) and verify signatures. - Generate cryptographically secure random data. You should consider using AWS CloudHSM instead of AWS KMS if you require: - Keys stored in dedicated, third-party validated hardware security modules under your exclusive control. - FIPS 140-2 compliance. - Integration with applications using PKCS#11, Java JCE, or Microsoft CNG interfaces. - High-performance in-VPC cryptographic acceleration (bulk crypto). Hence, the correct answer is to import the encryption keys from your on-premises key management service to AWS CloudHSM. Using AWS KMS to store and manage the encryption keys is incorrect. Although AWS KMS supports asymmetric encryption, it doesn't provide dedicated, third-party validated hardware security modules which are under your exclusive control. You have to use CloudHSM instead. Importing the encryption keys from your on-premises key management service to AWS Secrets Manager as KMS Keys is incorrect because you can't store KMS keys to AWS Secrets Manager. Developing a custom key management service using the AWS Encryption SDK is incorrect because this entails a lot of effort to implement. Moreover, the AWS Encryption SDK only encrypts your data using a symmetric key algorithm which doesn't comply with the requirements provided in the scenario. References: https://docs.aws.amazon.com/cloudhsm/latest/userguide/manage-keys.html https://aws.amazon.com/cloudhsm/faqs/ Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"])</script><script>self.__next_f.push([1,"12:Tab3,"])</script><script>self.__next_f.push([1,"To enable HTTPS connections to your website or application in AWS, you need an SSL/TLS server certificate. For certificates in a Region supported by AWS Certificate Manager (ACM), it is recommended that you use ACM to provision, manage, and deploy your server certificates. In unsupported Regions, you must use IAM as a certificate manager. ACM is the preferred tool to provision, manage, and deploy your server certificates. With ACM you can request a certificate or deploy an existing ACM or external certificate to AWS resources. Certificates provided by ACM are free and automatically renew. In a supported Region, you can use ACM to manage server certificates from the console or programmatically Use IAM as a certificate manager only when you must support HTTPS connections in a Region that is not supported by ACM. IAM securely encrypts your private keys and stores the encrypted version in IAM SSL certificate storage. IAM supports deploying server certificates in all Regions, but you must obtain your certificate from an external provider for use with AWS. You cannot upload an ACM certificate to IAM. Additionally, you cannot manage your certificates from the IAM Console. If you got your certificate from a third-party CA, import the certificate into ACM or upload it to the IAM certificate store. Hence, the correct answers are AWS Certificate Manager (ACM) and IAM certificate store. A private S3 bucket with versioning enabled is incorrect as S3 is not a suitable service to store the SSL certificate. You have to import it to either AWS Certificate Manager (ACM) or IAM certificate store. Amazon Cognito is incorrect because this is just a user identity and data synchronization service that helps you securely manage and synchronize app data for your users across their mobile devices. This service can't be used to store or import your SSL certificates. CloudFront is incorrect. Although you can upload certificates to CloudFront, it doesn't mean that you can import third-party SSL certificates on it. If you got your certificate from a third-party CA then you have to import the certificate into ACM or upload it to the IAM certificate store first. You would also not be able to export the certificate that you have loaded in CloudFront nor assign them to your EC2 or ELB instances as it would be tied to a single CloudFront distribution. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html https://aws.amazon.com/certificate-manager/ https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-procedures.html#cnames-and-https-uploading-certificates Check out this AWS Certificate Manager Cheat Sheet: https://tutorialsdojo.com/aws-certificate-manager/"])</script><script>self.__next_f.push([1,"13:Td7a,"])</script><script>self.__next_f.push([1,"A gateway response is identified by a response type defined by API Gateway. The response consists of an HTTP status code, a set of additional headers that are specified by parameter mappings, and a payload that is generated by a non-VTL (Apache Velocity Template Language) mapping template. You can set up a gateway response for a supported response type at the API level. Whenever API Gateway returns a response of the type, the header mappings and payload mapping templates defined in the gateway response are applied to return the mapped results to the API caller. The following are the Gateway response types which are associated with the HTTP 504 error in API Gateway: INTEGRATION_FAILURE - The gateway response for an integration failed error. If the response type is unspecified, this response defaults to the DEFAULT_5XX type. INTEGRATION_TIMEOUT - The gateway response for an integration timed-out error. If the response type is unspecified, this response defaults to the DEFAULT_5XX type. For the integration timeout, the range is from 50 milliseconds to 29 seconds for all integration types, including Lambda, Lambda proxy, HTTP, HTTP proxy, and AWS integrations. In this scenario, there is an issue where the users are getting HTTP 504 errors in the serverless application. This means the Lambda function is working fine at times, but there are instances when it throws an error. Based on this analysis, the most likely cause of the issue is the INTEGRATION_TIMEOUT error since you will only get an INTEGRATION_FAILURE error if your AWS Lambda integration does not work at all in the first place. Hence, the correct answer is: The underlying Lambda function has been running for more than 29 seconds causing the API Gateway request to time out. The option that says: The memory allocated for the Lambda function is insufficient is incorrect. The fact that no errors were found in the CloudWatch Logs suggests that the function is not the bottleneck. The option that says: The API Gateway automatically enabled throttling in peak times which caused the HTTP 504 errors is incorrect because a large number of incoming requests will most likely produce an HTTP 502 or 429 error but not a 504 error. If executing the function would cause you to exceed a concurrency limit at either the account level (ConcurrentInvocationLimitExceeded) or function level (ReservedFunctionConcurrentInvocationLimitExceeded), Lambda may return a TooManyRequestsException as a response. For functions with a long timeout, your client might be disconnected during synchronous invocation while it waits for a response and returns an HTTP 504 error. The option that says: There is an authorization failure occurring between API Gateway and the Lambda function is incorrect because an authentication issue usually produces HTTP 403 errors and not 504s. The gateway response for authorization failures for missing authentication token errors, invalid AWS signature errors, or Amazon Cognito authentication problems is HTTP 403, which is why this option is unlikely to cause this issue. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html https://aws.amazon.com/about-aws/whats-new/2017/11/customize-integration-timeouts-in-amazon-api-gateway/ https://docs.aws.amazon.com/apigateway/latest/developerguide/supported-gateway-response-types.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"])</script><script>self.__next_f.push([1,"14:T88e,"])</script><script>self.__next_f.push([1,"AWS Amplify is a set of purpose-built tools and features that enables frontend web and mobile developers to quickly and easily build full-stack applications on AWS. Amplify provides two services: -Amplify Hosting - provides a git-based workflow for hosting full-stack serverless web apps with continuous deployment. -Amplify Studio - a visual development environment that simplifies the creation of scalable, full-stack web and mobile apps. Use can use Amplify Studio to build your frontend UI with a set of ready-to-use UI components, create an app backend with AWS resources, and then connect the two together. Amplify Hosting provides deep integration with Cypress for End-to-End (E2E) testing, allowing developers to generate a UI report for their tests. To add Cypress tests to your application, you can update the build settings in the amplify.yml configuration file, which will enable Amplify to run the tests during the build process. Hence, the correct answers are: - Connect the Github repository to AWS Amplify Hosting - Update the amplify.yml file with appropriate configuration settings for Cypress. The option that says: Create an application in AWS Amplify Studio. Clone the application’s source code in a local environment and run amplify pull --appId APP_ID --envName ENV_NAME is incorrect. This is not necessary, as you will not be recreating the application from scratch. The option that says: Include the location of the Cypress configuration file in the aws-exports.js file is incorrect. The aws-exports.js is simply a configuration file containing information for an Amplify application's region, user pool, or identity pool. You can't include build settings for Cypress in this file. The option that says: Update the amplifyconfiguration.json with appropriate configuration settings for Cypress is incorrect because this file is just the same as aws-exports.js, but for Android and iOS projects. References: https://docs.aws.amazon.com/amplify/latest/userguide/welcome.html https://aws.amazon.com/blogs/mobile/running-end-to-end-cypress-tests-for-your-fullstack-ci-cd-deployment-with-amplify-console/ https://amplify-sns.workshop.aws/en/80_e2e_test/00_-cypress.html"])</script><script>self.__next_f.push([1,"15:T856,"])</script><script>self.__next_f.push([1,"If you are getting an Access Denied error when trying to upload a large file to your S3 bucket with an upload request that includes an AWS KMS key, then you have to confirm that you have permission to perform kms:Decrypt actions on the AWS KMS key that you're using to encrypt the object. To perform a multipart upload with encryption using an AWS KMS key, the requester must have permission to the kms:Decrypt and kms:GenerateDataKey* actions on the key. These permissions are required because Amazon S3 must decrypt and read data from the encrypted file parts before it completes the multipart upload. Hence, the correct answers in this scenario are: - The AWS CLI S3 commands perform a multipart upload when the file is large. - The developer does not have the kms:Decrypt permission. The option that says: The developer does not have the kms:Encrypt permission is incorrect. This permission is not necessary for you to have as it's typically used for direct encryption requests with the KMS key and is only applicable for small data up to 4KB. Amazon S3 operates differently; it generates a data key from your KMS key to encrypt files. Behind the scenes, Amazon S3 actually generates a data key from your KMS key to encrypt files. The option that says: The developer's IAM permission has an attached inline policy that restricts him from uploading a file to S3 with a size of 100 GB or more is incorrect because inline policies are just policies that you create, manage, and embed directly into a single user, group, or role. This is unlikely to happen as there is no direct way to restrict a user from uploading a file with a specific size constraint. The option that says: The maximum size that can be encrypted in KMS is only 100 GB is incorrect because there is no such limitation in KMS. References: https://aws.amazon.com/premiumsupport/knowledge-center/s3-large-file-encryption-kms-key/ https://docs.aws.amazon.com/cli/latest/userguide/cli-services-s3-commands.html#using-s3-commands-managing-objects Check out this AWS Key Management Service (KMS) Cheat Sheet: https://tutorialsdojo.com/aws-key-management-service-aws-kms/"])</script><script>self.__next_f.push([1,"16:Teb4,"])</script><script>self.__next_f.push([1,"Lambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points: - After CloudFront receives a request from a viewer (viewer request) - Before CloudFront forwards the request to the origin (origin request) - After CloudFront receives the response from the origin (origin response) - Before CloudFront forwards the response to the viewer In the given scenario, you can use Lambda@Edge to allow your Lambda functions to customize the content that CloudFront delivers and to execute the authentication process in AWS locations closer to the users. In addition, you can set up an origin failover by creating an origin group with two origins with one as the primary origin and the other as the second origin, which CloudFront automatically switches to when the primary origin fails. This will alleviate the occasional HTTP 504 errors that users are experiencing. Hence, the correct answers in this scenario are: - Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your Lambda functions to execute the authentication process in AWS locations closer to the users. - Configure an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses. The option that says: Launch your application to multiple AWS regions to serve your global users. Use a Route 53 record with latency routing policy to route incoming traffic to the region with the best latency to the user is incorrect. Although this may resolve the performance issue, this solution entails a significant implementation cost since you have to deploy your application to multiple AWS regions. Remember that the scenario asks for a solution that will improve the performance of the application with minimal cost. The option that says: Launch your application to multiple and geographically disperse VPCs on various AWS regions then create a transit VPC to easily connect all your resources. Use several Lambda functions in each region using the AWS Serverless Application Model (SAM) service to improve the overall application performance is incorrect. Although setting up multiple VPCs across various regions which are connected with a transit VPC is valid, this solution still entails higher setup and maintenance costs. A more cost-effective option would be to use Lambda@Edge instead. The option that says: Add a Cache-Control max-age directive to your objects in CloudFront and specify the longest practical value for max-age to increase the cache hit ratio of your CloudFront distribution is incorrect because improving the cache hit ratio for the CloudFront distribution is irrelevant in this scenario. You can improve your cache performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content. However, take note that the problem in the scenario is the slow authentication process of your global users and not just the caching of the static objects. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html Check out these Amazon CloudFront and AWS Lambda Cheat Sheets: https://tutorialsdojo.com/amazon-cloudfront/ https://tutorialsdojo.com/aws-lambda/"])</script><script>self.__next_f.push([1,"17:Tb04,"])</script><script>self.__next_f.push([1,"AWS CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified regions. A stack set lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. All the resources included in each stack are defined by the stack set's AWS CloudFormation template. As you create the stack set, you specify the template to use, as well as any parameters and capabilities that the template requires. Hence, the correct solution in this scenario is to update the stacks on multiple AWS accounts using CloudFormation StackSets. After you've defined a stack set, you can create, update, or delete stacks in the target accounts and regions you specify. When you create, update, or delete stacks, you can also specify operational preferences, such as the order of regions in which you want the operation to be performed, the failure tolerance beyond which stack operations stop, and the number of accounts in which operations are performed on stacks concurrently. Remember that a stack set is a regional resource so if you create a stack set in one region, you cannot see it or change it in other regions. The option that says: Creating and managing stacks on multiple AWS accounts using CloudFormation Change Sets is incorrect because Change Sets only allow you to preview how proposed changes to a stack might impact your running resources. In this scenario, the most suitable way to meet the requirement is to use StackSets. The option that says: Defining and managing stack instances on multiple AWS Accounts using CloudFormation Stack Instances is incorrect because a stack instance is simply a reference to a stack in a target account within a region. Remember that a stack instance is associated with one stack set which is why this is just one of the components of CloudFormation StackSets. The option that says: Use AWS CodePipeline to automate the deployment of CloudFormation templates across multiple accounts is incorrect. AWS CodePipeline can automate the deployment process, but it is primarily a CI/CD tool. While it can be configured to deploy CloudFormation templates, it does not inherently provide the same level of centralized management for multiple accounts and regions as StackSets does. References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-getting-started.html Check out this AWS CloudFormation Cheat Sheet: https://tutorialsdojo.com/aws-cloudformation/"])</script><script>self.__next_f.push([1,"18:Tb44,"])</script><script>self.__next_f.push([1,"AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. You can store values as plain text or encrypted data. You can then reference values by using the unique name that you specified when you created the parameter. Parameter Store offers the following benefits and features: - Use a secure, scalable, hosted secrets management service (No servers to manage). - Improve your security posture by separating your data from your code. - Store configuration data and secure strings in hierarchies and track versions. - Control and audit access at granular levels. - Configure change notifications and trigger automated actions. - Tag parameters individually, and then secure access from different levels, including operational, parameter, Amazon EC2 tag, or path levels. - Reference AWS Secrets Manager secrets by using Parameter Store parameters. Hence, the correct solution for this scenario is to use AWS Systems Manager Parameter Store as a Secure String Parameter. Encrypting the database credentials and storing them in an S3 bucket which the Lambda functions can fetch is incorrect because it is a security risk to store sensitive database passwords and credentials in S3, even though the data is encrypted. A more suitable and secure way is to use the AWS Secrets Manager or the Systems Manager Parameter Store. Storing the database credentials as environment variables with KMS encryption which will be shared by the Lambda functions is incorrect because even though the credentials will be encrypted, these environment variables will only be used by an individual Lambda function and cannot be shared. Using IAM DB Authentication in RDS to allow encrypted connections from each Lambda function is incorrect. While using IAM DB Authentication can significantly enhance security for database connections in AWS environments, it's more about controlling access rather than managing the sharing of encrypted credentials like a database connection string across multiple applications or services. Additionally, this method is limited to certain types of databases supported by AWS, such as Amazon RDS for MySQL and PostgreSQL, and might not be available for all database engines. Lastly, in the scenario where Lambda functions need to share and securely access a connection string, other AWS services like the Systems Manager Parameter Store would be more appropriate to meet all the specified needs. References: https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html https://aws.amazon.com/blogs/compute/sharing-secrets-with-aws-lambda-using-aws-systems-manager-parameter-store/ Check out this AWS Systems Manager Cheat Sheet: https://tutorialsdojo.com/aws-systems-manager/"])</script><script>self.__next_f.push([1,"19:Tb2a,"])</script><script>self.__next_f.push([1,"A DynamoDB stream is an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attribute(s) of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the \"before\" and \"after\" images of modified items. All data in DynamoDB Streams is subject to a 24-hour lifetime. You can retrieve and analyze the last 24 hours of activity for any given table; however, data older than 24 hours is susceptible to trimming (removal) at any moment. If you disable a stream on a table, the data in the stream will continue to be readable for 24 hours. After this time, the data expires and the stream records are automatically deleted. Note that there is no mechanism for manually deleting an existing stream; you just need to wait until the retention limit expires (24 hours), and all the stream records will be deleted. Hence, the correct answer is to decrease the interval of running your function to 24 hours because in DynamoDB Streams, data older than 24 hours is susceptible to trimming (removal) at any moment. Increasing the interval of running your function to 48 hours is incorrect because this will actually make the problem worse. Considering that the data in DynamoDB Streams only lasts for 24 hours, you should actually decrease the interval of running your function and not further increase it. Setting the value of StreamViewType parameter in DynamoDB Streams to NEW_AND_OLD_IMAGES is incorrect because this just configures DynamoDB to write both the new and the old item images of the item to the stream. Setting the Stream View Type is actually irrelevant in this scenario since the problem is about missing data and not missing old or new values of the items. Setting the value of StreamViewType parameter in DynamoDB Streams to NEW_IMAGE is incorrect because this just configures the DynamoDB to write only the new image of the item to the stream. Just as mentioned above, the root cause of the issue is the length of the interval of running the Lambda function and not the DynamoDB Streams configuration. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_StreamSpecification.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ AWS Lambda Integration with Amazon DynamoDB Streams: https://tutorialsdojo.com/aws-lambda-integration-with-amazon-dynamodb-streams/"])</script><script>self.__next_f.push([1,"1a:Tb1a,"])</script><script>self.__next_f.push([1,"Amazon API Gateway is a fully managed service for creating, deploying, and managing APIs at scale. It is an intermediary between clients and backend services, offering features like authorization, traffic management, and monitoring. API Gateway integrates with Amazon CloudWatch to provide metrics that help diagnose performance issues, analyze traffic patterns, and ensure reliable operation. API Gateway metrics include Latency, which measures the total time taken to process a request, and IntegrationLatency, which tracks the time API Gateway spends communicating with the backend service. Metrics like 4XXError and 5XXError provide insights into client-side and server-side errors, helping pinpoint request or integration failures. Count tracks the total requests, offering visibility into traffic patterns and usage trends. Additionally, CacheHitCount and CacheMissCount are useful for understanding the effectiveness of caching; high cache hits reduce backend load and improve response times, while cache misses indicate requests that bypassed the cache and were processed by the backend. By monitoring these metrics in CloudWatch and setting alarms on critical thresholds, organizations can optimize API performance, troubleshoot issues, and maintain high availability. For more insights, enabling logging in CloudWatch Logs can provide detailed data for debugging and root cause analysis. Hence, the correct answers are: - IntegrationLatency. - Latency. The option that says: 4XXError is incorrect. This metric primarily captures client-side errors, such as unauthorized access or invalid requests, and does not reflect delays or timeouts in API Gateway. Since the scenario focuses on analyzing the root cause of timeouts during high traffic, monitoring client-side errors is simply not relevant for identifying latency issues. The option that says: Count is incorrect. While this metric tracks the total number of requests received by API Gateway, it only provides insights into traffic volume. It does not indicate latency, processing delays, or timeouts. Although it is useful for understanding traffic trends, it is not directly helpful in diagnosing performance issues. The option that says: 5XXError is incorrect because this metric tracks server-side errors, such as integration failures or configuration issues, which are not evident in this scenario. The Lambda function completes within its specified time, and delays are observed at the API Gateway. Thus, monitoring server-side errors would simply not address the identified latency problem. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-metrics-and-dimensions.html https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"])</script><script>self.__next_f.push([1,"1b:T627,You can use the CloudWatch agent to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. The agent supports both Windows Server and Linux, and enables you to select the metrics to be collected, including sub-resource metrics such as per-CPU core. Aside from the usual metrics, it also tracks the memory, swap, and disk space utilization metrics of your server. Hence, the most suitable solution to use in this scenario is to: Install the Amazon CloudWatch Logs agent to the EC2 instance. The option that says: Use AWS CloudShell to consolidate all metrics in a single dashboard is incorrect because this service is simply a command-line interface used for managing AWS resources from a terminal. It is not a monitoring tool and does not collect or display EC2 instance metrics. The option that says: Using detailed monitoring in CloudWatch is incorrect because this will typically send metric data of the EC2 instance to CloudWatch in 1-minute periods instead of 5-minute intervals. The option that says: Install the AWS X-Ray daemon on the EC2 instance is incorrect. Although this is helpful for troubleshooting your application, it does not have the capability to track the memory and swap usage of the instance. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html Check out these Amazon EC2 and CloudWatch Cheat Sheets: https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/ https://tutorialsdojo.com/amazon-cloudwatch/1c:Td9b,"])</script><script>self.__next_f.push([1,"Server-side encryption is about protecting data at rest. AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. AWS KMS uses KMS keys to encrypt your Amazon S3 objects. You use AWS KMS via the AWS Management Console or AWS KMS APIs to centrally create encryption keys, define the policies that control how keys can be used, and audit key usage to prove they are being used correctly. You can use these keys to protect your data in Amazon S3 buckets. The first time you add an SSE-KMS–encrypted object to a bucket in a region, a default KMS key is created for you automatically. This key is used for SSE-KMS encryption unless you select a KMS key that you created separately using AWS Key Management Service. Creating your own KMS key gives you more flexibility, including the ability to create, rotate, disable, and define access controls and audit the encryption keys used to protect your data. To upload an object to the S3 bucket, which uses SSE-KMS, you have to send a request with an x-amz-server-side-encryption header with the value of aws:kms. There's also an optional x-amz-server-side-encryption-aws-kms-key-id header, which specifies the ID of the AWS KMS master encryption key that was used for the object. The Amazon S3 API also supports encryption context with the x-amz-server-side-encryption-context header. When you upload an object, you can specify the KMS key using the x-amz-server-side-encryption-aws-kms-key-id header. If the header is not present in the request, Amazon S3 assumes the default KMS key. Regardless, the KMS key ID that Amazon S3 uses for object encryption must match the KMS key ID in the policy, otherwise, Amazon S3 denies the request. Hence, the correct is: Include the x-amz-server-side-encryption header with a value of aws:kms in your upload request. The option that says: Including the x-amz-server-side​-encryption​-customer-algorithm, x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers with appropriate values in the upload request is incorrect because these headers are only required if your S3 bucket is using Server-Side Encryption with Customer-Provided Keys (SSE-C). The option that says: Including the x-amz-server-side-encryption header with a value of aws:kms as well as the x-amz-server-side-encryption-aws-kms-key-id header containing the ID of the default AWS KMS key in your upload request is incorrect. Although this is a valid option, you actually don't need to add the x-amz-server-side-encryption-aws-kms-key-id header if you will be using the default AWS KMS key. Take note that the scenario explicitly mentioned to provide a solution with the LEAST amount of configuration. The option that says: Including the x-amz-server-side-encryption header with a value of AES256 in your upload request is incorrect because the value should be set as aws:kms instead. The value of AES256 is only applicable for SSE-S3 and SSE-C. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html#RESTObjectPUT-responses-examples https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/ Check out these Amazon S3 and AWS KMS Cheat Sheets: https://tutorialsdojo.com/amazon-s3/ https://tutorialsdojo.com/aws-key-management-service-aws-kms/"])</script><script>self.__next_f.push([1,"1d:T6e1,Any Lambda function invoked asynchronously is retried twice before the event is discarded. If the retries fail and you're unsure why, use Dead Letter Queues (DLQ) to direct unprocessed events to an Amazon SQS queue or an Amazon SNS topic to analyze the failure. AWS Lambda directs events that cannot be processed to the specified Amazon SNS topic or Amazon SQS queue. Functions that don't specify a DLQ will discard events after they have exhausted their retries. Hence, the correct answer is to specify the Amazon Resource Name of the SQS Queue in the Lambda function's DeadLetterConfig parameter. Specifying the AWS Service Namespace of the SQS Queue in the AWS::Lambda::Function resource of the CloudFormation template that you'll use for deploying the function is incorrect because you have to use the Amazon Resource Name (ARN) of the SQS Queue and not the service namespace. Moreover, you have to configure this in the Lambda function's DeadLetterConfig parameter and not in the CloudFormation template. Specifying the AWS Service Namespace of the SQS Queue in the Lambda function's DeadLetterConfig parameter is incorrect because you have to specify the ARN of the queue and not its service namespace. Specifying the Amazon Resource Name of the SQS Queue in the Transform section of the AWS SAM template that you'll use for deploying the function is incorrect. Although it is right to use the Amazon Resource Name of the SQS Queue, you still have to set this in the Lambda function's DeadLetterConfig parameter and not on the AWS SAM template. References: https://docs.aws.amazon.com/lambda/latest/dg/dlq.html https://docs.aws.amazon.com/lambda/latest/dg/retries-on-errors.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/1e:Ta47,"])</script><script>self.__next_f.push([1,"A stream is composed of one or more shards, each of which provides a fixed unit of capacity. The total capacity of the stream is the sum of the capacities of its shards. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that shard. It also tracks the shards in the stream using an Amazon DynamoDB table. Typically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances. Since the question requires the system to smoothly process streaming data, a fair number of shards and instances are required. By launching 4 shards, the stream will have more capacity for reading and writing data. By launching 2 instances, each instance will focus on processing two shards. It also provides high availability in the event that one instance goes down. Therefore, the ratio of 4 shards : 2 instances is the correct answer. The 1 shard : 6 instances ratio is incorrect because having just one shard for the stream will be insufficient and in the event that your incoming data rate increases, this single shard will not be able to handle the load. The 6 shards : 1 instance ratio is incorrect because having just one instance to process multiple shards will be insufficient since the processing capacity of your system will be severely limited. You have to allocate more instances in proportion to the number of open shards in your data stream. Moreover, a single instance is not a highly available option since the application doesn't have a backup instance to process the shards in the event of an outage. The 4 shards : 8 instances ratio is incorrect because launching more instances than the number of open shards will not improve the processing of the stream as it is only useful for failure standby purposes. Take note that each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. In addition, this option is not the most cost-effective choice as well. References: https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/"])</script><script>self.__next_f.push([1,"1f:T85a,"])</script><script>self.__next_f.push([1,"You can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package. Layers let you keep your deployment package small, which makes development easier. You can avoid errors that can occur when you install and package dependencies with your function code. For Node.js, Python, and Ruby functions, you can develop your function code in the Lambda console as long as you keep your deployment package under 3 MB. A function can use up to 5 layers at a time. The total unzipped size of the function and all layers can't exceed the unzipped deployment package size limit of 250 MB. You can create layers, or use layers published by AWS and other AWS customers. Layers support resource-based policies for granting layer usage permissions to specific AWS accounts, AWS Organizations, or all accounts. Layers are extracted to the /opt directory in the function execution environment. Each runtime looks for libraries in a different location under /opt, depending on the language. Structure your layer so that function code can access libraries without additional configuration. Hence, the correct answer is to use Lambda Layers. Alias is incorrect because this is just like a pointer to a specific Lambda function version. This will not enable your function to pull in additional code and other dependencies from another source. Execution Context is incorrect because this is a temporary runtime environment that initializes any external dependencies of your Lambda function code, such as database connections or HTTP endpoints. Environment Variable is incorrect because this just enables you to dynamically pass settings to your function code and libraries, without making changes to your code. References: https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html https://docs.aws.amazon.com/lambda/latest/dg/limits.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"])</script><script>self.__next_f.push([1,"20:T9c8,"])</script><script>self.__next_f.push([1,"Amazon DynamoDB provides fast access to items in a table by specifying primary key values. However, many applications might benefit from having one or more secondary (or alternate) keys available, to allow efficient access to data with attributes other than the primary key. To address this, you can create one or more secondary indexes on a table, and issue Query or Scan requests against these indexes. A secondary index is a data structure that contains a subset of attributes from a table, along with an alternate key to support Query operations. You can retrieve data from the index using a Query, in much the same way as you use Query with a table. A table can have multiple secondary indexes, which gives your applications access to many different query patterns. It is considered \"global\" because queries on the index can span all of the data in the base table, across all partitions. To speed up queries on non-key attributes, you can create a global secondary index. A global secondary index contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table. The index key does not need to have any of the key attributes from the table; it doesn't even need to have the same key schema as a table. Hence, the correct answer in this scenario is to add a Global Secondary Index. Sparse index is incorrect because parse indexes are only useful for queries over a small subsection of a table. For any item in a table, DynamoDB writes a corresponding index entry only if the index sort key value is present in the item. If the sort key doesn't appear in every table item, the index is said to be \"sparse\". Local Secondary Index is incorrect because this is used for queries which use the same partition key value, and in addition, you can't add this index to an already existing table. A local secondary index has the same partition key as the base table, but has a different sort key. It is \"local\" in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value. Primary Index is incorrect because this one actually refers to the partition key, which is the FighterId attribute in this scenario. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"21:T44f,A developer has recently released a new Lambda function which calculates accruals, interests, and other financial data. This function must have a streamlined integration setup with API Gateway. The requirement is to pass the incoming request from the client as the input to the backend Lambda function, via HTTPS, in the following format: { \"resource\": \"Resource path\", \"path\": \"Path parameter\", \"httpMethod\": \"Incoming request's method name\" \"headers\": {String containing incoming request headers} \"multiValueHeaders\": {List of strings containing incoming request headers} \"queryStringParameters\": {query string parameters } \"multiValueQueryStringParameters\": {List of query string parameters} \"pathParameters\": {path parameters} \"stageVariables\": {Applicable stage variables} \"requestContext\": {Request context, including authorizer-returned key-value pairs} \"body\": \"A JSON string of the request payload.\" \"isBase64Encoded\": \"A boolean flag to indicate if the applicable request payload is Base64-encode\"} Which of the following options is the MOST appropriate method to use to meet this requirement?22:Tdc5,"])</script><script>self.__next_f.push([1,"You choose an API integration type according to the types of integration endpoint you work with and how you want data to pass to and from the integration endpoint. For a Lambda function, you can have two types of integration: - Lambda proxy integration - Lambda custom integration In Lambda proxy integration, the setup is simple. If your API does not require content encoding or caching, you only need to set the integration's HTTP method to POST, the integration endpoint URI to the ARN of the Lambda function invocation action of a specific Lambda function, and the credential to an IAM role with permissions to allow API Gateway to call the Lambda function on your behalf. In Lambda non-proxy (or custom) integration, in addition to the proxy integration setup steps, you also specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. For an AWS service action, you have the AWS integration of the non-proxy type only. API Gateway also supports the mock integration, where API Gateway serves as an integration endpoint to respond to a method request. The Lambda custom integration is a special case of the AWS integration, where the integration endpoint corresponds to the function-invoking action of the Lambda service. The Lambda proxy integration type (AWS_PROXY) lets an API method be integrated with the Lambda function invocation action with a flexible, versatile, and streamlined integration setup. This integration relies on direct interactions between the client and the integrated Lambda function. With this type of integration, also known as the Lambda proxy integration, you do not set the integration request or the integration response. API Gateway passes the incoming request from the client as the input to the backend Lambda function. The integrated Lambda function takes the input of this format and parses the input from all available sources, including request headers, URL path variables, query string parameters, and applicable body. The function returns the result following this output format. This is the preferred integration type to call a Lambda function through API Gateway and is not applicable to any other AWS service actions, including Lambda actions other than the function-invoking action. Hence, Lambda proxy integration is correct as it matches the description depicted in the scenario. Lambda custom integration is incorrect because this type requires you to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. HTTP custom integration is incorrect because this type is only used where you need to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. Take note that the scenario uses an application hosted in Lambda which is why you have to use Lambda integration instead. HTTP proxy integration is incorrect because the scenario uses an application hosted in Lambda which is why you have to use Lambda integration instead. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-simple-proxy-for-lambda-input-format Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"])</script><script>self.__next_f.push([1,"23:Tf5b,"])</script><script>self.__next_f.push([1,"CloudFront Functions allows you to write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. This feature is designed for operations that can be processed with low latency at the edge locations of AWS, such as: - Cache key normalization – You can transform HTTP request attributes (headers, query strings, cookies, even the URL path) to create an optimal cache key, which can improve your cache hit ratio. - Header manipulation – You can insert, modify, or delete HTTP headers in the request or response. For example, you can add a True-Client-IP header to every request. - Status code modification and body generation – You can evaluate headers and respond back to viewers with customized content. - URL redirects or rewrites – You can redirect viewers to other pages based on information in the request or rewrite all requests from one path to another. - Request authorization – You can validate hashed authorization tokens, such as JSON web tokens (JWT), by inspecting authorization headers or other request metadata. When you associate a CloudFront function with a CloudFront distribution, it allows CloudFront to intercept requests and responses at CloudFront edge locations. CloudFront functions can only be invoked during two specific events: when CloudFront receives a request from a viewer (viewer request) and before CloudFront returns the response to the viewer (viewer response). In the given scenario, by attaching the validation function to the Viewer Request event, requests can be authenticated right when they hit the CloudFront cache and before they are forwarded to your AWS Fargate service. This method not only helps in reducing unnecessary traffic to your Fargate tasks but also improves overall latency for valid requests, as they don't have to wait behind unauthenticated requests being processed by your backend infrastructure. In addition, because CloudFront Functions operate at the edge locations of AWS's infrastructure, they are highly scalable and can handle a very high number of requests per second. Hence, the correct answer is: Create a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution. The option that says: Create a Lambda function that performs JWT validation. Configure the ALB to route login requests to the Lambda function is incorrect. This option doesn't eliminate the problem of unauthenticated requests reaching the backend infrastructure. All requests, regardless of authentication, will still need to traverse the network to reach the ALB and Lambda, which is not operationally efficient and could increase latency. The option that says: Enable auto-scaling on the Fargate tasks is incorrect. While auto-scaling could help handle the increased load by adding more tasks as demand increases, it is a reactive measure and not a long-term solution. It doesn't prevent unauthenticated requests from consuming resources on Fargate. The option that says: Create a Lambda@Edge function for JWT validation. Attach it to the Origin Response event of the CloudFront distribution is incorrect. The Origin Response event is triggered after the request has been processed by your origin (in this case, the ALB) and the origin has returned a response to CloudFront. Validating the JWT at this stage would not reduce the number of unauthenticated requests reaching your Fargate service. It's too late in the request processing flow to prevent unauthenticated requests from consuming resources on your backend infrastructure. Moreover, this could also introduce errors in the CloudFront flow. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-functions.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/example-function-validate-token.html Check out this Amazon CloudFront Cheat Sheet: https://tutorialsdojo.com/amazon-cloudfront/"])</script><script>self.__next_f.push([1,"24:Tc50,"])</script><script>self.__next_f.push([1,"A write capacity unit (WCU) represents one write per second, for an item up to 1 KB in size. For example, suppose that you create a table with 10 write capacity units. This allows you to perform 10 writes per second, for items up to 1 KB in size per second. Item sizes for writes are rounded up to the next 1 KB multiple. For example, writing a 500-byte item consumes the same throughput as writing a 1 KB item. If you need to write an item that is larger than 1 KB, DynamoDB must consume additional write capacity units. Transactional write requests require 2 write capacity units to perform one write per second for items up to 1 KB. The total number of write capacity units required depends on the item size. For example, if your item size is 2 KB, you require 2 write capacity units to sustain one standard write request per second or 4 write capacity units for a transactional write request. Suppose that you want to write 100 items per second to your table, and that the items are 512 bytes in size. Each write requires one provisioned write capacity unit. First, you have to round up your item size to the nearest whole number per KB. Then to determine the write capacity unite per item, you need to divide the item size of the operation by 1 KB. Once you got the value, you just simply have to multiply it with the number of write request per second (1 x 100) hence, the WCU that you should provision is 100. Here are the 3 steps to get the total number of WCU needed for your application. In this scenario, you are storing 100 items to a DynamoDB table every second, where each item is 1.5 KB in size. To get the WCU, you just have to follow these 3 simple steps below: Step #1 Get the Average Item Size round up by 1 KB Average Item Size = 1.5 KB = 2 KB (Rounded up) Step #2 Get the WCU per Item by dividing the Average Item Size by 1 KB Divide the Average Item Size by 1KB and round up the result: = 2 KB / 1 KB = 2 WCU per Item Step #3 Multiply the WCU per item to the number of items to be written per second = 2 WCU per item × 100 writes per second = 200 WCU Hence, the correct answer is to increase the WCU to 200 because your DynamoDB table only got 100 WCU, which is causing the issue. Enabling DynamoDB Accelerator (DAX) is incorrect. Although it will significantly improve the read performance of the mobile app, the issue with the throttling write requests will still persist. You have to increase the WCU to properly address the issue in this scenario. Implementing database caching with an ElastiCache cluster is incorrect because just as mentioned above, the main issue here is the write performance of the DynamoDB table and not its read performance. Using strong consistency in the write operations is incorrect because a strong consistency model is primarily used for read operations and not for writes. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ItemSizeCalculations.Writes https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"25:Tc92,"])</script><script>self.__next_f.push([1,"In ElasticBeanstalk, you can choose from a variety of deployment methods: All at once – Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time while the deployment occurs. Rolling – Deploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch. Rolling with additional batch – Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during the deployment process. Immutable – Deploy the new version to a fresh group of instances by performing an immutable update. Blue/Green - Deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly. To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a rolling deployment with an additional batch. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances. Immutable environment updates are an alternative to rolling updates. Immutable environment updates ensure that configuration changes that require replacing instances are applied efficiently and safely. If an immutable environment update fails, the rollback process requires only terminating an Auto Scaling group. A failed rolling update, on the other hand, requires performing an additional rolling update to roll back the changes. To perform an immutable environment update, Elastic Beanstalk creates a second, temporary Auto Scaling group behind your environment's load balancer to contain the new instances. First, Elastic Beanstalk launches a single instance with the new configuration in the new group. This instance serves traffic alongside all of the instances in the original Auto Scaling group that are running the previous configuration. Hence, Immutable and Rolling with additional batch are the correct deployment methods to be used in this scenario. All at once is incorrect because this will deploy the new version to all existing instances immediately and will not create new EC2 instances. Hence, it is possible that there would be a degradation of the service since some instances would be unavailable during the deployment process. Rolling is incorrect because this will deploy the new version in batches only to existing instances, without provisioning new resources. The compute capacity of the environment would still be compromised in this method. Canary is incorrect because this type of deployment method is not readily available in Elastic Beanstalk, but primarily to Lambda. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/ Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"])</script><script>self.__next_f.push([1,"26:Tab4,"])</script><script>self.__next_f.push([1,"When you encrypt your data, it is protected, but you have to protect your encryption key. One strategy is to encrypt it. Envelope encryption is the practice of encrypting plaintext data with a data key and then encrypting the data key under another key. You can even encrypt the data encryption key under another encryption key, and encrypt that encryption key under another encryption key. But, eventually, one key must remain in plaintext so you can decrypt the keys and your data. This top-level plaintext key encryption key is known as the root key. AWS KMS helps you to protect your encryption keys by storing and managing them securely. Root keys stored in AWS KMS, known as AWS KMS keys, never leave the AWS KMS FIPS validated hardware security modules unencrypted. To use an AWS KMS key, you must call AWS KMS. It is recommended that you use the following pattern to encrypt data locally in your application: 1. Use the GenerateDataKey operation to get a data encryption key. 2. Use the plaintext data key (returned in the Plaintext field of the response) to encrypt data locally, then erase the plaintext data key from memory. 3. Store the encrypted data key (returned in the CiphertextBlob field of the response) alongside the locally encrypted data. To decrypt data locally: 1. Use the Decrypt operation to decrypt the encrypted data key. The operation returns a plaintext copy of the data key. 2. Use the plaintext data key to decrypt data locally, then erase the plaintext data key from memory. The option that says: Use the Decrypt operation to decrypt the plaintext data key is incorrect because there is no need to decrypt a plaintext, or 'unencrypted', data key. The correct way is to use the Decrypt operation to decrypt the encrypted data key. The option that says: Use the plaintext data key to decrypt data locally, then erase the encrypted data key from memory is incorrect. Although this is a valid option for the encryption process, you still must erase the plaintext data key instead of the encrypted one. Take note that you are asked to choose valid and secure steps as per the scenario. The option that says: Use the encrypted data key to decrypt data locally, then erase the encrypted data key from memory is incorrect because you cannot decrypt data using an encrypted data key. You have to decrypt the encrypted data key first and use the plaintext copy of the data key to decrypt the files. References: https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping Check out these Amazon S3 and AWS KMS Cheat Sheets: https://tutorialsdojo.com/amazon-s3/ https://tutorialsdojo.com/aws-key-management-service-aws-kms/"])</script><script>self.__next_f.push([1,"27:T74c,Today's applications have more demanding requirements than ever before. For example, an online game might start out with just a few users and a very small amount of data. However, if the game becomes successful, it can easily outstrip the resources of the underlying database management system. It is not uncommon for web-based applications to have hundreds, thousands, or millions of concurrent users, with terabytes or more of new data generated per day. Databases for such applications must handle tens (or hundreds) of thousands of reads and writes per second. Amazon DynamoDB is well-suited for these kinds of workloads. As a developer, you can start with a small amount of provisioned throughput and gradually increase it as your application becomes more popular. DynamoDB scales seamlessly to handle very large amounts of data and very large numbers of users. Amazon RDS is incorrect because this is a type of SQL database, and it can't scale seamlessly to handle very large amounts of data and the number of users, as compared to DynamoDB. Using Amazon Aurora database could also provide scalability, which may be at par with DynamoDB, but this will entail the additional cost of setting up several replicas in various AWS regions. Amazon Redshift is incorrect because this is a columnar-based database, which is more appropriate for online analytical processing (OLAP) applications. Amazon S3 is incorrect because this is primarily used for object storage and not as a database. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html https://aws.amazon.com/products/databases/ Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ Amazon DynamoDB Overview: https://youtu.be/3ZOyUNIeorU?si=ogJGx2E_ZfMFJQvc28:T7ec,You can implement an AWS Lambda runtime in any programming language. A runtime is a program that runs a Lambda function's handler method when the function is invoke"])</script><script>self.__next_f.push([1,"d. You can include a runtime in your function's deployment package in the form of an executable file named bootstrap. A runtime is responsible for running the function's setup code, reading the handler name from an environment variable, and reading invocation events from the Lambda runtime API. The runtime passes the event data to the function handler, and posts the response from the handler back to Lambda. Your custom runtime runs in the standard Lambda execution environment. It can be a shell script, a script in a language that's included in Amazon Linux, or a binary executable file that's compiled in Amazon Linux. Hence, the correct answer in this scenario is to create a new layer which contains the Custom Runtime for C++ and then launch a Lambda function which uses that runtime. Uploading the deployment package to S3 and then using CloudFormation to deploy Lambda function with a reference to the S3 URL of the package is incorrect because you have to implement a Custom Runtime in order to execute the C++ code. Take note that this programming language is not natively supported yet in Lambda, which is why the use of a Custom Runtime is essential. Creating a Lambda function with the C++ code and directly uploading it to AWS is incorrect because there is a 50 MB deployment package size limit in Lambda if you'll directly upload the package. Just as mentioned above, you have to implement a Custom Runtime for this scenario. Using AWS Serverless Application Model (AWS SAM) to deploy the Lambda function is incorrect because using SAM alone is not enough to run the C++ code in Lambda. You have to use a Custom Runtime. References: https://docs.aws.amazon.com/lambda/latest/dg/runtimes-custom.html https://docs.aws.amazon.com/lambda/latest/dg/runtimes-walkthrough.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/29:T8f5,"])</script><script>self.__next_f.push([1,"A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. When a client makes a request to one of your API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output. There are two types of Lambda authorizers: - A token-based Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token. - A request parameter-based Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of headers, query string parameters, stageVariables, and $context variables. It is possible to use an AWS Lambda function from an AWS account that is different from the one in which you created your Lambda authorizer function, by using a Cross-Account Lambda Authorizer. Therefore, the correct answer is to use a Request Parameter-based Authorization as it uses a combination of headers, query string parameters, stageVariables, and $context variables for authentication. Amazon Cognito User Pools Authorizer is incorrect because this is just an alternative to using IAM roles and policies or Lambda authorizers. An Amazon Cognito user pool is primarily used to control who can access your API in Amazon API Gateway using identity token or access token, and not header and query string parameters from the API caller. Token-based authorization is incorrect because this is only suitable if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML. Cross-Account Lambda Authorizer is incorrect because this just enables you to use an AWS Lambda function from a different AWS account as your API authorizer function. Moreover, this is not a valid Lambda authorizer type. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-lambda-authorizer-input.html https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-lambda-authorizer-cross-account-lambda-authorizer.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"])</script><script>self.__next_f.push([1,"2a:Tc32,"])</script><script>self.__next_f.push([1,"AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications that run in AWS. You create a template that describes all the AWS resources that you want, and AWS CloudFormation takes care of provisioning and configuring those resources for you. A Cloudformation template is a JSON- or YAML-formatted text file that describes your AWS infrastructure. The template includes several sections for you to define your infrastructure code. For serverless applications (also referred to as Lambda-based applications), the Transform section specifies the version of the AWS Serverless Application Model (AWS SAM) to use. When you specify a transform, you can use AWS SAM syntax to declare resources in your template. The model defines the syntax that you can use and how it is processed. More specifically, the AWS::Serverless transform, which is a macro hosted by AWS CloudFormation, takes an entire template written in the AWS Serverless Application Model (AWS SAM) syntax and transforms and expands it into a compliant AWS CloudFormation template. In the following example, the template uses AWS SAM syntax to simplify the declaration of a Lambda function and its execution role. Transform: AWS::Serverless-2016-10-31Resources: MyServerlessFunctionLogicalID: Type: AWS::Serverless::Function Properties: Handler: index.handler Runtime: nodejs8.10 CodeUri: 's3://testBucket/mySourceCode.zip' Hence, the correct answer is: Add a Transform section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use. The option that says: Add a Parameters section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use is incorrect because this is just the part of the template that contains values to pass to your template at runtime when you create or update a stack. The option that says: Add a Resources section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use. is incorrect because this is primarily used to specify the stack resources and their properties. The option that says: Add a Mappings section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use is incorrect because this just lists a mapping of keys and associated values that you can use to specify conditional parameter values, similar to a lookup table. You can match a key to a corresponding value by using the Fn::FindInMap intrinsic function in the Resources and Outputs sections. References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-aws-serverless.html Check out this AWS CloudFormation Cheat Sheet: https://tutorialsdojo.com/aws-cloudformation/ Check out this AWS Serverless Application Model Cheat Sheet: https://tutorialsdojo.com/aws-serverless-application-model-sam/"])</script><script>self.__next_f.push([1,"2b:Tb49,"])</script><script>self.__next_f.push([1,"You can use an AWS Lambda function to process records in an Amazon Kinesis data stream. With Kinesis, you can collect data from many sources and process them with multiple consumers. Lambda supports standard data stream iterators and HTTP/2 stream consumers. Lambda reads records from the data stream and invokes your function synchronously with an event that contains stream records. Lambda reads records in batches and invokes your function to process records from the batch. Concurrent executions refers to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the it will differ depending on whether or not your Lambda function is processing events from a poll-based event source. For Lambda functions that process Kinesis or DynamoDB streams, the number of shards is the unit of concurrency. If your stream has 100 active shards, there will be at most 100 Lambda function invocations running concurrently. This is because Lambda processes each shard’s events in sequence. Hence, the correct answer in this scenario is that: there will be at most 100 Lambda function invocations running concurrently. The option that says: the Lambda function has 500 concurrent executions is incorrect because the number of concurrent executions for poll-based event sources is different from push-based event sources. This number of concurrent executions would have been correct if the Lambda function is integrated with a push-based even source such as API Gateway or Amazon S3 Events. Remember that the Kinesis and Lambda integration is using a poll-based event source, which means that the number of shards is the unit of concurrency for the function. The option that says: the Lambda function will throttle the incoming requests due to the excessive number of Kinesis shards is incorrect because, by default, AWS Lambda will automatically scale the function's concurrency execution in response to increased traffic, up to your concurrency limit. Moreover, having 100 shards is not excessive at all as long as there is a sufficient number of workers or consumers of the stream. The option that says: the Kinesis shards must be merged to increase the data capacity of the stream as well as the concurrency execution of the Lambda function is incorrect because, in the first place, you have to split the shards in order to increase the data capacity of the stream and not merge them. Since the Lambda function is using a poll-based event source mapping for Kinesis, the number of shards is the unit of concurrency for the function. References: https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html https://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html https://docs.aws.amazon.com/lambda/latest/dg/scaling.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"])</script><script>self.__next_f.push([1,"2c:Tbcc,"])</script><script>self.__next_f.push([1,"By default, the DynamoDB write operations (PutItem, UpdateItem, DeleteItem) are unconditional: each of these operations will overwrite an existing item that has the specified primary key. DynamoDB optionally supports conditional writes for these operations. A conditional write will succeed only if the item attributes meet one or more expected conditions. Otherwise, it returns an error. Conditional writes are helpful in many situations. For example, you might want a PutItem operation to succeed only if there is not already an item with the same primary key. Or you could prevent an UpdateItem operation from modifying an item if one of its attributes has a certain value. Conditional writes are helpful in cases where multiple users attempt to modify the same item. Therefore, configuring the database calls of the application to use conditional updates and conditional writes with a condition expression that will check if the new bid submitted by the customer is greater than the current bid is the most suitable solution in this scenario. Using DynamoDB Streams and a Lambda function to track the current bid price and compare against all of the new bids submitted by the customers is incorrect because DynamoDB Streams is primarily used to capture a time-ordered sequence of item-level modifications in the table. Although you can use a Lambda function that checks if the current and new bid prices are correct, this solution is cumbersome to implement as opposed to using conditional writes. Using an optimistic locking strategy in your database calls to ensure that the new bid submitted by the customer is greater than the current bid is incorrect because the Optimistic locking strategy simply ensures that the client-side item that you are updating (or deleting) is the same as the item in DynamoDB. It doesn't have the capability to specify certain conditions to meet the requirement. Enabling DynamoDB Transactions to automatically check the minimum acceptable price as well as the current and new bid price is incorrect because the DynamoDB Transactions feature simply provides developers atomicity, consistency, isolation, and durability (ACID) across one or more tables within a single AWS account and region. Although this can be used in building applications that require coordinated inserts, deletes, or updates to multiple items as part of a single logical business operation, the implementation of this solution entails a lot of work. Using conditional updates and conditional writes is still a more effective solution in this scenario instead of using DynamoDB Transactions. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ AWS Lambda Integration with Amazon DynamoDB Streams: https://tutorialsdojo.com/aws-lambda-integration-with-amazon-dynamodb-streams/"])</script><script>self.__next_f.push([1,"2d:Ta47,"])</script><script>self.__next_f.push([1,"A local secondary index maintains an alternate sort key for a given partition key value. It also contains a copy of some or all of the attributes from its base table; you specify which attributes are projected into the local secondary index when you create the table. The data in a local secondary index is organized by the same partition key as the base table, but with a different sort key. This lets you access data items efficiently across this different dimensions. For greater query or scan flexibility, you can create up to five local secondary indexes per table. A local secondary index (LSI) is \"local\" in the sense that every partition of an LSI is scoped to a base table partition that has the same partition key value. It lets you query over a single partition, as specified by the partition key value in the query. When you query a local secondary index, you can choose either eventual consistency or strong consistency. Queries or scans on a local secondary index consume read capacity units from the base table. When you write to a table, its local secondary indexes are also updated; these updates consume write capacity units from the base table. Take note that local secondary indexes are created at the same time you create a table. You cannot add a local secondary index to an existing table, nor can you delete any local secondary indexes that currently exist. Hence, the correct option in this scenario is to add a local secondary index before the table is created. Adding a local secondary index after the table has been created is incorrect because as mentioned above, you cannot add a local secondary index to an existing table. This index must be created at the same time you create your table. Adding a global secondary index before the table is created is incorrect because a global secondary index (GSI) is primarily used if you want to query over the entire table, across all partitions. GSI only supports eventual consistency and not strong consistency. You have to use a local secondary index instead. Adding a global secondary index after the table has been created is incorrect because you should use a local secondary index (LSI) instead of a global secondary index (GSI) just as mentioned above. Although you can add a GSI to an already existing table, unlike LSI, this option is still not sufficient to meet the specified requirement. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"2e:Tcbf,"])</script><script>self.__next_f.push([1,"Amazon RDS Read Replicas provide enhanced performance and durability for the database (DB) instances. This feature makes it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. You can reduce the load on your source DB instance by routing read queries from your applications to the read replica. Read replicas allow you to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Because read replicas can be promoted to master status, they are useful as part of a sharding implementation. To shard your database, add a read replica and promote it to master status, then, from each of the resulting DB Instances, delete the data that belongs to the other shard. In-memory data caching can be one of the most effective strategies to improve your overall application performance and reduce your database costs. Caching can be applied to any type of database, including relational databases such as Amazon RDS or NoSQL databases such as Amazon DynamoDB, MongoDB, and Apache Cassandra. The best part of caching is that it’s minimally invasive to implement, and by doing so, your application performance regarding both scale and speed is dramatically improved. Amazon ElastiCache offers fully managed Redis and Memcached. Seamlessly deploy, run, and scale popular open-source compatible in-memory data stores. Build data-intensive apps or improve the performance of your existing apps by retrieving data from high throughput and low latency in-memory data stores. Hence, the correct answers in this scenario are: - Set up read replicas for the RDS database instance and route read queries to these replicas. - Implement database caching using Amazon ElastiCache. The option that says: Replace the database with Amazon MemoryDB for Redis is incorrect because Redshift is primarily used for online analytics processing applications (OLAP) and as a data warehouse. Hence, this will not improve the read performance of your application. The option that says: Cache the database response using Amazon CloudFront is incorrect. Although CloudFront can provide caching and for CDN, it is not suitable to be used for database caching. Using Read Replicas and ElastiCache are more appropriate features to be used in this scenario. The option that says: Implement a Multi-AZ deployment configuration for the RDS DB instance is incorrect because configuring a Multi-AZ RDS just improves the availability of the database but does not drastically improve the read performance, which Read Replicas can provide. References: https://aws.amazon.com/caching/database-caching/ https://aws.amazon.com/rds/details/read-replicas/ https://aws.amazon.com/elasticache/ Check out these Amazon RDS and Elasticache Cheat Sheets: https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/ https://tutorialsdojo.com/amazon-elasticache/ Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"])</script><script>self.__next_f.push([1,"2f:T778,Cross-region replication (CRR) enables automatic, asynchronous copying of objects across buckets in different AWS Regions. Buckets configured for cross-region replication can be owned by the same AWS account or by different accounts. Cross-region replication is enabled with a bucket-level configuration. You add the replication configuration to your source bucket. To enable the cross-region replication feature in S3, the following items should be met: - The source and destination buckets must have versioning enabled. - The source and destination buckets must be in different AWS Regions. - Amazon S3 must have permissions to replicate objects from that source bucket to the destination bucket on your behalf. Hence, the most likely root cause for this scenario is that the versioning is not enabled in the bucket. The option that says: Amazon S3 Object Lock is enabled in the bucket is incorrect because this feature simply enables you to store objects using a write-once-read-many (WORM) model. You can use it to prevent an object from being deleted or overwritten for a fixed amount of time or indefinitely, but it will not affect your cross-region replication configuration. The option that says: The bucket should be configured as a static web hosting first is incorrect because this feature won't affect the cross-region replication in S3. This is primarily used to make your S3 bucket a static website, as its name implies. The option that says: S3 Transfer Acceleration is not enabled in the bucket is incorrect because this just enables fast, easy, and secure transfers of files to and from your bucket. Just like the other options, it does not affect cross-region replication in any way. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html https://docs.aws.amazon.com/AmazonS3/latest/dev/crr-how-setup.html Check out this Amazon S3 Cheat Sheet: https://tutorialsdojo.com/amazon-s3/30:T6a0,Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your clien"])</script><script>self.__next_f.push([1,"t and your Amazon S3 bucket. Transfer Acceleration leverages Amazon CloudFront’s globally distributed AWS Edge Locations. As data arrives at an AWS Edge Location, data is routed to your Amazon S3 bucket over an optimized network path. Hence, the correct answer is: S3 Transfer Acceleration. AWS Transfer for SFTP is incorrect because this is just a fully managed service that enables the transfer of files directly into and out of Amazon S3 using the Secure File Transfer Protocol (SFTP) which is also known as Secure Shell (SSH) File Transfer Protocol. It does not provide a fast, easy, and secure way to transfer files over long distances between your client and your Amazon S3 bucket. AWS Direct Connect is incorrect because you have users all around the world and not just on your on-premises data center. Direct Connect would be too costly and is definitely not suitable for this purpose. Amazon CloudFront is incorrect because this service is primarily used to serve static content and not as a transfer accelerator going to or from Amazon S3. CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. References: http://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration-examples.html Check out this Amazon S3 Cheat Sheet: https://tutorialsdojo.com/amazon-s3/ Comparison of AWS Services Cheat Sheets: https://tutorialsdojo.com/comparison-of-aws-services/31:Tadc,"])</script><script>self.__next_f.push([1,"One read request unit represents one strongly consistent read request, or two eventually consistent read requests, for an item up to 4 KB in size. Transactional read requests require 2 read request units to perform one read for items up to 4 KB. If you need to read an item that is larger than 4 KB, DynamoDB needs additional read request units. The total number of read request units required depends on the item size, and whether you want an eventually consistent or strongly consistent read. For example, suppose that you create a table with 10 provisioned read capacity units. This allows you to perform 10 strongly consistent reads per second, or 20 eventually consistent reads per second, for items up to 4 KB. Reading an item larger than 4 KB consumes more read capacity units. For example, a strongly consistent read of an item that is 8 KB (4 KB × 2) consumes 2 read capacity units. An eventually consistent read on that same item consumes only 1 read capacity unit. Item sizes for reads are rounded up to the next 4 KB multiple. For example, reading a 3,500-byte item consumes the same throughput as reading a 4 KB item. To get the number of RCU required to handle 150 eventually consistent read requests with an average item size of 3.5 KB, you simply have to do the following steps: Step #1 Get the Average Item Size by rounding up to 4 KB = 3.5 KB = 4 KB (rounded up) Step #2 Get the RCU per Item by dividing the Average Item Size by 8 KB = 4 KB / 8 KB = 0.5 Step #3 Multiply the RCU per item to the number of items to be written per second = 150 x 0.5 = 75 eventually consistent read requests Hence, the correct answer is 75. 150 is incorrect because this is the value for strongly consistent read requests based on the given RCU. Take note that for Step #2, you have to divide the Average Item Size by 8 KB and not by 4 KB, if you are calculating for eventual consistency. 300 is incorrect because this is the value for transactional read requests based on the given RCU. Take note that for Step #2, you have to divide the Average Item Size by 8 KB and not by 2 KB, if you are calculating for eventual consistency. 600 is incorrect because this is the value for transactional write requests, which is irrelevant since only the RCU is provided in the scenario. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ItemSizeCalculations.Reads Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ Calculating the Required Read and Write Capacity Unit for your DynamoDB Table: https://tutorialsdojo.com/calculating-the-required-read-and-write-capacity-unit-for-your-dynamodb-table/"])</script><script>self.__next_f.push([1,"32:Te91,"])</script><script>self.__next_f.push([1,"Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates. For example, you can define a stage variable in a stage configuration, and then set its value as the URL string of an HTTP integration for a method in your REST API. Later, you can reference the URL string using the associated stage variable name from the API setup. This way, you can use the same API setup with a different endpoint at each stage by resetting the stage variable value to the corresponding URLs. You can also access stage variables in the mapping templates, or pass configuration parameters to your AWS Lambda or HTTP backend. With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints. For example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, http://tutorialsdojo.com). In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage, and calls a different web host (for example, beta.tutorialsdojo.com). Similarly, stage variables can be used to specify a different AWS Lambda function name for each stage in your API. You can also use stage variables to pass configuration parameters to a Lambda function through your mapping templates. For example, you may want to re-use the same Lambda function for multiple stages in your API, but the function should read data from a different Amazon DynamoDB table depending on which stage is being called. In the mapping templates that generate the request for the Lambda function, you can use stage variables to pass the table name to Lambda. Stage variables are not applied to the security definitions section of the API specification. For example, you cannot use different Amazon Cognito user pools for different stages. Hence, the correct answer in this scenario is to use stage variables. Setting up an API Gateway Private Integration to the Lambda function is incorrect because this is just used to expose your HTTP/HTTPS resources behind an Amazon VPC to allow access to clients outside of your VPC. Creating environment variables in the Lambda function is incorrect because using an environment variable alone is not enough to meet this requirement. You have to integrate your Lambda function with API Gateway by using a stage variable, along with a proper mapping configuration. Setting up traffic shifting with Lambda Aliases is incorrect because this is just like a pointer to a specific Lambda function version. By using aliases, you can access the Lambda function which the alias is pointing to, without the caller knowing the specific version the alias is pointing to. Take note that we are not talking about different versions of the Lambda functions here as the scenario explicitly mentioned that we have passed the configuration parameters from API Gateway to the Lambda function. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html https://docs.aws.amazon.com/apigateway/latest/developerguide/amazon-api-gateway-using-stage-variables.html https://docs.aws.amazon.com/lambda/latest/dg/lambda-configuration.html Check out this Amazon API Gateway and AWS Lambda Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/ https://tutorialsdojo.com/aws-lambda/"])</script><script>self.__next_f.push([1,"33:T6b0,AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define. This makes it a good choice for automating your CI/CD process and centrally monitoring application activity. Moreover, AWS CodePipeline integrates with AWS CloudWatch, which provides a reliable, scalable, and flexible monitoring solution. You can create dashboards in CloudWatch to centrally monitor application activity and manage day-to-day development tasks. The option that says: AWS Fault Injection Simulator is incorrect because this is just a managed service that is commonly used in chaos engineering, and not for application development. It enables you to perform fault injection experiments on your AWS workloads to improve the performance and resiliency of your applications. The option that says: Elastic Beanstalk is incorrect because it is an orchestration service to quickly deploy and manage applications in AWS. The option that says: Amazon CodeGuru is incorrect because this is simply a developer tool that provides intelligent recommendations to improve the quality of your codebase and for identifying an application's most \"expensive\" lines of code in terms of resource intensiveness, CPU performance, and code efficiency. References: https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html https://aws.amazon.com/codepipeline/ Check out this AWS CodePipeline Cheat Sheet: https://tutorialsdojo.com/aws-codepipeline/34:T817,"])</script><script>self.__next_f.push([1,"There are two primary reasons why records may be delivered more than one time to your Amazon Kinesis Data Streams application: producer retries and consumer retries. Your application must anticipate and appropriately handle processing individual records multiple times. Consider a producer that experiences a network-related timeout after it makes a call to PutRecord, but before it can receive an acknowledgment from Amazon Kinesis Data Streams. The producer cannot be sure if the record was delivered to Kinesis Data Streams. Assuming that every record is important to the application, the producer would have written to retry the call with the same data. If both PutRecord calls on that same data were successfully committed to Kinesis Data Streams, then there will be two Kinesis Data Streams records. Although the two records have identical data, they also have unique sequence numbers. Applications that need strict guarantees should embed a primary key within the record to remove duplicates later when processing. Note that the number of duplicates due to producer retries is usually low compared to the number of duplicates due to consumer retries. Hence, the correct answer in this scenario is to embed a primary key within the record to remove duplicates later when processing. Adding more shards is incorrect because this is not a suitable solution for handling duplicate records in the Kinesis data stream. This is primarily used to increase the rate of data flowing through the stream. Splitting shards of the data stream is incorrect because this is used to increase the capacity of the stream and not to avoid any duplicate data. Merging shards of the data stream is incorrect because this is primarily used to make better use of the unused capacity in the stream and to save on costs. References: https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/"])</script><script>self.__next_f.push([1,"35:T9e0,"])</script><script>self.__next_f.push([1,"To share information between stacks, export a stack's output values. Other stacks that are in the same AWS account and region can import the exported values. For example, you might have a single networking stack that exports the IDs of a subnet and security group for public web servers. Stacks with a public webserver can easily import those networking resources. You don't need to hard code resource IDs in the stack's template or pass IDs as input parameters. To export a stack's output value, use the Export field in the Output section of the stack's template. To import those values, use the Fn::ImportValue function in the template for the other stacks. In this scenario, we can expose the API endpoint to other stacks by adding the Export property in the Outputs section. In the example below, we use 'SimpleAPI' as the name of the value to be exported: To reference the endpoint's value in other templates, simply use the Fn::ImportValue function and specify SimpleAPI as its parameter. Hence, the correct answer is: Include the Export property in the original template's Outputs section. Then use the Fn::ImportValue function in other templates to retrieve the exported value. The option that says: Include the Export property in the original template's Outputs section. Then use the Ref function in other templates to retrieve the exported value is incorrect. The Ref function is not capable of returning resource/parameter values from other templates. The option that says: Specify HelloWorldApi as the parameter when using the Fn::ImportValue function in other templates is incorrect. Fn::ImportValue is not a standalone command. It cannot be used to directly reference values from the Outputs section. You must first declare the name of the resource to be exported using the Export property. The option that says: Add the AWS::Include transform in the original template to directly import the HelloWorldFunction resource to other templates is incorrect. The AWS::Include transform is used for referencing templates stored in Amazon S3. This allows you to reuse resources in other CloudFormation templates. Keep in mind that in this scenario, you only need to solve the issue of exporting the API endpoint in other templates. References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputs-section-structure.html Check out this AWS CloudFormation Cheat Sheet: https://tutorialsdojo.com/aws-cloudformation/"])</script><script>self.__next_f.push([1,"36:T10a5,"])</script><script>self.__next_f.push([1,"CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can use CodeDeploy. CodeDeploy provides two deployment type options: In-place deployment: The application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete. Only deployments that use the EC2/On-Premises compute platform can use in-place deployments. AWS Lambda compute platform deployments cannot use an in-place deployment type. Blue/green deployment: The behavior of your deployment depends on which compute platform you use: - Blue/green on an EC2/On-Premises compute platform: The instances in a deployment group (the original environment) are replaced by a different set of instances (the replacement environment). If you use an EC2/On-Premises compute platform, be aware that blue/green deployments work with Amazon EC2 instances only. - Blue/green on an AWS Lambda compute platform: Traffic is shifted from your current serverless environment to one with your updated Lambda function versions. You can specify Lambda functions that perform validation tests and choose the way in which the traffic shift occurs. All AWS Lambda compute platform deployments are blue/green deployments. For this reason, you do not need to specify a deployment type. - Blue/green on an Amazon ECS compute platform: Traffic is shifted from the task set with the original version of a containerized application in an Amazon ECS service to a replacement task set in the same service. The protocol and port of a specified load balancer listener are used to reroute production traffic. During deployment, a test listener can be used to serve traffic to the replacement task set while validation tests are run. The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The CodeDeploy agent communicates outbound using HTTPS over port 443. It is also important to note that the CodeDeploy agent is required only if you deploy to an EC2/On-Premises compute platform. The agent is not required for deployments that use the Amazon ECS or AWS Lambda compute platform. Therefore, the supported deployment types in CodeDeploy are: - Blue/green deployments to ECS - In-place deployments to on-premises servers Rolling deployments to ECS is incorrect because only blue/green deployment is allowed if you used the AWS CodeDeploy service to deploy the new version of your application to ECS. Take note that in CodeDeploy, only the EC2/On-Premises compute platform can use both in-place deployments and blue/green deployment. For Lambda and ECS, you can only do a blue/green deployment in CodeDeploy. This type of deployment is actually done in Elastic Beanstalk for Multicontainer docker environment which implicitly uses ECS. In-place deployments to AWS Lambda is incorrect because AWS Lambda compute platform deployments cannot use an in-place deployment type. Blue/green deployments to on-premises servers is incorrect because, in CodeDeploy, blue/green deployments only work with Amazon EC2 instances only. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/ Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/ Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy: https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/ Comparison of AWS Services Cheat Sheets: https://tutorialsdojo.com/comparison-of-aws-services/"])</script><script>self.__next_f.push([1,"37:T796,The write-through strategy adds data or updates data in the cache whenever data is written to the database. With this strategy, the data in the cache is never stale since the data in the cache is updated every time it is written to the database. This is why the data in the cache is always current. One of its disadvantages is that, since most data are never read in the cluster, you are actually wasting resources and disk space. This issue can be rectified by simply adding TTL to minimize wasted space. Hence, the correct answer is to implement a Write-Through caching strategy in the application and enable TTL. Implementing Lazy Loading in the application in conjunction with the Write Through caching strategy is incorrect. Although the Write Through caching strategy can ensure that the cache always have the current data, the Lazy Loading strategy is not helpful in minimizing wasted space in the cluster. This combination is only helpful in scenarios where you have missing data which continues to be missing until it is added or updated on the database. Using a Write-Through caching strategy is incorrect. Although the caching strategy will meet the first requirement of ensuring that the data in the cache is always current, it fails the second requirement of minimizing wasted space in the cluster as this strategy does not automatically delete the data that are never read. You should use a combination of Write-Through and TTL instead. Using a Lazy Loading caching strategy is incorrect because it is just a strategy to load data into the cache only when necessary. It doesn't meet the requirement of the scenario, which is to not have stale data in the cache. References: https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.WriteThrough https://aws.amazon.com/caching/implementation-considerations/ Check out this Amazon Elasticache Cheat Sheet: https://tutorialsdojo.com/amazon-elasticache/38:Td30,"])</script><script>self.__next_f.push([1,"Amazon DynamoDB is integrated with AWS Lambda so that you can create triggers—pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables. If you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records. The Lambda function can perform any actions you specify, such as sending a notification or initiating a workflow. For example, you can write a Lambda function to simply copy each stream record to persistent storage, such as Amazon Simple Storage Service (Amazon S3), to create a permanent audit trail of write activity in your table. Or suppose you have a mobile gaming app that writes to a GameScores table. Whenever the TopScore attribute of the GameScores table is updated, a corresponding stream record is written to the table's stream. This event could then trigger a Lambda function that posts a congratulatory message on a social media network. (The function would simply ignore any stream records that are not updates to GameScores or that do not modify the TopScore attribute.) Hence, the correct answer is: Enable DynamoDB Streams and configure it as the event source for the Lambda function. The option that says: Use Amazon EventBridge (Amazon CloudWatch Events) to track all new data in your table and configure it as the event source for the Lambda function is incorrect because the Amazon EventBridge (Amazon CloudWatch Events) service does not have the capability to track any new inserts or updates on the DynamoDB table. Although Amazon EventBridge (Amazon CloudWatch Events) delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources, it cannot provide tracking of the DynamoDB's table activities. The option that says: Enable DynamoDB Transactions and configure it as the event source for the Lambda function is incorrect because Amazon DynamoDB transactions just simplifies the developer experience of making coordinated, all-or-nothing changes to multiple items both within and across tables. This feature is primarily used to provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, enabling you to maintain data correctness in your applications easily. The option that says: Use Amazon Kinesis Data Streams to track all new data in your table and configure it as the event source for the Lambda function is incorrect because using DynamoDB Streams is a better option than Kinesis. In addition, Kinesis does not have the capability to immediately track any new inserts or updates on the DynamoDB table. References: https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html Check out these AWS Lambda and Amazon DynamoDB Cheat Sheets: https://tutorialsdojo.com/aws-lambda/ https://tutorialsdojo.com/amazon-dynamodb/ AWS Lambda Integration with Amazon DynamoDB Streams: https://tutorialsdojo.com/aws-lambda-integration-with-amazon-dynamodb-streams/"])</script><script>self.__next_f.push([1,"39:Ta21,"])</script><script>self.__next_f.push([1,"With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. You can also perform most deployment tasks, such as changing the size of your fleet of Amazon EC2 instances or monitoring your application, directly from the Elastic Beanstalk web interface (console). To use Elastic Beanstalk, you create an application, upload an application version in the form of an application source bundle (for example, a Java .war file) to Elastic Beanstalk, and then provide some information about the application. Elastic Beanstalk automatically launches an environment and creates and configures the AWS resources needed to run your code. After your environment is launched, you can then manage your environment and deploy new application versions. Hence, the correct answer in this scenario is Elastic Beanstalk. AWS CloudFormation is incorrect. Although the CloudFormation service provides deployment capabilities, you will still have to design a custom template that contains the required AWS resources for your application needs. Hence, this will require more time to complete instead of just directly using Elastic Beanstalk. AWS SAM is incorrect because the AWS Serverless Application Model (AWS SAM) is just an open-source framework that you can use to build serverless applications on AWS. You can't host your application in AWS, unlike Elastic Beanstalk, and it does not automatically handle the details of capacity provisioning, load balancing, scaling, and application health monitoring. AWS CodeDeploy is incorrect because this is primarily used for deployment and not as an orchestration service for your applications. AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/ Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy: https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/"])</script><script>self.__next_f.push([1,"3a:Te67,"])</script><script>self.__next_f.push([1,"Partitions are usually throttled when they are accessed by your downstream applications much more frequently than other partitions (that is, a \"hot\" partition), or when workloads rely on short periods of time with high usage (a \"burst\" of read or write activity). This problem can be more pronounced in stateful applications, where maintaining session data or transactions may cause repeated access to the same partition. To avoid hot partitions and throttling, you must optimize your table and partition structure. DynamoDB adaptive capacity automatically boosts throughput capacity to high-traffic partitions. However, each partition is still subject to the hard limit. This means that adaptive capacity can't solve larger issues with your table or partition design. To avoid hot partitions and throttling, optimize your table and partition structure. To solve this issue, consider one or more of the following solutions: - Increase the amount of read or write capacity for your table to anticipate short-term spikes or bursts in read or write operations. If you decide later you don't need the additional capacity, decrease it. Take note that Before deciding on how much to increase read or write capacity, consider the best practices in designing your partition keys. - Implement error retries and exponential backoff. This technique improves reliability in stateful applications by allowing retries after progressively longer wait times between errors. AWS SDKs have built-in support for this logic. - Distribute your read operations and write operations as evenly as possible across your table. A hot partition can degrade the overall performance, particularly for stateful applications that repeatedly access certain keys. - Implement a caching solution, such as DynamoDB Accelerator (DAX) or Amazon ElastiCache. For stateful workloads with frequent reads to session or static data, caching can significantly reduce database access. Hence, the correct answers are: - Implement error retries and exponential backoff. - Refactor your application to distribute your read and write operations as evenly as possible across your table. The option that says: Using a DynamoDB Accelerator (DAX) is incorrect. Although it can reduce read load by caching frequently accessed items, it will simply incurs additional costs to maintain the DAX cluster. Since the issue can be addressed by improving workload distribution without extra cost, DAX is not a cost-effective solution for this scenario. The option that says: Increasing the amount of read or write capacity for your table is incorrect because, while it can alleviate throttling by temporarily handling higher traffic, it is an expensive solution that contradicts the minimal-cost requirement. Scaling read and write capacity (RCU and WCU) can lead to significant increases in operational costs, especially for stateful applications with continuous usage patterns. The option that says: Implementing read sharding to distribute workloads evenly is incorrect because the issue primarily involves write operations in a stateful workload. Instead, write sharding should be used to better distribute writes across the partition key space. This can be achieved by appending either random or calculated suffixes to partition key values, which spreads the data more evenly across partitions. References: https://aws.amazon.com/premiumsupport/knowledge-center/throttled-ddb/ https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-sharding.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"3b:Td41,"])</script><script>self.__next_f.push([1,"An event-driven architecture uses events to trigger and communicate between decoupled services and is common in modern applications built with microservices. An event is a change in state or an update, like an item being placed in a shopping cart on an e-commerce website. Events can either carry the state (the item purchased, its price, and a delivery address) or events can be identifiers (a notification that an order was shipped). Event-driven architectures have three key components: event sources/producers, event routers, and event consumers. A producer publishes an event to the router, which filters and pushes the events to consumers. Producer services and consumer services are decoupled, which allows them to be scaled, updated, and deployed independently. In the scenario, the DynamoDB table is the source of events. Updates made to an inventory item are tracked by DynamoDB Streams. Leveraging an Event-Driven pattern, a Lambda function can immediately be triggered by this change in the stream. The function can then execute the required business logic for inventory replenishment and send a notification to an Amazon SNS topic to promptly notify the business owner about the inventory status. This immediate response ensures timely replenishment actions and notifications. Hence, the correct answer is: An Event-Driven pattern using DynamoDB Streams for capturing inventory changes, Lambda function for executing business logic, and Amazon SNS for push notifications. The option that says: A Scheduled pattern using Amazon EventBridge Scheduler for checking inventory levels, Lambda function for executing business logic, and Amazon SNS for push notifications is incorrect. With this approach, if an item's stock goes below the threshold immediately after a check, the system won't be aware of it until the next scheduled check, making it not a suitable pattern for replenishing inventory in a timely manner. The option that says: A Batch pattern with Amazon SQS for queuing changes, Lambda function for executing business logic, and Amazon SNS for email notifications is incorrect because this pattern is designed to process a group of records collectively. It means that immediate action might not be taken when a single item's stock goes below the threshold. Instead, it would wait for a batch of records to be queued up, making it not ideal for a scenario where timely action on individual stock items is critical. The option that says: A Fan-out pattern where Amazon SNS broadcasts orders, triggering Lambda functions for business logic execution and Amazon SNS for push notifications is incorrect. This pattern is primarily suited for scenarios where one event needs to initiate multiple independent actions in parallel. For instance, converting a video clip in different formats. In contrast, the scenario requires succeeding actions that are dependent: after processing the stock level, a notification is sent if a certain condition is met. References: https://aws.amazon.com/what-is/eda/ https://docs.aws.amazon.com/lambda/latest/operatorguide/event-driven-architectures.html https://aws.amazon.com/blogs/architecture/lets-architect-designing-event-driven-architectures/ Check out these Cheat Sheets for AWS Lambda, Amazon DynamoDB, and Amazon SNS: https://tutorialsdojo.com/aws-lambda/ https://tutorialsdojo.com/amazon-dynamodb/ https://tutorialsdojo.com/amazon-sns/"])</script><script>self.__next_f.push([1,"3c:T9f2,"])</script><script>self.__next_f.push([1,"When a task that uses the EC2 launch type is launched, Amazon ECS must determine where to place the task based on the requirements specified in the task definition, such as CPU and memory. Similarly, when you scale down the task count, Amazon ECS must determine which tasks to terminate. You can apply task placement strategies and constraints to customize how Amazon ECS places and terminates tasks. Task placement strategies and constraints are not supported for tasks using the Fargate launch type. By default, Fargate tasks are spread across Availability Zones. Cluster queries are expressions that enable you to group objects. For example, you can group container instances by attributes such as Availability Zone, instance type, or custom metadata. You can add custom metadata to your container instances, known as attributes. Each attribute has a name and an optional string value. You can use the built-in attributes provided by Amazon ECS or define custom attributes. After you have defined a group of container instances, you can customize Amazon ECS to place tasks on container instances based on group. Running tasks manually is ideal in certain situations. For example, suppose that you are developing a task but you are not ready to deploy this task with the service scheduler. Perhaps your task is a one-time or periodic batch job that does not make sense to keep running or restart when it finishes. Hence, the correct ECS feature which provides you with expressions that you can use to group container instances by a specific attribute is Cluster Query Language. Task Group is incorrect because this is just a set of related tasks. This does not provide expressions that enable you to group objects. All tasks with the same task group name are considered as a set when performing spread placement. Task Placement Constraint is incorrect because it is just a rule that is considered during task placement. Although it uses cluster queries when you are placing tasks on container instances based on a specific expression, it does not provide the actual expressions which are used to group those container instances. Task Placement Strategies is incorrect because this is just an algorithm for selecting instances for task placement or tasks for termination. References: https://aws.amazon.com/blogs/compute/amazon-ecs-task-placement/ https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cluster-query-language.html Check out this Amazon ECS Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/"])</script><script>self.__next_f.push([1,"3d:Ta8a,"])</script><script>self.__next_f.push([1,"Server-side encryption is about protecting data at rest. Using server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your own encryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it writes to disks, and decryption, when you access your objects. Therefore, you don't need to maintain any code to perform data encryption and decryption. The only thing you do is manage the encryption keys you provide. When you upload an object, Amazon S3 uses the encryption key you provide to apply AES-256 encryption to your data and removes the encryption key from memory. It is important to note that Amazon S3 does not store the encryption key you provide. Instead, it is stored in a randomly salted HMAC value of the encryption key in order to validate future requests. The salted HMAC value cannot be used to derive the value of the encryption key or to decrypt the contents of the encrypted object. That means, if you lose the encryption key, you lose the object. When you retrieve an object, you must provide the same encryption key as part of your request. Amazon S3 first verifies that the encryption key you provided matches, and then decrypts the object before returning the object data to you. Hence, the valid consideration that the developer should keep in mind when implementing this architecture is: if you lose the encryption key, you lose the object. The option that says: the salted HMAC value can be used to derive the value of the encryption key is incorrect because the salted HMAC is just used to validate future encryption requests. It cannot be used to derive the value of the encryption key or to decrypt the contents of the encrypted object. The option that says: the salted HMAC value can be used to decrypt the contents of the encrypted object is incorrect because just as mentioned above, the HMAC cannot be used to derive the value of the encryption key or to decrypt the contents of the encrypted object. The option that says: if you lose the encryption key, the salted HMAC value can be used to decrypt the object is incorrect because if you lose the encryption key, you lose the object. You cannot use the salted HMAC value to decrypt the object. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html#RESTObjectPUT-responses-examples https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/ Check out these Amazon S3 and AWS KMS Cheat Sheets: https://tutorialsdojo.com/amazon-s3/ https://tutorialsdojo.com/aws-key-management-service-aws-kms/"])</script><script>self.__next_f.push([1,"3e:T832,"])</script><script>self.__next_f.push([1,"Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes Delay queues are similar to visibility timeouts because both features make messages unavailable to consumers for a specific period of time. The difference between the two is that, for delay queues, a message is hidden when it is first added to queue, whereas for visibility timeouts a message is hidden only after it is consumed from the queue. To set delay seconds on individual messages rather than on an entire queue, use message timers to allow Amazon SQS to use the message timer's DelaySeconds value instead of the delay queue's DelaySeconds value. Hence, the correct answer is to use a Delay Queue. Short Polling is incorrect because this is just the default configuration of SQS that queries only a subset of its servers (based on a weighted random distribution), to determine whether any messages are available for a response. Visibility Timeouts is incorrect because, with this configuration, a message is hidden only after it is consumed from the queue, and not before. Take note that the difference between the two is that, for delay queues, a message is hidden when it is first added to the queue, whereas for visibility timeouts a message is hidden only after it is consumed from the queue. Long Polling is incorrect because this just helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-delay-queue.html Check out this Amazon SQS Cheat Sheet: https://tutorialsdojo.com/amazon-sqs/"])</script><script>self.__next_f.push([1,"3f:Tb27,"])</script><script>self.__next_f.push([1,"Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources. To configure your bucket to allow cross-origin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support each origin, and other operation-specific information. You can add up to 100 rules to the configuration. You add the XML document as the cors subresource to the bucket either programmatically or by using the Amazon S3 console as shown below: A CORS configuration is an XML file that contains a series of rules within a \u003cCORSRule\u003e. A configuration can have up to 100 rules. A rule is defined by one of the following tags: AllowedOrigin - Specifies domain origins that you allow to make cross-domain requests. AllowedMethod - Specifies a type of request you allow (GET, PUT, POST, DELETE, HEAD) in cross-domain requests. AllowedHeader - Specifies the headers allowed in a preflight request. Below are some of the CORSRule elements: MaxAgeSeconds - Specifies the amount of time in seconds (in this example, 3000) that the browser caches an Amazon S3 response to a preflight OPTIONS request for the specified resource. By caching the response, the browser does not have to send preflight requests to Amazon S3 if the original request will be repeated. ExposeHeader - Identifies the response headers (in this example, x-amz-server-side-encryption, x-amz-request-id, and x-amz-id-2) that customers are able to access from their applications (for example, from a JavaScript XMLHttpRequest object). Hence, the correct answers in this scenario are: - It allows a user to view, add, remove or update objects inside the S3 bucket from the domain tutorialsdojo.com - This will cause the browser to cache an Amazon S3 response of a preflight OPTIONS request for 1 hour The option that says: the request will fail if the x-amz-meta-custom-header header is not included is incorrect because the ExposeHeader element refers to the header that will be exposed to the response and not a constraint for the request. The option that says: this configuration authorizes the user to perform actions on the S3 bucket is incorrect because this configuration actually does the opposite. It doesn't authorize the user to perform actions on the S3 bucket. The option that says: all HTTP Methods are allowed is incorrect because the configuration didn't include the HEAD HTTP method. References: http://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/cors.html"])</script><script>self.__next_f.push([1,"40:Tabe,"])</script><script>self.__next_f.push([1,"Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status. When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available. Amazon Cognito lets you save end-user data in datasets containing key-value pairs. This data is associated with an Amazon Cognito identity, so that it can be accessed across logins and devices. To sync this data between the Amazon Cognito service and an end user’s devices, invoke the synchronize method. Each dataset can have a maximum size of 1 MB. You can associate up to 20 datasets with an identity. The Amazon Cognito Sync client creates a local cache for the identity data. Your app talks to this local cache when it reads and writes keys. This guarantees that all of your changes made on the device are immediately available on the device, even when you are offline. When the synchronize method is called, changes from the service are pulled to the device, and any local changes are pushed to the service. At this point, the changes are available to other devices to synchronize. Amazon Cognito automatically tracks the association between identity and devices. Using the push synchronization, or push sync, feature, you can ensure that every instance of a given identity is notified when identity data changes. Push sync ensures that whenever the sync store data changes for a particular identity, all devices associated with that identity receive a silent push notification informing them of the change. Hence, the correct answer is to Amazon Cognito Sync. Amazon Cognito User Pools is incorrect because this is just a user directory which allows your users to sign in to your web or mobile app through Amazon Cognito. Amazon Cognito Identity Pools is incorrect because this simply enables you to create unique identities for your users and federate them with identity providers where you can obtain temporary, limited-privilege AWS credentials to access other AWS services. AWS Device Farm is incorrect because this is only an app testing service that lets you test and interact with your Android, iOS, and web apps on many devices at once, or reproduce issues on a device in real-time. References: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html https://docs.aws.amazon.com/cognito/latest/developerguide/push-sync.html Check out this Amazon Cognito Cheat Sheet: https://tutorialsdojo.com/amazon-cognito/"])</script><script>self.__next_f.push([1,"41:Tc60,"])</script><script>self.__next_f.push([1,"Amazon Kinesis Data Streams is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. You can configure hundreds of thousands of data producers to continuously put data into a Kinesis data stream. Data will be available within milliseconds to your Amazon Kinesis applications, and those applications will receive data records in the order they were generated. The purpose of resharding in Amazon Kinesis Data Streams is to enable your stream to adapt to changes in the rate of data flow. You split shards to increase the capacity (and cost) of your stream. You merge shards to reduce the cost (and capacity) of your stream. One approach to resharding could be to split every shard in the stream—which would double the stream's capacity. However, this might provide more additional capacity than you actually need and, therefore, create unnecessary costs. You can also use metrics to determine which are your \"hot\" or \"cold\" shards, that is, shards that are receiving much more data or much less data than expected. You could then selectively split the hot shards to increase capacity for the hash keys that target those shards. Similarly, you could merge cold shards to make better use of their unused capacity. You can obtain some performance data for your stream from the Amazon CloudWatch metrics that Kinesis Data Streams publishes. However, you can also collect some of your own metrics for your streams. One approach would be to log the hash key values generated by the partition keys for your data records. Recall that you specify the partition key at the time that you add the record to the stream. Hence, the correct answer is: Split every shard in the stream. The option that says: Upgrade the instance type of the EC2 instances is incorrect. Although it will improve the processing time of the data in the stream, it will not increase the capacity of the stream itself. You have to reshard the stream in order to increase or decrease its capacity, and not just upgrade the EC2 instances which process the data in the stream. The option that says: Merge every shard in the stream is incorrect because merging shards will actually decrease the capacity of the stream rather than increase it. This is only useful if you want to save costs or if the data stream is underutilized, which are both not indicated in the scenario. The option that says: Integrate Amazon Data Firehose with the Amazon Kinesis Data Stream to increase the capacity of the stream is incorrect because Data Firehose just provides a way to reliably transform and load streaming data into data stores and analytics tools. This method will not increase the capacity of the stream as it doesn't mention anything about resharding. References: https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/ Kinesis Scaling, Resharding and Parallel Processing: https://tutorialsdojo.com/kinesis-scaling-resharding-and-parallel-processing/"])</script><script>self.__next_f.push([1,"42:T7bb,You can use the API Gateway Import API feature to import a REST API from an external definition file into API Gateway. Currently, the Import API feature supports OpenAPI v2.0 and OpenAPI v3.0 definition files. You can update an API by overwriting it with a new definition or merge a definition with an existing API. You specify the options using a mode query parameter in the request URL. You can paste a Swagger API definition in the AWS Console to create a new API and populate it with the resources and methods from your Swagger or OpenAPI definition, just as shown below: You can also import your Swagger definition through the AWS CLI and SDKs. Hence, the correct answer in this scenario is to import their Swagger or OpenAPI definitions to API Gateway using the AWS Console. Using CodeDeploy to migrate and deploy the company's web services to API Gateway is incorrect because using CodeDeploy alone is not enough to deploy new custom APIs. This is mainly used in conjunction with AWS SAM where you can add deployment preferences to manage the way traffic is shifted during an AWS Lambda application deployment. Using AWS SAM to migrate and deploy the company's web services to API Gateway is incorrect. Although using AWS SAM is the preferred way to deploy your serverless application, it is not the easiest way to import the Swagger API definitions file. As mentioned above, you can simply import Swagger or OpenAPI files directly to AWS. Creating models and templates for request and response mappings based on the company's API definitions is incorrect because this is primarily done for API Gateway integration to other services and not for importing API definitions file. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-import-api.html https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-from-example.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/43:T878,"])</script><script>self.__next_f.push([1,"An AppSpec file is a YAML or JSON formatted file used by AWS CodeDeploy to manage and direct the deployment process. It provides instructions regarding how the deployment service should handle application content and lifecycle event hooks. Depending on the type of deployment, the content and structure of the AppSpec file will differ, tailored to the specific requirements and nature of each service. For ECS deployments, the resources section specifies the Amazon ECS service to deploy and has the following structure: Out of these properties, only the TaskDefinition, ContainerName, ContainerPort are required by CodeDeploy for ECS deployments. TaskDefinition - This is the task definition for the Amazon ECS service to deploy. It is specified with the ARN of the task definition. ContainerName - This is the name of the Amazon ECS container that contains your Amazon ECS application. It must be a container specified in your Amazon ECS task definition. ContainerPort - This is the port on the container where traffic will be routed. Hence, the correct answers are: - TaskDefinition - ContainerName - ContainerPort targetversion is incorrect. This property is specifically related to Lambda deployments when using CodeDeploy. It specifies which version of a Lambda function to route traffic to. However, for ECS deployments, it's not a required property in the AppSpec file. alias is incorrect because this property is only relevant to Lambda deployments using CodeDeploy. In AWS Lambda, an alias is a pointer to a specific Lambda function version. In the context of ECS deployments, this property is not required in the AppSpec file. NetworkConfiguration is incorrect. While it can be included in the AppSpec file for ECS deployments using CodeDeploy, it's just optional. If not specified, the ECS service's current network configuration is used. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-resources.html https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-example.html#appspec-file-example-ecs Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/"])</script><script>self.__next_f.push([1,"44:Tf26,"])</script><script>self.__next_f.push([1,"The visibility timeout is a period of time during which Amazon SQS prevents other consuming components from receiving and processing a message. When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn't automatically delete the message. Because Amazon SQS is a distributed system, there's no guarantee that the consumer actually receives the message (for example, due to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must delete the message from the queue after receiving and processing it. Immediately after the message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The maximum is 12 hours. The visibility timeout begins when Amazon SQS returns a message. During this time, the consumer processes and deletes the message. However, if the consumer fails before deleting the message and your system doesn't call the DeleteMessage action for that message before the visibility timeout expires, the message becomes visible to other consumers and the message is received again. If a message must be received only once, your consumer should delete it within the duration of the visibility timeout. Every Amazon SQS queue has the default visibility timeout setting of 30 seconds. You can change this setting for the entire queue. Typically, you should set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue. When receiving messages, you can also set a special visibility timeout for the returned messages without changing the overall queue timeout. Hence, the best solution in this scenario is to set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue. Configuring the queue to use short polling by setting the WaitTimeSeconds parameter of the ReceiveMessage request to 0 is incorrect. Although the implementation steps for short polling is accurate, this is not enough to keep other consumers from processing the undeleted message that became available again in the queue. This is just the default configuration of SQS that queries only a subset of its servers (based on a weighted random distribution), to determine whether any messages are available for a response. Hence, this is irrelevant in this scenario. Configuring the queue to use long polling by setting the Receive Message Wait Time parameter to a value greater than 0 is incorrect. Although the implementation steps for long polling is accurate, this configuration just helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). A more appropriate solution in this scenario is to configure the visibility timeout of the messages. Postponing the delivery of new messages by using a delay queue is incorrect. Although a visibility timeout and delay queue are almost the same, there are still some key differences between these two in the scenario which warrants the use of the former rather than the latter. For delay queues, a message is hidden when it is first added to queue, whereas for visibility timeouts, a message is hidden only after it is consumed from the queue which is what the scenario depicts. References: https://aws.amazon.com/sqs/faqs/ https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html Check out this Amazon SQS Cheat Sheet: https://tutorialsdojo.com/amazon-sqs/"])</script><script>self.__next_f.push([1,"45:T90b,"])</script><script>self.__next_f.push([1,"Unlike standard queues, FIFO queues don't introduce duplicate messages. FIFO queues help you avoid sending duplicates to a queue. If you retry the SendMessage action within the 5-minute deduplication interval, Amazon SQS doesn't introduce any duplicates into the queue. To configure deduplication, you must do one of the following: - Enable content-based deduplication. This instructs Amazon SQS to use a SHA-256 hash to generate the message deduplication ID using the body of the message - but not the attributes of the message. - Explicitly provide the message deduplication ID (or view the sequence number) for the message. The message deduplication ID is the token used for deduplication of sent messages. If a message with a particular message deduplication ID is sent successfully, any messages sent with the same message deduplication ID are accepted successfully but aren't delivered during the 5-minute deduplication interval. Message deduplication applies to an entire queue, not to individual message groups. Amazon SQS continues to keep track of the message deduplication ID even after the message is received and deleted. Hence, the correct answer in this scenario is to use a FIFO (First-In-First-Out) Queue and provide the Message Deduplication ID for each message. Using a FIFO (First-In-First-Out) Queue by disabling the content-based deduplication is incorrect. Although the use of FIFO queue is valid, it is wrong to disable the content-based deduplication. This should be enabled to avoid duplicate messages in the queue. Using a Standard Queue and providing the Message Group ID for each message is incorrect because you should use a FIFO queue instead to avoid duplicate messages. Using a Standard Queue and providing the Message Deduplication ID for each message is incorrect. Although it is a valid answer to provide the Message Deduplication ID, this feature can't be enabled for Standard Queues. You have to use the FIFO queues instead for this scenario. References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html#FIFO-queues-exactly-once-processing https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html Check out this Amazon SQS Cheat Sheet: https://tutorialsdojo.com/amazon-sqs/"])</script><script>self.__next_f.push([1,"46:Td22,"])</script><script>self.__next_f.push([1,"When a task that uses the EC2 launch type is launched, Amazon ECS must determine where to place the task based on the requirements specified in the task definition, such as CPU and memory. Similarly, when you scale down the task count, Amazon ECS must determine which tasks to terminate. You can apply task placement strategies and constraints to customize how Amazon ECS places and terminates tasks. By default, tasks are randomly placed with RunTask or spread across Availability Zones with CreateService. Spread is typically used to achieve high availability by making sure that multiple copies of a task are scheduled across multiple instances based on attributes such as Availability Zones. A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service. Amazon ECS supports the following task placement strategies: binpack - Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use. random - Place tasks randomly. spread - Place tasks evenly based on the specified value. Accepted values are attribute key-value pairs, instanceId, or host. The spread strategy, contrary to the binpack strategy, tries to put your tasks on as many different instances as possible. It is typically used to achieve high availability and mitigate risks, by making sure that you don’t put all your task-eggs in the same instance-baskets. Spread across Availability Zones, therefore, is the default placement strategy used for services. When using the spread strategy, you must also indicate a field parameter. It is used to indicate the bins that you are considering. The accepted values are instanceID, host, or a custom attribute key:value pairs such as attribute:ecs.availability-zone to balance tasks across zones. There are several AWS attributes that start with the ecs prefix, but you can be creative and create your own attributes. Hence, the task placement configuration which has a value of \"field\": \"attribute:ecs.availability-zone\", \"type\": \"spread\" is correct, because this is using the appropriate strategy for task placement. The configuration which has a value of \"field\": \"instanceId\", \"type\": \"spread\" is incorrect because although it is using a spread task placement strategy, it distributes tasks evenly across all instances and not to Availability Zones. The configuration which has a value of \"field\": \"memory\", \"type\": \"binpack\" is incorrect because this is using a strategy that bin packs tasks based on memory. Take note that the scenario is asking for a configuration which will evenly place the tasks across multiple Availability Zones, and not based on memory. The configuration which has a value of \"type\": \"random\" is incorrect because this will just place the tasks on instances randomly. You have to use the spread task placement strategy instead to meet the requirements of the scenario. References: https://aws.amazon.com/blogs/compute/amazon-ecs-task-placement/ https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html Check out this Amazon ECS Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/"])</script><script>self.__next_f.push([1,"47:T91f,"])</script><script>self.__next_f.push([1,"You can use AWS Identity and Access Management (IAM) to grant X-Ray permissions to users and compute resources in your account. IAM controls access to the X-Ray service at the API level to enforce permissions uniformly, regardless of which client (console, AWS SDK, AWS CLI) your users employ. To use the X-Ray console to view service maps and segments, you only need read permissions. To enable console access, add the AWSXrayReadOnlyAccess managed policy to your IAM user. For local development and testing, create an IAM user with read and write permissions. Generate access keys for the user and store them in the standard AWS SDK location. You can use these credentials with the X-Ray daemon, the AWS CLI, and the AWS SDK. To deploy your instrumented app to AWS, create an IAM role with write permissions and assign it to the resources running your application. AWSXRayDaemonWriteAccess includes permission to upload traces and some read permissions as well to support the use of sampling rules. The read and write policies do not include permission to configure encryption key settings and sampling rules. Use AWSXrayFullAccess to access these settings or add configuration APIs in a custom policy. For encryption and decryption with a customer managed key that you create, you also need permission to use the key. Hence, the AWSXrayReadOnlyAccess managed policy is the most appropriate one to grant to the developer in order for her to access the X-Ray console. This also abides with the standard security advice of granting least privilege, or granting only the permissions required to perform a task. AWSXRayDaemonWriteAccess is incorrect because this policy is more suitable if you want to grant permission to upload traces to X-Ray. AWSXrayFullAccess is incorrect. Although this can provide the required access to the developer, it does not abide with the standard security advice of granting least privilege. Hence, this is not the most appropriate policy to use. AmazonS3ReadOnlyAccess is incorrect because this policy just provides the instance permission to download the X-Ray daemon from Amazon S3. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-permissions.html https://docs.aws.amazon.com/xray/latest/devguide/security.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/"])</script><script>self.__next_f.push([1,"48:T830,"])</script><script>self.__next_f.push([1,"To create, update, or delete an item in a DynamoDB table, use one of the following operations: - PutItem - UpdateItem - DeleteItem For each of these operations, you need to specify the entire primary key, not just part of it. For example, if a table has a composite primary key (partition key and sort key), you must supply a value for the partition key and a value for the sort key. To return the number of write capacity units consumed by any of these operations, set the ReturnConsumedCapacity parameter to one of the following: TOTAL — returns the total number of write capacity units consumed. INDEXES — returns the total number of write capacity units consumed, with subtotals for the table and any secondary indexes that were affected by the operation. NONE — no write capacity details are returned. (This is the default.) Hence, the correct answer is to add the ReturnConsumedCapacity parameter with a value of INDEXES in every write request. Adding the ReturnValues parameter with a value of INDEXES in every write request is incorrect because you should use a ReturnConsumedCapacity parameter instead. Adding the ReturnConsumedCapacity parameter with a value of TOTAL in every write request is incorrect because this will not return the consumed WCU subtotals for the table and any secondary indexes that were affected by the operation just as what is required by the application. You have to use INDEXES instead. Adding the ReturnValues parameter with a value of TOTAL in every write request is incorrect because you should use a ReturnConsumedCapacity parameter instead. In addition, the value of the parameter is also incorrect as it doesn't return the consumed WCU subtotals for the table and any secondary indexes that were affected by the operation. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.WritingData https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_PutItem.html#API_PutItem_RequestParameters Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"])</script><script>self.__next_f.push([1,"49:Teb3,"])</script><script>self.__next_f.push([1,"You can use the existing Parameters section of your CloudFormation template to define Systems Manager parameters, along with other parameters. Systems Manager parameters are a unique type that is different from existing parameters because they refer to actual values in the Parameter Store. The value for this type of parameter would be the Systems Manager (SSM) parameter key instead of a string or other value. CloudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation. If the parameter referenced in the template does not exist in Systems Manager, there will be synchronous validation error that will be thrown. Also, if you have defined any parameter value validations (AllowedValues, AllowedPattern, etc.) for Systems Manager parameters, they will be performed against SSM keys which are given as input values for template parameters, not actual values stored in Systems Manager. Parameters stored in Systems Manager are mutable. Any time you use a template containing Systems Manager parameters to create/update your stacks, CloudFormation uses the values for these Systems Manager parameters at the time of the create/update operation. So, as parameters are updated in Systems Manager, you can have the new value of the parameter take effect by just executing a stack update operation. The Parameters section in the output for Describe API will show an additional ‘ResolvedValue’ field that contains the resolved value of the Systems Manager parameter that was used for the last stack operation. Hence, the correct answer is to set up CloudFormation with Systems Manager Parameter Store to retrieve the latest AMI IDs for your template. Whenever you decide to update the EC2 instances, call the update-stack API in CloudFormation in your CloudFormation template. The option that says: Set up your Systems Manager State Manager to store the latest AMI IDs and integrate it with your CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template is incorrect because the Systems Manager State Manager service simply automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. This can't be used as a parameter store that refers to the latest AMI of your application. The option that says: Integrate AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments is incorrect because AWS Service Catalog is not suitable in this scenario since this service just allows organizations to create and manage catalogs of IT services that are approved for use on AWS. In addition, AWS Config is simply a service that enables you to assess, audit, and evaluate the configurations of your AWS resources, which clearly is irrelevant in this case as the developer won't be able to use this to store the latest AMI IDs. The option that says: Integrate CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments is incorrect because, just as mentioned above, the AWS Service Catalog just allows organizations to create and manage catalogs of IT services that are approved for use on AWS. A more appropriate solution for this scenario would be to use the Systems Manager Parameter Store to retrieve the latest AMI IDs for your template. References: https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/ https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html Check out this AWS Systems Manager Cheat Sheet: https://tutorialsdojo.com/aws-systems-manager/"])</script><script>self.__next_f.push([1,"4a:T8f2,"])</script><script>self.__next_f.push([1,"Amazon CloudWatch agent enables you to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. The agent supports both Windows Server and Linux and allows you to select the metrics to be collected, including sub-resource metrics such as per-CPU core. Metrics are data about the performance of your systems. By default, several services provide free metrics for resources (such as Amazon EC2 instances, Amazon EBS volumes, and Amazon RDS DB instances). You can also enable detailed monitoring on some resources, such as your Amazon EC2 instances, or publish your own application metrics. Amazon CloudWatch can load all the metrics in your account (both AWS resource metrics and application metrics that you provide) for search, graphing, and alarms. However, take note that CloudWatch does not monitor the memory, swap, and disk space utilization of your instances. If you need to track these metrics, you can install a CloudWatch agent in your EC2 instances. Hence, the correct answer is: CloudWatch does not track memory utilization by default. The option that says: The detailed monitoring is not enabled in CloudWatch is incorrect because this will just send metric data for your instance to CloudWatch in 1-minute periods, but not including the memory utilization. The option that says: X-Ray Daemon is not installed to the EC2 instances is incorrect because X-Ray is primarily used for troubleshooting applications and not to monitor the actual EC2 instances. Even if the X-Ray Daemon is installed and is running in the instance, it will still not send the memory utilization metrics to CloudWatch. The option that says: The .ebextensions/xray-daemon.config file in Elastic Beanstalk is missing is incorrect because just like what is mentioned above, X-Ray will not send the memory utilization of the EC2 instance. References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html Check out these Amazon EC2 and CloudWatch Cheat Sheets: https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/ https://tutorialsdojo.com/amazon-cloudwatch/"])</script><script>self.__next_f.push([1,"4b:T1179,"])</script><script>self.__next_f.push([1,"The SDKs included with X-Ray are part of a tightly integrated instrumentation solution offered by AWS. The AWS Distro for OpenTelemetry is part of a broader industry solution in which X-Ray is only one of many tracing solutions. You can implement end-to-end tracing in X-Ray using either approach, but it’s important to understand the differences in order to determine the most useful approach for you. It is recommended to instrument your application with the AWS Distro for OpenTelemetry if you need the following: -The ability to send traces to multiple different tracing backends without having to re-instrument your code -Support for a large number of library instrumentations for each language, maintained by the OpenTelemetry community -Fully managed Lambda layers that package everything you need to collect telemetry data without requiring code changes when using Java, Python, or Node.js Conversely, it is recommended to choose an X-Ray SDK for instrumenting your application if you need the following: -A tightly integrated single-vendor solution -Integration with X-Ray centralized sampling rules, including the ability to configure sampling rules from the X-Ray console and automatically use them across multiple hosts, when using Node.js, Python, Ruby, or .NET An account alias substitutes for an account ID in the web address for your account. You can create and manage an account alias from the AWS Management Console, AWS CLI, or AWS API. Your sign-in page URL has the following format by default: https://Your_AWS_Account_ID.signin.aws.amazon.com/console/ If you create an AWS account alias for your AWS account ID, your sign-in page URL looks like the following example. https://Your_Alias.signin.aws.amazon.com/console/ The original URL containing your AWS account ID remains active and can be used after you create your AWS account alias. For example, the following create-account-alias command creates the alias tutorialsdojo for your AWS account: aws iam create-account-alias --account-alias tutorialsdojo Hence, for this scenario, the correct answer is: Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls. The option that says: Use the https://061218980612.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure the Amazon CloudWatch Evidently to trace all the downstream API calls is incorrect because Amazon CloudWatch Evidently is not capable of tracing any API calls. This particular service is used to safely validate your new features by serving them to a specified percentage of your users while you roll out the feature. The option that says: Use the https://finance-dept.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the downstream API calls is incorrect because the AWS Identity and Access Management (IAM) Roles Anywhere is mainly used to bridge the trust model of IAM and Public Key Infrastructure (PKI) but not for tracing the downstream call. The model connects the role, the IAM Roles Anywhere service principal, and identities encoded in X509 certificates, that are issued by a Certificate Authority (CA). The option that says: Use the https://finance-dept.aws.amazon.com/console sign-in page URL for the AWS account. Install and configure the AWS X-Ray auto-instrumentation Java agent to trace all the downstream API calls is incorrect. Although it is right that the AWS X-Ray auto-instrumentation agent for Java is capable of providing a tracing solution that instruments your Java web applications with minimal development effort, it still doesn't have the ability to send traces to multiple different tracing backends without having to re-instrument the application. A more suitable option is to set up the AWS Distro for OpenTelemetry. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-instrumenting-your-app.html#xray-instrumenting-choosing https://docs.aws.amazon.com/IAM/latest/UserGuide/console_account-alias.html https://docs.aws.amazon.com/cli/latest/reference/iam/create-account-alias.html Check out this AWS IAM and AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/ https://tutorialsdojo.com/aws-x-ray/"])</script><script>self.__next_f.push([1,"4c:Tc6e,"])</script><script>self.__next_f.push([1,"Optimistic locking is a strategy to ensure that the client-side item that you are updating (or deleting) is the same as the item in DynamoDB. Optimistic concurrency depends on checking a value upon save to ensure that it has not changed. If you use this strategy, then your database writes are protected from being overwritten by the writes of others — and vice-versa. By default, the DynamoDB write operations (PutItem, UpdateItem, DeleteItem) are unconditional: each of these operations will overwrite an existing item that has the specified primary key. DynamoDB optionally supports conditional writes for these operations. A conditional write will succeed only if the item attributes meet one or more expected conditions. Otherwise, it returns an error. Conditional writes are helpful in many situations. For example, you might want a PutItem operation to succeed only if there is not already an item with the same primary key. Or you could prevent an UpdateItem operation from modifying an item if one of its attributes has a certain value. Conditional writes are helpful in cases where multiple users attempt to modify the same item. In the AWS SDK for PHP, there is a PessimisticLockingStrategy class for DynamoDB. This locking strategy uses pessimistic locking (similar to how the native PHP session handler works) to ensure that sessions are not edited while another process is reading/writing to it. Pessimistic locking can be expensive and can increase latencies, especially in cases where the user can access the session more than once at the same time (e.g. ajax, iframes, or multiple browser tabs). Hence, using a combination of optimistic locking and conditional writes is the correct answer in this scenario. Using a combination of pessimistic concurrency and conditional writes is incorrect as this will just prevent a value from being updated by locking the item or row in the database. This can block users from reading, updating, or deleting an entry depending on the lock type which is not suitable for the multithreaded application. You have to use optimistic locking strategy and conditional writes instead. Using batch operations is incorrect because this will just reduce the number of network round trips when reading or writing multiple items from your application to DynamoDB. This will not improve the concurrency of your multithreaded application. Using atomic counters is incorrect because this is just a numeric attribute that is unconditionally incremented without interfering with other write requests. The more suitable solution to use in this solution is conditional writes. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBContext.VersionSupport.htm https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html https://docs.aws.amazon.com/aws-sdk-php/v2/api/class-Aws.DynamoDb.Session.LockingStrategy.PessimisticLockingStrategy.html Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"])</script><script>self.__next_f.push([1,"4d:Tae7,"])</script><script>self.__next_f.push([1,"You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled. A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the Cache-Control: max-age=0 header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint. To grant permission for a client, attach a policy of the following format to an IAM execution role for the user: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"execute-api:InvalidateCache\" ], \"Resource\": [ \"arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier\" ] } ]} Hence, the option which has the same policy shown above is the correct answer since attaching the policy that allows the action \"execute-api:InvalidateCache\" to your IAM execution role will allow the API Gateway execution service to invalidate the cache results for requests on the specified resource (or resources). In the list of options, you have to take a look at the values of the \"Effect\" and \"Action\" fields. The option which has a policy of \"Effect\": \"Deny\", \"Action\": [\"execute-api:InvalidateCache\"] is incorrect because this will deny any request to invalidate cache results in API Gateway. The option which has a policy of \"Effect\": \"Deny\", \"Action\": [\"execute-api:*\"] is incorrect because this will deny all requests to your API Gateway such as API invocation, cache invalidation, and all other actions. The option which has a policy of \"Effect\": \"Allow\", \"Action\": [\"execute-api:Invoke\"] is incorrect because this will just allow API invocation requests in API Gateway and not cache invalidation requests. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#override-api-gateway-stage-cache-for-method-cache https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-control-access-using-iam-policies-to-invoke-api.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"])</script><script>self.__next_f.push([1,"4e:Ta2b,"])</script><script>self.__next_f.push([1,"Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type. You can also use Elastic Beanstalk to host Docker applications in AWS. It is an application management platform that helps customers easily deploy and scale web applications and services. It keeps the provisioning of building blocks (e.g., EC2, RDS, Elastic Load Balancing, Auto Scaling, CloudWatch), deployment of applications, and health monitoring abstracted from the user so they can just focus on writing code. You simply specify which container images are to be deployed, the CPU and memory requirements, the port mappings, and the container links. Elastic Beanstalk will automatically handle all the details such as provisioning an Amazon ECS cluster, balancing load, auto-scaling, monitoring, and placing your containers across your cluster. Elastic Beanstalk is ideal if you want to leverage the benefits of containers but just want the simplicity of deploying applications from development to production by uploading a container image. You can work with Amazon ECS directly if you want more fine-grained control for custom application architectures. Hence, the correct answer in this scenario is ECS. Elastic Beanstalk is incorrect. Although it can be used to host Docker applications, it is ideal to be used if you want the simplicity of deploying applications from development to production by uploading a container image. It does not provide fine-grained control for custom application architectures unlike ECS. AWS SAM is incorrect because the AWS Serverless Application Model (AWS SAM) is just an open-source framework that you can use to build serverless applications on AWS and not to host Docker applications. EC2 is incorrect. Although you can run Docker in your EC2 instances, it does not provide a highly scalable, fast, container management service in comparison to ECS. Take note that in itself, EC2 is not scalable and should be paired with Auto Scaling and ELB. References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html https://aws.amazon.com/ecs/faqs/ Check out this Amazon ECS Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/"])</script><script>self.__next_f.push([1,"5:[\"$\",\"$Le\",null,{\"quiz\":{\"id\":\"aws-developer-10\",\"title\":\"AWS Certified Developer Associate Practice Exams 4\",\"description\":\"Additional practice questions covering AWS development topics.\",\"questions\":[{\"question\":\"A web application running in Amazon Elastic Beanstalk reads and writes a large number of related items in DynamoDB and processes each item one at a time. The network overhead of these transactions causes degradation in the application's performance. You were instructed by your manager to quickly refactor the application but without introducing major code changes such as implementing concurrency management or multithreading. Which of the following solutions is the EASIEST method to implement that will improve the application performance in a cost-effective manner?\",\"answers\":[{\"text\":\"Use DynamoDB Batch Operations API for GET, PUT, and DELETE operations.\",\"isCorrect\":true},{\"text\":\"Upgrade the EC2 instances to a higher instance type.\",\"isCorrect\":false},{\"text\":\"Enable DynamoDB Streams.\",\"isCorrect\":false},{\"text\":\"Refactor the application to use DynamoDB transactional read and write APIs .\",\"isCorrect\":false}],\"explanation\":\"$f\"},{\"question\":\"There is a requirement to improve the performance of your serverless application in AWS by increasing the allocated CPU available for your Lambda functions. Which of the following is the MOST appropriate solution that you should implement to meet this requirement?\",\"answers\":[{\"text\":\"Increase the memory configuration of your function.\",\"isCorrect\":true},{\"text\":\"Use Lambda layers to optimize the performance of the Lambda function.\",\"isCorrect\":false},{\"text\":\"Manually configure the CPU settings of the Lambda function to the maximum value.\",\"isCorrect\":false},{\"text\":\"Configure the concurrency limit of your function to be the same as the account level concurrency limit.\",\"isCorrect\":false}],\"explanation\":\"$10\"},{\"question\":\"A cryptocurrency exchange portal has a key management service hosted in their on-premises data center, which stores encryption keys and uses an RSA asymmetric encryption algorithm. The company has recently implemented a hybrid cloud architecture in AWS and you were assigned to migrate the exchange portal to their cloud infrastructure. For security compliance, the keys should be stored in dedicated, third-party validated hardware security modules under your exclusive control. Which of the following is the BEST solution that you should implement to meet the above requirement?\",\"answers\":[{\"text\":\"Import the encryption keys from your on-premises key management service to AWS CloudHSM.\",\"isCorrect\":true},{\"text\":\"Import the encryption keys from your on-premises key management service to AWS Secrets Manager as KMS Keys.\",\"isCorrect\":false},{\"text\":\"Use AWS KMS to store and manage the encryption keys.\",\"isCorrect\":false},{\"text\":\"Develop a custom key management service using the AWS Encryption SDK.\",\"isCorrect\":false}],\"explanation\":\"$11\"},{\"question\":\"A company has a static website running in an Auto Scaling group of EC2 instances which they want to convert as a dynamic e-commerce web portal. One of the requirements is to use HTTPS to improve the security of their portal and also improve their search ranking as a reputable and secure site. A developer recently requested an SSL/TLS certificate from a third-party certificate authority (CA) which is ready to be imported to AWS. Which of the following services can the developer use to safely import the SSL/TLS certificate? (Select TWO.)\",\"answers\":[{\"text\":\"IAM certificate store\",\"isCorrect\":true},{\"text\":\"AWS Certificate Manager\",\"isCorrect\":true},{\"text\":\"Amazon Cognito\",\"isCorrect\":false},{\"text\":\"CloudFront\",\"isCorrect\":false},{\"text\":\"A private S3 bucket with versioning enabled\",\"isCorrect\":false}],\"explanation\":\"$12\"},{\"question\":\"A serverless application, which uses a Lambda function integrated with API Gateway, provides data to a front-end application written in ReactJS. The users are complaining that they are getting HTTP 504 errors intermittently when they are using the application in peak times. The developer found no errors in the CloudWatch logs of the Lambda function. Which of the following is the MOST likely cause of this issue?\",\"answers\":[{\"text\":\"There is an authorization failure occurring between API Gateway and the Lambda function.\",\"isCorrect\":false},{\"text\":\"The API Gateway automatically enabled throttling in peak times which caused the HTTP 504 errors.\",\"isCorrect\":false},{\"text\":\"The memory allocated for the Lambda function is insufficient\",\"isCorrect\":false},{\"text\":\"The underlying Lambda function has been running for more than 29 seconds causing the API Gateway request to time out.\",\"isCorrect\":true}],\"explanation\":\"$13\"},{\"question\":\"A developer is creating a React application whose source code is hosted in GitHub. To help ensure proper functionality and identify any UI issues before going live, the developer must perform end-to-end (E2E) testing using Cypress. Which combination of actions should the developer take? (Select Two)\",\"answers\":[{\"text\":\"Update the amplify.yml file with appropriate configuration settings for Cypress.\",\"isCorrect\":true},{\"text\":\"Update the amplifyconfiguration.json with appropriate configuration settings for Cypress.\",\"isCorrect\":false},{\"text\":\"Include the location of the Cypress configuration file in the aws-exports.js file.\",\"isCorrect\":false},{\"text\":\"Create an application in AWS Amplify Studio. Clone the application’s source code in a local environment and run amplify pull --appId APP_ID --envName ENV_NAME\",\"isCorrect\":false},{\"text\":\"Connect the Github repository to AWS Amplify Hosting\",\"isCorrect\":true}],\"explanation\":\"$14\"},{\"question\":\"A developer runs a shell script that uses the aws s3 cp CLI to upload a large file to an S3 bucket. The S3 bucket is configured with Server-side encryption with AWS Key Management Service (SSE-KMS). An Access Denied error always shows up whenever the developer uploads a file with a size of 100 GB or more. However, whenever he uploads a smaller file, the request succeeds. Which of the following are possible reasons why this issue is happening? (Select TWO.)\",\"answers\":[{\"text\":\"The developer does not have the kms:Encrypt permission.\",\"isCorrect\":false},{\"text\":\"The AWS CLI S3 commands perform a multipart upload when the file is large.\",\"isCorrect\":true},{\"text\":\"The developer's IAM permission has an attached inline policy that restricts him from uploading a file to S3 with a size of 100 GB or more.\",\"isCorrect\":false},{\"text\":\"The maximum size that can be encrypted in KMS is only 100 GB.\",\"isCorrect\":false},{\"text\":\"The developer does not have the kms:Decrypt permission.\",\"isCorrect\":true}],\"explanation\":\"$15\"},{\"question\":\"A company has an application that is using CloudFront to serve their static contents to their users around the globe. They are receiving a number of bad reviews from their customers lately because it takes a lot of time to log into their website. Sometimes, their users are also getting HTTP 504 errors which is why the developer was instructed to fix this problem immediately. Which of the following combination of options should the developer use together to set up a cost-effective solution for this scenario? (Select TWO.)\",\"answers\":[{\"text\":\"Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your Lambda functions to execute the authentication process in AWS locations closer to the users.\",\"isCorrect\":true},{\"text\":\"Launch your application to multiple AWS regions to serve your global users. Use a Route 53 record with latency routing policy to route incoming traffic to the region with the best latency to the user.\",\"isCorrect\":false},{\"text\":\"Add a Cache-Control max-age directive to your objects in CloudFront and specify the longest practical value for max-age to increase the cache hit ratio of your CloudFront distribution.\",\"isCorrect\":false},{\"text\":\"Configure an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses.\",\"isCorrect\":true},{\"text\":\"Launch your application to multiple and geographically disperse VPCs on various AWS regions then create a transit VPC to easily connect all your resources. Use several Lambda functions in each region using the AWS Serverless Application Model (SAM) service to improve the overall application performance.\",\"isCorrect\":false}],\"explanation\":\"$16\"},{\"question\":\"An application architect manages several AWS accounts for staging, testing, and production environments, which are used by several development teams. For application deployments, the developers use the similar base CloudFormation template for their applications. Which of the following can allow the developer to effectively manage the updates on this template across all AWS accounts with minimal effort?\",\"answers\":[{\"text\":\"Update the stacks on multiple AWS accounts using CloudFormation StackSets.\",\"isCorrect\":true},{\"text\":\"Define and manage stack instances on multiple AWS Accounts using CloudFormation Stack Instances.\",\"isCorrect\":false},{\"text\":\"Create and manage stacks on multiple AWS accounts using CloudFormation Change Sets.\",\"isCorrect\":false},{\"text\":\"Use AWS CodePipeline to automate the deployment of CloudFormation templates across multiple accounts.\",\"isCorrect\":false}],\"explanation\":\"$17\"},{\"question\":\"Your team is developing a serverless application, which is composed of multiple Lambda functions which process data from an SQS queue and stores the results to an RDS database. To comply with the strict IT policy of the company, you were instructed to configure these functions to share the same connection string that should be properly secured and encrypted. What should you do to protect, encrypt, and share your database credentials in AWS?\",\"answers\":[{\"text\":\"Store the database credentials as environment variables with KMS encryption which will be shared by the Lambda functions.\",\"isCorrect\":false},{\"text\":\"Use AWS Systems Manager Parameter Store as a Secure String Parameter.\",\"isCorrect\":true},{\"text\":\"Use IAM DB Authentication in RDS to allow encrypted connections from each Lambda function.\",\"isCorrect\":false},{\"text\":\"Encrypt the database credentials and store them in an S3 bucket which the Lambda functions can fetch.\",\"isCorrect\":false}],\"explanation\":\"$18\"},{\"question\":\"You are developing an application that continuously collects data about player-game interactions and feeds the real-time data into your gaming platform. There is a requirement to make the system highly scalable to accommodate the sudden influx of gamers that will use the platform. Which AWS service will help you achieve this?\",\"answers\":[{\"text\":\"AWS Kinesis Data Stream\",\"isCorrect\":true},{\"text\":\"AWS Elastic Map Reduce\",\"isCorrect\":false},{\"text\":\"AWS Redshift\",\"isCorrect\":false},{\"text\":\"AWS DynamoDB\",\"isCorrect\":false}],\"explanation\":\"Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more. All other options are incorrect because these services do not provide real-time processing, unlike Kinesis. References: https://aws.amazon.com/kinesis/data-streams/ https://aws.amazon.com/kinesis/data-streams/faqs/ Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/\"},{\"question\":\"A company is developing an online system that lets patients schedule appointments with their preferred doctors at medical centers all over the country. The company uses Amazon DynamoDB as its primary database. The DynamoDB Streams feature is enabled on the DynamoDB table to capture all changes made to the booking data. A Lambda function integrated with Amazon EventBridge (Amazon CloudWatch Events) is used to process the data stream every 36 hours and then store the results in an S3 bucket. There are a lot of updated items in DynamoDB that are not sent to the S3 bucket, even though there are no errors in the logs. Which of the following is the MOST appropriate solution for this issue?\",\"answers\":[{\"text\":\"Set the value of StreamViewType parameter in DynamoDB Streams to NEW_IMAGE.\",\"isCorrect\":false},{\"text\":\"Set the value of StreamViewType parameter in DynamoDB Streams to NEW_AND_OLD_IMAGES.\",\"isCorrect\":false},{\"text\":\"Decrease the interval of running your function to 24 hours.\",\"isCorrect\":true},{\"text\":\"Increase the interval of running your function to 48 hours.\",\"isCorrect\":false}],\"explanation\":\"$19\"},{\"question\":\"A company is developing a real-time analytics platform that allows users to submit data through the Amazon API Gateway. The data is processed by AWS Lambda and stored in Amazon S3. However, during high-traffic periods, users experience timeouts despite the Lambda function completing tasks on time. The company needs to analyze API Gateway metrics in Amazon CloudWatch to determine the cause of these timeouts. Which two API Gateway metrics in Amazon CloudWatch should be reviewed to troubleshoot the delay issues? (Select TWO.)\",\"answers\":[{\"text\":\"5XXError\",\"isCorrect\":false},{\"text\":\"Count\",\"isCorrect\":false},{\"text\":\"IntegrationLatency\",\"isCorrect\":true},{\"text\":\"Latency\",\"isCorrect\":true},{\"text\":\"4XXError\",\"isCorrect\":false}],\"explanation\":\"$1a\"},{\"question\":\"A web application is currently deployed on an On-Demand Linux EC2 instance that connects to an Amazon RDS database. Users have frequently reported that the application crashes intermittently. The support team has reviewed the logs in CloudWatch but has been unable to identify the root cause. To enhance troubleshooting, the team needs to monitor additional metrics, including memory utilization, swap usage, and the count of idle and active processes running on the instance. Which solution would be the MOST appropriate to implement in this situation?\",\"answers\":[{\"text\":\"Install the Amazon CloudWatch Logs agent to the EC2 instance.\",\"isCorrect\":true},{\"text\":\"Install the AWS X-Ray daemon on the EC2 instance.\",\"isCorrect\":false},{\"text\":\"Use detailed monitoring in CloudWatch.\",\"isCorrect\":false},{\"text\":\"Use AWS CloudShell to consolidate all metrics in a single dashboard.\",\"isCorrect\":false}],\"explanation\":\"$1b\"},{\"question\":\"You are developing a new batch job for the enterprise application suite in your company, which is hosted in an Auto Scaling group of EC2 instances behind an ELB. The application is using an S3 bucket configured with Server-Side Encryption with AWS KMS Keys (SSE-KMS). The batch job must upload files to the bucket using the default AWS KMS key to protect the data at rest. What should you do to satisfy this requirement with the LEAST amount of configuration?\",\"answers\":[{\"text\":\"Include the x-amz-server-side-encryption header with a value of aws:kms as well as the x-amz-server-side-encryption-aws-kms-key-id header containing the ID of the default AWS KMS key in your upload request.\",\"isCorrect\":false},{\"text\":\"Include the x-amz-server-side​-encryption​-customer-algorithm, x-amz-server-side-encryption-customer-key, and x-amz-server-side-encryption-customer-key-MD5 headers with appropriate values in the upload request.\",\"isCorrect\":false},{\"text\":\"Include the x-amz-server-side-encryption header with a value of AES256 in your upload request.\",\"isCorrect\":false},{\"text\":\"Include the x-amz-server-side-encryption header with a value of aws:kms in your upload request.\",\"isCorrect\":true}],\"explanation\":\"$1c\"},{\"question\":\"A company is developing a distributed system which will use a Lambda function that will be invoked asynchronously. In the event of failure, the function must be retried twice before sending the unprocessed events to an Amazon SQS queue through the use of Dead Letter Queue (DLQ). Which of the following is the correct way to implement a DLQ in Lambda?\",\"answers\":[{\"text\":\"Specify the Amazon Resource Name of the SQS Queue in the Lambda function's DeadLetterConfig parameter.\",\"isCorrect\":true},{\"text\":\"Specify the AWS Service Namespace of the SQS Queue in the Lambda function's DeadLetterConfig parameter.\",\"isCorrect\":false},{\"text\":\"Specify the AWS Service Namespace of the SQS Queue in the AWS::Lambda::Function resource of the CloudFormation template that you'll use for deploying the function.\",\"isCorrect\":false},{\"text\":\"Specify the Amazon Resource Name of the SQS Queue in the Transform section of the AWS SAM template that you'll use for deploying the function.\",\"isCorrect\":false}],\"explanation\":\"$1d\"},{\"question\":\"A developer will be building a game data feed application which will continuously collect data about player-game interactions and feed the data into your gaming platform. The application uses the Kinesis Client Library to process the data stream from the Amazon Kinesis Data Streams and stores the data to Amazon DynamoDB. It is required that the system should have enough shards and EC2 instances in order to handle failover and adequately process the amount of data coming in and out of the stream. Which of the following ratio of the number of Kinesis shards to EC2 worker instances should the developer implement to achieve the above requirement in the most cost-effective and highly available way?\",\"answers\":[{\"text\":\"4 shards : 8 instances\",\"isCorrect\":false},{\"text\":\"6 shards : 1 instance\",\"isCorrect\":false},{\"text\":\"4 shards : 2 instances\",\"isCorrect\":true},{\"text\":\"1 shard : 6 instances\",\"isCorrect\":false}],\"explanation\":\"$1e\"},{\"question\":\"A Lambda function is over 80 MB in size, which exceeds the deployment package size limit for direct uploads. You want to refactor the function to pull in additional code and other dependencies from another source, which will reduce the size of the deployment. Which feature of Lambda should you use in order to implement the above task?\",\"answers\":[{\"text\":\"Layers\",\"isCorrect\":true},{\"text\":\"Execution Context\",\"isCorrect\":false},{\"text\":\"Alias\",\"isCorrect\":false},{\"text\":\"Environment Variable\",\"isCorrect\":false}],\"explanation\":\"$1f\"},{\"question\":\"A mobile game has a DynamoDB table named TutorialsDojoScores which keeps track of the users and their respective scores. Each item in the table is identified by the FighterId attribute as its partition key and the FightTitle attribute as the sort key. A developer needs to retrieve data from non-key attributes of the table named DojoTopScores and DojoDateTime attributes. Which type of index should the developer add in the table to speed up queries on non-key attributes?\",\"answers\":[{\"text\":\"Primary Index\",\"isCorrect\":false},{\"text\":\"Global Secondary Index\",\"isCorrect\":true},{\"text\":\"Local Secondary Index\",\"isCorrect\":false},{\"text\":\"Sparse Index\",\"isCorrect\":false}],\"explanation\":\"$20\"},{\"question\":\"$21\",\"answers\":[{\"text\":\"Lambda custom integration\",\"isCorrect\":false},{\"text\":\"HTTP Proxy integration\",\"isCorrect\":false},{\"text\":\"HTTP custom integration\",\"isCorrect\":false},{\"text\":\"Lambda proxy integration\",\"isCorrect\":true}],\"explanation\":\"$22\"},{\"question\":\"A company has a latency-sensitive service running on AWS Fargate, which is fronted by an Application Load Balancer (ALB). A CloudFront distribution uses the ALB as its origin and presents a custom domain for clients to access the service. The service authenticates requests by validating the JSON Web Token (JWT) obtained from the Authorization header sent by clients. Lately, there has been a significant influx of login attempts from unauthenticated users, which increases the CPU utilization of the Fargate tasks. Which solution would reduce the load on the Fargate tasks in the most operationally efficient manner?\",\"answers\":[{\"text\":\"Create a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.\",\"isCorrect\":true},{\"text\":\"Create a Lambda@Edge function for JWT validation. Attach it to the Origin Response event of the CloudFront distribution.\",\"isCorrect\":false},{\"text\":\"Create a Lambda function that performs JWT validation. Configure the ALB to route login requests to the Lambda function.\",\"isCorrect\":false},{\"text\":\"Enable auto-scaling on the Fargate tasks.\",\"isCorrect\":false}],\"explanation\":\"$23\"},{\"question\":\"A mobile game has a serverless backend in AWS which is composed of Lambda, API Gateway, and DynamoDB. It writes 100 items per second to the DynamoDB table and the size is 1.5 KB per item. The table has a provisioned WCU of 100 but the write requests are still being throttled by DynamoDB. What is the MOST suitable solution in order to rectify this throttling issue?\",\"answers\":[{\"text\":\"Implement database caching with an ElastiCache cluster.\",\"isCorrect\":false},{\"text\":\"Use strong consistency in the write operations.\",\"isCorrect\":false},{\"text\":\"Increase the WCU to 200.\",\"isCorrect\":true},{\"text\":\"Enable DynamoDB Accelerator (DAX).\",\"isCorrect\":false}],\"explanation\":\"$24\"},{\"question\":\"A single docker container environment is hosted in Elastic Beanstalk. Your manager instructed you to ensure that the compute resources maintain full capacity during deployments to avoid any degradation of the service or possible down time. Which of the following deployment methods should you use to satisfy the given requirement? (Select TWO.)\",\"answers\":[{\"text\":\"Rolling\",\"isCorrect\":false},{\"text\":\"Rolling with additional batch\",\"isCorrect\":true},{\"text\":\"Immutable\",\"isCorrect\":true},{\"text\":\"Canary\",\"isCorrect\":false},{\"text\":\"All at once\",\"isCorrect\":false}],\"explanation\":\"$25\"},{\"question\":\"A developer is working on an application that will process files encrypted with a data key generated from a KMS key. The application needs to decrypt the files locally before it can proceed with the processing of the files. Which of the following are valid and secure steps in decrypting data? (Select TWO.)\",\"answers\":[{\"text\":\"Use the Decrypt operation to decrypt the encrypted data key.\",\"isCorrect\":true},{\"text\":\"Use the Decrypt operation to decrypt the plaintext data key.\",\"isCorrect\":false},{\"text\":\"Use the encrypted data key to decrypt data locally, then erase the encrypted data key from memory.\",\"isCorrect\":false},{\"text\":\"Use the plaintext data key to decrypt data locally, then erase the encrypted data key from memory.\",\"isCorrect\":false},{\"text\":\"Use the plaintext data key to decrypt data locally, then erase the plaintext data key from memory.\",\"isCorrect\":true}],\"explanation\":\"$26\"},{\"question\":\"An EBS-backed EC2 instance has been recently reported to contain a malware that could spread to your other instances. To fix this security vulnerability, you will need to attach its root EBS volume to a new EC2 instance which hosts a security program that can scan viruses, worms, Trojan horses, or spyware. What steps would you take to detach the root volume from the compromised EC2 instance?\",\"answers\":[{\"text\":\"Unmount the volume from the OS and then detach.\",\"isCorrect\":false},{\"text\":\"Detach the volume from the AWS Console. AWS takes care of unmounting the volume for you.\",\"isCorrect\":false},{\"text\":\"Stop the instance then detach the volume.\",\"isCorrect\":true},{\"text\":\"Unmount the volume, stop the instance, and then detach.\",\"isCorrect\":false}],\"explanation\":\"You can detach an Amazon EBS volume from an instance explicitly or by terminating the instance. However, if the instance is running, you must first unmount the volume from the instance. If an EBS volume is the root device of an instance, you must stop the instance before you can detach the volume. The options that say unmount the volume from the OS and then detach and unmount the volume, stop the instance, and then detach are both incorrect because you can’t unmount the root volume on a running instance. The option that says: Detach the volume from the AWS Console. AWS takes care of unmounting the volume for you is incorrect because unmounting the volume is not managed by AWS. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-detaching-volume.html Check out this Amazon EC2 Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/\"},{\"question\":\"A developer is working on an online game based on a popular movie, which may have a few users in its first few weeks of release. However, it is expected to grow and reach millions of concurrent users, with terabytes or more of new data generated per day. The database must seamlessly handle hundreds of thousands of reads and writes per second. Which of the following would be the MOST ideal data store to choose for this application?\",\"answers\":[{\"text\":\"Amazon RDS\",\"isCorrect\":false},{\"text\":\"Amazon S3\",\"isCorrect\":false},{\"text\":\"Amazon DynamoDB\",\"isCorrect\":true},{\"text\":\"Amazon Redshift\",\"isCorrect\":false}],\"explanation\":\"$27\"},{\"question\":\"A leading technology company is building a serverless application in AWS using the C++ programming language. The application will use DynamoDB as its data store, Lambda as its compute service, and API Gateway as its API Proxy. You are tasked to handle the deployment of the compute resources to AWS. Which of the following steps should you implement to properly deploy the serverless application?\",\"answers\":[{\"text\":\"Use AWS Serverless Application Model (AWS SAM) to deploy the Lambda function.\",\"isCorrect\":false},{\"text\":\"Upload the deployment package to S3 and then use CloudFormation to deploy Lambda function with a reference to the S3 URL of the package.\",\"isCorrect\":false},{\"text\":\"Create a Lambda function with the C++ code and directly upload it to AWS.\",\"isCorrect\":false},{\"text\":\"Create a new layer which contains the Custom Runtime for C++ and then launch a Lambda function which uses that runtime.\",\"isCorrect\":true}],\"explanation\":\"$28\"},{\"question\":\"A developer is using API Gateway Lambda Authorizer to securely authenticate the API requests to their web application. The authentication process should be implemented using a custom authorization scheme which accepts header and query string parameters from the API caller. Which of the following methods should the developer use to properly implement the above requirement?\",\"answers\":[{\"text\":\"Amazon Cognito User Pools Authorizer\",\"isCorrect\":false},{\"text\":\"Token-based Authorization\",\"isCorrect\":false},{\"text\":\"Request Parameter-based Authorization\",\"isCorrect\":true},{\"text\":\"Cross-Account Lambda Authorizer\",\"isCorrect\":false}],\"explanation\":\"$29\"},{\"question\":\"You are using AWS Serverless Application Model (AWS SAM) to build and deploy applications in your serverless infrastructure. Your manager instructed you to create a CloudFormation template that includes your SAM script and other service configurations. This template will be used to launch a similar infrastructure in another region. What should you do in order to accomplish this task?\",\"answers\":[{\"text\":\"Add a Mappings section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.\",\"isCorrect\":false},{\"text\":\"Add a Parameters section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.\",\"isCorrect\":false},{\"text\":\"Add a Resources section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.\",\"isCorrect\":false},{\"text\":\"Add a Transform section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.\",\"isCorrect\":true}],\"explanation\":\"$2a\"},{\"question\":\"You are using an AWS Lambda function to process records in an Amazon Kinesis Data Streams stream which has 100 active shards. The Lambda function takes an average of 10 seconds to process the data and the stream is receiving 50 new items per second. Which of the following statements are TRUE regarding this scenario?\",\"answers\":[{\"text\":\"The Lambda function will throttle the incoming requests due to the excessive number of Kinesis shards.\",\"isCorrect\":false},{\"text\":\"There will be at most 100 Lambda function invocations running concurrently.\",\"isCorrect\":true},{\"text\":\"The Kinesis shards must be merged to increase the data capacity of the stream as well as the concurrency execution of the Lambda function.\",\"isCorrect\":false},{\"text\":\"The Lambda function has 500 concurrent executions.\",\"isCorrect\":false}],\"explanation\":\"$2b\"},{\"question\":\"A commercial bank is developing an online auction application with a DynamoDB database that will allow customers to bid for real estate properties from the comforts of their homes. The application should allow the minimum acceptable price established by the bank prior to the auction. The opening bid entered by the staff must be at least the minimum bid and the new bids submitted by the customers should be greater than the current bid. The application logic has already been implemented but the DynamoDB database calls should also be tailored to meet the requirements. Which of the following is the MOST effective solution that will satisfy the requirement in this scenario?\",\"answers\":[{\"text\":\"Configure the database calls of the application to use conditional updates and conditional writes with a condition expression that will check if the new bid submitted by the customer is greater than the current bid.\",\"isCorrect\":true},{\"text\":\"Enable DynamoDB Transactions to automatically check the minimum acceptable price as well as the current and new bid price.\",\"isCorrect\":false},{\"text\":\"Use DynamoDB Streams and a Lambda function to track the current bid price and compare against all of the new bids submitted by the customers.\",\"isCorrect\":false},{\"text\":\"Use an optimistic locking strategy in your database calls to ensure that the new bid submitted by the customer is greater than the current bid.\",\"isCorrect\":false}],\"explanation\":\"$2c\"},{\"question\":\"You are developing a Node.js application which uses a DynamoDB database. The architecture should be designed to allow you to query over a single partition of the table, as specified by the partition key value in the query. It should also support both eventually consistent and strongly consistent reads. What should you do to satisfy this requirement?\",\"answers\":[{\"text\":\"Add a local secondary index before the table is created.\",\"isCorrect\":true},{\"text\":\"Add a global secondary index before the table is created.\",\"isCorrect\":false},{\"text\":\"Add a local secondary index after the table has been created.\",\"isCorrect\":false},{\"text\":\"Add a global secondary index after the table has been created.\",\"isCorrect\":false}],\"explanation\":\"$2d\"},{\"question\":\"An application experiences a sluggish response whenever there is a surge in requests involving read queries. The developer has already attempted to improve performance by optimizing the queries. However, the problem still persists even after applying the change. The application is hosted in an Amazon ECS Cluster and uses a MySQL database backed by Amazon RDS. Which of the following could the developer do to resolve the performance issue? (Select TWO.)\",\"answers\":[{\"text\":\"Implement database caching using Amazon ElastiCache.\",\"isCorrect\":true},{\"text\":\"Implement a Multi-AZ deployment configuration for the RDS DB instance.\",\"isCorrect\":false},{\"text\":\"Cache the database response using Amazon CloudFront.\",\"isCorrect\":false},{\"text\":\"Replace the database with Amazon MemoryDB for Redis\",\"isCorrect\":false},{\"text\":\"Set up read replicas for the RDS database instance and route read queries to these replicas.\",\"isCorrect\":true}],\"explanation\":\"$2e\"},{\"question\":\"A developer has been instructed to configure Cross-Region Replication (CRR) to their S3 bucket as part of the company's disaster recovery plan. She is using the put-bucket-replication AWS CLI to enable CRR on the bucket but it fails whenever she attempts to issue the command. However, the same command works for the other S3 buckets. Which of the following options is the MOST likely reason for this issue?\",\"answers\":[{\"text\":\"S3 Transfer Acceleration is not enabled in the bucket.\",\"isCorrect\":false},{\"text\":\"Versioning is not enabled in the bucket.\",\"isCorrect\":true},{\"text\":\"Amazon S3 Object Lock is enabled in the bucket.\",\"isCorrect\":false},{\"text\":\"The bucket should be configured as a static web hosting first.\",\"isCorrect\":false}],\"explanation\":\"$2f\"},{\"question\":\"A global financial company has hundreds of users from all over the world who regularly upload terabytes of transactional data to a centralized Amazon S3 bucket. Users from different parts of the globe are experiencing delays in uploading data, which in turn affects processing times. The goal is to improve data throughput and ensure consistently fast data transfer to the S3 bucket regardless of the user's location. Which of the following should be used to satisfy the above requirement?\",\"answers\":[{\"text\":\"S3 Transfer Acceleration\",\"isCorrect\":true},{\"text\":\"AWS Transfer for SFTP\",\"isCorrect\":false},{\"text\":\"Amazon CloudFront\",\"isCorrect\":false},{\"text\":\"AWS Direct Connect\",\"isCorrect\":false}],\"explanation\":\"$30\"},{\"question\":\"A developer is building an online game in AWS which will be using a NoSQL database with DynamoDB. Each player data has an average size of 3.5 KB and it is expected that the game will send 150 eventually consistent read requests per second. How may Read Capacity Units (RCU) should the developer provision to the table?\",\"answers\":[{\"text\":\"150\",\"isCorrect\":false},{\"text\":\"300\",\"isCorrect\":false},{\"text\":\"75\",\"isCorrect\":true},{\"text\":\"600\",\"isCorrect\":false}],\"explanation\":\"$31\"},{\"question\":\"A company decided to re-use the same Lambda function for multiple stages of their API, but the function should read data from a different Amazon DynamoDB table depending on which stage is being called. In order to accomplish this, they instructed the developer to pass configuration parameters to a Lambda function through mapping templates in API Gateway. Which of the following is the MOST suitable solution that the developer should use to meet this requirement?\",\"answers\":[{\"text\":\"Use Stage Variables.\",\"isCorrect\":true},{\"text\":\"Set up traffic shifting with Lambda Aliases.\",\"isCorrect\":false},{\"text\":\"Create environment variables in the Lambda function.\",\"isCorrect\":false},{\"text\":\"Set up an API Gateway Private Integration to the Lambda function.\",\"isCorrect\":false}],\"explanation\":\"$32\"},{\"question\":\"A company has a development team that’s heavily relying on AWS CodeBuild, and CodeDeploy. The management would like to further automate its CI/CD process. They requested a system that monitors the status of each code change, from the moment it's committed through to its deployment. Which of the following AWS services will help you achieve this?\",\"answers\":[{\"text\":\"AWS Fault Injection Simulator\",\"isCorrect\":false},{\"text\":\"AWS CodePipeline\",\"isCorrect\":true},{\"text\":\"Amazon CodeGuru\",\"isCorrect\":false},{\"text\":\"AWS Elastic Beanstalk\",\"isCorrect\":false}],\"explanation\":\"$33\"},{\"question\":\"A clickstream application uses Amazon Kinesis Data Stream for real-time processing. PutRecord API calls are being used by the producer to send data to the stream. However, there are cases where the producer intermittently restarted while doing the processing, which resulted in sending the same data twice to the stream. This inadvertently causes duplication of entries in the data stream, which affects the processing of the consumers. Which of the following should you implement to resolve this issue?\",\"answers\":[{\"text\":\"Split shards of the data stream.\",\"isCorrect\":false},{\"text\":\"Merge shards of the data stream.\",\"isCorrect\":false},{\"text\":\"Embed a primary key within the record.\",\"isCorrect\":true},{\"text\":\"Add more shards.\",\"isCorrect\":false}],\"explanation\":\"$34\"},{\"question\":\"A developer wants to deploy a REST API using the CloudFormation template shown below: Which changes should be done so that the newly created API endpoint can be referenced to other stacks?\",\"answers\":[{\"text\":\"Include the Export property in the original template's Outputs section. Then use the Fn::ImportValue function in other templates to retrieve the exported value.\",\"isCorrect\":true},{\"text\":\"Specify HelloWorldApias parameter when using the Fn::ImportValue function in other templates.\",\"isCorrect\":false},{\"text\":\"Add the AWS::Include transform in the original template to directly import the HelloWorldFunction resource to other templates.\",\"isCorrect\":false},{\"text\":\"Include the Export property in the original template's Outputs section. Then use the Ref function in other templates to retrieve the exported value.\",\"isCorrect\":false}],\"explanation\":\"$35\"},{\"question\":\"The current application deployment process of a company is tedious and is prone to errors. They asked a developer to set up CodeDeploy as their deployment service, which can automate their application deployments on their hybrid cloud architecture. Which of the following deployment types does CodeDeploy support? (Select TWO.)\",\"answers\":[{\"text\":\"Rolling deployments to ECS.\",\"isCorrect\":false},{\"text\":\"In-place deployments to AWS Lambda.\",\"isCorrect\":false},{\"text\":\"Blue/green deployments to ECS.\",\"isCorrect\":true},{\"text\":\"In-place deployments to on-premises servers\",\"isCorrect\":true},{\"text\":\"Blue/green deployments to on-premises servers.\",\"isCorrect\":false}],\"explanation\":\"$36\"},{\"question\":\"An application is hosted in Elastic Beanstalk with an ElastiCache cluster that acts as a database cache layer for accessing its data in DynamoDB. It is currently configured to write the data to the cache only if there is a cache miss, which causes the data in the cache to become stale. A developer is instructed to ensure that the data in the cache is always current and to minimize wasted space in the cluster by automatically deleting the data that are never read. What is the BEST way to implement this to satisfy the given requirement?\",\"answers\":[{\"text\":\"Use a Write Through caching strategy.\",\"isCorrect\":false},{\"text\":\"Implement a Write Through caching strategy in the application and enable TTL in Elasticache.\",\"isCorrect\":true},{\"text\":\"Implement Lazy Loading in the application in conjunction with the Write Through caching strategy.\",\"isCorrect\":false},{\"text\":\"Use a Lazy Loading caching strategy.\",\"isCorrect\":false}],\"explanation\":\"$37\"},{\"question\":\"You are developing an online learning platform using Lambda, Elastic Beanstalk, and DynamoDB. There is a requirement that whenever a new customer is added to the DynamoDB table, it will invoke a Lambda function that sends a welcome email to the customer. Which of the following is the MOST suitable solution that you should use to implement this feature?\",\"answers\":[{\"text\":\"Enable DynamoDB Transactions and configure it as the event source for the Lambda function.\",\"isCorrect\":false},{\"text\":\"Use Amazon EventBridge (Amazon CloudWatch Events) to track all new data in your table and configure it as the event source for the Lambda function.\",\"isCorrect\":false},{\"text\":\"Enable DynamoDB Streams and configure it as the event source for the Lambda function.\",\"isCorrect\":true},{\"text\":\"Use Amazon Kinesis Data Streams to track all new data in your table and configure it as the event source for the Lambda function.\",\"isCorrect\":false}],\"explanation\":\"$38\"},{\"question\":\"A startup has an urgent requirement to deploy their new NodeJS application to AWS. You were assigned to perform the deployment to a service where you don't need to worry about the underlying infrastructure that runs the application. The service must also automatically handle provisioning, load balancing, scaling, and application health monitoring. Which service will you use to easily deploy and manage the application?\",\"answers\":[{\"text\":\"AWS CloudFormation\",\"isCorrect\":false},{\"text\":\"AWS Elastic Beanstalk\",\"isCorrect\":true},{\"text\":\"AWS CodeDeploy\",\"isCorrect\":false},{\"text\":\"AWS SAM\",\"isCorrect\":false}],\"explanation\":\"$39\"},{\"question\":\"The read and write operations to an Amazon DynamoDB table are throttled, causing errors in a stateful application that maintains user sessions. Despite checking Amazon CloudWatch metrics, the consumed capacity units have not exceeded the provisioned capacity. Upon further investigation, it is found that a \\\"hot partition\\\" is being accessed more frequently than others by downstream services. What should be done to resolve this issue with MINIMAL cost? (Select TWO.)\",\"answers\":[{\"text\":\"Use DynamoDB Accelerator (DAX).\",\"isCorrect\":false},{\"text\":\"Increase the amount of read or write capacity for your table.\",\"isCorrect\":false},{\"text\":\"Refactor your application to distribute your read and write operations as evenly as possible across your table.\",\"isCorrect\":true},{\"text\":\"Implement read sharding to distribute workloads evenly.\",\"isCorrect\":false},{\"text\":\"Implement error retries and exponential backoff.\",\"isCorrect\":true}],\"explanation\":\"$3a\"},{\"question\":\"A small retail business hired a developer to replace their spreadsheet-based inventory tracking system. They want an automated system that does the following: Process an inventory replenishment when a stock level goes below a certain threshold Send a notification to the business owner about the inventory and replenishment status. The inventory data has already been migrated to an Amazon DynamoDB table. Which architectural pattern should the developer adopt to meet the requirements?\",\"answers\":[{\"text\":\"An Event-Driven pattern using DynamoDB Streams for capturing inventory changes, Lambda function for executing business logic, and Amazon SNS for push notifications.\",\"isCorrect\":true},{\"text\":\"A Scheduled pattern using Amazon EventBridge Scheduler for checking inventory levels, Lambda function for executing business logic, and Amazon SNS for push notifications.\",\"isCorrect\":false},{\"text\":\"A Fan-out pattern where Amazon SNS broadcasts orders, triggering Lambda functions for business logic execution and Amazon SNS for push notifications.\",\"isCorrect\":false},{\"text\":\"A Batch pattern with Amazon SQS for queuing changes, Lambda function for executing business logic, and Amazon SNS for email notifications.\",\"isCorrect\":false}],\"explanation\":\"$3b\"},{\"question\":\"A developer is designing an application which will be hosted in ECS and uses an EC2 launch type. You need to group your container instances by certain attributes such as Availability Zone, instance type, or custom metadata. After you have defined a group of container instances, you will need to customize Amazon ECS to place tasks on container instances based on the group you specified. Which of the following ECS features provides you with expressions that you can use to group container instances by a specific attribute?\",\"answers\":[{\"text\":\"Task Placement Constraints\",\"isCorrect\":false},{\"text\":\"Task Placement Strategies\",\"isCorrect\":false},{\"text\":\"Cluster Query Language\",\"isCorrect\":true},{\"text\":\"Task Groups\",\"isCorrect\":false}],\"explanation\":\"$3c\"},{\"question\":\"A developer is designing a multitiered system which utilizes various AWS resources. The application will be hosted in Elastic Beanstalk, which uses an RDS database and an S3 bucket that is configured to use Server-Side Encryption with Customer-Provided Encryption Keys (SSE-C). In this configuration, Amazon S3 does not store the encryption key you provide but instead, stores a randomly salted hash-based message authentication code (HMAC) value of the encryption key in order to validate future requests. Which of the following is a valid consideration that the developer should keep in mind when implementing this architecture?\",\"answers\":[{\"text\":\"The salted HMAC value can be used to derive the value of the encryption key.\",\"isCorrect\":false},{\"text\":\"The salted HMAC value can be used to decrypt the contents of the encrypted object.\",\"isCorrect\":false},{\"text\":\"If you lose the encryption key, the salted HMAC value can be used to decrypt the object.\",\"isCorrect\":false},{\"text\":\"If you lose the encryption key, you lose the object.\",\"isCorrect\":true}],\"explanation\":\"$3d\"},{\"question\":\"There is a requirement to postpone the delivery of new messages to an SQS queue for a number of seconds. You must configure the queue to ensure that any messages that you send remain invisible to consumers for a duration of time specified. Which of the following SQS feature should you use to meet this requirement?\",\"answers\":[{\"text\":\"Delay Queue\",\"isCorrect\":true},{\"text\":\"Short Polling\",\"isCorrect\":false},{\"text\":\"Visibility Timeouts\",\"isCorrect\":false},{\"text\":\"Long Polling\",\"isCorrect\":false}],\"explanation\":\"$3e\"},{\"question\":\"You were recently hired as a developer for a leading insurance firm in Asia which has a hybrid cloud architecture with AWS. The project that was assigned to you involves setting up a static website using Amazon S3 with a CORS configuration as shown below: \u003c?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?\u003e\u003cCORSConfiguration xmlns=\\\"http://s3.amazonaws.com/doc/2006-03-01/\\\"\u003e \u003cCORSRule\u003e \u003cAllowedOrigin\u003ehttps://tutorialsdojo.com\u003c/AllowedOrigin\u003e \u003cAllowedMethod\u003eGET\u003c/AllowedMethod\u003e \u003cAllowedMethod\u003ePUT\u003c/AllowedMethod\u003e \u003cAllowedMethod\u003ePOST\u003c/AllowedMethod\u003e \u003cAllowedMethod\u003eDELETE\u003c/AllowedMethod\u003e \u003cAllowedHeader\u003e*\u003c/AllowedHeader\u003e \u003cExposeHeader\u003eETag\u003c/ExposeHeader\u003e \u003cExposeHeader\u003ex-amz-meta-custom-header\u003c/ExposeHeader\u003e \u003cMaxAgeSeconds\u003e3600\u003c/MaxAgeSeconds\u003e \u003c/CORSRule\u003e\u003c/CORSConfiguration\u003e Which of the following statements are TRUE with regards to this S3 configuration? (Select TWO.)\",\"answers\":[{\"text\":\"This configuration authorizes the user to perform actions on the S3 bucket.\",\"isCorrect\":false},{\"text\":\"It allows a user to view, add, remove or update objects inside the S3 bucket from the domain tutorialsdojo.com.\",\"isCorrect\":true},{\"text\":\"All HTTP Methods are allowed.\",\"isCorrect\":false},{\"text\":\"This will cause the browser to cache the response of the preflight OPTIONS request for 1 hour.\",\"isCorrect\":true},{\"text\":\"The request will fail if the x-amz-meta-custom-header header is not included.\",\"isCorrect\":false}],\"explanation\":\"$3f\"},{\"question\":\"An online role-playing video game requires cross-device syncing of application-related user data. It must synchronize the user profile data across mobile devices without requiring your own backend. When the device is online, it should synchronize data and notify other devices immediately that an update is available. Which of the following is the most suitable feature that you have to use to meet this requirement?\",\"answers\":[{\"text\":\"Amazon Cognito Sync\",\"isCorrect\":true},{\"text\":\"Amazon Cognito User Pools\",\"isCorrect\":false},{\"text\":\"AWS Device Farm\",\"isCorrect\":false},{\"text\":\"Amazon Cognito Identity Pools\",\"isCorrect\":false}],\"explanation\":\"$40\"},{\"question\":\"The developer has built a real-time IoT device monitoring application that leverages Amazon Kinesis Data Stream to ingest data. The application uses several EC2 instances for processing. Recently, the developer has observed a steady increase in the rate of data flowing into the stream, indicating that the stream's capacity must be scaled up to sustain optimal performance. What should the developer do to increase the capacity of the stream?\",\"answers\":[{\"text\":\"Split every shard in the stream.\",\"isCorrect\":true},{\"text\":\"Merge every shard in the stream.\",\"isCorrect\":false},{\"text\":\"Integrate Amazon Data Firehose with the Amazon Kinesis Data Stream to increase the capacity of the stream.\",\"isCorrect\":false},{\"text\":\"Upgrade the instance type of the EC2 instances.\",\"isCorrect\":false}],\"explanation\":\"$41\"},{\"question\":\"A company is using OpenAPI, which is also known as Swagger, for the API specifications of their REST web services that are hosted on their on-premises data center. They want to migrate their system to AWS using Lambda and API Gateway. In line with this, you are instructed to create a new API and populate it with the resources and methods from their Swagger definition. Which of the following is the EASIEST way to accomplish this task?\",\"answers\":[{\"text\":\"Import their Swagger or OpenAPI definitions to API Gateway using the AWS Console.\",\"isCorrect\":true},{\"text\":\"Use AWS SAM to migrate and deploy the company's web services to API Gateway.\",\"isCorrect\":false},{\"text\":\"Create models and templates for request and response mappings based on the company's API definitions.\",\"isCorrect\":false},{\"text\":\"Use CodeDeploy to migrate and deploy the company's web services to API Gateway.\",\"isCorrect\":false}],\"explanation\":\"$42\"},{\"question\":\"A developer is tasked with automating the deployment of a new microservice in an ECS cluster using AWS CodeDeploy. The developer is writing the AppSpec file to instruct CodeDeploy on how to handle the deployment. Which sets of properties are REQUIRED in the resources section to successfully deploy the microservice? (Select THREE.)\",\"answers\":[{\"text\":\"ContainerName\",\"isCorrect\":true},{\"text\":\"NetworkConfiguration\",\"isCorrect\":false},{\"text\":\"targetversion\",\"isCorrect\":false},{\"text\":\"ContainerPort\",\"isCorrect\":true},{\"text\":\"alias\",\"isCorrect\":false},{\"text\":\"TaskDefinition\",\"isCorrect\":true}],\"explanation\":\"$43\"},{\"question\":\"A batch application is hosted in an Auto Scaling group of On-Demand EC2 instances which consumes and processes the messages from an SQS queue. The system works well but there are times that the consumers process the same message twice. Upon investigation, you found out that if the consumer takes a long time to process the message, that exact same message becomes available again to other consumers, which causes duplicate processing. Which of the following is the BEST solution that the developer should implement to meet this requirement?\",\"answers\":[{\"text\":\"Postpone the delivery of new messages by using a delay queue.\",\"isCorrect\":false},{\"text\":\"Configure the queue to use long polling by setting the Receive Message Wait Time parameter to a value greater than 0.\",\"isCorrect\":false},{\"text\":\"Set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue.\",\"isCorrect\":true},{\"text\":\"Configure the queue to use short polling by setting the WaitTimeSeconds parameter of the ReceiveMessage request to 0.\",\"isCorrect\":false}],\"explanation\":\"$44\"},{\"question\":\"You are managing an application which is composed of an SQS queue and an Auto Scaling group of EC2 instances. Recently, your customers are complaining that there are a lot of incidents where their orders are being erroneously sent twice. What should you do to rectify this problem?\",\"answers\":[{\"text\":\"Use a FIFO (First-In-First-Out) Queue by disabling the content-based deduplication.\",\"isCorrect\":false},{\"text\":\"Use a Standard Queue and provide the Message Group ID for each message.\",\"isCorrect\":false},{\"text\":\"Use a Standard Queue and provide the Message Deduplication ID for each message.\",\"isCorrect\":false},{\"text\":\"Use a FIFO (First-In-First-Out) Queue and provide the Message Deduplication ID for each message.\",\"isCorrect\":true}],\"explanation\":\"$45\"},{\"question\":\"You are developing a high-traffic online stocks trading application, which will be hosted in an ECS Cluster and will be accessed by thousands of investors for intraday stocks trading. Each task of the cluster should be evenly placed across multiple Availability Zones to avoid any service disruptions. Which of the following is the MOST suitable placementStrategy configuration that you should use in your task definition?\",\"answers\":[{\"text\":\"\\\"placementStrategy\\\": [ { \\\"field\\\": \\\"memory\\\", \\\"type\\\": \\\"binpack\\\" }]\",\"isCorrect\":false},{\"text\":\"\\\"placementStrategy\\\": [ { \\\"type\\\": \\\"random\\\" }]\",\"isCorrect\":false},{\"text\":\"\\\"placementStrategy\\\": [ { \\\"field\\\": \\\"instanceId\\\", \\\"type\\\": \\\"spread\\\" }]\",\"isCorrect\":false},{\"text\":\"\\\"placementStrategy\\\": [ { \\\"field\\\": \\\"attribute:ecs.availability-zone\\\", \\\"type\\\": \\\"spread\\\" }]\",\"isCorrect\":true}],\"explanation\":\"$46\"},{\"question\":\"A newly hired developer has been instructed to debug an application. She tried to access the X-Ray console to view service maps and segments but her current access is insufficient. Which of the following is the MOST appropriate managed policy that should be granted to the developer?\",\"answers\":[{\"text\":\"AmazonS3ReadOnlyAccess\",\"isCorrect\":false},{\"text\":\"AWSXrayFullAccess\",\"isCorrect\":false},{\"text\":\"AWSXRayDaemonWriteAccess\",\"isCorrect\":false},{\"text\":\"AWSXrayReadOnlyAccess\",\"isCorrect\":true}],\"explanation\":\"$47\"},{\"question\":\"A software engineer is developing a serverless application which will use a DynamoDB database. One of the requirements is that each write request should return the total number of write capacity units consumed, with subtotals for the table and any secondary indexes that were affected by the operation. What should be done to accomplish this feature?\",\"answers\":[{\"text\":\"Add the ReturnValues parameter with a value of TOTAL in every write request.\",\"isCorrect\":false},{\"text\":\"Add the ReturnConsumedCapacity parameter with a value of TOTAL in every write request.\",\"isCorrect\":false},{\"text\":\"Add the ReturnValues parameter with a value of INDEXES in every write request.\",\"isCorrect\":false},{\"text\":\"Add the ReturnConsumedCapacity parameter with a value of INDEXES in every write request.\",\"isCorrect\":true}],\"explanation\":\"$48\"},{\"question\":\"For application deployments, a company is using CloudFormation templates, which are regularly updated to map the latest AMI IDs. A developer was assigned to automate the process since the current set up takes a lot of time to execute on a regular basis. Which of the following is the MOST suitable solution that the developer should implement to satisfy this requirement?\",\"answers\":[{\"text\":\"Integrate CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments.\",\"isCorrect\":false},{\"text\":\"Set up your Systems Manager State Manager to store the latest AMI IDs and integrate it with your CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template.\",\"isCorrect\":false},{\"text\":\"Integrate AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments.\",\"isCorrect\":false},{\"text\":\"Set up CloudFormation with Systems Manager Parameter Store to retrieve the latest AMI IDs for your template. Whenever you decide to update the EC2 instances, call the update-stack API in CloudFormation in your CloudFormation template.\",\"isCorrect\":true}],\"explanation\":\"$49\"},{\"question\":\"A financial company has a cryptocurrency application that has been hosted in Elastic Beanstalk for a couple of months. Recently, the application's performance has been degrading, so you decided to check the CPU and memory utilization of the underlying EC2 instances in CloudWatch. You can see the CPU utilization of the instances but not the memory utilization. Which of the following is the MOST likely cause of this issue?\",\"answers\":[{\"text\":\"The .ebextensions/xray-daemon.config file in Elastic Beanstalk is missing.\",\"isCorrect\":false},{\"text\":\"The detailed monitoring is not enabled in CloudWatch.\",\"isCorrect\":false},{\"text\":\"X-Ray Daemon is not installed on the EC2 instances.\",\"isCorrect\":false},{\"text\":\"CloudWatch does not track memory utilization by default.\",\"isCorrect\":true}],\"explanation\":\"$4a\"},{\"question\":\"A company has an AWS account with an ID of 061218980612 and has a centralized Java web application hosted in AWS Elastic Beanstalk that is used by different departments. The developer used the iam create-account-alias --account-alias finance-dept AWS CLI command to create a user-friendly identifier for the finance department. For faster troubleshooting, the application must also be configured to easily trace all its downstream requests, such as Apache HTTP requests, AWS SDK requests, and SQL queries made using a JDBC driver. The ability to send traces to multiple different tracing backends without having to re-instrument the application code is required as well. Which of the following options is the MOST suitable solution that the developer implements?\",\"answers\":[{\"text\":\"Use the https://061218980612.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure the Amazon CloudWatch Evidently to trace all the downstream API calls.\",\"isCorrect\":false},{\"text\":\"Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.\",\"isCorrect\":true},{\"text\":\"Use the https://finance-dept.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the downstream API calls.\",\"isCorrect\":false},{\"text\":\"Use the https://finance-dept.aws.amazon.com/console sign-in page URL for the AWS account. Install and configure the AWS X-Ray auto-instrumentation Java agent to trace all the downstream API calls.\",\"isCorrect\":false}],\"explanation\":\"$4b\"},{\"question\":\"A developer is designing a multi-threaded e-commerce application that will be reading and writing data on a DynamoDB table. There will be a lot of people who will use the application to update the price of items in the table at the same time. The application should prevent an update operation from modifying an item if one of its attributes has a certain value. Which of the following is the most suitable solution that the developer should use in this application?\",\"answers\":[{\"text\":\"Use batch operations.\",\"isCorrect\":false},{\"text\":\"Use optimistic locking and conditional writes.\",\"isCorrect\":true},{\"text\":\"Use pessimistic locking and conditional writes.\",\"isCorrect\":false},{\"text\":\"Use atomic counters.\",\"isCorrect\":false}],\"explanation\":\"$4c\"},{\"question\":\"A developer has enabled API Caching on his application endpoints in Amazon API Gateway. For testing purposes, he wants to fetch the latest data, and not the cache data, whenever he sends a GET request with a Cache-Control: max-age=0 header to a specific resource. Which of the following policies will allow him to invalidate the cache for requests?\",\"answers\":[{\"text\":\"{ \\\"Version\\\": \\\"2012-10-17\\\", \\\"Statement\\\": [ { \\\"Effect\\\": \\\"Deny\\\", \\\"Action\\\": [ \\\"execute-api:*\\\" ], \\\"Resource\\\": [ \\\"arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier\\\" ] } ]}\",\"isCorrect\":false},{\"text\":\"{ \\\"Version\\\": \\\"2012-10-17\\\", \\\"Statement\\\": [ { \\\"Effect\\\": \\\"Deny\\\", \\\"Action\\\": [ \\\"execute-api:InvalidateCache\\\" ], \\\"Resource\\\": [ \\\"arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier\\\" ] } ]}\",\"isCorrect\":false},{\"text\":\"{ \\\"Version\\\": \\\"2012-10-17\\\", \\\"Statement\\\": [ { \\\"Effect\\\": \\\"Allow\\\", \\\"Action\\\": [ \\\"execute-api:Invoke\\\" ], \\\"Resource\\\": [\\\"arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier\\\" ] } ]}\",\"isCorrect\":false},{\"text\":\"{ \\\"Version\\\": \\\"2012-10-17\\\", \\\"Statement\\\": [ { \\\"Effect\\\": \\\"Allow\\\", \\\"Action\\\": [ \\\"execute-api:InvalidateCache\\\" ], \\\"Resource\\\": [ \\\"arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier\\\" ] } ]}\",\"isCorrect\":true}],\"explanation\":\"$4d\"},{\"question\":\"A developer is currently building a scalable microservices architecture where complex applications are decomposed into smaller, independent services. Docker will be used as its application container to provide an optimal way of running small, decoupled services. The developer should also have fine-grained control over the custom application architecture. Which of the following services is the MOST suitable one to use?\",\"answers\":[{\"text\":\"EC2\",\"isCorrect\":false},{\"text\":\"ECS\",\"isCorrect\":true},{\"text\":\"Elastic Beanstalk\",\"isCorrect\":false},{\"text\":\"AWS SAM\",\"isCorrect\":false}],\"explanation\":\"$4e\"}]}}]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n7:null\n"])</script><script>self.__next_f.push([1,"8:null\nc:[[\"$\",\"title\",\"0\",{\"children\":\"v0 App\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Created with v0\"}],[\"$\",\"meta\",\"2\",{\"name\":\"generator\",\"content\":\"v0.app\"}]]\n"])</script></body></html>