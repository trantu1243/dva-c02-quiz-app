"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[530],{2166:(e,t,a)=>{a.d(t,{E:()=>o});let o=[{id:"aws-developer-1",title:"AWS Developer Associate (DVA-C02) Practice Test Latest 2025 1",description:"Test your knowledge of AWS services including DynamoDB, ELB, and IAM best practices.",questions:[{question:"A nightly batch job loads 1 million new records into a DynamoDB table. The records are only needed for one hour, and the table needs to be empty by the next night's batch job. Which is the MOST efficient and cost-effective method to provide an empty table?",answers:[{text:"Use BatchWriteItem to empty all of the rows",isCorrect:!1},{text:"Create and then delete the table after the task has completed",isCorrect:!0},{text:"Use DeleteItem using a ConditionExpression",isCorrect:!1},{text:"Write a recursive function that scans and calls out DeleteItem",isCorrect:!1}],explanation:'The key requirements here are to be efficient and costeffective. Therefore, it’s important to choose the option that requires the fewest API calls. As the table is only required for a short period of time, the most efficient and costeffective option is to simply delete and recreate the table.The following API actions can be used to perform this operation programmatically:• CreateTable The CreateTable operation adds a new table to your account.• DeleteTable The DeleteTable operation deletes a table and all of its items.This solution means fewer API calls and also the table is not consuming RCUs/WCUs whilst not being used. Therefore, the best option is to create and then delete the table after the task has completed.CORRECT: "Create and then delete the table after the task has completed" is the correct answer.INCORRECT: "Use DeleteItem using a ConditionExpression" is incorrect as this will use more RCUs and WCUs and is not cost effective.INCORRECT: "Use BatchWriteItem to empty all of the rows" is incorrect. The BatchWriteItem operation puts or deletes multiple items (not rows) in one or more tables. This would use more RCUs and WCUs and is not costeffective.INCORRECT: "Write a recursive function that scans and calls out DeleteItem" is incorrect as scans are the least efficient and costeffective option as all items must be retrieved from the table.References: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Operations.html'},{question:"An application includes multiple Auto Scaling groups of Amazon EC2 instances. Each group corresponds to a different subdomain of example.com, including forum.example.com and myaccount.example.com. An Elastic Load Balancer will be used to distribute load from a single HTTPS listener. Which type of Elastic Load Balancer MUST a Developer use in this scenario?",answers:[{text:"Task Load Balancer",isCorrect:!1},{text:"Network Load Balancer",isCorrect:!1},{text:"Classic Load Balancer",isCorrect:!1},{text:"Application Load Balancer",isCorrect:!0}],explanation:'With an Application Load Balancer it is possible to route requests based on the domain name specified in the Host header. This means you can route traffic coming in to forum.example.com and myaccount.example.com to different target groups.The Application Load Balancer is the only Elastic Load Balancer provided by AWS that can perform hostbased routing.CORRECT: "Application Load Balancer" is the correct answer.INCORRECT: "Network Load Balancer" is incorrect as this type of ELB routes traffic based on information at the connection layer (L4).INCORRECT: "Classic Load Balancer" is incorrect as it does not support any kind of host or pathbased routing or even target groups.INCORRECT: "Task Load Balancer" is incorrect as this is not a type of ELB.References: https://aws.amazon.com/blogs/aws/newhostbasedroutingsupportforawsapplicationloadbalancers/'},{question:"A company is reviewing their security practices. According to AWS best practice, how should access keys be managed to improve security? (Select TWO.)",answers:[{text:"Embed access keys directly into code",isCorrect:!1},{text:"Rotate access keys daily",isCorrect:!1},{text:"Use the same access key in all applications for consistency",isCorrect:!1},{text:"Use different access keys for different applications",isCorrect:!0},{text:"Delete all access keys for the root account IAM user",isCorrect:!0}],explanation:'When you access AWS programmatically, you use an access key to verify your identity and the identity of your applications. An access key consists of an access key ID (something like AKIAIOSFODNN7EXAMPLE) and a secret access key (something like wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY).Anyone who has your access key has the same level of access to your AWS resources that you do. Steps to protect access keys include the following:• Remove (or Don\'t Generate) Account Access Key – this is especially important for the root account.• Use Temporary Security Credentials (IAM Roles) Instead of LongTerm Access Keys.• Don\'t embed access keys directly into code.• Use different access keys for different applications.• Rotate access keys periodically.• Remove unused access keys.• Configure multifactor authentication for your most sensitive operations.CORRECT: "Delete all access keys for the root account IAM user" is the correct answer.CORRECT: "Use different access keys for different applications" is the correct answer.INCORRECT: "Embed access keys directly into code" is incorrect. This is not a best practice as this is something that should be avoided as much as possible.INCORRECT: "Rotate access keys daily" is incorrect. Though this would be beneficial from a security perspective it may be hard to manage so this is not an AWS recommended best practice. AWS recommend you rotate access keys “periodically”, not “daily”.INCORRECT: "Use the same access key in all applications for consistency" is incorrect. The best practice is to use different access keys for different applications.References: https://docs.aws.amazon.com/general/latest/gr/awsaccesskeysbestpractices.html#iamuseraccesskeys'},{question:"A Developer is deploying an Amazon EC2 update using AWS CodeDeploy. In the appspec.yml file, which of the following is a valid structure for the order of hooks that should be specified?",answers:[{text:"BeforeAllowTraffic > AfterAllowTraffic",isCorrect:!1},{text:"BeforeBlockTraffic > AfterBlockTraffic > BeforeAllowTraffic > AfterAllowTraffic",isCorrect:!1},{text:"BeforeInstall > AfterInstall > ApplicationStart > ValidateService",isCorrect:!0},{text:"BeforeInstall > AfterInstall > AfterAllowTestTraffic > BeforeAllowTraffic > AfterAllowTraffic",isCorrect:!1}],explanation:'The content in the \'hooks\' section of the AppSpec file varies, depending on the compute platform for your deployment.The \'hooks\' section for an EC2/OnPremises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts.The \'hooks\' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment.Therefore, in this scenario a valid structure for the order of hooks that should be specified in the appspec.yml file is:BeforeInstall > AfterInstall > ApplicationStart > ValidateServiceCORRECT: "BeforeInstall > AfterInstall > ApplicationStart > ValidateService" is the correct answer.INCORRECT: "BeforeInstall > AfterInstall > AfterAllowTestTraffic > BeforeAllowTraffic > AfterAllowTraffic" is incorrect as this would be valid for Amazon ECS.INCORRECT: "BeforeAllowTraffic > AfterAllowTraffic" is incorrect as this would be valid for AWS Lambda.INCORRECT: "BeforeBlockTraffic > AfterBlockTraffic > BeforeAllowTraffic > AfterAllowTraffic" is incorrect as this is a partial listing of hooks for Amazon EC2 but is incomplete.References: https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfilestructurehooks.html'},{question:"A Developer is creating a social networking app for games that uses a single Amazon DynamoDB table. All users' saved game data is stored in the single table, but users should not be able to view each other's data. How can the Developer restrict user access so they can only view their own data?",answers:[{text:"Use an identitybased policy that restricts read access to the table to specific principals",isCorrect:!1},{text:"Use separate access keys for each user to call the API and restrict access to specific items based on access key ID",isCorrect:!1},{text:"Restrict access to specific items based on certain primary key values",isCorrect:!0},{text:"Read records from DynamoDB and discard irrelevant data clientside",isCorrect:!1}],explanation:'In DynamoDB, you have the option to specify conditions when granting permissions using an IAM policy. For example, you can:• Grant permissions to allow users readonly access to certain items and attributes in a table or a secondary index.• Grant permissions to allow users writeonly access to certain attributes in a table, based upon the identity of that user.To implement this kind of finegrained access control, you write an IAM permissions policy that specifies conditions for accessing security credentials and the associated permissions. You then apply the policy to IAM users, groups, or roles that you create using the IAM console. Your IAM policy can restrict access to individual items in a table, access to the attributes in those items, or both at the same time.You use the IAM Condition element to implement a finegrained access control policy. By adding a Condition element to a permissions policy, you can allow or deny access to items and attributes in DynamoDB tables and indexes, based upon your particular business requirements. You can also grant permissions on a table, but restrict access to specific items in that table based on certain primary key values.CORRECT: "Restrict access to specific items based on certain primary key values" is the correct answer.INCORRECT: "Use separate access keys for each user to call the API and restrict access to specific items based on access key ID" is incorrect. You cannot restrict access based on access key ID.INCORRECT: "Use an identitybased policy that restricts read access to the table to specific principals" is incorrect as this would only restrict read access to the entire table, not to specific items in the table.INCORRECT: "Read records from DynamoDB and discard irrelevant data clientside" is incorrect as this is inefficient and insecure as it will use more RCUs and has more potential to leak the information.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifyingconditions.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/usingidentitybasedpolicies.html'},{question:"A Developer wants the ability to roll back to a previous version of an AWS Lambda function in the event of errors caused by a new deployment. How can the Developer achieve this with MINIMAL impact on users?",answers:[{text:"Change the application to use the $LATEST version. Update and save code. If too many errors are encountered, modify and save the code",isCorrect:!1},{text:"Change the application to use a version ARN that points to the latest published version. Deploy the new version of the code. Update the application to point to the ARN of the new version of the code. If too many errors are encountered, point the application back to the ARN of the previous version",isCorrect:!1},{text:"Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to direct 10% of users to the newly deployed version. If too many errors are encountered, send 100% of traffic to the previous version",isCorrect:!0},{text:"Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to use the newly deployed version. If too many errors are encountered, point the alias back to the previous version",isCorrect:!1}],explanation:'You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN.You can update the versions that an alias points to and you can also add multiple versions and use weightings to direct a percentage of traffic to a new version of the code.For this example the best choice is to use an alias and direct 10% of traffic to the new version. If errors are encountered the rollback is easy (change the pointer in the alias) and a minimum of impact has been made to users.CORRECT: "Change the application to use an alias that points to the current version. Deploy the new version of the code.Update the alias to direct 10% of users to the newly deployed version. If too many errors are encountered, send 100% of traffic to the previous version" is the correct answer.INCORRECT: "Change the application to use an alias that points to the current version. Deploy the new version of the code.Update the alias to use the newly deployed version. If too many errors are encountered, point the alias back to the previous version" is incorrect. This is not the best answer as 100% of the users will be directed to the new version so if any errors do occur more users will be affected.INCORRECT: "Change the application to use a version ARN that points to the latest published version. Deploy the new version of the code. Update the application to point to the ARN of the new version of the code. If too many errors are encountered, point the application back to the ARN of the previous version" is incorrect. This answer involves a lot of updates to the application that could be completely avoided by using an alias.INCORRECT: "Change the application to use the $LATEST version. Update and save code. If too many errors are encountered, modify and save the code" is incorrect as this is against best practice. The $LATEST is the unpublished version of the code where you make changes. You should publish to a version when the code is ready for deployment.References: https://docs.aws.amazon.com/lambda/latest/dg/configurationaliases.html'},{question:"A retail organization stores stock information in an Amazon RDS database. An application reads and writes data to the database. A Developer has been asked to provide read access to the database from a reporting application in another region. Which configuration would provide BEST performance for the reporting application without impacting the performance of the main database?",answers:[{text:"Implement a crossregion read replica in the region where the reporting application will run",isCorrect:!0},{text:"Implement a read replica in another AZ and configure the reporting application to connect to the read replica using a VPN connection",isCorrect:!1},{text:"Implement a crossregion multiAZ deployment in the region where the reporting application will run",isCorrect:!1},{text:"Create a snapshot of the database and create a new database from the snapshot in the region where the reporting application will run",isCorrect:!1}],explanation:'With Amazon RDS, you can create a MariaDB, MySQL, Oracle, or PostgreSQL read replica in a different AWS Region than the source DB instance. Creating a crossRegion read replica isn\'t supported for SQL Server on Amazon RDS.You create a read replica in a different AWS Region to do the following:• Improve your disaster recovery capabilities.• Scale read operations into an AWS Region closer to your users.• Make it easier to migrate from a data center in one AWS Region to a data center in another AWS Region.Creating a read replica in a different AWS Region from the source instance is similar to creating a replica in the same AWSRegion. You can use the AWS Management Console, run the createdbinstancereadreplica command, or call the CreateDBInstanceReadReplica API operation.Creating read replica in the region where the reporting application is going to run will provide the best performance as latency will be much lower than connecting across regions. As the database is a replica it will also be continuously updated using asynchronous replication so the reporting application will have the latest data available.CORRECT: "Implement a crossregion read replica in the region where the reporting application will run" is the correct answer.INCORRECT: "Implement a crossregion multiAZ deployment in the region where the reporting application will run" is incorrect as multiAZ is used across availability zones, not regions.INCORRECT: "Create a snapshot of the database and create a new database from the snapshot in the region where the reporting application will run" is incorrect as this would be OK from a performance perspective but the database would not receive ongoing updates from the main database so the data would quickly become out of date.INCORRECT: "Implement a read replica in another AZ and configure the reporting application to connect to the read replica using a VPN connection" is incorrect as this would result in much higher latency than having the database in the local region close to the reporting application and would impact performance.References: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html'},{question:"A Development team are developing a microservices application that will use Docker containers on Amazon ECS. There will be 6 distinct services included in the architecture. Each service requires specific permissions to various AWS services. What is the MOST secure way to grant the services the necessary permissions?",answers:[{text:"Create a new Identity and Access Management (IAM) instance profile containing the required permissions for the various ECS services, then associate that instance role with the underlying EC2 instances",isCorrect:!1},{text:"Create six separate IAM roles, each containing the required permissions for the associated ECS service, then create an IAM group and configure the ECS cluster to reference that group",isCorrect:!1},{text:"Create six separate IAM roles, each containing the required permissions for the associated ECS service, then configure each ECS task definition to reference the associated IAM role",isCorrect:!0},{text:"Create a single IAM policy and use principal statements referencing the ECS tasks and assigning the required permissions, then apply the policy to the ECS service",isCorrect:!1}],explanation:'With IAM roles for Amazon ECS tasks, you can specify an IAM role that can be used by the containers in a task. Applications must sign their AWS API requests with AWS credentials, and this feature provides a strategy for managing credentials for your applications to use, similar to the way that Amazon EC2 instance profiles provide credentials to EC2 instances. Instead of creating and distributing your AWS credentials to the containers or using the EC2 instance’s role, you can associate an IAM role with an ECS task definition or RunTask API operation. The applications in the task’s containers can then use the AWS SDK or CLI to make API requests to authorized AWS services.Therefore, the most secure solution is to use a separate IAM role with the specific permissions required for an individual service and associate that role to the relevant ECS task definition. This should then be repeated for the remaining 5 services.CORRECT: "Create six separate IAM roles, each containing the required permissions for the associated ECS service, then configure each ECS task definition to reference the associated IAM role" is the correct answer.INCORRECT: "Create six separate IAM roles, each containing the required permissions for the associated ECS service, then create an IAM group and configure the ECS cluster to reference that group" is incorrect. The IAM role should be applied to the ECS task definition, not the ECS cluster.INCORRECT: "Create a new Identity and Access Management (IAM) instance profile containing the required permissions for the various ECS services, then associate that instance role with the underlying EC2 instances" is incorrect. With IAM Roles for Tasks you apply the permissions directly to the task definition. This means multiple services can share the underlying EC2 instance and only have the minimum privileges required.INCORRECT: "Create a single IAM policy and use principal statements referencing the ECS tasks and assigning the required permissions, then apply the policy to the ECS service" is incorrect. Identitybased policies attached to the ECS service can be used to control permissions for viewing, launching, and managing resources within ECS. However, for this solution we need to control the permissions for an ECS task to access other AWS services. For this we need to use IAM Roles for Tasks.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskiamroles.html'},{question:"A company is building an application to track athlete performance using an Amazon DynamoDB table. Each item in the table is identified by a partition key (user_id) and a sort key (sport_name). The table design is shown below: &bull; Partition key: user_id &bull; Sort Key: sport_name &bull; Attributes: score, score_datetime A Developer is asked to write a leaderboard application to display the top performers (user_id) based on the score for each sport_name. What process will allow the Developer to extract results MOST efficiently from the DynamoDB table?",answers:[{text:"Create a global secondary index with a partition key of sport_name and a sort key of score, and get the results",isCorrect:!0},{text:"Use a DynamoDB query operation with the key attributes of user_id and sport_name and order the results based on the score attribute",isCorrect:!1},{text:"Use a DynamoDB scan operation to retrieve scores and user_id based on sport_name, and order the results based on the score attribute",isCorrect:!1},{text:"Create a local secondary index with a primary key of sport_name and a sort key of score and get the results based on the score attribute",isCorrect:!1}],explanation:'The Developer needs to be able to sort the scores for each sport and then extract the highest performing athletes. In this case BOTH the partition key and sort key must be different which means a Global Secondary index is required (as a Local Secondary index only has a different sort key). The GSI would be configured as follows: &bull; Partition key: sport_name &bull; Sort Key: score The results will then be listed in order of the highest score for each sport which is exactly what is required. CORRECT: "Create a global secondary index with a partition key of sport_name and a sort key of score, and get the results" is the correct answer. INCORRECT: "Create a local secondary index with a primary key of sport_name and a sort key of score and get the results based on the score attribute" is incorrect as an LSI cannot be created after table creation and also only has a different sort key, not a different partition key. INCORRECT: "Use a DynamoDB query operation with the key attributes of user_id and sport_name and order the results based on the score attribute" is incorrect as this is less efficient compared to using a GSI. INCORRECT: "Use a DynamoDB scan operation to retrieve scores and user_id based on sport_name, and order the results based on the score attribute" is incorrect as this is the least efficient option as a scan returns every item in the table (more RCUs). References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html'},{question:"A company runs many microservices applications that use Docker containers. The company are planning to migrate the containers to Amazon ECS. The workloads are highly variable and therefore the company prefers to be charged per running task. Which solution is the BEST fit for the company's requirements?",answers:[{text:"An Amazon ECS Service with Auto Scaling",isCorrect:!1},{text:"Amazon ECS with the EC2 launch type",isCorrect:!1},{text:"An Amazon ECS Cluster with Auto Scaling",isCorrect:!1},{text:"Amazon ECS with the Fargate launch type",isCorrect:!0}],explanation:'The key requirement is that the company should be charged per running task. Therefore, the best answer is to use Amazon ECS with the Fargate launch type as with this model AWS charge you for running tasks rather than running container instances.The Fargate launch type allows you to run your containerized applications without the need to provision and manage the backend infrastructure. You just register your task definition and Fargate launches the container for you. The Fargate LaunchType is a serverless infrastructure managed by AWS.CORRECT: "Amazon ECS with the Fargate launch type" is the correct answer.INCORRECT: "Amazon ECS with the EC2 launch type" is incorrect as with this launch type you pay for running container instances (EC2 instances).INCORRECT: "An Amazon ECS Service with Auto Scaling" is incorrect as this does not specify the launch type. You can run an ECS Service on the Fargate or EC2 launch types.INCORRECT: "An Amazon ECS Cluster with Auto Scaling" is incorrect as this does not specify the launch type. You can run an ECS Cluster on the Fargate or EC2 launch types.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html'},{question:"A Development team manage a hybrid cloud environment. They would like to collect systemlevel metrics from onpremises servers and Amazon EC2 instances. How can the Development team collect this information MOST efficiently?",answers:[{text:"Install the CloudWatch agent on the onpremises servers and EC2 instances",isCorrect:!0},{text:"Install the CloudWatch agent on the EC2 instances and use a cron job on the onpremises servers",isCorrect:!1},{text:"Use CloudWatch for monitoring EC2 instances and custom AWS CLI scripts using the putmetricdata API",isCorrect:!1},{text:"Use CloudWatch detailed monitoring for both EC2 instances and onpremises servers",isCorrect:!1}],explanation:'The unified CloudWatch agent can be installed on both onpremises servers and Amazon EC2 instances using multiple operating system versions. It enables you to do the following:• Collect more systemlevel metrics from Amazon EC2 instances across operating systems. The metrics can include inguest metrics, in addition to the metrics for EC2 instances.• Collect systemlevel metrics from onpremises servers. These can include servers in a hybrid environment as well as servers not managed by AWS.• Retrieve custom metrics from your applications or services using the StatsD and collectd protocols.• Collect logs from Amazon EC2 instances and onpremises servers, running either Linux or Windows Server.Therefore, the Development team should install the CloudWatch agent on the onpremises servers and EC2 instances. This will allow them to collect systemlevel metrics from servers and instances across the hybrid cloud environment.CORRECT: "Install the CloudWatch agent on the onpremises servers and EC2 instances" is the correct answer.INCORRECT: "Use CloudWatch for monitoring EC2 instances and custom AWS CLI scripts using the putmetricdata API" is incorrect as this is not the most efficient option as you must write and maintain custom scripts. It is better to use the CloudWatch agent as it provides all the functionality required.INCORRECT: "Install the CloudWatch agent on the EC2 instances and use a cron job on the onpremises servers" is incorrect as the answer does not even specify what the cron job is going to do / use for gathering and sending the data.INCORRECT: "Use CloudWatch detailed monitoring for both EC2 instances and onpremises servers" is incorrect as this would not do anything for the onpremises instances.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/InstallCloudWatchAgent.html'},{question:"A monitoring application that keeps track of a large eCommerce website uses Amazon Kinesis for data ingestion. During periods of peak data rates, the producers are not making best use of the available shards. What step will allow the producers to better utilize the available shards and increase write throughput to the Kinesis data stream?",answers:[{text:"Increase the shard count of the stream using UpdateShardCount",isCorrect:!1},{text:"Install the Kinesis Producer Library (KPL) for ingesting data into the stream",isCorrect:!0},{text:"Ingest multiple records into the stream in a single call using BatchWriteItem",isCorrect:!1},{text:"Create an SQS queue and decouple the producers from the Kinesis data stream",isCorrect:!1}],explanation:'An Amazon Kinesis Data Streams producer is an application that puts user data records into a Kinesis data stream (also called data ingestion). The Kinesis Producer Library (KPL) simplifies producer application development, allowing developers to achieve high write throughput to a Kinesis data stream.The KPL is an easytouse, highly configurable library that helps you write to a Kinesis data stream. It acts as an intermediary between your producer application code and the Kinesis Data Streams API actions. The KPL performs the following primary tasks:• Writes to one or more Kinesis data streams with an automatic and configurable retry mechanism• Collects records and uses PutRecords to write multiple records to multiple shards per request• Aggregates user records to increase payload size and improve throughput• Integrates seamlessly with the Kinesis Client Library (KCL) to deaggregate batched records on the consumer• Submits Amazon CloudWatch metrics on your behalf to provide visibility into producer performanceThe question states that the producers are not making best use of the available shards. Therefore, we understand that there are adequate shards available but the producers are either not discovering them or are not writing records at sufficient speed to best utilize the shards.We therefore need to install the Kinesis Producer Library (KPL) for ingesting data into the stream.CORRECT: "Install the Kinesis Producer Library (KPL) for ingesting data into the stream" is the correct answer.INCORRECT: "Create an SQS queue and decouple the producers from the Kinesis data stream " is incorrect. In this case we need to ensure our producers are discovering shards and writing records to best utilize those shards.INCORRECT: "Increase the shard count of the stream using UpdateShardCount" is incorrect. The problem statement is that the producers are not making best use of the available shards. We don’t need to add more shards, we need to make sure the producers are discovering and then fully utilizing the shards that are available.INCORRECT: "Ingest multiple records into the stream in a single call using BatchWriteItem" is incorrect. This API is used with DynamoDB, not Kinesis.References: https://docs.aws.amazon.com/streams/latest/dev/developingproducerswithkpl.html'},{question:"A serverless application composed of multiple Lambda functions has been deployed. A developer is setting up AWS CodeDeploy to manage the deployment of code updates. The developer would like a 10% of the traffic to be shifted to the new version in equal increments, 10 minutes apart. Which setting should be chosen for configuring how traffic is shifted?",answers:[{text:"Allatonce",isCorrect:!1},{text:"Blue/green",isCorrect:!1},{text:"Canary",isCorrect:!1},{text:"Linear",isCorrect:!0}],explanation:'A deployment configuration is a set of rules and success and failure conditions used by CodeDeploy during a deployment. These rules and conditions are different, depending on whether you deploy to an EC2/OnPremises compute platform or an AWS Lambda compute platform.As you can see from the table above, the linear option shifts a specific amount of traffic in equal increments of time. Therefore, the following option should be chosen:CodeDeployDefault.LambdaLinear10PercentEvery10MinutesCORRECT: "Linear" is the correct answer.INCORRECT: "Canary" is incorrect as it does not shift traffic in equal increments.INCORRECT: "Allatonce" is incorrect as it shifts all traffic at once.INCORRECT: "Blue/green" is incorrect as it is a type of deployment, not a setting for traffic shifting.References: https://docs.aws.amazon.com/codedeploy/latest/userguide/deploymentconfigurations.html'},{question:"A website delivers images stored in an Amazon S3 bucket. The site uses Amazon Cognitoenabled and guest users without logins need to be able to view the images from the S3 bucket. How can a Developer enable access for guest users to the AWS resources?",answers:[{text:"Create a new user pool, enable access to unauthenticated identities, and grant access to AWS resources",isCorrect:!1},{text:"Create a new user pool, disable authentication access, and grant access to AWS resources",isCorrect:!1},{text:"Create a new identity pool, enable access to unauthenticated identities, and grant access to AWS resources",isCorrect:!0},{text:"Create a blank user ID in a user pool, add to the user group, and grant access to AWS resources",isCorrect:!1}],explanation:'Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token. An identity pool is a store of user identity data specific to your account.Amazon Cognito identity pools support both authenticated and unauthenticated identities. Authenticated identities belong to users who are authenticated by any supported identity provider. Unauthenticated identities typically belong to guest users.• To configure authenticated identities with a public login provider, see Identity Pools (Federated Identities) ExternalIdentity Providers.• To configure your own backend authentication process, see Developer Authenticated Identities (Identity Pools).Therefore, the Developer should create a new identity pool, enable access to unauthenticated identities, and grant access to AWS resources.CORRECT: "Create a new identity pool, enable access to unauthenticated identities, and grant access to AWS resources" is the correct answer.INCORRECT: "Create a blank user ID in a user pool, add to the user group, and grant access to AWS resources" is incorrect as you must use identity pools for unauthenticated users.INCORRECT: "Create a new user pool, enable access to unauthenticated identities, and grant access to AWS resources" is incorrect as you must use identity pools for unauthenticated users.INCORRECT: "Create a new user pool, disable authentication access, and grant access to AWS resources" is incorrect as you must use identity pools for unauthenticated users.References: https://docs.aws.amazon.com/cognito/latest/developerguide/identitypools.html'},{question:"A Developer is creating an application that uses Amazon EC2 instances and must be highly available and fault tolerant. How should the Developer configure the VPC?",answers:[{text:"Create a cluster placement group for the EC2 instances",isCorrect:!1},{text:"Create a subnet in each availability zone in the region",isCorrect:!0},{text:"Create multiple subnets within a single availability zone in the region",isCorrect:!1},{text:"Create an Internet Gateway for every availability zone",isCorrect:!1}],explanation:'To ensure high availability and fault tolerance the Developer should create a subnet within each availability zone. The EC2 instances should then be distributed between these subnets.The Developer would likely use Amazon EC2 Auto Scaling which will automatically launch instances in each subnet and then Elastic Load Balancing to distributed incoming traffic.CORRECT: "Create a subnet in each availability zone in the region" is the correct answer.INCORRECT: "Create multiple subnets within a single availability zone in the region" is incorrect as this will not provide fault tolerance in the event that the AZ becomes unavailable.INCORRECT: "Create an Internet Gateway for every availability zone" is incorrect as there is a single Internet Gateway per VPC.INCORRECT: "Create a cluster placement group for the EC2 instances" is incorrect as this is used for ensuring low latency access between EC2 instances in a single availability zone.References: https://d1.awsstatic.com/whitepapers/awsbuildingfaulttolerantapplications.pdf'},{question:"An organization has encrypted a large quantity of data. To protect their data encryption keys they are planning to use envelope encryption. Which of the following processes is a correct implementation of envelope encryption?",answers:[{text:"Encrypt plaintext data with a data key and then encrypt the data key with a toplevel encrypted master key",isCorrect:!1},{text:"Encrypt plaintext data with a master key and then encrypt the master key with a toplevel plaintext data key",isCorrect:!1},{text:"Encrypt plaintext data with a master key and then encrypt the master key with a toplevel encrypted data key",isCorrect:!1},{text:"Encrypt plaintext data with a data key and then encrypt the data key with a toplevel plaintext master key.",isCorrect:!0}],explanation:'When you encrypt your data, your data is protected, but you have to protect your encryption key. One strategy is to encrypt it. Envelope encryption is the practice of encrypting plaintext data with a data key, and then encrypting the data key under another key.You can even encrypt the data encryption key under another encryption key and encrypt that encryption key under another encryption key. But, eventually, one key must remain in plaintext so you can decrypt the keys and your data. This toplevel plaintext key encryption key is known as the master key.Envelope encryption offers several benefits:• Protecting data keysWhen you encrypt a data key, you don\'t have to worry about storing the encrypted data key, because the data key is inherently protected by encryption. You can safely store the encrypted data key alongside the encrypted data.• Encrypting the same data under multiple master keys Encryption operations can be time consuming, particularly when the data being encrypted are large objects. Instead of reencrypting raw data multiple times with different keys, you can reencrypt only the data keys that protect the raw data.• Combining the strengths of multiple algorithmsIn general, symmetric key algorithms are faster and produce smaller ciphertexts than public key algorithms. But public key algorithms provide inherent separation of roles and easier key management. Envelope encryption lets youcombine the strengths of each strategy.As described above, the process that should be implemented is to encrypt plaintext data with a data key and then encrypt the data key with a toplevel plaintext master key.CORRECT: "Encrypt plaintext data with a data key and then encrypt the data key with a toplevel plaintext master key" is the correct answer.INCORRECT: "Encrypt plaintext data with a master key and then encrypt the master key with a toplevel plaintext data key" is incorrect as the master key is the toplevel key.INCORRECT: "Encrypt plaintext data with a data key and then encrypt the data key with a toplevel encrypted master key" is incorrect as the toplevel master key must be unencrypted so it can be used to decrypt data.INCORRECT: "Encrypt plaintext data with a master key and then encrypt the master key with a toplevel encrypted data key" is incorrect as the master key is the toplevel key.References: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping'},{question:"A developer is creating a multitier web application. The frontend will place messages in an Amazon SQS queue for the backend to process. Each job includes a file that is 1GB in size. What MUST the developer do to ensure this works as expected?",answers:[{text:"Store the large files in DynamoDB and use the SQS Extended Client Library for Java to manage SQS messages",isCorrect:!1},{text:"Increase the maximum message size of the queue from 256KB to 1GB",isCorrect:!1},{text:"Create a FIFO queue that supports large files",isCorrect:!1},{text:"Store the large files in Amazon S3 and use the SQS Extended Client Library for Java to manage SQS messages",isCorrect:!0}],explanation:'You can use Amazon S3 and the Amazon SQS Extended Client Library for Java to manage Amazon SQS messages. This is especially useful for storing and consuming messages up to 2 GB in size. Unless your application requires repeatedly creating queues and leaving them inactive or storing large amounts of data in your queue, consider using Amazon S3 for storing your data.You can use the Amazon SQS Extended Client Library for Java library to do the following:• Specify whether messages are always stored in Amazon S3 or only when the size of a message exceeds 256 KB.• Send a message that references a single message object stored in an Amazon S3 bucket.• Get the corresponding message object from an Amazon S3 bucket.• Delete the corresponding message object from an Amazon S3 bucket.Note: Amazon SQS only supports messages up to 256KB in size. Therefore, the extended client library for Java must be used.CORRECT: "Store the large files in Amazon S3 and use the SQS Extended Client Library for Java to manage SQS messages" is the correct answer.INCORRECT: "Increase the maximum message size of the queue from 256KB to 1GB" is incorrect as you cannot increase the maximum message size above 256KB.INCORRECT: "Store the large files in DynamoDB and use the SQS Extended Client Library for Java to manage SQS messages" is incorrect as you should store the files in Amazon S3.INCORRECT: "Create a FIFO queue that supports large files " is incorrect as FIFO queues also have a maximum message size of 256KB.References:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqss3messages.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/workingjavaexampleusings3forlargesqsmessages.html'},{question:"An application uses Amazon EC2 instances, AWS Lambda functions and an Amazon SQS queue. The Developer must ensure all communications are within an Amazon VPC using private IP addresses. How can this be achieved? (Select TWO.)",answers:[{text:"Add the AWS Lambda function to the VPC",isCorrect:!0},{text:"Create a VPN and connect the services to the VPG",isCorrect:!1},{text:"Create a VPC endpoint for AWS Lambda",isCorrect:!1},{text:"Create the Amazon SQS queue within a VPC",isCorrect:!1},{text:"Create a VPC endpoint for Amazon SQS",isCorrect:!0}],explanation:'This solution can be achieved by adding the AWS Lambda function to a VPC through the function configuration and by creating a VPC endpoint for Amazon SQS. This will result in the services using purely private IP addresses to communicate without traversing the public Internet.CORRECT: "Add the AWS Lambda function to the VPC" is a correct answer.CORRECT: "Create a VPC endpoint for Amazon SQS" is also a correct answer.INCORRECT: "Create the Amazon SQS queue within a VPC" is incorrect as you cannot create a queue within a VPC as Amazon SQS is a public service.INCORRECT: "Create a VPC endpoint for AWS Lambda" is incorrect as you cannot create a VPC endpoint for AWS Lambda. You can, however, connect a Lambda function to a VPC.INCORRECT: "Create a VPN and connect the services to the VPG" is incorrect as you cannot create a VPN between each of these services.References:https://aws.amazon.com/aboutaws/whatsnew/2018/12/amazonsqsvpcendpointsawsprivatelink/ https://docs.aws.amazon.com/lambda/latest/dg/configurationvpc.html'},{question:"An application collects data from sensors in a manufacturing facility. The data is stored in an Amazon SQS Standard queue by an AWS Lambda function and an Amazon EC2 instance processes the data and stores it in an Amazon RedShift data warehouse. A fault in the sensors' software is causing occasional duplicate messages to be sent. Timestamps on the duplicate messages show they are generated within a few seconds of the primary message. How a can a Developer prevent duplicate data being stored in the data warehouse?",answers:[{text:"Use a FIFO queue and configure the Lambda function to add a message group ID to the messages generated by each individual sensor",isCorrect:!1},{text:"Use a FIFO queue and configure the Lambda function to add a message deduplication token to the message body",isCorrect:!0},{text:"Configure a redrive policy, specify a destination DeadLetter queue, and set the maxReceiveCount to 1",isCorrect:!1},{text:"Send a ChangeMessageVisibility call with VisibilityTimeout set to 30 seconds after the receipt of every message from the queue",isCorrect:!1}],explanation:'FIFO (FirstInFirstOut) queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can\'t be tolerated.In FIFO queues, messages are ordered based on message group ID. If multiple hosts (or different threads on the same host) send messages with the same message group ID to a FIFO queue, Amazon SQS stores the messages in the order in which they arrive for processing. To ensure that Amazon SQS preserves the order in which messages are sent and received, ensure that each producer uses a unique message group ID to send all its messages.FIFO queue logic applies only per message group ID. Each message group ID represents a distinct ordered message group within an Amazon SQS queue. For each message group ID, all messages are sent and received in strict order. However, messages with different message group ID values might be sent and received out of order. You must associate a message group ID with a message. If you don\'t provide a message group ID, the action fails. If you require a single group of ordered messages, provide the same message group ID for messages sent to the FIFO queue.Therefore, the Developer can use a FIFO queue and configure the Lambda function to add a message deduplication token to the message body. This will ensure that the messages are deduplicated before being picked up for processing by the Amazon EC2 instance.CORRECT: "Use a FIFO queue and configure the Lambda function to add a message deduplication token to the message body" is the correct answer.INCORRECT: "Use a FIFO queue and configure the Lambda function to add a message group ID to the messages generated by each individual sensor" is incorrect. The message group ID is the tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are always processed one by one, in a strict order relative to the message group.INCORRECT: "Send a ChangeMessageVisibility call with VisibilityTimeout set to 30 seconds after the receipt of every message from the queue" is incorrect as this will just change the visibility timeout for the message which will prevent others from seeing it until it has been processed and deleted from the queue. This doesn’t stop a message with duplicate data being processed.INCORRECT: "Configure a redrive policy, specify a destination DeadLetter queue, and set the maxReceiveCount to 1" is incorrect as without a FIFO queue and a message deduplication ID duplicate messages will still enter the queue. The redrive policy only applies to individual messages for which processing has failed a number of times as specified in the maxReceiveCount.References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/usingmessagededuplicationidproperty.html'},{question:"An application will ingest data at a very high throughput from several sources and stored in an Amazon S3 bucket for subsequent analysis. Which AWS service should a Developer choose for this requirement?",answers:[{text:"Amazon Kinesis Data Analytics",isCorrect:!1},{text:"Amazon Kinesis Data Firehose",isCorrect:!0},{text:"Amazon Simple Queue Service (SQS)",isCorrect:!1},{text:"Amazon S3 Transfer Acceleration",isCorrect:!1}],explanation:'Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores and analytics tools.It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near realtime analytics with existing business intelligence tools and dashboards.A destination is the data store where your data will be delivered. Firehose Destinations include:• Amazon S3.• Amazon Redshift.• Amazon Elasticsearch Service.• Splunk.For Amazon S3 destinations, streaming data is delivered to your S3 bucket. If data transformation is enabled, you can optionally back up source data to another Amazon S3 bucket:The best choice of AWS service for this scenario is to use Amazon Kinesis Data Firehose as it can ingest large amounts of data at extremely high throughput and load that data into an Amazon S3 bucketCORRECT: "Amazon Kinesis Data Firehose" is the correct answer.INCORRECT: "Amazon S3 Transfer Acceleration" is incorrect as this is a service used for improving the performance of uploads into Amazon S3. It is not suitable for ingesting streaming data.INCORRECT: "Amazon Kinesis Data Analytics" is incorrect as this service is used for processing and analyzing realtime, streaming data. The easiest way to load streaming data into a data store for analysing at a later time is Kinesis Data FirehoseINCORRECT: "Amazon Simple Queue Service (SQS)" is incorrect as this is not the best solution for this scenario. With SQS you need a producer to place the messages on the queue and then consumers to process the messages and load them into Amazon S3. Kinesis Data Firehose can do this natively without the need for consumers.References: https://aws.amazon.com/kinesis/datafirehose/'},{question:"A Developer created an AWS Lambda function and then attempted to add an on failure destination but received the following error:The function's execution role does not have permissions to call SendMessage on arn:aws:sqs:useast1:515148212435:FailureDestinationHow can the Developer resolve this issue MOST securely?",answers:[{text:"Add the AWSLambdaSQSQueueExecutionRole AWS managed policy to the function’s execution role",isCorrect:!1},{text:"Add a permissions policy to the SQS queue allowing the SendMessage action and specify the AWS account number",isCorrect:!1},{text:"Create a customer managed policy with all read/write permissions to SQS and attach the policy to the function’s execution role",isCorrect:!0},{text:"Add the Lambda function to a group with administrative privileges",isCorrect:!1}],explanation:'The Lambda function needs the privileges to use the SendMessage API action on the Amazon SQS queue. The permissions should be assigned to the function’s execution role. The AWSLambdaSQSQueueExecutionRole AWS managed policy cannot be used as this policy does not provide the SendMessage action.The Developer should therefore create a customer managed policy with read/write permissions to SQS and attach the policy to the function’s execution role.CORRECT: "Create a customer managed policy with all read/write permissions to SQS and attach the policy to the function’s execution role" is the correct answer.INCORRECT: "Add the AWSLambdaSQSQueueExecutionRole AWS managed policy to the function’s execution role" is incorrect as this does not provide the necessary permissions.INCORRECT: "Add a permissions policy to the SQS queue allowing the SendMessage action and specify the AWS account number" is incorrect as this would allow any resource in the AWS account to write to the queue which is less secure.INCORRECT: "Add the Lambda function to a group with administrative privileges" is incorrect as you cannot add a Lambda function to an IAM group.References: https://docs.aws.amazon.com/lambda/latest/dg/lambdaintroexecutionrole.html'},{question:"A Developer is creating an AWS Lambda function that will process medical images. The function is dependent on several libraries that are not available in the Lambda runtime environment. Which strategy should be used to create the Lambda deployment package?",answers:[{text:"Create a ZIP file with the source code and a script that installs the dependent libraries at runtime",isCorrect:!1},{text:"Create a ZIP file with the source code. Stage the dependent libraries on an Amazon S3 bucket indicated by the Lambda environment variable LIBRARY_PATH",isCorrect:!1},{text:"Create a ZIP file with the source code and all dependent libraries",isCorrect:!0},{text:"Create a ZIP file with the source code and a buildspec.yaml file that installs the dependent libraries on AWS Lambda",isCorrect:!1}],explanation:'A deployment package is a ZIP archive that contains your function code and dependencies. You need to create a deployment package if you use the Lambda API to manage functions, or if you need to include libraries and dependencies other than the AWS SDK.You can upload the package directly to Lambda, or you can use an Amazon S3 bucket, and then upload it to Lambda. If the deployment package is larger than 50 MB, you must use Amazon S3.If your function depends on libraries not included in the Lambda runtime, you can install them to a local directory and include them in your deployment package.CORRECT: "Create a ZIP file with the source code and all dependent libraries" is the correct answer.INCORRECT: "Create a ZIP file with the source code and a script that installs the dependent libraries at runtime" is incorrect as though it is possible to call a script within the function code, this would need to run every time and pull in the files which would cause latency.INCORRECT: "Create a ZIP file with the source code. Stage the dependent libraries on an Amazon S3 bucket indicated by the Lambda environment variable LIBRARY_PATH" is incorrect as you cannot map an external path to a Lambda function using an environment variable.INCORRECT: "Create a ZIP file with the source code and a buildspec.yaml file that installs the dependent libraries on AWS Lambda" is incorrect as a buildspec.yaml file that is used by AWS CodeBuild to run a build. The libraries need to be included in the package zip file for the Lambda function.References: https://docs.aws.amazon.com/lambda/latest/dg/pythonpackage.html'},{question:"A company has sensitive data that must be encrypted. The data is made up of 1 GB objects and there is a total of 150 GB of data. What is the BEST approach for a Developer to encrypt the data using AWS KMS?",answers:[{text:"Make a GenerateDataKey API call that returns a plaintext key and an encrypted copy of a data key. Use the plaintext key to encrypt the data",isCorrect:!0},{text:"Make a GenerateDataKeyWithoutPlaintext API call that returns an encrypted copy of a data key. Use the encrypted key to encrypt the data",isCorrect:!1},{text:"Make an Encrypt API call to encrypt the plaintext data as ciphertext using a customer master key (CMK)",isCorrect:!1},{text:"Make an Encrypt API call to encrypt the plaintext data as ciphertext using a customer master key (CMK) with imported key material",isCorrect:!1}],explanation:'To encrypt large quantities of data with the AWS Key Management Service (KMS), you must use a data encryption key rather than a customer master keys (CMK). This is because a CMK can only encrypt up to 4KB in a single operation and in this scenario the objects are 1 GB in size.To create a data key, call the GenerateDataKey operation. AWS KMS uses the CMK that you specify to generate a data key. The operation returns a plaintext copy of the data key and a copy of the data key encrypted under the CMK. AWS KMS cannot use a data key to encrypt data. But you can use the data key outside of KMS, such as by using OpenSSL or a cryptographic library like the AWS Encryption SDK. Data can then be encrypted using the plaintext data key as depicted below:Therefore, the Developer should make a GenerateDataKey API call that returns a plaintext key and an encrypted copy of a data key, and then use the plaintext key to encrypt the data.CORRECT: "Make a GenerateDataKey API call that returns a plaintext key and an encrypted copy of a data key. Use the plaintext key to encrypt the data" is the correct answer.INCORRECT: "Make an Encrypt API call to encrypt the plaintext data as ciphertext using a customer master key (CMK)" is incorrect as you cannot use a CMK to encrypt objects over 4 KB in size.INCORRECT: "Make an Encrypt API call to encrypt the plaintext data as ciphertext using a customer master key (CMK) with imported key material" is incorrect as you cannot use a CMK to encrypt objects over 4 KB in size.INCORRECT: "Make a GenerateDataKeyWithoutPlaintext API call that returns an encrypted copy of a data key. Use the encrypted key to encrypt the data" is incorrect as you need to encrypt data with a plaintext data key.References: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#datakeys'},{question:"A company has released a new application on AWS. The company are concerned about security and require a tool that can automatically assess applications for exposure, vulnerabilities, and deviations from best practices. Which AWS service should they use?",answers:[{text:"AWS Secrets Manager",isCorrect:!1},{text:"AWS Shield",isCorrect:!1},{text:"Amazon Inspector",isCorrect:!0},{text:"AWS WAF",isCorrect:!1}],explanation:'Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices.After performing an assessment, Amazon Inspector produces a detailed list of security findings prioritized by level of severity. These findings can be reviewed directly or as part of detailed assessment reports which are available via the Amazon Inspector console or API.CORRECT: "Amazon Inspector" is the correct answer.INCORRECT: "AWS Shield" is incorrect as this service is used to protect from distributed denial of service (DDoS) attacks.INCORRECT: "AWS WAF" is incorrect as this is a web application firewall.INCORRECT: "AWS Secrets Manager" is incorrect as this service is used to store secure secrets.References: https://aws.amazon.com/inspector/'},{question:'An application will generate thumbnails from objects uploaded to an Amazon S3 bucket. The Developer has created the bucket configuration and the AWS Lambda function and has formulated the following AWS CLI command:aws lambda addpermission functionname CreateThumbnail principal s3.amazonaws.com statementid s3invoke action"lambda:InvokeFunction" sourcearn arn:aws:s3:::digitalcloudbucketsource sourceaccount 523107438921What will be achieved by running the AWS CLI command?',answers:[{text:"The Amazon S3 service principal (s3.amazonaws.com) will be granted permissions to perform the create an eventsource mapping with the digitalcloudbucketsource bucket",isCorrect:!1},{text:"The Lambda function CreateThumbnail will be granted permissions to access the objects in the digitalcloudbucket source bucket",isCorrect:!1},{text:"A Lambda function will be created called CreateThumbnail with an Amazon SNS event source mapping that executes the function when objects are uploaded",isCorrect:!1},{text:"The Amazon S3 service principal (s3.amazonaws.com) will be granted permissions to perform the lambda:InvokeFunction action",isCorrect:!0}],explanation:'In this scenario the Developer is using an AWS Lambda function to process images that are uploaded to an Amazon S3 bucket.The AWS Lambda function has been created and the notification settings on the bucket have been configured. The last thing to do is to grant permissions for the Amazon S3 service principal to invoke the function.The Lambda CLI addpermission command grants the Amazon S3 service principal (s3.amazonaws.com) permissions to perform the lambda:InvokeFunction action.CORRECT: "The Amazon S3 service principal (s3.amazonaws.com) will be granted permissions to perform the lambda:InvokeFunction action" is the correct answer.INCORRECT: "The Lambda function CreateThumbnail will be granted permissions to access the objects in the digitalcloudbucketsource bucket" is incorrect as the CLI command grants S3 the ability to execute the Lambda function.INCORRECT: "The Amazon S3 service principal (s3.amazonaws.com) will be granted permissions to perform the create an eventsource mapping with the digitalcloudbucketsource bucket" is incorrect as event source mappings are created with services such as Kinesis, DynamoDB, and SQS.INCORRECT: "A Lambda function will be created called CreateThumbnail with an Amazon SNS event source mapping that executes the function when objects are uploaded" is incorrect. This solution does not use Amazon SNS, the S3 notification invokes the Lambda function directly.References: https://docs.aws.amazon.com/lambda/latest/dg/withs3example.html'},{question:"A company use Amazon CloudFront to deliver application content to users around the world. A Developer has made an update to some files in the origin however users have reported that they are still getting the old files. How can the Developer ensure that the old files are replaced in the cache with the LEAST disruption?",answers:[{text:"Invalidate the files from the edge caches",isCorrect:!0},{text:"Add code to Lambda@Edge that updates the files in the cache",isCorrect:!1},{text:"Disable the CloudFront distribution and enable it again to update all the edge locations",isCorrect:!1},{text:"Create a new origin with the new files and remove the old origin",isCorrect:!1}],explanation:'If you need to remove files from CloudFront edge caches before they expire you can invalidate the files from the edge caches.To invalidate files, you can specify either the path for individual files or a path that ends with the * wildcard, which might apply to one file or to many, as shown in the following examples:• /images/image1.jpg• /images/image*• /images/*You can submit a specified number of invalidation paths each month for free. If you submit more than the allotted number of invalidation paths in a month, you pay a fee for each invalidation path that you submit.CORRECT: "Invalidate the files from the edge caches" is the correct answer.INCORRECT: "Create a new origin with the new files and remove the old origin" is incorrect as this would be more disruptive and costly as the entire cache would need to be updated.INCORRECT: "Disable the CloudFront distribution and enable it again to update all the edge locations" is incorrect as this will cause an outage (disruption) and will not replace files that have not yet expired.INCORRECT: "Add code to Lambda@Edge that updates the files in the cache" is incorrect as there’s no value in running code in Lambda@Edge to update the files. Instead the files in the cache can be invalidated.References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html'},{question:"A Development team wants to instrument their code to provide more detailed information to AWS XRay than simple outgoing and incoming requests. This will generate large amounts of data, so the Development team wants to implement indexing so they can filter the data. What should the Development team do to achieve this?",answers:[{text:"Add metadata to the segment document",isCorrect:!1},{text:"Install required plugins for the appropriate AWS SDK",isCorrect:!1},{text:"Add annotations to the segment document",isCorrect:!0},{text:"Configure the necessary XRay environment variables",isCorrect:!1}],explanation:'AWS XRay makes it easy for developers to analyze the behavior of their production, distributed applications with endtoend tracing capabilities. You can use XRay to identify performance bottlenecks, edge case errors, and other hard to detect issues.When you instrument your application, the XRay SDK records information about incoming and outgoing requests, the AWS resources used, and the application itself. You can add other information to the segment document as annotations and metadata. Annotations and metadata are aggregated at the trace level and can be added to any segment or subsegment.Annotations are simple keyvalue pairs that are indexed for use with filter expressions. Use annotations to record data that you want to use to group traces in the console, or when calling the GetTraceSummaries API. XRay indexes up to 50 annotations per trace.Metadata are keyvalue pairs with values of any type, including objects and lists, but that are not indexed. Use metadata to record data you want to store in the trace but don\'t need to use for searching traces.You can view annotations and metadata in the segment or subsegment details in the XRay console.In this scenario, we need to add annotations to the segment document so that the data that needs to be filtered is indexed.CORRECT: "Add annotations to the segment document" is the correct answer.INCORRECT: "Add metadata to the segment document" is incorrect as metadata is not indexed for filtering.INCORRECT: "Configure the necessary XRay environment variables" is incorrect as this will not result in indexing of the required data.INCORRECT: "Install required plugins for the appropriate AWS SDK" is incorrect as there are no plugin requirements for the AWS SDK to support this solution as the annotations feature is available in AWS XRay.References: https://docs.aws.amazon.com/xray/latest/devguide/xrayconcepts.html#xrayconceptsannotations'},{question:"A solution requires a serverless service for receiving streaming data and loading it directly into an Amazon Elasticsearch datastore. Which AWS service would be suitable for this requirement?",answers:[{text:"Amazon Simple Queue Service (SQS)",isCorrect:!1},{text:"Amazon Kinesis Data Firehose",isCorrect:!0},{text:"Amazon Kinesis Data Streams",isCorrect:!1},{text:"Amazon Kinesis Data Analytics",isCorrect:!1}],explanation:'Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near realtime analytics with existing business intelligence tools and dashboards you’re already using today.Firehose is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.CORRECT: "Amazon Kinesis Data Firehose" is the correct answer.INCORRECT: "Amazon Kinesis Data Streams" is incorrect as with Kinesis Data Streams you need consumers running on EC2 instances or AWS Lambda for processing the data from the stream. It therefore will not load data directly to a datastore.INCORRECT: "Amazon Kinesis Data Analytics" is incorrect as this service is used for performing analytics on streaming data using Structured Query Language (SQL queries.INCORRECT: "Amazon Simple Queue Service (SQS)" is incorrect as this is a message queueing service. You would need servers to place messages on the queue and then other servers to process messages from the queue and store them in Elasticsearch.References: https://aws.amazon.com/kinesis/datafirehose/faqs/'},{question:"A Developer needs to update an Amazon ECS application that was deployed using AWS CodeDeploy. What file does the Developer need to update to push the change through CodeDeploy?",answers:[{text:"buildspec.yml",isCorrect:!1},{text:"dockerrun.aws.json",isCorrect:!1},{text:"ebextensions.config",isCorrect:!1},{text:"appspec.yml",isCorrect:!0}],explanation:'In CodeDeploy, a revision contains a version of the source files CodeDeploy will deploy to your instances or scripts CodeDeploy will run on your instances. You plan the revision, add an AppSpec file to the revision, and then push the revision to Amazon S3 or GitHub. After you push the revision, you can deploy it.For a deployment to an Amazon ECS compute platform:• The AppSpec file specifies the Amazon ECS task definition used for the deployment, a container name and port mapping used to route traffic, and optional Lambda functions run after deployment lifecycle events.• A revision is the same as an AppSpec file.• An AppSpec file can be written using JSON or YAML.• An AppSpec file can be saved as a text file or entered directly into a console when you create a deployment.Therefore, the appspec.yml file needs to be updated by the Developer.CORRECT: "appspec.yml" is the correct answer.INCORRECT: "dockerrun.aws.json" is incorrect. A Dockerrun.aws.json file describes how to deploy a remote Docker image as an Elastic Beanstalk application.INCORRECT: "buildspec.yml" is incorrect. A build spec is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build using AWS CodeBuild.INCORRECT: "ebextensions.config" is incorrect. The .ebextensions folder in the source code for an Elastic Beanstalk application is used for .config files that configure the environment and customize resources.References: https://docs.aws.amazon.com/codedeploy/latest/userguide/applicationrevisionsappspecfile.html'},{question:"A Developer is creating a script to automate the deployment process for a serverless application. The Developer wants to use an existing AWS Serverless Application Model (SAM) template for the application. What should the Developer use for the project? (Select TWO.)",answers:[{text:"Create a ZIP package locally and call aws serverlessrepo createapplication to create the application",isCorrect:!1},{text:"Create a ZIP package and upload it to Amazon S3. Call aws cloudformation createstack to create the application",isCorrect:!1},{text:"Call sam package to create the deployment package. Call sam deploy to deploy the package afterward",isCorrect:!0},{text:"Call aws s3 cp to upload the AWS SAM template to Amazon S3. Call aws lambda updatefunctioncode to create the application",isCorrect:!1},{text:"Call aws cloudformation package to create the deployment package. Call aws cloudformation deploy to deploy the package afterward",isCorrect:!0}],explanation:'The AWS Serverless Application Model (SAM) is an opensource framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax, enabling you to build serverless applications faster.To get started with building SAMbased applications, use the AWS SAM CLI. SAM CLI provides a Lambdalike execution environment that lets you locally build, test, and debug applications defined by SAM templates. You can also use the SAM CLI to deploy your applications to AWS.With the SAM CLI you can package and deploy your source code using two simple commands:• sam package• sam deployAlternatively, you can use:• aws cloudformation package• aws cloudformation deployTherefore, the Developer can use either the sam or aws cloudformation CLI commands to package and deploy the serverless application.CORRECT: "Call aws cloudformation package to create the deployment package. Call aws cloudformation deploy to deploy the package afterward" is a correct answer.CORRECT: "Call sam package to create the deployment package. Call sam deploy to deploy the package afterward" is a correct answer.INCORRECT: "Call aws s3 cp to upload the AWS SAM template to Amazon S3. Call aws lambda updatefunctioncode to create the application" is incorrect as this is not how to use a SAM template. With SAM the commands mentioned above must be run.INCORRECT: "Create a ZIP package locally and call aws serverlessrepo createapplication to create the application" is incorrect as this is not the correct way to use a SAM template.INCORRECT: "Create a ZIP package and upload it to Amazon S3. Call aws cloudformation createstack to create the application" is incorrect as this is not required when deploying a SAM template.References: https://aws.amazon.com/serverless/sam/'},{question:"A mobile application runs as a serverless application on AWS. A Developer needs to create a push notification feature that sends periodic message to subscribers. How can the Developer send the notification from the application?",answers:[{text:"Publish a message to an Amazon SQS Queue",isCorrect:!1},{text:"Publish a notification to an Amazon SNS Topic",isCorrect:!0},{text:"Publish a message to an Amazon SWF Workflow",isCorrect:!1},{text:"Publish a notification to Amazon CloudWatch Events",isCorrect:!1}],explanation:'With Amazon SNS, you have the ability to send push notification messages directly to apps on mobile devices. Push notification messages sent to a mobile endpoint can appear in the mobile app as message alerts, badge updates, or even sound alerts.You send push notification messages to both mobile devices and desktops using one of the following supported push notification services:• Amazon Device Messaging (ADM)• Apple Push Notification Service (APNs) for both iOS and Mac OS X• Baidu Cloud Push (Baidu)• Firebase Cloud Messaging (FCM)• Microsoft Push Notification Service for Windows Phone (MPNS)• Windows Push Notification Services (WNS)To send a notification to an Amazon SNS subscriber, the application needs to send the notification to an Amazon SNS Topic. Amazon SNS will then send the notification to the relevant subscribers.CORRECT: "Publish a notification to an Amazon SNS Topic" is the correct answer.INCORRECT: "Publish a message to an Amazon SQS Queue" is incorrect as SQS is a message queue service, not a notification service.INCORRECT: "Publish a notification to Amazon CloudWatch Events" is incorrect as CloudWatch Events will not be able to send notifications to mobile app users.INCORRECT: "Publish a message to an Amazon SWF Workflow" is incorrect as SWF is a workflow orchestration service and it is not used for publishing messages to mobile app users.References: https://docs.aws.amazon.com/sns/latest/dg/snshowusernotificationswork.html'},{question:"A Developer has code running on Amazon EC2 instances that needs readonly access to an Amazon DynamoDB table. What is the MOST secure approach the Developer should take to accomplish this task?",answers:[{text:"Use an IAM role with an AmazonDynamoDBReadOnlyAccess policy applied to the EC2 instances",isCorrect:!0},{text:"Create a user access key for each EC2 instance with readonly access to DynamoDB. Place the keys in the code. Redeploy the code as keys rotate",isCorrect:!1},{text:"Use an IAM role with Administrator access applied to the EC2 instance",isCorrect:!1},{text:"Run all code with only AWS account root user access keys to ensure maximum access to services",isCorrect:!1}],explanation:'According to the principal of least privilege the Developer needs to provide the minimum permissions that application requires.The application needs readonly access and therefore an IAM role with an AmazonDynamoDBReadOnlyAccess policy applied that only provides readonly access to DynamoDB is secure.This role can be applied to the EC2 instance through the management console or programmatically by creating an instance profile and attaching the role to the instance profile. The EC2 instance can then assume the role and get readonly access to DynamoDB.CORRECT: "Use an IAM role with an AmazonDynamoDBReadOnlyAccess policy applied to the EC2 instances" is the correct answer.INCORRECT: "Create a user access key for each EC2 instance with readonly access to DynamoDB. Place the keys in the code. Redeploy the code as keys rotate" is incorrect as access keys are less secure than using roles as the keys are stored in the code.INCORRECT: "Run all code with only AWS account root user access keys to ensure maximum access to services" is incorrect as this is highly insecure as the access keys are stored in code and these access keys provide full permissions to the AWS account.INCORRECT: "Use an IAM role with Administrator access applied to the EC2 instance" is incorrect as this does not follow the principal of least privilege and is therefore less secure. The role used should have readonly access to DynamoDB.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/usingidentitybasedpolicies.html'},{question:"A company has a global presence and managers must submit large quantities of reporting data to an Amazon S3 bucket located in the useast1 region on weekly basis. Uploads have been slow recently, how can you improve data throughput and upload times?",answers:[{text:"Use S3 Multipart upload",isCorrect:!1},{text:"Use an AWS Managed VPN",isCorrect:!1},{text:"Enable S3 Transfer Acceleration on the S3 bucket",isCorrect:!0},{text:"Create an AWS Direct Connect connection from each remote office",isCorrect:!1}],explanation:'Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.You might want to use Transfer Acceleration on a bucket for various reasons, including the following:• You have customers that upload to a centralized bucket from all over the world.• You transfer gigabytes to terabytes of data on a regular basis across continents.• You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.Therefore, Amazon S3 Transfer Acceleration is an ideal solution for this use case and will result in improved throughput and upload times.CORRECT: "Enable S3 Transfer Acceleration on the S3 bucket" is the correct answer.INCORRECT: "Use S3 Multipart upload" is incorrect. Multipart upload will perform multiple uploads in parallel which does improve performance however Transfer Acceleration will utilize CloudFront and result in much improved performance over multipart upload.INCORRECT: "Create an AWS Direct Connect connection from each remote office" is incorrect as Direct Connect is used to connect from a data center into an AWS region that is local to the data center, not somewhere else in the world (though Direct Connect Gateway can do this). This would also be an extremely expensive solution.INCORRECT: "Use an AWS Managed VPN" is incorrect as this is used to create an encrypted tunnel into a VPC and will not result in improved upload performance for S3 uploads.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/transferacceleration.html'},{question:"A Development team are creating a financial trading application. The application requires submillisecond latency for processing trading requests. Amazon DynamoDB is used to store the trading data. During load testing the Development team found that in periods of high utilization the latency is too high and read capacity must be significantly overprovisioned to avoid throttling. How can the Developers meet the latency requirements of the application?",answers:[{text:"Use Amazon DynamoDB Accelerator (DAX) to cache the data",isCorrect:!0},{text:"Create a Global Secondary Index (GSI) for the trading data",isCorrect:!1},{text:"Use exponential backoff in the application code for DynamoDB queries",isCorrect:!1},{text:"Store the trading data in Amazon S3 and use Transfer Acceleration",isCorrect:!1}],explanation:'Amazon DynamoDB is designed for scale and performance. In most cases, the DynamoDB response times can be measured in singledigit milliseconds. However, there are certain use cases that require response times in microseconds. For these use cases, DynamoDB Accelerator (DAX) delivers fast response times for accessing eventually consistent data. DAX is a DynamoDBcompatible caching service that enables you to benefit from fast inmemory performance for demanding applications. DAX addresses three core scenarios:1. As an inmemory cache, DAX reduces the response times of eventually consistent read workloads by an order of magnitude from singledigit milliseconds to microseconds.2. DAX reduces operational and application complexity by providing a managed service that is APIcompatible with DynamoDB. Therefore, it requires only minimal functional changes to use with an existing application.3. For readheavy or bursty workloads, DAX provides increased throughput and potential operational cost savings by reducing the need to overprovision read capacity units. This is especially beneficial for applications that requirerepeated reads for individual keys.In this scenario the question is calling for submillisecond (e.g. microsecond) latency and this is required for read traffic as evidenced by the need to overprovision reads. Therefore, DynamoDB DAX would be the best solution for reducing the latency and meeting the requirements.CORRECT: "Use Amazon DynamoDB Accelerator (DAX) to cache the data" is the correct answer.INCORRECT: "Create a Global Secondary Index (GSI) for the trading data" is incorrect as a GSI is used to speed up queries on nonkey attributes. There is no requirement here for a Global Secondary Index.INCORRECT: "Use exponential backoff in the application code for DynamoDB queries" is incorrect as this may reduce the requirement for overprovisioning reads but it will not solve the problem of reducing latency. With this solution the application performance will be worse, it’s a case of reducing cost along with performance.INCORRECT: "Store the trading data in Amazon S3 and use Transfer Acceleration" is incorrect as this will not reduce the latency of the application. Transfer Acceleration is used for improving performance of uploads of data to Amazon S3.References: https://aws.amazon.com/dynamodb/dax/'},{question:"An independent software vendor (ISV) uses Amazon S3 and Amazon CloudFront to distribute software updates. They would like to provide their premium customers with access to updates faster. What is the MOST efficient way to distribute these updates only to the premium customers? (Select TWO.)",answers:[{text:"Use an access control list (ACL) on the Amazon S3 bucket to restrict access based on IP address",isCorrect:!1},{text:"Create a signed URL with access to the content and distribute it to the premium customers",isCorrect:!0},{text:"Use an IAM policy to restrict access to the content using a condition attribute and specify the IP addresses of the premium customers",isCorrect:!1},{text:"Create a signed cookie and associate it with the Amazon S3 distribution",isCorrect:!1},{text:"Create an origin access identity (OAI) and associate it with the distribution and configure permissions",isCorrect:!0}],explanation:'To restrict access to content that you serve from Amazon S3 buckets, you create CloudFront signed URLs or signed cookies to limit access to files in your Amazon S3 bucket, and then you create a special CloudFront user called an origin access identity (OAI) and associate it with your distribution. Then you configure permissions so that CloudFront can use the OAI to access and serve files to your users, but users can\'t use a direct URL to the S3 bucket to access a file there. Taking these steps help you maintain secure access to the files that you serve through CloudFront.CORRECT: "Create a signed URL with access to the content and distribute it to the premium customers" is the correct answer.CORRECT: "Create an origin access identity (OAI) and associate it with the distribution and configure permissions" is the correct answer.INCORRECT: "Create a signed cookie and associate it with the Amazon S3 distribution" is incorrect as you cannot associated signed cookies with Amazon S3 and a distribution is a CloudFront concept, not an S3 concept.INCORRECT: "Use an access control list (ACL) on the Amazon S3 bucket to restrict access based on IP address" is incorrect as you cannot restrict access to buckets by IP address when using an ACL.INCORRECT: "Use an IAM policy to restrict access to the content using a condition attribute and specify the IP addresses of the premium customers " is incorrect. You can restrict access to buckets using policy statements with conditions based on source IP address. However, this is cumbersome to manage as IP addresses change (and you need to know all your customer’s IPs in the first place). Also, because the content is being cached on CloudFront, this would not stop others accessing it anyway.References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/privatecontentsignedurls.html#privatecontentchoosingcannedcustompolicy https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/privatecontentrestrictingaccesstos3.html'},{question:"A mobile application has thousands of users. Each user may use multiple devices to access the application. The Developer wants to assign unique identifiers to these users regardless of the device they use. Which of the below is the BEST method to obtain unique identifiers?",answers:[{text:"Use IAMgenerated access key IDs for the users as the unique identifier, but do not store secret keys",isCorrect:!1},{text:"Implement developerauthenticated identities by using Amazon Cognito and get credentials for these identities",isCorrect:!0},{text:"Assign IAM users and roles to the users. Use the unique IAM resource ID as the unique identifier",isCorrect:!1},{text:"Create a user table in Amazon DynamoDB with keyvalue pairs of users and their devices. Use these keys as unique identifiers",isCorrect:!1}],explanation:'Amazon Cognito lets you add user signup, signin, and access control to your web and mobile apps quickly and easily. Amazon Cognito provides authentication, authorization, and user management for your web and mobile apps.Amazon Cognito identity pools enable you to create unique identities for your users and authenticate them with identity providers. With an identity, you can obtain temporary, limitedprivilege AWS credentials to access other AWS services.Amazon Cognito supports developer authenticated identities, in addition to web identity federation through Facebook (Identity Pools), Google (Identity Pools), and Login with Amazon (Identity Pools).With developer authenticated identities, you can register and authenticate users via your own existing authentication process, while still using Amazon Cognito to synchronize user data and access AWS resources. Using developer authenticated identities involves interaction between the end user device, your backend for authentication, and Amazon Cognito.In this scenario, this would be the best method of obtaining unique identifiers for each user. This is natively supported through Amazon Cognito.CORRECT: "Implement developerauthenticated identities by using Amazon Cognito and get credentials for these identities" is the correct answer.INCORRECT: "Create a user table in Amazon DynamoDB with keyvalue pairs of users and their devices. Use these keys as unique identifiers" is incorrect. This is not the best method of implementing this requirement as it requires more custom implementation and management.INCORRECT: "Use IAMgenerated access key IDs for the users as the unique identifier, but do not store secret keys" is incorrect. As this is a mobile application it is a good use case for Amazon Cognito so authentication can be handled without needing to create IAM users.INCORRECT: "Assign IAM users and roles to the users. Use the unique IAM resource ID as the unique identifier" is incorrect as this mobile application is a good use case for Amazon Cognito. With Cognito the authentication can be handled using identities in Cognito itself or a federated identity provider. Therefore, the users will not have identities in IAM.References: https://docs.aws.amazon.com/cognito/latest/developerguide/developerauthenticatedidentities.html'},{question:"A company is migrating an onpremises web application to AWS. The web application runs on a single server and stores session data in memory. On AWS the company plan to implement multiple Amazon EC2 instances behind an Elastic Load Balancer (ELB). The company want to refactor the application so that data is resilient if an instance fails and user downtime is minimized. Where should the company move session data to MOST effectively reduce downtime and make users' session data more fault tolerant?",answers:[{text:"The web server’s primary disk",isCorrect:!1},{text:"A second Amazon EBS volume",isCorrect:!1},{text:"An Amazon EC2 instance dedicated to session data",isCorrect:!1},{text:"An Amazon ElastiCache for Redis cluster",isCorrect:!0}],explanation:'ElastiCache is a fully managed, low latency, inmemory data store that supports either Memcached or Redis. The Redis engine supports multiAZ and high availability.With ElastiCache the company can move the session data to a highperformance, inmemory data store that is well suited to this use case. This will provide high availability for the session data in the case of EC2 instance failure and will reduce downtime for users.CORRECT: "An Amazon ElastiCache for Redis cluster" is the correct answer.INCORRECT: "A second Amazon EBS volume" is incorrect as the session data needs to be highly available so should not be stored on an EC2 instance.INCORRECT: "The web server’s primary disk" is incorrect as the session data needs to be highly available so should not be stored on an EC2 instance.INCORRECT: "An Amazon EC2 instance dedicated to session data" is incorrect as the session data needs to be highly available so should not be stored on an EC2 instance.References: https://aws.amazon.com/elasticache/features/'},{question:"A manufacturing company is creating a new RESTful API that their customers can use to query the status of orders. The endpoint for customer queries will be https://www.manufacturerdomain.com/status/customerID. Which of the following application designs will meet the requirements? (Select TWO.)",answers:[{text:"Amazon SQS; Amazon SNS",isCorrect:!1},{text:"Elastic Load Balancing; Amazon EC2",isCorrect:!0},{text:"Amazon API Gateway; AWS Lambda",isCorrect:!0},{text:"Amazon S3; Amazon CloudFront",isCorrect:!1},{text:"Amazon ElastiCache; Amazon Elacticsearch Service",isCorrect:!1}],explanation:'This scenario includes a web application that will use RESTful API calls to determine the status of orders and dynamically return the results back to the company\'s customers. Therefore, the two best options are as per below: &bull; Amazon API Gateway; AWS Lambda - this choice includes API Gateway which is provides managed REST APIs and Lambda which can run the backend code for the application. This is a good solution for this scenario. &bull; Elastic Load Balancing; Amazon EC2 - with this choice the ELB can load balance to one or more EC2 instances which can run the RESTful APIs and compute functions. This is also a good choice but could be more costly (operationally and financially). None of the other options provide a workable solution for this scenario. CORRECT: "Elastic Load Balancing; Amazon EC2" is a correct answer. CORRECT: "Amazon API Gateway; AWS Lambda" is a correct answer. INCORRECT: "Amazon SQS; Amazon SNS" is incorrect as these services are used for queuing and sending notifications. They are not suitable for hosting a REST API. INCORRECT: "Amazon ElastiCache; Amazon Elacticsearch Service" is incorrect as ElastiCache is an inmemory caching service and Elasticsearch is used for searching. These do not provide a suitable solution for this scenario. INCORRECT: "Amazon S3; Amazon CloudFront" is incorrect as though you can host a static website on Amazon S3 with CloudFront caching the content, this is a static website only and you cannot host an API. References: https://aws.amazon.com/ec2/features/ https://aws.amazon.com/elasticloadbalancing/ https://aws.amazon.com/apigateway/features/ https://aws.amazon.com/lambda/features/'},{question:"A Developer has used a thirdparty tool to build, bundle, and package a software package onpremises. The software package is stored in a local file system and must be deployed to Amazon EC2 instances. How can the application be deployed onto the EC2 instances?",answers:[{text:"Use AWS CodeBuild to commit the package and automatically deploy the software package.",isCorrect:!1},{text:"Use AWS CodeDeploy and point it to the local file system to deploy the software package.",isCorrect:!1},{text:"Upload the bundle to an Amazon S3 bucket and specify the S3 location when doing a deployment using AWS CodeDeploy.",isCorrect:!0},{text:"Create a repository using AWS CodeCommit to automatically trigger a deployment to the EC2 instances.",isCorrect:!1}],explanation:'AWS CodeDeploy can deploy software packages using an archive that has been uploaded to an Amazon S3 bucket. The archive file will typically be a .zip file containing the code and files required to deploy the software package. CORRECT: "Upload the bundle to an Amazon S3 bucket and specify the S3 location when doing a deployment using AWS CodeDeploy" is the correct answer. INCORRECT: "Use AWS CodeDeploy and point it to the local file system to deploy the software package" is incorrect. You cannot point CodeDeploy to a local file system running onpremises. INCORRECT: "Create a repository using AWS CodeCommit to automatically trigger a deployment to the EC2 instances" is incorrect. CodeCommit is a source control system. In this case the source code has already been package using a thirdparty tool. INCORRECT: "Use AWS CodeBuild to commit the package and automatically deploy the software package" is incorrect. CodeBuild does not commit packages (CodeCommit does) or deploy the software. It is a build service. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/tutorialswindowsuploadapplication.html'},{question:"An Amazon ElastiCache cluster has been placed in front of a large Amazon RDS database. To reduce cost the ElastiCache cluster should only cache items that are actually requested. How should ElastiCache be optimized?",answers:[{text:"Use a lazy loading caching strategy",isCorrect:!0},{text:"Enable a TTL on cached data",isCorrect:!1},{text:"Only cache database writes",isCorrect:!1},{text:"Use a writethrough caching strategy",isCorrect:!1}],explanation:'There are two caching strategies available: Lazy Loading and WriteThrough:Lazy Loading• Loads the data into the cache only when necessary (if a cache miss occurs).• Lazy loading avoids filling up the cache with data that won’t be requested.• If requested data is in the cache, ElastiCache returns the data to the application.• If the data is not in the cache or has expired, ElastiCache returns a null.• The application then fetches the data from the database and writes the data received into the cache so that it is available for next time.• Data in the cache can become stale if Lazy Loading is implemented without other strategies (such as TTL).Write Through• When using a write through strategy, the cache is updated whenever a new write or update is made to the underlying database.• Allows cache data to remain uptodate.• Can add wait time to write operations in your application.• Without a TTL you can end up with a lot of cached data that is never read.CORRECT: "Use a lazy loading caching strategy" is the correct answer.INCORRECT: "Use a writethrough caching strategy" is incorrect as this will load all database items into the cache increasing cost.INCORRECT: "Only cache database writes" is incorrect as you cannot cache writes, only reads.INCORRECT: "Enable a TTL on cached data" is incorrect. This would help expire stale items but it is not a cache optimization strategy that will cache only items that are requested.References: https://docs.aws.amazon.com/AmazonElastiCache/latest/memug/Strategies.html'},{question:"A developer is building a Docker application on Amazon ECS that will use an Application Load Balancer (ALB). The developer needs to configure the port mapping between the host port and container port. Where is this setting configured?",answers:[{text:"Service scheduler",isCorrect:!1},{text:"Task definition",isCorrect:!0},{text:"Host definition",isCorrect:!1},{text:"Container instance",isCorrect:!1}],explanation:'Port mappings allow containers to access ports on the host container instance to send or receive traffic. Port mappings are specified as part of the container definition.The container definition settings are specified within the task definition. The relevant settings are: containerPort the port number on the container that is bound to the userspecified or automatically assigned host port. hostPort the port number on the container instance to reserve for your container.With an ALB you can use Dynamic port mapping which makes it easier to run multiple tasks on the same Amazon ECS service on an Amazon ECS cluster. This is configured by setting the host port to 0, as in the image below:CORRECT: "Task definition" is the correct answer.INCORRECT: "Host definition" is incorrect as there’s no such thing.INCORRECT: "Service scheduler" is incorrect as the service scheduler is responsible for scheduling tasks and placing those tasks.INCORRECT: "Container instance" is incorrect as you don’t specify any settings on the container instance to control the host and container port mappings.References:https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PortMapping.html https://aws.amazon.com/premiumsupport/knowledgecenter/dynamicportmappingecs/'},{question:"A Developer is creating an application that will utilize an Amazon DynamoDB table for storing session data. The data being stored is expected to be around 4.5KB in size and the application will make 20 eventually consistent reads/sec, and 12 standard writes/sec. How many RCUs/WCUs are required?",answers:[{text:"20 RCU and 60 WCU",isCorrect:!0},{text:"40 RCU and 120 WCU",isCorrect:!1},{text:"40 RCU and 60 WCU",isCorrect:!1},{text:"10 RCU and 24 WCU",isCorrect:!1}],explanation:'With provisioned capacity mode, you specify the number of data reads and writes per second that you require for your application.Read capacity unit (RCU):• Each API call to read data from your table is a read request.• Read requests can be strongly consistent, eventually consistent, or transactional.• For items up to 4 KB in size, one RCU can perform one strongly consistent read request per second.• Items larger than 4 KB require additional RCUs.• For items up to 4 KB in size, one RCU can perform two eventually consistent read requests per second.• Transactional read requests require two RCUs to perform one read per second for items up to 4 KB.• For example, a strongly consistent read of an 8 KB item would require two RCUs, an eventually consistent read of an 8 KB item would require one RCU, and a transactional read of an 8 KB item would require four RCUs.Write capacity unit (WCU):• Each API call to write data to your table is a write request.• For items up to 1 KB in size, one WCU can perform one standard write request per second.• Items larger than 1 KB require additional WCUs.• Transactional write requests require two WCUs to perform one write per second for items up to 1 KB.• For example, a standard write request of a 1 KB item would require one WCU, a standard write request of a 3 KB item would require three WCUs, and a transactional write request of a 3 KB item would require six WCUs.To determine the number of RCUs required to handle 20 eventually consistent reads per/second with an average item size of4.5KB, perform the following steps:1. Determine the average item size by rounding up the next multiple of 4KB (4.5KB rounds up to 8KB).2. Determine the RCU per item by dividing the item size by 8KB (8KB/8KB = 1).3. Multiply the value from step 2 with the number of reads required per second (1x20 = 20).To determine the number of WCUs required to handle 12 standard writes per/second with an average item size of 8KB, simply multiply the average item size by the number of writes required (5x12=60).CORRECT: "20 RCU and 60 WCU" is the correct answer.INCORRECT: "40 RCU and 60 WCU" is incorrect. This would be the correct answer for strongly consistent reads and standard writes.INCORRECT: "40 RCU and 120 WCU" is incorrect. This would be the correct answer for strongly consistent reads and transactional writes.INCORRECT: "6 RCU and 18 WCU" is incorrect.References: https://aws.amazon.com/dynamodb/pricing/provisioned/'},{question:"A Developer is publishing custom metrics for Amazon EC2 using the Amazon CloudWatch CLI. The Developer needs to add further context to the metrics being published by organizing them by EC2 instance and Auto Scaling Group. What should the Developer add to the CLI command when publishing the metrics using putmetricdata",answers:[{text:"The namespace parameter",isCorrect:!1},{text:"The statisticvalues parameter",isCorrect:!1},{text:"The metricname parameter",isCorrect:!1},{text:"The dimensions parameter",isCorrect:!0}],explanation:'You can publish your own metrics to CloudWatch using the AWS CLI or an API. You can view statistical graphs of your published metrics with the AWS Management Console. CloudWatch stores data about a metric as a series of data points. Each data point has an associated time stamp. You can even publish an aggregated set of data points called a statistic set.In custom metrics, the dimensions parameter is common. A dimension further clarifies what the metric is and what data it stores. You can have up to 10 dimensions in one metric, and each dimension is defined by a name and value pair.As you can see in the above example there are two dimensions associated with the EC2 namespace. These organize the metrics by Auto Scaling Group and PerInstance metrics. Therefore the Developer should the dimensions parameter.CORRECT: "The dimensions parameter" is the correct answer.INCORRECT: "The namespace parameter" is incorrect as a namespace is a container for CloudWatch metrics. To add further context the Developer should use a dimension.INCORRECT: "The statisticvalues parameter" is incorrect as this is a parameter associated with the publishing of statistic sets.INCORRECT: "The metricname parameter" is incorrect as this simply provides the name for the metric that is being published.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html'},{question:"A company is migrating an application with a website and MySQL database to the AWS Cloud. The company require the application to be refactored so it offers high availability and fault tolerance. How should a Developer refactor the application? (Select TWO.)",answers:[{text:"Migrate the website to an Auto Scaling group of EC2 instances across multiple AZs and use an Elastic Load Balancer",isCorrect:!0},{text:"Migrate the website to an Auto Scaling group of EC2 instances across a single AZ and use an Elastic Load Balancer",isCorrect:!1},{text:"Migrate the MySQL database to an Amazon RDS instance with a Read Replica in another AZ",isCorrect:!1},{text:"Migrate the MySQL database to an Amazon RDS MultiAZ deployment",isCorrect:!0},{text:"Migrate the MySQL database to an Amazon DynamoDB with Global Tables",isCorrect:!1}],explanation:'The key requirements are to add high availability and fault tolerance to the application. To do this the Developer should put the website into an Auto Scaling group of EC2 instances across multiple AZs. An Elastic Load Balancer can be deployed in front of the EC2 instances to distribute incoming connections. This solution is highly available and fault tolerant.For the MySQL database the Developer should use Amazon RDS with the MySQL engine. To provide fault tolerance the Developer should configure Amazon RDS as a MultiAZ deployment which will create a standby instance in another AZ that can be failed over to.CORRECT: "Migrate the website to an Auto Scaling group of EC2 instances across multiple AZs and use an Elastic Load Balancer" is a correct answer.CORRECT: "Migrate the MySQL database to an Amazon RDS MultiAZ deployment" is also a correct answer.INCORRECT: "Migrate the website to an Auto Scaling group of EC2 instances across a single AZ and use an Elastic Load Balancer" is incorrect as to be fully fault tolerant the solution should be spread across multiple AZs.INCORRECT: "Migrate the MySQL database to an Amazon RDS instance with a Read Replica in another AZ" is incorrect as read replicas are used for performance, not fault toleranceINCORRECT: "Migrate the MySQL database to an Amazon DynamoDB with Global Tables" is incorrect as the MySQL database is a relational database so it is a better fit to be migrated to Amazon RDS rather than DynamoDB.References: https://d1.awsstatic.com/whitepapers/awsbuildingfaulttolerantapplications.pdf'},{question:"An Amazon RDS database is experiencing a high volume of read requests that are slowing down the database. Which fully managed, inmemory AWS database service can assist with offloading reads from the RDS database?",answers:[{text:"Memcached on Amazon EC2",isCorrect:!1},{text:"Amazon ElastiCache Redis",isCorrect:!0},{text:"Amazon Aurora Serverless",isCorrect:!1},{text:"Amazon RDS Read Replica",isCorrect:!1}],explanation:'ElastiCache is a web service that makes it easy to deploy and run Memcached or Redis protocolcompliant server nodes in the cloud. The inmemory caching provided by ElastiCache can be used to significantly improve latency and throughput for many readheavy application workloads or computeintensive workloads.This is a fully managed AWS service and is ideal for offloading reads from the main database to reduce the performance impact.CORRECT: "Amazon ElastiCache Redis" is the correct answer.INCORRECT: "Amazon RDS Read Replica" is incorrect as it is not an inmemory database. RDS Read Replicas can be used for offloading reads from the main database, however.INCORRECT: "Amazon Aurora Serverless" is incorrect. Aurora Serverless is not an inmemory solution, nor is it suitable for functioning as a method of offloading reads from RDS databases.INCORRECT: "Memcached on Amazon EC2" is incorrect as this is an implementation of Memcached running on EC2 and therefore is not a fully managed AWS service.References: https://aws.amazon.com/elasticache/redis/'},{question:"An organization has an Amazon S3 bucket containing premier content that they intend to make available to only paid subscribers of their website. The objects in the S3 bucket are private to prevent inadvertent exposure of the premier content to nonpaying website visitors. How can the organization provide only paid subscribers the ability to download the premier content in the S3 bucket?",answers:[{text:"Apply a bucket policy that grants anonymous users to download the content from the S3 bucket",isCorrect:!1},{text:"Generate a presigned object URL for the premier content file when a paid subscriber requests a download",isCorrect:!0},{text:"Add a bucket policy that requires MultiFactor Authentication for requests to access the S3 bucket objects",isCorrect:!1},{text:"Enable serverside encryption on the S3 bucket for data protection against the nonpaying website visitors",isCorrect:!1}],explanation:'When Amazon S3 objects are private, only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a presigned URL, using their own security credentials, to grant timelimited permission to download the objects.When you create a presigned URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The presigned URLs are valid only for the specified duration.Anyone who receives the presigned URL can then access the object. In this scenario, a presigned URL can be generated only for paying customers and they will be the only website visitors who can view the premier content.CORRECT: "Generate a presigned object URL for the premier content file when a paid subscriber requests a download" is the correct answer.INCORRECT: "Apply a bucket policy that grants anonymous users to download the content from the S3 bucket" is incorrect as this would provide everyone the ability to download the content.INCORRECT: "Add a bucket policy that requires MultiFactor Authentication for requests to access the S3 bucket objects" is incorrect as this would be very difficult to manage. Using presigned URLs that are dynamically generated by an application for premier users would be much simpler.INCORRECT: "Enable serverside encryption on the S3 bucket for data protection against the nonpaying website visitors" is incorrect as this is encryption at rest and S3 will simply unencrypt the objects when users attempt to read them. This provides privacy protection for data at rest but does not restrict access.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html'},{question:"A company is designing a new application that will store thousands of terabytes of data. They need a fully managed NoSQL data store that provides lowlatency and can store keyvalue pairs. Which type of database should they use?",answers:[{text:"Amazon ElastiCache",isCorrect:!1},{text:"Amazon DynamoDB",isCorrect:!0},{text:"Amazon S3",isCorrect:!1},{text:"Amazon RDS",isCorrect:!1}],explanation:'Amazon DynamoDB is a fully managed NoSQL database. With DynamoDB, you can create database tables that can store and retrieve any amount of data and serve any level of request traffic. You can scale up or scale down your tables\' throughput capacity without downtime or performance degradation.DynamoDB is a keyvalue database. A keyvalue database is a type of nonrelational database that uses a simple keyvalue method to store data. A keyvalue database stores data as a collection of keyvalue pairs in which a key serves as a unique identifier. Both keys and values can be anything, ranging from simple objects to complex compound objects.CORRECT: "Amazon DynamoDB" is the correct answer.INCORRECT: "Amazon RDS" is incorrect as RDS is a SQL (not a NoSQL) type of database.INCORRECT: "Amazon ElastiCache" is incorrect as ElastiCache is a SQL (not a NoSQL) type of database. ElastiCache is an inmemory database typically used for caching data.INCORRECT: "Amazon S3" is incorrect as S3 is not a NoSQL database. S3 is an object storage system.References:https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html https://aws.amazon.com/nosql/keyvalue/'},{question:"A company has a website that is developed in PHP and WordPress and is launched using AWS Elastic Beanstalk. There is a new version of the website that needs to be deployed in the Elastic Beanstalk environment. The company cannot tolerate having the website offline if an update fails. Deployments must have minimal impact and rollback as soon as possible. What deployment method should be used?",answers:[{text:"Immutable",isCorrect:!0},{text:"All at once",isCorrect:!1},{text:"Rolling",isCorrect:!1},{text:"Snapshots",isCorrect:!1}],explanation:'AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments.All at once:• Deploys the new version to all instances simultaneously.Rolling:• Update a few instances at a time (bucket), and then move onto the next bucket once the first bucket is healthy(downtime for 1 bucket at a time).Rolling with additional batch:• Like Rolling but launches new instances in a batch ensuring that there is full availability.Immutable:• Launches new instances in a new ASG and deploys the version update to these instances before swapping traffic tothese instances once healthy.• Zero downtime.Blue / Green deployment:• Zero downtime and release facility.• Create a new “stage” environment and deploy updates there.For this scenario, the best choice is Immutable as this is the safest option when you cannot tolerate downtime and also provides a simple way of rolling back should an issue occur.CORRECT: "Immutable" is the correct answer.INCORRECT: "All at once" is incorrect as this will take all instances down and cause a total outage.INCORRECT: "Snapshots" is incorrect as this is not a deployment method you can use with Elastic Beanstalk.INCORRECT: "Rolling" is incorrect as this will reduce the capacity of the application and it is more difficult to roll back as you must redeploy the old version to the instances.References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.rollingversiondeploy.html'},{question:"A developer is designing a web application that will be used by thousands of users. The users will sign up using their email addresses and the application will store attributes for each user. Which service should the developer use to enable users to sign up for the web application?",answers:[{text:"AWS AppSync",isCorrect:!1},{text:"Amazon Cognito Sync",isCorrect:!1},{text:"Amazon Cognito user pool",isCorrect:!0},{text:"AWS Inspector",isCorrect:!1}],explanation:'A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Google, Facebook, Amazon, or Apple, and through SAML identity providers.Whether your users sign in directly or through a third party, all members of the user pool have a directory profile that you can access through a Software Development Kit (SDK).User pools provide:• Signup and signin services.• A builtin, customizable web UI to sign in users.• Social signin with Facebook, Google, Login with Amazon, and Sign in with Apple, as well as signin with SAML identity providers from your user pool.• User directory management and user profiles.• Security features such as multifactor authentication (MFA), checks for compromised credentials, account takeover protection, and phone and email verification.• Customized workflows and user migration through AWS Lambda triggers.After successfully authenticating a user, Amazon Cognito issues JSON web tokens (JWT) that you can use to secure and authorize access to your own APIs, or exchange for AWS credentials.Therefore, an Amazon Cognito user pool is the best solution for enabling signup to the new web application.CORRECT: "Amazon Cognito user pool" is the correct answer.INCORRECT: "Amazon Cognito Sync" is incorrect as it is used to synchronize user profile data across mobile devices and the web without requiring your own backend.INCORRECT: "AWS Inspector" is incorrect. Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS.INCORRECT: "AWS AppSync" is incorrect. AWS AppSync simplifies application development by letting you create a flexible API to securely access, manipulate, and combine data from one or more data sources.References: https://docs.aws.amazon.com/cognito/latest/developerguide/cognitouseridentitypools.html'},{question:"A serverless application uses an AWS Lambda function, Amazon API Gateway API and an Amazon DynamoDB table. The Lambda function executes 10 times per second and takes 3 seconds to complete each execution. How many concurrent executions will the Lambda function require?",answers:[{text:"10",isCorrect:!1},{text:"30",isCorrect:!0},{text:"12",isCorrect:!1},{text:"3",isCorrect:!1}],explanation:'Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function\'s concurrency.To calculate the concurrency requirements for the Lambda function simply multiply the number of executions per second (10) by the time it takes to complete the execution (3).Therefore, for this scenario the calculation is 10 x 3 = 30.CORRECT: "30" is the correct answer.INCORRECT: "10" is incorrect. Please use the formula above to calculate concurrency requirements.INCORRECT: "12" is incorrect. Please use the formula above to calculate concurrency requirements.INCORRECT: "3" is incorrect. Please use the formula above to calculate concurrency requirements.References: https://docs.aws.amazon.com/lambda/latest/dg/invocationscaling.html'},{question:"A Developer is migrating Docker containers to Amazon ECS. A large number of containers will be deployed onto an existing ECS cluster that uses container instances of different instance types. Which task placement strategy can be used to minimize the number of container instances used based on available memory?",answers:[{text:"distinctInstance",isCorrect:!1},{text:"spread",isCorrect:!1},{text:"random",isCorrect:!1},{text:"binpack",isCorrect:!0}],explanation:'When a task that uses the EC2 launch type is launched, Amazon ECS must determine where to place the task based on the requirements specified in the task definition, such as CPU and memory. Similarly, when you scale down the task count, Amazon ECS must determine which tasks to terminate. You can apply task placement strategies and constraints to customize how Amazon ECS places and terminates tasks. Task placement strategies and constraints are not supported for tasks using the Fargate launch type. By default, Fargate tasks are spread across Availability Zones.A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. For example, Amazon ECS can select instances at random, or it can select instances such that tasks are distributed evenly across a group of instances.Amazon ECS supports the following task placement strategies:• binpackPlace tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.• randomPlace tasks randomly.• spreadPlace tasks evenly based on the specified value. Accepted values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availabilityzone. Service tasks are spread based on the tasks from that service. Standalone tasks are spread based on the tasks from the same task group.The Developer should use the binpack task placement strategy using available memory to determine the placement of tasks.This will minimize the number of container instances required.CORRECT: "binpack" is the correct answer.INCORRECT: "random" is incorrect as this would just randomly assign the tasks across the available container instances in the cluster.INCORRECT: "spread" is incorrect as this would attempt to spread the tasks across the cluster instances for better high availability.INCORRECT: "distinctInstance" is incorrect as this is a task placement constraint, not a strategy. This constraint would result in the tasks being each placed on a separate instance which would not assist with meeting the requirements.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskplacementstrategies.html'},{question:"A company will be hiring a large number of Developers for a series of projects. The Develops will bring their own devices to work and the company want to ensure consistency in tooling. The Developers must be able to write, run, and debug applications with just a browser, without needing to install or maintain a local Integrated Development Environment (IDE). Which AWS service should the Developers use?",answers:[{text:"AWS Cloud9",isCorrect:!0},{text:"AWS XRay",isCorrect:!1},{text:"AWS CodeCommit",isCorrect:!1},{text:"AWS CodeDeploy",isCorrect:!1}],explanation:'AWS Cloud9 is an integrated development environment, or IDE. The AWS Cloud9 IDE offers a rich codeediting experience with support for several programming languages and runtime debuggers, and a builtin terminal. It contains a collection of tools that you use to code, build, run, test, and debug software, and helps you release software to the cloud.You access the AWS Cloud9 IDE through a web browser. You can configure the IDE to your preferences. You can switch color themes, bind shortcut keys, enable programming languagespecific syntax coloring and code formatting, and more.CORRECT: "AWS Cloud9" is the correct answer.INCORRECT: "AWS CodeCommit" is incorrect. AWS CodeCommit is a fullymanaged source control service that hosts secure Gitbased repositories. It is not an IDE.INCORRECT: "AWS CodeDeploy" is incorrect. CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, onpremises instances, serverless Lambda functions, or Amazon ECS services.INCORRECT: "AWS XRay" is incorrect. AWS XRay helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture.References: https://docs.aws.amazon.com/cloud9/latest/userguide/welcome.html'},{question:"A Developer is creating an AWS Lambda function to process a stream of data from an Amazon Kinesis Data Stream. When the Lambda function parses the data and encounters a missing field, it exits the function with an error. The function is generating duplicate records from the Kinesis stream. When the Developer looks at the stream output without the Lambda function, there are no duplicate records. What is the reason for the duplicates?",answers:[{text:"The Lambda function is not keeping up with the amount of data coming from the stream",isCorrect:!1},{text:"The Lambda event source used asynchronous invocation, resulting in duplicate records",isCorrect:!1},{text:"The Lambda function did not handle the error, and the Lambda service attempted to reprocess the data",isCorrect:!0},{text:"The Lambda function did not advance the Kinesis stream point to the next record after the error",isCorrect:!1}],explanation:'When you invoke a function, two types of error can occur. Invocation errors occur when the invocation request is rejected before your function receives it. Function errors occur when your function\'s code or runtime returns an error.Depending on the type of error, the type of invocation, and the client or service that invokes the function, the retry behavior and the strategy for managing errors varies. Function errors occur when your function code or the runtime that it uses return an error.In this case, with an event source mapping from a stream (Kinesis Data Stream), Lambda retries the entire batch of items.Therefore, the best explanation is that the Lambda function did not handle the error, and the Lambda service attempted to reprocess the data.CORRECT: "The Lambda function did not handle the error, and the Lambda service attempted to reprocess the data" is the correct answer.INCORRECT: "The Lambda function did not advance the Kinesis stream point to the next record after the error" is incorrect. Lambda does not advance a stream “point” to the next record. It processed records in batches.INCORRECT: "The Lambda event source used asynchronous invocation, resulting in duplicate records" is incorrect as Lambda processes records from Kinesis Data Streams synchronously.INCORRECT: "The Lambda function is not keeping up with the amount of data coming from the stream" is incorrect as Lambda can scale seamlessly to handle the load coming from the stream.References: https://docs.aws.amazon.com/lambda/latest/dg/invocationretries.html'},{question:"A company has hired a team of remote Developers. The Developers need to work programmatically with AWS resources from their laptop computers. Which security components MUST the Developers use to authenticate? (Select TWO.)",answers:[{text:"MFA device",isCorrect:!1},{text:"Console password",isCorrect:!1},{text:"Access key ID",isCorrect:!0},{text:"IAM user ID",isCorrect:!1},{text:"Secret access key",isCorrect:!0}],explanation:'Access keys consist of two parts: an access key ID (for example, AKIAIOSFODNN7EXAMPLE) and a secret access key (for example, wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY). You use access keys to sign programmatic requests that you make to AWS if you use AWS CLI commands (using the SDKs) or using AWS API operations.For this scenario, the Developers will be connecting programmatically to AWS resources and will therefore be required to use an access key ID and secret access key.CORRECT: "Access key ID" is a correct answer.CORRECT: "Secret access key" is a correct answer.INCORRECT: "Console password " is incorrect as this is used for accessing AWS via the console with an IAM user ID and is not used for programmatic access.INCORRECT: "IAM user ID" is incorrect as the IAM user ID is used with the password (see above) to access the AWS management console.INCORRECT: "MFA device" is incorrect as this is not required for making programmatic requests but can be added for additional securityReferences: https://docs.aws.amazon.com/general/latest/gr/awsseccredtypes.html'},{question:"The manager of a development team is setting up a shared S3 bucket for team members. The manager would like to use a single policy to allow each user to have access to their objects in the S3 bucket. Which feature can be used to generalize the policy?",answers:[{text:"Condition",isCorrect:!1},{text:"Variable",isCorrect:!0},{text:"Resource",isCorrect:!1},{text:"Principal",isCorrect:!1}],explanation:'In some cases, you might not know the exact name of the resource when you write the policy. You might want to generalize the policy so it works for many users without having to make a unique copy of the policy for each user. For example, consider writing a policy to allow each user to have access to his or her own objects in an Amazon S3 bucket.Instead of that explicitly specifies the user\'s name as part of the resource, create a single group policy that works for any user in that group. You can do this by using policy variables, a feature that lets you specify placeholders in a policy. When the policy is evaluated, the policy variables are replaced with values that come from the context of the request itself.When this policy is evaluated, IAM replaces the variable ${aws:username}with the friendly name of the actual current user. This means that a single policy applied to a group of users can control access to a bucket by using the username as part of the resource\'s name.CORRECT: "Variable" is the correct answer.INCORRECT: "Condition" is incorrect. The Condition element (or Condition block) lets you specify conditions for when a policy is in effect.INCORRECT: "Principal" is incorrect. You can use the Principal element in a policy to specify the principal that is allowed or denied access to a resource. However, in this scenario a variable is needed to create a generic policy that can provide the necessary permissions to different principals using variables.INCORRECT: "Resource" is incorrect. The Resource element specifies the object or objects that the statement covers.References: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html'},{question:"An application resizes images that are uploaded to an Amazon S3 bucket. Amazon S3 event notifications are used to trigger an AWS Lambda function that resizes the images. The processing time for each image is less than one second. A large amount of images are expected to be received in a short burst of traffic. How will AWS Lambda accommodate the workload?",answers:[{text:"Lambda will collect and then batch process the images in a single execution",isCorrect:!1},{text:"Lambda will process the images sequentially in the order they are received",isCorrect:!1},{text:"Lambda will scale out and execute the requests concurrently",isCorrect:!0},{text:"Lambda will scale the memory allocated to the function to increase the amount of CPU available to process many images",isCorrect:!1}],explanation:'The first time you invoke your function, AWS Lambda creates an instance of the function and runs its handler method to process the event. When the function returns a response, it stays active and waits to process additional events. If you invoke the function again while the first event is being processed, Lambda initializes another instance, and the function processes the two events concurrently.Your functions’ concurrency is the number of instances that serve requests at a given time. For an initial burst of traffic, your functions’ cumulative concurrency in a Region can reach an initial level of between 500 and 3000, which varies per Region.Burst Concurrency Limits:• 3000 – US West (Oregon), US East (N. Virginia), Europe (Ireland).• 1000 – Asia Pacific (Tokyo), Europe (Frankfurt).• 500 – Other Regions.After the initial burst, your functions’ concurrency can scale by an additional 500 instances each minute. This continues until there are enough instances to serve all requests, or until a concurrency limit is reached.The default account limit is up to 1000 executions per second, per region (can be increased).CORRECT: "Lambda will scale out and execute the requests concurrently" is the correct answer.INCORRECT: "Lambda will process the images sequentially in the order they are received" is incorrect as Lambda uses concurrency to process multiple events in parallel.INCORRECT: "Lambda will collect and then batch process the images in a single execution" is incorrect as Lambda never collects requests and then processes them at a later time. Lambda always uses concurrency to process requests in parallel.INCORRECT: "Lambda will scale the memory allocated to the function to increase the amount of CPU available to process many images" is incorrect as Lambda does not automatically scale memory/CPU and processes requests in parallel, not sequentially.References: https://docs.aws.amazon.com/lambda/latest/dg/configurationconcurrency.html'},{question:'A Developer has created a task definition that includes the following JSON code:"placementConstraints": [{"expression": "attribute:ecs.instancetype =~ t2.*","type": "memberOf"}]What will be the effect for tasks using this task definition?',answers:[{text:"They will be added to distinct instances using the T2 instance type",isCorrect:!1},{text:"They will be placed only on container instances using the T2 instance type",isCorrect:!0},{text:"They will be placed only on container instances of T2 or T3 instance types",isCorrect:!1},{text:"They will be spread across all instances except for T2 instances",isCorrect:!1}],explanation:'A task placement constraint is a rule that is considered during task placement. Task placement constraints can be specified when either running a task or creating a new service.The memberOf task placement constraint places tasks on container instances that satisfy an expression.The memberOf task placement constraint can be specified with the following actions:• Running a task• Creating a new service• Creating a new task definition• Creating a new revision of an existing task definitionThe example JSON code uses the memberOf constraint to place tasks on T2 instances. It can be specified with the following actions: CreateService, UpdateService, RegisterTaskDefinition, and RunTask.CORRECT: "They will be placed only on container instances using the T2 instance type" is the correct answer.INCORRECT: "They will be added to distinct instances using the T2 instance type" is incorrect. The memberOf constraint does not choose distinct instances.INCORRECT: "They will be placed only on container instances of T2 or T3 instance types" is incorrect as only T2 instance types will be used. The wildcard means any T2 instance type such as t2.micro or t2.large.INCORRECT: "They will be spread across all instances except for T2 instances" is incorrect as this code ensures the instances WILL be placed on T2 instance types.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskplacementconstraints.html'},{question:"A legacy application is being refactored into a microservices architecture running on AWS. The microservice will include several AWS Lambda functions. A Developer will use AWS Step Functions to coordinate function execution. How should the Developer proceed?",answers:[{text:"Create an AWS CloudFormation stack using a YAMLformatted template",isCorrect:!1},{text:"Create a workflow using the StartExecution API action",isCorrect:!1},{text:"Create a state machine using the Amazon States Language",isCorrect:!0},{text:"Create a layer in AWS Lambda and add the functions to the layer",isCorrect:!1}],explanation:'AWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or task, allowing you to scale and change applications quickly.The following are key features of AWS Step Functions:• Step Functions is based on the concepts of tasks and state machines.• You define state machines using the JSONbased Amazon States Language.• The Step Functions console displays a graphical view of your state machine\'s structure. This provides a way to visually check your state machine\'s logic and monitor executions.The Developer needs to create a state machine using the Amazon States Language as this is how you can create an executable state machine that includes the Lambda functions that must be coordinated.CORRECT: "Create a state machine using the Amazon States Language" is the correct answer.INCORRECT: "Create an AWS CloudFormation stack using a YAMLformatted template" is incorrect as AWS Step Functions does not use CloudFormation. The Developer needs to create a state machine.INCORRECT: "Create a workflow using the StartExecution API action" is incorrect as workflows are associated with Amazon SWF whereas the StartExecution API action is a Step Functions action for executing a state machine.INCORRECT: "Create a layer in AWS Lambda and add the functions to the layer" is incorrect as a layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies that you can use to pull additional code into a Lambda function.References: https://docs.aws.amazon.com/stepfunctions/latest/dg/welcome.html'},{question:"An Auto Scaling Group (ASG) of Amazon EC2 instances is being created for processing messages from an Amazon SQS queue. To ensure the EC2 instances are cost-effective a Developer would like to configure the ASG to maintain aggregate CPU utilization at 70%. Which type of scaling policy should the Developer choose?",answers:[{text:"Simple Scaling Policy",isCorrect:!1},{text:"Step Scaling Policy",isCorrect:!1},{text:"Scheduled Scaling Policy",isCorrect:!1},{text:"Target Tracking Scaling Policy",isCorrect:!0}],explanation:'With target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. In addition to keeping the metric close to the target value, a target tracking scaling policy also adjusts to the changes in the metric due to a changing load pattern.For example, you can use target tracking scaling to:• Configure a target tracking scaling policy to keep the average aggregate CPU utilization of your Auto Scaling group at 40 percent.• Configure a target tracking scaling policy to keep the request count per target of your Elastic Load Balancing target group at 1000 for your Auto Scaling group.The target tracking scaling policy is therefore the best choice for this scenario.CORRECT: "Target Tracking Scaling Policy" is the correct answer.INCORRECT: "Step Scaling Policy" is incorrect. (explanation below)INCORRECT: "Simple Scaling Policy" is incorrect. (explanation below)With step scaling and simple scaling, you choose scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process. You also define how your Auto Scaling group should be scaled when a threshold is in breach for a specified number of evaluation periods.INCORRECT: "Scheduled Scaling Policy" is incorrect as this is used to schedule a scaling action at a specific time and date rather than dynamically adjusting according to load.References: https://docs.aws.amazon.com/autoscaling/ec2/userguide/asscalingtargettracking.html'},{question:"A Developer is creating a serverless website with content that includes HTML files, images, videos, and JavaScript (clientside scripts). Which combination of services should the Developer use to create the website?",answers:[{text:"Amazon ECS and Redis",isCorrect:!1},{text:"AWS Lambda and Amazon API Gateway",isCorrect:!1},{text:"Amazon EC2 and Amazon ElastiCache",isCorrect:!1},{text:"Amazon S3 and Amazon CloudFront",isCorrect:!0}],explanation:'You can use Amazon S3 to host a static website. On a static website, individual webpages include static content. They might also contain clientside scripts.To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document.To get content closer to users for better performance you can also use Amazon CloudFront in front of the S3 static website. To serve a static website hosted on Amazon S3, you can deploy a CloudFront distribution using one of these configurations:• Using a REST API endpoint as the origin with access restricted by an origin access identity (OAI)• Using a website endpoint as the origin with anonymous (public) access allowed• Using a website endpoint as the origin with access restricted by a Referer headerTherefore, the combination of services should be Amazon S3 and Amazon CloudFrontCORRECT: "Amazon S3 and Amazon CloudFront" is the correct answer.INCORRECT: "Amazon EC2 and Amazon ElastiCache" is incorrect. The website is supposed to be serverless and neither of these services are serverless as they both use Amazon EC2 instances.INCORRECT: "Amazon ECS and Redis" is incorrect. These services are also not serverless. Also Redis is an inmemory cache and is typically placed in front of a database, not a Docker container.INCORRECT: "AWS Lambda and Amazon API Gateway" is incorrect. These are both serverless services however for serving content such as HTML files, images, videos, and clientside JavaScript, Amazon S3 and CloudFront are more appropriate.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html'},{question:"A Developer has lost their access key ID and secret access key for programmatic access. What should the Developer do?",answers:[{text:"Generate a new key pair from the EC2 management console",isCorrect:!1},{text:"Reset the AWS account access keys",isCorrect:!1},{text:"Disable and delete the user’s access key and generate a new set",isCorrect:!0},{text:"Contact AWS support and request a password reset",isCorrect:!1}],explanation:'Access keys consist of two parts:• The access key identifier. This is not a secret, and can be seen in the IAM console wherever access keys are listed, such as on the user summary page.• The secret access key. This is provided when you initially create the access key pair. Just like a password, it cannot be retrieved later. If you lost your secret access key, then you must create a new access key pair. If you already have the maximum number of access keys, you must delete an existing pair before you can create another.Therefore, the Developer should disable and delete their access keys and generate a new set.CORRECT: "Disable and delete the users’ access key and generate a new set" is the correct answer.INCORRECT: "Contact AWS support and request a password reset" is incorrect as a user name and password are used for console access, not programmatic access.INCORRECT: "Generate a new key pair from the EC2 management console" is incorrect as a key pair is used for accessing EC2 instances, not for programmatic access to work with AWS services.INCORRECT: "Reset the AWS account access keys" is incorrect as these are the access keys associated with the root account rather than the users’ individual IAM account.References: https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_general.html#troubleshoot_general_accesskeys'},{question:"A Developer is creating multiple AWS Lambda functions that will be using an external library that is not included in the standard Lambda libraries. What is the BEST way to make these libraries available to the functions?",answers:[{text:"Create a deployment package that includes the external library",isCorrect:!1},{text:"Include the external library with the function code",isCorrect:!1},{text:"Store the files in Amazon S3 and reference them from your function code",isCorrect:!1},{text:"Create a layer in Lambda that includes the external library",isCorrect:!0}],explanation:'You can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package.Layers let you keep your deployment package small, which makes development easier. You can avoid errors that can occur when you install and package dependencies with your function code.When a Lambda function configured with a Lambda layer is executed, AWS downloads any specified layers and extracts them to the /opt directory on the function execution environment. Each runtime then looks for a languagespecific folder under the /opt directory.One of the best practices for AWS Lambda functions is to minimize your deployment package size to its runtime necessities in order to reduce the amount of time that it takes for your deployment package to be downloaded and unpacked ahead of invocation.Therefore, it is preferable to use layers to store the external libraries to optimize performance of the function. Using layers means that the external library will also be available to all of the Lambda functions that the Developer is creating.CORRECT: "Create a layer in Lambda that includes the external library" is the correct answer.INCORRECT: "Include the external library with the function code" is incorrect as you should not include an external library within the function code. Even if possible this would result in bloated code that could slow down execution time.INCORRECT: "Create a deployment package that includes the external library" is incorrect as the best practice is to minimize package sizes to runtime necessities. Also, this would require including the library in all function deployment packages whereas with layers we can create a single layer that is used by all functions.INCORRECT: "Store the files in Amazon S3 and reference them from your function code" is incorrect as this would likely result in increased latency of your function execution. Instead you should either package the library in the deployment package for your function or use layers (preferable in this scenario).References: https://docs.aws.amazon.com/lambda/latest/dg/configurationlayers.html'},{question:"A serverless application uses Amazon API Gateway, AWS Lambda and DynamoDB. The application writes statistical data that is constantly received from sensors. The data is analyzed soon after it is written to the database and is then not required. What is the EASIEST method to remove stale data and optimize database size?",answers:[{text:"Enable the TTL attribute and add expiry timestamps to items",isCorrect:!0},{text:"Scan the table for stale data and delete it once every hour",isCorrect:!1},{text:"Use atomic counters to decrement the data when it becomes stale",isCorrect:!1},{text:"Delete the table and recreate it every hour",isCorrect:!1}],explanation:'Time to Live (TTL) for Amazon DynamoDB lets you define when items in a table expire so that they can be automatically deleted from the database. With TTL enabled on a table, you can set a timestamp for deletion on a peritem basis, allowing you to limit storage usage to only those records that are relevant.TTL is useful if you have continuously accumulating data that loses relevance after a specific time period (for example, session data, event logs, usage patterns, and other temporary data). If you have sensitive data that must be retained only for a certain amount of time according to contractual or regulatory obligations, TTL helps you ensure that it is removed promptly and asscheduled.Therefore, the best answer is to enable the TTL attribute and add expiry timestamps to items.CORRECT: "Enable the TTL attribute and add expiry timestamps to items" is the correct answer.INCORRECT: "Use atomic counters to decrement the data when it becomes stale" is incorrect. Atomic counters are useful for incrementing or decrementing the value of an attribute. A good use case is counting website visitors.INCORRECT: "Scan the table for stale data and delete it once every hour" is incorrect as this is costly in terms of RCUs and WCUs. It also may result in data that has just been written but not analyzed yet.INCORRECT: "Delete the table and recreate it every hour" is incorrect. The table is constantly being written to and the analysis of data happens soon after the data is written. Therefore, there isn’t a good time to delete and recreate the table as data loss is likely to occur at any time.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html'},{question:"An application that is being migrated to AWS and refactored requires a storage service. The storage service should provide a standards-based REST web service interface and store objects based on keys. Which AWS service would be MOST suitable?",answers:[{text:"Amazon EFS",isCorrect:!1},{text:"Amazon EBS",isCorrect:!1},{text:"Amazon DynamoDB",isCorrect:!1},{text:"Amazon S3",isCorrect:!0}],explanation:'Amazon S3 is object storage built to store and retrieve any amount of data from anywhere on the Internet. Amazon S3 uses standardsbased REST and SOAP interfaces designed to work with any internetdevelopment toolkit.Amazon S3 is a simple keybased object store. The key is the name of the object and the value is the actual data itself. Keys can be any string, and they can be constructed to mimic hierarchical attributes.CORRECT: "Amazon S3" is the correct answer.INCORRECT: "Amazon DynamoDB" is incorrect. DynamoDB is a key/value database service that provides tables to store your data. This is not the most suitable solution for this requirement as the cost will be higher and there are more design considerations that need to be addressed.INCORRECT: "Amazon EBS" is incorrect as this is a blockbased storage system with which you attach volumes to Amazon EC2 instances. It is not a keybased object storage system.INCORRECT: "Amazon EFS" is incorrect as this is a filesystem that you mount to Amazon EC2 instances, it is also not a keybased object storage system.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html'},{question:"A Java based application generates email notifications to customers using Amazon SNS. The emails must contain links to access data in a secured Amazon S3 bucket. What is the SIMPLEST way to maintain security of the bucket whilst allowing the customers to access specific objects?",answers:[{text:"Use the AWS SDK for Java to update the bucket Access Control List to allow the customers to access the bucket",isCorrect:!1},{text:"Use the AWS SDK for Java to assume a role with AssumeRole to gain temporary security credentials",isCorrect:!1},{text:"Use the AWS SDK for Java with GeneratePresignedUrlRequest to create a presigned URL",isCorrect:!0},{text:"Use the AWS SDK for Java with the AWS STS service to gain temporary security credentials",isCorrect:!1}],explanation:'A presigned URL gives you access to the object identified in the URL, provided that the creator of the presigned URL has permissions to access that object. That is, if you receive a presigned URL to upload an object, you can upload the object only if the creator of the presigned URL has the necessary permissions to upload that object.You can use the AWS SDK for Java to generate a presigned URL that you, or anyone you give the URL, can use to upload an object to Amazon S3. When you use the URL to upload an object, Amazon S3 creates the object in the specified bucket.If an object with the same key that is specified in the presigned URL already exists in the bucket, Amazon S3 replaces the existing object with the uploaded object. To successfully complete an upload, you must do the following:• Specify the HTTP PUT verb when creating the GeneratePresignedUrlRequest and HttpURLConnection objects.• Interact with the HttpURLConnection object in some way after finishing the upload. The following example accomplishes this by using the HttpURLConnection object to check the HTTP response code.CORRECT: "Use the AWS SDK for Java with GeneratePresignedUrlRequest to create a presigned URL" is the correct answer.INCORRECT: "Use the AWS SDK for Java to update the bucket Access Control List to allow the customers to access the bucket" is incorrect. Bucket ACLs are used to grant access to predefined groups and accounts and are not suitable for this purpose.INCORRECT: "Use the AWS SDK for Java with the AWS STS service to gain temporary security credentials" is incorrect as this requires the creation of policies and security credentials and is not as simple as creating a presigned URL.INCORRECT: "Use the AWS SDK for Java to assume a role with AssumeRole to gain temporary security credentials" is incorrect as this requires the creation of policies and security credentials and is not as simple as creating a presigned URL.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURLJavaSDK.html'}]},{id:"aws-developer-2",title:"AWS Developer Associate (DVA-C02) Practice Test Latest 2025 2",description:"Additional practice questions covering AWS development topics.",questions:[{question:"An application is being instrumented to send trace data using AWS XRay. A Developer needs to upload segment documents using JSONformatted strings to XRay using the API. Which API action should the developer use?",answers:[{text:"The GetTraceSummaries API action",isCorrect:!1},{text:"The UpdateGroup API action",isCorrect:!1},{text:"The PutTraceSegments API action",isCorrect:!0},{text:"The PutTelemetryRecords API action",isCorrect:!1}],explanation:'You can send trace data to XRay in the form of segment documents. A segment document is a JSON formatted string that contains information about the work that your application does in service of a request. Your application can record data about the work that it does itself in segments, or work that uses downstream services and resources in subsegments.Segments record information about the work that your application does. A segment, at a minimum, records the time spent on a task, a name, and two IDs. The trace ID tracks the request as it travels between services. The segment ID tracks the work done for the request by a single service.Example Minimal complete segment:You can upload segment documents with the PutTraceSegments API. The API has a single parameter, TraceSegmentDocuments, that takes a list of JSON segment documents.Therefore, the Developer should use the PutTraceSegments API action.CORRECT: "The PutTraceSegments API action" is the correct answer.INCORRECT: "The PutTelemetryRecords API action" is incorrect as this is used by the AWS XRay daemon to upload telemetry.INCORRECT: "The UpdateGroup API action" is incorrect as this updates a group resource.INCORRECT: "The GetTraceSummaries API action" is incorrect as this retrieves IDs and annotations for traces available for a specified time frame using an optional filter.References: https://docs.aws.amazon.com/xray/latest/devguide/xrayapisendingdata.html'},{question:"A developer is creating a serverless application that will use a DynamoDB table. The average item size is 9KB. The application will make 4 strongly consistent reads/sec, and 2 standard write/sec. How many RCUs/WCUs are required?",answers:[{text:"12 RCU and 18 WCU",isCorrect:!0},{text:"12 RCU and 36 WCU",isCorrect:!1},{text:"6 RCU and 18 WCU",isCorrect:!1},{text:"24 RCU and 18 WCU",isCorrect:!1}],explanation:'With provisioned capacity mode, you specify the number of data reads and writes per second that you require for your application.Read capacity unit (RCU):• Each API call to read data from your table is a read request.• Read requests can be strongly consistent, eventually consistent, or transactional.• For items up to 4 KB in size, one RCU can perform one strongly consistent read request per second.• Items larger than 4 KB require additional RCUs.• For items up to 4 KB in size, one RCU can perform two eventually consistent read requests per second.• Transactional read requests require two RCUs to perform one read per second for items up to 4 KB.• For example, a strongly consistent read of an 8 KB item would require two RCUs, an eventually consistent read of an 8 KB item would require one RCU, and a transactional read of an 8 KB item would require four RCUs.Write capacity unit (WCU):• Each API call to write data to your table is a write request.• For items up to 1 KB in size, one WCU can perform one standard write request per second.• Items larger than 1 KB require additional WCUs.• Transactional write requests require two WCUs to perform one write per second for items up to 1 KB.• For example, a standard write request of a 1 KB item would require one WCU, a standard write request of a 3 KB item would require three WCUs, and a transactional write request of a 3 KB item would require six WCUs.To determine the number of RCUs required to handle 4 strongly consistent reads per/second with an average item size of 9KB, perform the following steps:1. Determine the average item size by rounding up the next multiple of 4KB (9KB rounds up to 12KB).2. Determine the RCU per item by dividing the item size by 4KB (12KB/4KB = 3).3. Multiply the value from step 2 with the number of reads required per second (3x4 = 12).To determine the number of WCUs required to handle 2 standard writes per/second with an average item size of 9KB, simply multiply the average item size by the number of writes required (9x2=18).CORRECT: "12 RCU and 18 WCU" is the correct answer.INCORRECT: "24 RCU and 18 WCU" is incorrect. This would be the correct answer for transactional reads and standard writes.INCORRECT: "12 RCU and 36 WCU" is incorrect. This would be the correct answer for strongly consistent reads and transactional writes.INCORRECT: "6 RCU and 18 WCU" is incorrect. This would be the correct answer for eventually consistent reads and standard writesReferences: https://aws.amazon.com/dynamodb/pricing/provisioned/'},{question:"A gaming application displays the results of games in a leaderboard. The leaderboard is updated by 4 KB messages that are retrieved from an Amazon SQS queue. The updates are received infrequently but the Developer needs to minimize the time between the messages arriving in the queue and the leaderboard being updated. Which technique provides the shortest delay in updating the leaderboard?",answers:[{text:"Retrieve the messages from the queue using short polling every 10 seconds",isCorrect:!1},{text:"Store the message payload in Amazon S3 and use the SQS Extended Client Library for Java",isCorrect:!1},{text:"Reduce the size of the messages with compression before sending them",isCorrect:!1},{text:"Retrieve the messages from the queue using long polling every 15 seconds",isCorrect:!0}],explanation:'The process of consuming messages from a queue depends on whether you use short or long polling. By default, Amazon SQS uses short polling, querying only a subset of its servers (based on a weighted random distribution) to determine whether any messages are available for a response.You can use long polling to reduce your costs while allowing your consumers to receive messages as soon as they arrive in the queue. When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds.Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren\'t included in a response). It also returns messages as soon as they become available.CORRECT: "Retrieve the messages from the queue using long polling every 15 seconds" is the correct answer.INCORRECT: "Retrieve the messages from the queue using short polling every 10 seconds" is incorrect as short polling is configured when the WaitTimeSeconds parameter of a ReceiveMessage request is set to 0. Any number above zero indicates long polling is in effect.INCORRECT: "Reduce the size of the messages with compression before sending them" is incorrect as this will not mean messages are picked up earlier and there is no reason to compress messages that are 4 KB in size.INCORRECT: "Store the message payload in Amazon S3 and use the SQS Extended Client Library for Java" is incorrect as this is unnecessary for messages of this size and will also not result in the shortest delay when updating the leaderboard.References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsshortandlongpolling.html'},{question:"A website consisting of HTML, images, and clientside JavaScript is being hosted on Amazon S3. The website will be used globally, what's the best way to MINIMIZE latency for global users?",answers:[{text:"Host the website from multiple buckets around the world and use Route 53 geolocationbased routing",isCorrect:!1},{text:"Create a CloudFront distribution and configure the S3 website as an origin",isCorrect:!0},{text:"Enable S3 transfer acceleration",isCorrect:!1},{text:"Create an ElastiCache cluster and configure the S3 website as an origin",isCorrect:!1}],explanation:'To serve a static website hosted on Amazon S3, you can deploy a CloudFront distribution using one of these configurations:• Using a REST API endpoint as the origin with access restricted by an origin access identity (OAI)• Using a website endpoint as the origin with anonymous (public) access allowed• Using a website endpoint as the origin with access restricted by a Referer headerAll assets of this website are static (HTML, images, clientside JavaScript), therefore this website is compatible with both S3 static websites and Amazon CloudFront. The simplest way to minimize latency is to create a CloudFront distribution and configure the static website as an origin.CORRECT: "Create a CloudFront distribution and configure the S3 website as an origin" is the correct answer.INCORRECT: "Host the website from multiple buckets around the world and use Route 53 geolocationbased routing" is incorrect as this not a good way to solve this problem. With this configuration you would need to keep multiple copies of the website files in sync (and pay for more storage space) which is less than ideal.INCORRECT: "Enable S3 transfer acceleration" is incorrect as transfer acceleration is used for improving the speed of uploads to an S3 bucket, not downloads.INCORRECT: "Create an ElastiCache cluster and configure the S3 website as an origin" is incorrect as you cannot use an ElastiCache cluster as the frontend to an S3 static website (nor does it solve the problem of reducing latency around the world).References: https://aws.amazon.com/premiumsupport/knowledgecenter/cloudfrontservestaticwebsite/'},{question:"A Developer has joined a team and needs to connect to the AWS CodeCommit repository using SSH. What should the Developer do to configure access using Git?",answers:[{text:"Generate an SSH public and private key. Upload the public key to the Developer’s IAM account",isCorrect:!0},{text:"On the Developer’s IAM account, under security credentials, choose to create an access key and secret ID",isCorrect:!1},{text:"Create an account on Github and user those login credentials to login to AWS CodeCommit",isCorrect:!1},{text:"On the Developer’s IAM account, under security credentials, choose to create HTTPS Git credentials for AWS CodeCommit",isCorrect:!1}],explanation:'You need to configure your Git client to communicate with CodeCommit repositories. As part of this configuration, you provide IAM credentials that CodeCommit can use to authenticate you. IAM supports CodeCommit with three types of credentials:• Git credentials, an IAM generated user name and password pair you can use to communicate with CodeCommit repositories over HTTPS.• SSH keys, a locally generated publicprivate key pair that you can associate with your IAM user to communicate with CodeCommit repositories over SSH.• AWS access keys, which you can use with the credential helper included with the AWS CLI to communicate with CodeCommit repositories over HTTPS.As the Developer is going to use SSH, he first needs to generate an SSH private and public key. These can then be used for authentication. The method of creating these depends on the operating system the Developer is using. Then, the Developer can upload the public key (by copying the contents of the file) into his IAM account under security credentials.CORRECT: "Generate an SSH public and private key. Upload the public key to the Developer’s IAM account" is the correct answer.INCORRECT: "On the Developer’s IAM account, under security credentials, choose to create HTTPS Git credentials for AWS CodeCommit" is incorrect as this method is used for creating credentials when you want to connect to CodeCommit using HTTPS.INCORRECT: "Create an account on Github and user those login credentials to login to AWS CodeCommit" is incorrect as you cannot login to AWS CodeCommit using credentials from Github.INCORRECT: "On the Developer’s IAM account, under security credentials, choose to create an access key and secret ID" is incorrect as though you can use access keys to authenticated to CodeCommit, this requires the credential helper, and enables access over HTTPS.References:https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_sshkeys.html https://docs.aws.amazon.com/codecommit/latest/userguide/settingupsshunixes.html'},{question:"A developer has created a Docker image and uploaded it to an Amazon Elastic Container Registry (ECR) repository. How can the developer pull the image to his workstation using the docker client?",answers:[{text:"Run aws ecr getloginpassword use the output to login in then issue a docker pull command specifying the image name using registry/repository[:tag]",isCorrect:!0},{text:"Run docker login with an IAM key pair then issue a docker pull command specifying the image name using registry/repository[@digest]",isCorrect:!1},{text:"Run aws ecr describeimages repositoryname repositoryname",isCorrect:!1},{text:"Run the docker pull command specifying the image name using registry/repository[:tag]",isCorrect:!1}],explanation:'If you would like to run a Docker image that is available in Amazon ECR, you can pull it to your local environment with the docker pull command. You can do this from either your default registry or from a registry associated with another AWS account.Docker CLI does not support standard AWS authentication methods, so client authentication must be handled so that ECR knows who is requesting to push or pull an image. To do this you can issue the aws ecr getlogin or aws ecr getloginpassword (AWS CLI v2) and then use the output to login using docker login and then issue a docker pull command specifying the image name using registry/repository[:tag]CORRECT: "Run aws ecr getloginpassword use the output to login in then issue a docker pull command specifying the image name using registry/repository[:tag]" is the correct answer.INCORRECT: "Run the docker pull command specifying the image name using registry/repository[:tag]" is incorrect as you first need to authenticate to get an access token so you can pull the image down.INCORRECT: "Run aws ecr describeimages repositoryname repositoryname" is incorrect as this would just list the images available in the repository.INCORRECT: "Run docker login with an IAM key pair then issue a docker pull command specifying the image name using registry/repository[@digest]" is incorrect as you cannot run docker login with an IAM key pair.References: https://docs.aws.amazon.com/AmazonECR/latest/userguide/dockerpullecrimage.html'},{question:'A Developer has created a task definition that includes the following JSON code:"placementConstraints": [{"expression": "task:group == databases","type": "memberOf"}]What will be the effect for tasks using this task definition?',answers:[{text:"They will not be placed on container instances in the “databases” task group",isCorrect:!1},{text:"They will be placed on container instances in the “databases” task group",isCorrect:!0},{text:"They will not be allowed to run unless they have the “databases” tag assigned",isCorrect:!1},{text:"They will become members of a task group called “databases”",isCorrect:!1}],explanation:'A task placement constraint is a rule that is considered during task placement. Task placement constraints can be specified when either running a task or creating a new service.The memberOf task placement constraint places tasks on container instances that satisfy an expression.The memberOf task placement constraint can be specified with the following actions:• Running a task• Creating a new service• Creating a new task definition• Creating a new revision of an existing task definitionThe example JSON code uses the memberOf constraint to place tasks on instances in the databases task group. It can be specified with the following actions: CreateService, UpdateService, RegisterTaskDefinition, and RunTask.CORRECT: "They will be placed on container instances in the “databases” task group" is the correct answer.INCORRECT: "They will become members of a task group called “databases”" is incorrect. They will be placed on container instances in the “databases” task group.INCORRECT: "They will not be placed on container instances in the “databases” task group" is incorrect. This statement ensures the tasks ARE placed on the container instances in the “databases” task group.INCORRECT: "They will not be allowed to run unless they have the “databases” tag assigned" is incorrect. This JSON code is not related to tagging of the tasks.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskplacementconstraints.html'},{question:"A Developer is migrating Docker containers to Amazon ECS. A large number of containers will be deployed across some newly deployed ECS containers instances using the same instance type. High availability is provided within the microservices architecture. Which task placement strategy requires the LEAST configuration for this scenario?",answers:[{text:"binpack",isCorrect:!1},{text:"random",isCorrect:!0},{text:"Fargate",isCorrect:!1},{text:"spread",isCorrect:!1}],explanation:'When a task that uses the EC2 launch type is launched, Amazon ECS must determine where to place the task based on the requirements specified in the task definition, such as CPU and memory. Similarly, when you scale down the task count, Amazon ECS must determine which tasks to terminate. You can apply task placement strategies and constraints to customize how Amazon ECS places and terminates tasks. Task placement strategies and constraints are not supported for tasks using the Fargate launch type. By default, Fargate tasks are spread across Availability Zones.A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. For example, Amazon ECS can select instances at random, or it can select instances such that tasks are distributed evenly across a group of instances.Amazon ECS supports the following task placement strategies:• binpackPlace tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.• randomPlace tasks randomly.• spreadPlace tasks evenly based on the specified value. Accepted values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availabilityzone. Service tasks are spread based on the tasks from that service. Standalone tasks are spread based on the tasks from the same task group.Therefore, for this scenario the random task placement strategy is most suitable as it requires the least configuration.CORRECT: "random" is the correct answer.INCORRECT: "spread" is incorrect. As high availability is taken care of within the containers there is no need to use a spread strategy to ensure HA.INCORRECT: "binpack" is incorrect as there is no need to pack the containers onto the fewest instances based on CPU or memory.INCORRECT: "Fargate" is incorrect as this is not a task placement strategy, it is a serverless service for running containers.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskplacementstrategies.html'},{question:"A company uses an Amazon EC2 web application with Amazon CloudFront to distribute content to its customers globally. The company requires that all traffic is encrypted between the customers and CloudFront, and CloudFront and the web application. What steps need to be taken to enforce this encryption? (Select TWO.)",answers:[{text:"Enable Field Level Encryption",isCorrect:!1},{text:"Use AWS KMS to enforce encryption",isCorrect:!1},{text:"Set the Viewer Protocol Policy to “HTTPS Only” or “Redirect HTTP to HTTPS”",isCorrect:!0},{text:"Set the Origin Protocol Policy to “HTTPS Only”",isCorrect:!0},{text:"Change the HTTP port to 443 in the Origin Settings",isCorrect:!1}],explanation:'To ensure encryption between the origin (Amazon EC2) and CloudFront you need to set the Origin Protocol Policy to “HTTPS Only” This is configured in the origin settings and can be seen in the image below:To ensure encryption between CloudFront and the end users you need to change the Viewer Protocol Policy to “HTTPS Only” or “Redirect HTTP to HTTPS”. This is configured in the cache behavior and can be seen in the image below:CORRECT: "Set the Origin Protocol Policy to “HTTPS Only”" is the correct answer.CORRECT: "Set the Viewer Protocol Policy to “HTTPS Only” or “Redirect HTTP to HTTPS”" is also a correct answer.INCORRECT: "Enable Field Level Encryption" is incorrect. This is used to add another layer of security to sensitive data such as credit card numbers.INCORRECT: "Change the HTTP port to 443 in the Origin Settings" is incorrect. You should not change the HTTP port to 443, instead change Origin Protocol Policy to HTTPS.INCORRECT: "Use AWS KMS to enforce encryption" is incorrect. AWS KMS is not used for enforcing encryption on CloudFront. AWS KMS is used for creating and managing encryption keys.References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/usinghttps.html'},{question:"A Developer manages a website running behind an Elastic Load Balancer in the useast1 region. The Developer has recently deployed an identical copy of the website in uswest1 and needs to send 20% of the traffic to the new site. How can the Developer achieve this requirement?",answers:[{text:"Use a blue/green deployment with Amazon CodeDeploy",isCorrect:!1},{text:"Use an Amazon Route 53 Weighted Routing Policy",isCorrect:!0},{text:"Use an Amazon Route 53 Geolocation Routing Policy",isCorrect:!1},{text:"Use a blue/green deployment with Amazon Elastic Beanstalk",isCorrect:!1}],explanation:'Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software.In this case the Developer can use a weighted routing policy to direct 20% of the incoming traffic to the new site as required.CORRECT: "Use an Amazon Route 53 Weighted Routing Policy" is the correct answer.INCORRECT: "Use an Amazon Route 53 Geolocation Routing Policy" is incorrect as the Developer should use a weighted routing policy for this requirement as a specified percentage of traffic needs to be directed to the new website.INCORRECT: "Use a blue/green deployment with Amazon Elastic Beanstalk" is incorrect as the question does not state that Elastic Beanstalk is being used and the new website has already been deployed.INCORRECT: "Use a blue/green deployment with Amazon CodeDeploy" is incorrect as the question does not state that Amazon CodeDeploy is being used and the website has already been deployed.References: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routingpolicy.html#routingpolicyweighted'},{question:"A company needs to encrypt a large quantity of data. The data encryption keys must be generated from a dedicated, tamper-resistant hardware device. To deliver these requirements, which AWS service should the company use?",answers:[{text:"AWS KMS",isCorrect:!1},{text:"AWS Certificate Manager",isCorrect:!1},{text:"AWS CloudHSM",isCorrect:!0},{text:"AWS IAM",isCorrect:!1}],explanation:'The AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) instances within the AWS cloud.A Hardware Security Module (HSM) provides secure key storage and cryptographic operations within a tamperresistant hardware device. CloudHSM allows you to securely generate, store, and manage cryptographic keys used for data encryption in a way that keys are accessible only by you.CORRECT: "AWS CloudHSM" is the correct answer.INCORRECT: "AWS KMS" is incorrect as it uses shared infrastructure (multitenant) and is therefore not a dedicated HSM.INCORRECT: "AWS Certificate Manager" is incorrect as this is used to generate and manage SSL/TLS certificates, it does not generate data encryption keys.INCORRECT: "AWS IAM" is incorrect as this service is not involved with generating encryption keys.References: https://aws.amazon.com/cloudhsm/faqs/'},{question:"A Developer has created a serverless function that processes log files. The function should be invoked once every 15 minutes. How can the Developer automatically invoke the function using serverless services?",answers:[{text:"Launch an EC2 Linux instance and add a command to periodically invoke the function to its /etc/crontab file",isCorrect:!1},{text:"Create an Amazon SNS rule to send a notification to Lambda to instruct it to run",isCorrect:!1},{text:"Create an Amazon CloudWatch Events rule that is scheduled to run and invoke the function",isCorrect:!0},{text:"Configure the Lambda scheduler to run based on recurring time value",isCorrect:!1}],explanation:'Amazon CloudWatch Events delivers a near realtime stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams.You can use Amazon CloudWatch Events to invoke the Lambda function on a recurring schedule of 15 minutes. This solution is entirely automated and serverless.CORRECT: "Create an Amazon CloudWatch Events rule that is scheduled to run and invoke the function" is the correct answer.INCORRECT: "Launch an EC2 Linux instance and add a command to periodically invoke the function to its /etc/crontab file " is incorrect as this is automatic but it is not serverless.INCORRECT: "Configure the Lambda scheduler to run based on recurring time value" is incorrect as there is no Lambda scheduler that can be used.INCORRECT: "Create an Amazon SNS rule to send a notification to Lambda to instruct it to run" is incorrect as you cannot invoke a function by sending a notification to it from Amazon SNS.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html https://digitalcloud.training/certificationtraining/awsdeveloperassociate/awscompute/awslambda/'},{question:"An Amazon Kinesis Data Stream has recently been configured to receive data from sensors in a manufacturing facility. A consumer EC2 instance is configured to process the data every 48 hours and save processing results to an Amazon RedShift data warehouse. Testing has identified a large amount of data is missing. A review of monitoring logs has identified that the sensors are sending data correctly and the EC2 instance is healthy. What is the MOST likely explanation for this issue?",answers:[{text:"Records are retained for 24 hours in the Kinesis Data Stream by default",isCorrect:!0},{text:"Amazon RedShift is not suitable for storing streaming data",isCorrect:!1},{text:"Amazon Kinesis has too many shards provisioned",isCorrect:!1},{text:"The EC2 instance is failing intermittently",isCorrect:!1}],explanation:'Amazon Kinesis Data Streams supports changes to the data record retention period of your stream. A Kinesis data stream is an ordered sequence of data records meant to be written to and read from in real time. Data records are therefore stored in shards in your stream temporarily. The time period from when a record is added to when it is no longer accessible is called the retention period. A Kinesis data stream stores records from 24 hours by default, up to 168 hours.You can increase the retention period up to 168 hours using the IncreaseStreamRetentionPeriod operation. You can decrease the retention period down to a minimum of 24 hours using the DecreaseStreamRetentionPeriod operation. The request syntax for both operations includes the stream name and the retention period in hours. Finally, you can check the current retention period of a stream by calling the DescribeStream operation.Both operations are easy to use. The following is an example of changing the retention period using the AWS CLI:Therefore, the most likely explanation is that the message retention period is set at the 24hour default.CORRECT: "Records are retained for 24 hours in the Kinesis Data Stream by default" is the correct answer.INCORRECT: "Amazon RedShift is not suitable for storing streaming data" is incorrect. In this architecture Amazon Kinesis is responsible for receiving streaming data and storing it in a stream. The EC2 instances can then process and store the data in a number of different destinations including Amazon RedShift.INCORRECT: "The EC2 instance is failing intermittently" is incorrect as the question states that a review of monitoring logs indicates that the EC2 instance is healthy. If it was failing intermittently this should be recorded in the logs.INCORRECT: "Amazon Kinesis has too many shards provisioned" is incorrect as this would just mean that the Kinesis Stream has more capacity, not less.References: https://docs.aws.amazon.com/streams/latest/dev/kinesisextendedretention.html'},{question:"A Developer has noticed some suspicious activity in her AWS account and is concerned that the access keys associated with her IAM user account may have been compromised. What is the first thing the Developer do in should do in this situation?",answers:[{text:"Delete her IAM user account",isCorrect:!1},{text:"Report the incident to AWS Support",isCorrect:!1},{text:"Delete the compromised access keys",isCorrect:!0},{text:"Change her IAM User account password",isCorrect:!1}],explanation:'In this case the Developer’s access keys may have been compromised so the first step would be to invalidate the access keys by deleting them.The next step would then be to determine if any temporary security credentials have been issued an invalidating those too to prevent any further misuse.The user account and user account password have not been compromised so they do not need to be deleted / changed as a first step. However, changing the account password would typically be recommended as a best practice in this situation.CORRECT: "Delete the compromised access keys" is the correct answer.INCORRECT: "Delete her IAM user account" is incorrect. This user account has not been compromised based on the available information, just the access keys. Deleting the access keys will prevent further misuse of the AWS account.INCORRECT: "Report the incident to AWS Support" is incorrect is a good practice but not the first step. The Developer should first attempt to mitigate any further misuse of the account by deleting the access keys.INCORRECT: "Change her IAM User account password" is incorrect as she does not have any evidence that the account has been compromised, just the access keys. However, it would be a good practice to change the password, just not the first thing to do.References: https://aws.amazon.com/blogs/security/whattodoifyouinadvertentlyexposeanawsaccesskey/'},{question:"A developer is making some updates to an AWS Lambda function that is part of a serverless application and will be saving a new version. The application is used by hundreds of users and the developer needs to be able to test the updates and be able to rollback if there any issues with user experience. What is the SAFEST way to do this with minimal changes to the application code?",answers:[{text:"Create an alias and point it to the new and previous versions. Assign a weight of 20% to the new version to direct less traffic. Update the application code to point to the new alias",isCorrect:!0},{text:"Create an alias and point it to the new version. Update the application code to point to the new alias",isCorrect:!1},{text:"Create A records in Route 53 for each function version’s ARN. Use a weighted routing policy to direct 20% of traffic to the new version. Add the DNS records to the application code",isCorrect:!1},{text:"Update the application code to point to the new version",isCorrect:!1}],explanation:'You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN.You can point an alias a multiple versions of your function code and then assign a weighting to direct certain amounts of traffic to each version. This enables a blue/green style of deployment and means it’s easy to roll back to the older version by simply updating the weighting if issues occur with user experience.CORRECT: "Create an alias and point it to the new and previous versions. Assign a weight of 20% to the new version to direct less traffic. Update the application code to point to the new alias" is the correct answer.INCORRECT: "Create an alias and point it to the new version. Update the application code to point to the new alias" is incorrect as it is better to point the alias at both the new and previous versions of the function code so that it is easier to roll back with fewer application code changes.INCORRECT: "Update the application code to point to the new version" is incorrect as if you do this you will have to change the application code again to roll back in the event of issues. You will also need to update the application code every time you publish a new version, so this is not a best practice strategy.INCORRECT: "Create A records in Route 53 for each function version’s ARN. Use a weighted routing policy to direct 20% of traffic to the new version. Add the DNS records to the application code" is incorrect as you cannot create Route 53 DNS records that point to an ARN.References:https://docs.aws.amazon.com/lambda/latest/dg/configurationaliases.html https://docs.aws.amazon.com/lambda/latest/dg/configurationversions.html'},{question:"A Developer needs to setup a new serverless application that includes AWS Lambda and Amazon API Gateway as part of a single stack. The Developer needs to be able to locally build and test the serverless applications before deployment on AWS. Which service should the Developer use?",answers:[{text:"AWS Serverless Application Model (SAM)",isCorrect:!0},{text:"AWS CodeBuild",isCorrect:!1},{text:"AWS CloudFormation",isCorrect:!1},{text:"AWS Elastic Beanstalk",isCorrect:!1}],explanation:'The AWS Serverless Application Model (AWS SAM) is an opensource framework that you can use to build serverless applications on AWS. A serverless application is a combination of Lambda functions, event sources, and other resources that work together to perform tasks.AWS SAM provides you with a simple and clean syntax to describe the functions, APIs, permissions, configurations, and events that make up a serverless application.The AWS SAM CLI lets you locally build, test, and debug serverless applications that are defined by AWS SAM templates. The CLI provides a Lambdalike execution environment locally. It helps you catch issues upfront by providing parity with the actual Lambda execution environment.CORRECT: "AWS Serverless Application Model (SAM)" is the correct answer.INCORRECT: "AWS CloudFormation" is incorrect as you cannot perform local build and test with AWS CloudFormation.INCORRECT: "AWS Elastic Beanstalk" is incorrect as you cannot deploy serverless applications or perform local build and test with Elastic Beanstalk.INCORRECT: "AWS CodeBuild" is incorrect as you cannot perform local build and test with AWS CodeBuild.References: https://docs.aws.amazon.com/serverlessapplicationmodel/latest/developerguide/whatissam.html'},{question:"A Developer is looking for a way to use shorthand syntax to express functions, APIs, databases, and event source mappings. The Developer will test using AWS SAM to create a simple Lambda function using Nodejs.12x. What is the SIMPLEST way for the Developer to get started with a Hello World Lambda function?",answers:[{text:"Install the AWS SAM CLI, run sam init and use one of the AWS Quick Start Templates",isCorrect:!0},{text:"Install the AWS CLI, run aws sam init and use one of the AWS Quick Start Templates",isCorrect:!1},{text:"Use AWS CloudFormation to deploy a Hello World stack using AWS SAM",isCorrect:!1},{text:"Use the AWS Management Console to access AWS SAM and deploy a Hello World function",isCorrect:!1}],explanation:'The sam init command initializes a serverless application with an AWS SAM template. The template provides a folder structure for your Lambda functions and is connected to an event source such as APIs, S3 buckets, or DynamoDB tables. This application includes everything you need to get started and to eventually extend it into a productionscale application.This is the simplest way for the Developer to quickly get started with testing AWS SAM. Before the Developer can use the “sam” commands it is necessary to install the AWS SAM CLI. This is separate to the AWS CLI.CORRECT: "Install the AWS SAM CLI, run sam init and use one of the AWS Quick Start Templates" is the correct answer.INCORRECT: "Install the AWS CLI, run aws sam init and use one of the AWS Quick Start Templates" is incorrect as “sam init” is not an AWS CLI command, therefore you cannot put “aws” in front of “sam”.INCORRECT: "Use the AWS Management Console to access AWS SAM and deploy a Hello World function" is incorrect as you cannot access AWS SAM through the console. You can, however, access the Serverless Application Repository through the console and deploy SAM templates.INCORRECT: "Use AWS CloudFormation to deploy a Hello World stack using AWS SAM" is incorrect as though AWS SAM does use CloudFormation you cannot deploy SAM templates through the AWS CloudFormation console. You must use the SAM CLI or deploy using the Serverless Application Repository.https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html'},{question:'A Developer needs to access AWS CodeCommit over SSH. The SSH keys configured to access AWS CodeCommit are tied to a user with the following permissions:{"version": "20121017""Statement": [{"Effect": "Allow","Action": ["codecommit:BatchGetRepositories","codecommit:Get*""codecommit:List*","codecommit:GitPull"],"Resource": "*"}]}The Developer needs to create/delete branches.Which specific IAM permissions need to be added based on the principle of least privilege?',answers:[{text:"“codecommit:CreateBranch” and “codecommit:DeleteBranch”",isCorrect:!0},{text:"“codecommit:Put*:”",isCorrect:!1},{text:"“codecommit:*”",isCorrect:!1},{text:"“codecommit:Update*”",isCorrect:!1}],explanation:'The permissions assigned to the user account are missing the privileges to create and delete branches in AWS CodeCommit.The Developer needs to be assigned these permissions but according to the principal of least privilege it’s important to ensure no additional permissions are assigned.The following API actions can be used to work with branches:• CreateBranch , which creates a branch in a specified repository.• DeleteBranch , which deletes the specified branch in a repository unless it is the default branch.• GetBranch , which returns information about a specified branch.• ListBranches , which lists all branches for a specified repository.• UpdateDefaultBranch , which changes the default branch for a repository.Therefore, the best answer is to add the “codecommit:CreateBranch” and “codecommit:DeleteBranch” permissions to the permissions policy.CORRECT: "codecommit:CreateBranch” and “codecommit:DeleteBranch" is the correct answer.INCORRECT: "codecommit:Put*:" is incorrect. The wildcard (*) will allow any API action starting with “Put”, however the only options are putfile and putrepositorytriggers, neither of which is related to branches.INCORRECT: "codecommit:Update*" is incorrect. The wildcard (*) will allow any API action starting with “Update”, however none of the options available are suitable for working with branches.INCORRECT: "codecommit:*" is incorrect as this would allow any API action which does not follow the principal of least privilege.References: https://docs.aws.amazon.com/cli/latest/reference/codecommit/index.html#cliawscodecommit'},{question:"An organization needs to add encryption intransit to an existing website running behind an Elastic Load Balancer. The website's Amazon EC2 instances are CPUconstrained and therefore load on their CPUs should not be increased. What should be done to secure the website? (Select TWO.)",answers:[{text:"Configure an Elastic Load Balancer with a KMS CMK",isCorrect:!1},{text:"Install SSL certificates on the EC2 instances",isCorrect:!1},{text:"Configure an Elastic Load Balancer with SSL passthrough",isCorrect:!1},{text:"Configure SSL certificates on an Elastic Load Balancer",isCorrect:!0},{text:"Configure an Elastic Load Balancer with SSL termination",isCorrect:!0}],explanation:'The company need to add security to their website by encrypting traffic intransit using HTTPS. This requires adding SSL/TLS certificates to enable the encryption. The process of encrypting and decrypting data is CPU intensive and therefore the company need to avoid adding certificates to the EC2 instances as that will place further load on their CPUs.Therefore, the solution is to configure SSL certificates on the Elastic Load Balancer and then configure SSL termination. This can be done by adding a certificate to a HTTPS listener on the load balancer.CORRECT: "Configure SSL certificates on an Elastic Load Balancer" is a correct answer.CORRECT: "Configure an Elastic Load Balancer with SSL termination" is a correct answer.INCORRECT: "Configure an Elastic Load Balancer with SSL passthrough" is incorrect as with passthrough the SSL session must be terminated on the EC2 instances which should be avoided as they are CPUconstrained.INCORRECT: "Configure an Elastic Load Balancer with a KMS CMK" is incorrect as a KMS CMK is used to encrypt data at rest, it is not used for intransit encryption.INCORRECT: "Install SSL certificates on the EC2 instances" is incorrect as this would increase the load on the CPUsReferences: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/createhttpslistener.html'},{question:"A developer has created a YAML template file that includes the following header: 'AWS::Serverless20161031'. Which commands should the developer use to deploy the application?",answers:[{text:"aws cloudformation package and aws cloudformation createstack",isCorrect:!1},{text:"aws cloudformation createstackset",isCorrect:!1},{text:"sam package and sam build",isCorrect:!1},{text:"sam package and sam deploy",isCorrect:!0}],explanation:'The AWS Serverless Application Model (SAM) is an opensource framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML.The “Transform” header indicates that the developer is creating a SAM template as it has the value: Transform: \'AWS::Serverless20161031\'Therefore there are two sets of commands that can be used to package and deploy using SAM:Use either:• sam package• sam deployOr use:• aws cloudformation package• aws cloudformation deployCORRECT: "sam package and sam deploy" is the correct answer.INCORRECT: "sam package and sam build" is incorrect as “sam build” is used to build your Lambda function code, not to package and deploy it.INCORRECT: "aws cloudformation createstackset" is incorrect as this creates a stack set and is not used when deploying using AWS SAM.INCORRECT: "aws cloudformation package and aws cloudformation createstack" is incorrect as when using AWS SAM you should use “aws cloudformation deploy” instead for the second command.References: https://docs.aws.amazon.com/serverlessapplicationmodel/latest/developerguide/serverlesssamclicommandreference.html'},{question:"An IT automation architecture uses many AWS Lambda functions invoking one another as a large state machine. The coordination of this state machine is legacy custom code that breaks easily. Which AWS Service can help refactor and manage the state machine?",answers:[{text:"AWS CodeBuild",isCorrect:!1},{text:"AWS CloudFormation",isCorrect:!1},{text:"AWS Step Functions",isCorrect:!0},{text:"AWS CodePipeline",isCorrect:!1}],explanation:'AWS Step Functions lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. Using Step Functions, you can design and run workflows that stitch together services, such as AWS Lambda, AWS Fargate, and Amazon SageMaker, into featurerich applications.Workflows are made up of a series of steps, with the output of one step acting as input into the next. Application development is simpler and more intuitive using Step Functions, because it translates your workflow into a state machine diagram that is easy to understand, easy to explain to others, and easy to change.Step Functions automatically triggers and tracks each step, and retries when there are errors, so your application executes in order and as expected. With Step Functions, you can craft longrunning workflows such as machine learning model training, report generation, and IT automation.Therefore, AWS Step Functions is the best AWS service to use when refactoring the application away from the legacy code.CORRECT: "AWS Step Functions" is the correct answer.INCORRECT: "AWS CloudFormation" is incorrect as CloudFormation is used for deploying resources no AWS but not for ongoing automation.INCORRECT: "AWS CodePipeline" is incorrect as this is used as part of a continuous integration and delivery (CI/CD) pipeline to deploy software updates to applications.INCORRECT: "AWS CodeBuild" is incorrect as this an AWS build/test service.References: https://aws.amazon.com/stepfunctions/'},{question:"Data must be loaded into an application each week for analysis. The data is uploaded to an Amazon S3 bucket from several offices around the world. Latency is slowing the uploads and delaying the analytics job. What is the SIMPLEST way to improve upload times?",answers:[{text:"Upload to Amazon CloudFront and then download from the local cache to the S3 bucket",isCorrect:!1},{text:"Upload to a local Amazon S3 bucket within each region and enable CrossRegion Replication (CRR)",isCorrect:!1},{text:"Upload via a managed AWS VPN connection",isCorrect:!1},{text:"Upload using Amazon S3 Transfer Acceleration",isCorrect:!0}],explanation:'Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.You might want to use Transfer Acceleration on a bucket for various reasons, including the following:• You have customers that upload to a centralized bucket from all over the world.• You transfer gigabytes to terabytes of data on a regular basis across continents.• You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.CORRECT: "Upload using Amazon S3 Transfer Acceleration" is the correct answer.INCORRECT: "Upload to a local Amazon S3 bucket within each region and enable CrossRegion Replication (CRR)" is incorrect as this would not speed up the upload as the process introduces more latency.INCORRECT: "Upload via a managed AWS VPN connection" is incorrect as this still uses the public Internet and there’s no real latency advantages here.INCORRECT: "Upload to Amazon CloudFront and then download from the local cache to the S3 bucket" is incorrect. This is going to require some time to propagate to the cache and requires some manual work in retrieving the data. The simplest solution is to use S3 Transfer Acceleration which basically does this for you. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/transferacceleration.html'},{question:"A company currently runs a number of legacy automated batch processes for system update management and operational activities. The company are looking to refactor these processes and require a service that can coordinate multiple AWS services into serverless workflows. What is the MOST suitable service for this requirement?",answers:[{text:"Amazon SWF",isCorrect:!1},{text:"AWS Step Functions",isCorrect:!0},{text:"AWS Lambda",isCorrect:!1},{text:"AWS Batch",isCorrect:!1}],explanation:'AWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or task, allowing you to scale and change applications quickly.Step Functions provides a reliable way to coordinate components and step through the functions of your application. Step Functions offers a graphical console to visualize the components of your application as a series of steps. It automatically triggers and tracks each step, and retries when there are errors, so your application executes in order and as expected, every time. Step Functions logs the state of each step, so when things go wrong, you can diagnose and debug problems quickly.CORRECT: "AWS Step Functions" is the correct answer.INCORRECT: "Amazon SWF" is incorrect. You can think of Amazon SWF as a fullymanaged state tracker and task coordinator in the Cloud. It does not coordinate serverless workflows.INCORRECT: "AWS Batch" is incorrect as this is used to run batch computing jobs on Amazon EC2 and is therefore not serverless.INCORRECT: "AWS Lambda" is incorrect as though it is serverless, it does not provide a native capability to coordinate multiple AWS services.References: https://docs.aws.amazon.com/stepfunctions/latest/dg/welcome.html'},{question:"A Developer is managing an application that includes an Amazon SQS queue. The consumers that process the data from the queue are connecting in short cycles and the queue often does not return messages. The cost for API calls is increasing. How can the Developer optimize the retrieval of messages and reduce cost?",answers:[{text:"Call the SetQueueAttributes API with the DelaySeconds parameter set to 900",isCorrect:!1},{text:"Call the SetQueueAttributes API with the maxReceiveCount set to 20",isCorrect:!1},{text:"Call the ReceiveMessage API with the WaitTimeSeconds parameter set to 20",isCorrect:!0},{text:"Call the ReceiveMessage API with the VisibilityTimeout parameter set to 30",isCorrect:!1}],explanation:'The process of consuming messages from a queue depends on whether you use short or long polling. By default, Amazon SQS uses short polling, querying only a subset of its servers (based on a weighted random distribution) to determine whether any messages are available for a response. You can use long polling to reduce your costs while allowing your consumers to receive messages as soon as they arrive in the queue.When you consume messages from a queue using short polling, Amazon SQS samples a subset of its servers (based on a weighted random distribution) and returns messages from only those servers. Thus, a particular ReceiveMessage request might not return all of your messages. However, if you have fewer than 1,000 messages in your queue, a subsequent request will return your messages. If you keep consuming from your queues, Amazon SQS samples all of its servers, and you receive all of your messages.When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren\'t included in a response)Therefore, the Developer should call the ReceiveMessage API with the WaitTimeSeconds parameter set to 20 to enable long polling.CORRECT: "Call the ReceiveMessage API with the WaitTimeSeconds parameter set to 20 " is the correct answer.INCORRECT: "Call the ReceiveMessage API with the VisibilityTimeout parameter set to 30" is incorrectINCORRECT: "Call the SetQueueAttributes API with the DelaySeconds parameter set to 900" is incorrectINCORRECT: "Call the SetQueueAttributes API with the maxReceiveCount set to 20" is incorrectReferences:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsshortandlongpolling.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html'},{question:"A company has several AWS accounts used by different departments. Developers use the same CloudFormation template to deploy an application across accounts. What can the developers use to deploy and manage the application with the LEAST operational effort?",answers:[{text:"Synchronize the applications in multiple accounts using AWS AppSync",isCorrect:!1},{text:"Create a CloudFormation Stack in an administrator account and use StackSets to update the stacks across multiple accounts",isCorrect:!0},{text:"Migrate the application into an Elastic Beanstalk environment that is shared between multiple accounts",isCorrect:!1},{text:"Create a CloudFormation Stack in an administrator account and use CloudFormation Change Sets to modify stacks across multiple accounts",isCorrect:!1}],explanation:'AWS CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation.Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified regions.Using StackSets for this scenario will work well and result in the least operational overhead in creating, updating and deleting CloudFormation stacks across multiple accounts.CORRECT: "Create a CloudFormation Stack in an administrator account and use StackSets to update the stacks across multiple accounts" is the correct answer.INCORRECT: "Create a CloudFormation Stack in an administrator account and use CloudFormation Change Sets to modify stacks across multiple accounts" is incorrect. Change sets allow you to preview how proposed changes to a stack might impact your running resources.INCORRECT: "Migrate the application into an Elastic Beanstalk environment that is shared between multiple accounts" is incorrect because we don’t even know if the application is compatible with Elastic Beanstalk and you cannot “share” environments between multiple accounts.INCORRECT: "Synchronize the applications in multiple accounts using AWS AppSync" is incorrect. AWS AppSync can perform synchronization and realtime updates between applications but it requires development and is not suitable for solving this challenge.References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/whatiscfnstacksets.html'},{question:"A Developer must run a shell script on Amazon EC2 Linux instances each time they are launched by an Amazon EC2 Auto Scaling group. What is the SIMPLEST way to run the script?",answers:[{text:"Run the script using the AWS Systems Manager Run Command",isCorrect:!1},{text:"Configure Amazon CloudWatch Events to trigger the AWS CLI when an instance is launched and run the script",isCorrect:!1},{text:"Package the script in a zip file with some AWS Lambda source code. Upload to Lambda and run the function when instances are launched",isCorrect:!1},{text:"Add the script to the user data when creating the launch configuration",isCorrect:!0}],explanation:'The simplest option is to add the script to the user data when creating the launch configuration. User data is information that is parsed when the EC2 instances are launched. When you add a script to the user data in a launch configuration all instances that are launched by that Auto Scaling group will run the script.CORRECT: "Add the script to the user data when creating the launch configuration" is the correct answer.INCORRECT: "Configure Amazon CloudWatch Events to trigger the AWS CLI when an instance is launched and run the script" is incorrect as you cannot trigger the AWS CLI using CloudWatch Events and the script may not involve AWS CLI commands.INCORRECT: "Package the script in a zip file with some AWS Lambda source code. Upload to Lambda and run the function when instances are launched" is incorrect as Lambda does not run shell scripts. You could program the requirements into the function code however you still need a trigger which is not mentioned in this option.INCORRECT: "Run the script using the AWS Systems Manager Run Command" is incorrect as this is not the simplest method. For most Linux AMIs (except Amazon Linux) the developer’s would need to install the agent on the operating system. They would also then need to create a mechanism of triggering the run command.References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/userdata.html'},{question:"An application uses multiple Lambda functions to write data to an Amazon RDS database. The Lambda functions must share the same connection string. What is the BEST solution to ensure security and operational efficiency?",answers:[{text:"Embed the connection string within the Lambda function code",isCorrect:!1},{text:"Use KMS encrypted environment variables within each Lambda function",isCorrect:!1},{text:"Use a CloudHSM encrypted environment variable that is shared between the functions",isCorrect:!1},{text:"Create a secure string parameter using AWS systems manager parameter store",isCorrect:!0}],explanation:'AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values.You can store values as plaintext (unencrypted data) or ciphertext (encrypted data). You can then reference values by using the unique name that you specified when you created the parameter.A secure string parameter is any sensitive data that needs to be stored and referenced in a secure manner. If you have data that you don\'t want users to alter or reference in plaintext, such as passwords or license keys, create those parameters using the SecureString datatype.If you choose the SecureString datatype when you create a parameter, then Parameter Store uses an AWS Key Management Service (KMS) customer master key (CMK) to encrypt the parameter value.This is the most secure and operationally efficient way to meet this requirement. The connection string will be encrypted and only needs to be managed in one place where it can be shared by the multiple Lambda functions.CORRECT: "Create a secure string parameter using AWS systems manage parameter store" is the correct answer.INCORRECT: "Use KMS encrypted environment variables within each Lambda function" is incorrect as this would require more operational overhead when managing any changes to the connection string.INCORRECT: "Use a CloudHSM encrypted environment variable that is shared between the functions" is incorrect as you cannot encrypt Lambda environment variables with CloudHSM (use KMS instead).INCORRECT: "Embed the connection string within the Lambda function code" is incorrect as this is not secure or operationally efficient.References:https://docs.aws.amazon.com/systemsmanager/latest/userguide/systemsmanagerparameterstore.html https://docs.aws.amazon.com/systemsmanager/latest/userguide/sysmanparamstoresecurestring.html'},{question:"An application is running on a fleet of EC2 instances running behind an Elastic Load Balancer (ELB). The EC2 instances session data in a shared Amazon S3 bucket. Security policy mandates that data must be encrypted in transit. How can the Developer ensure that all data that is sent to the S3 bucket is encrypted in transit?",answers:[{text:"Create an S3 bucket policy that denies any S3 Put request that does not include the xamzserversideencryption",isCorrect:!1},{text:"Create an S3 bucket policy that denies traffic where SecureTransport is true",isCorrect:!1},{text:"Create an S3 bucket policy that denies traffic where SecureTransport is false",isCorrect:!0},{text:"Configure HTTP to HTTPS redirection on the Elastic Load Balancer",isCorrect:!1}],explanation:'At the Amazon S3 bucket level, you can configure permissions through a bucket policy. For example, you can limit access to the objects in a bucket by IP address range or specific IP addresses. Alternatively, you can make the objects accessible only through HTTPS.The following bucket policy allows access to Amazon S3 objects only through HTTPS (the policy was generated with the AWS Policy Generator).Here the bucket policy explicitly denies ("Effect": "Deny") all read access ("Action": "s3:GetObject") from anybody who browses("Principal": "*") to Amazon S3 objects within an Amazon S3 bucket if they are not accessed through HTTPS("aws:SecureTransport": "false").CORRECT: "Create an S3 bucket policy that denies traffic where SecureTransport is false" is the correct answer.INCORRECT: "Create an S3 bucket policy that denies traffic where SecureTransport is true" is incorrect. This will not work as it is denying traffic that IS encrypted in transit.INCORRECT: "Create an S3 bucket policy that denies any S3 Put request that does not include the xamzserversideencryption" is incorrect. This will ensure that the data is encrypted at rest, but not intransit.INCORRECT: "Configure HTTP to HTTPS redirection on the Elastic Load Balancer" is incorrect. This will ensure the client traffic reaching the ELB is encrypted however we need to ensure the traffic from the EC2 instances to S3 is encrypted and the ELB is not involved in this communication.References: https://aws.amazon.com/blogs/security/howtousebucketpoliciesandapplydefenseindepthtohelpsecureyouramazons3data/'},{question:"An application is running on a cluster of Amazon EC2 instances. The application has received an error when trying to read objects stored within an Amazon S3 bucket. The bucket is encrypted with serverside encryption and AWS KMS managed keys (SSEKMS). The error is as follows: Service: AWSKMS; Status Code: 400, Error Code: ThrottlingException Which combination of steps should be taken to prevent this failure? (Select TWO.)",answers:[{text:"Use more than once customer master key (CMK) to encrypt S3 data",isCorrect:!1},{text:"Contact AWS support to request an S3 rate limit increase",isCorrect:!1},{text:"Contact AWS support to request an AWS KMS rate limit increase",isCorrect:!0},{text:"Import a customer master key (CMK) with a larger key size",isCorrect:!1},{text:"Perform error retries with exponential backoff in the application code",isCorrect:!0}],explanation:'AWS KMS establishes quotas for the number of API operations requested in each second. When you exceed an API request quota, AWS KMS throttles the request, that is, it rejects an otherwise valid request and returns a ThrottlingException error. As the error indicates, one of the recommendations is to reduce the frequency of calls which can be implemented by using exponential backoff logic in the application code. It is also possible to contact AWS and request an increase in the quota. CORRECT: "Contact AWS support to request an AWS KMS rate limit increase" is a correct answer. CORRECT: "Perform error retries with exponential backoff in the application code" is a correct answer. INCORRECT: "Contact AWS support to request an S3 rate limit increase" is incorrect as the error indicates throttling in AWS KMS. INCORRECT: "Import a customer master key (CMK) with a larger key size" is incorrect as the key size does not affect the quota for requests to AWS KMS. INCORRECT: "Use more than once customer master key (CMK) to encrypt S3 data" is incorrect as the issue is not the CMK it is the request quota on AWS KMS. References: https://docs.aws.amazon.com/kms/latest/developerguide/requestspersecond.html'},{question:"A Developer has created an AWS Lambda function in a new AWS account. The function is expected to be invoked 40 times per second and the execution duration will be around 100 seconds. What MUST the Developer do to ensure there are no errors?",answers:[{text:"Implement error handling within the function code",isCorrect:!1},{text:"Implement a Dead Letter Queue to capture invocation errors",isCorrect:!1},{text:"Implement tracing with XRay",isCorrect:!1},{text:"Contact AWS Support to increase the concurrent execution limits",isCorrect:!0}],explanation:'Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function\'s concurrency.In this scenario the Lambda function will be invoked 40 times per second and run for 100 seconds. Therefore, there can be up to 4,000 executions running concurrently which is above the default perregion limit of 1,000 concurrent executions.This can be easily rectified by contacting AWS support and requesting the concurrent execution limit to be increased.CORRECT: "Contact AWS Support to increase the concurrent execution limits" is the correct answer.INCORRECT: "Implement error handling within the function code" is incorrect. Though this could be useful it is not something that must be done based on what we know about this scenario.INCORRECT: "Implement a Dead Letter Queue to capture invocation errors" is incorrect as this would be implemented for message handling requirements.INCORRECT: "Implement tracing with XRay" is incorrect. XRay can be used to analyze and debug distributed applications. We don’t know of any specific issues with this function yet so this is not something that must be done.References: https://docs.aws.amazon.com/lambda/latest/dg/configurationconcurrency.html'},{question:"A Developer is deploying an Amazon ECS update using AWS CodeDeploy. In the appspec.yaml file, which of the following is a valid structure for the order of hooks that should be specified?",answers:[{text:"BeforeAllowTraffic > AfterAllowTraffic",isCorrect:!1},{text:"BeforeInstall > AfterInstall > AfterAllowTestTraffic > BeforeAllowTraffic > AfterAllowTraffic",isCorrect:!0},{text:"BeforeInstall > AfterInstall > ApplicationStart > ValidateService",isCorrect:!1},{text:"BeforeBlockTraffic > AfterBlockTraffic > BeforeAllowTraffic > AfterAllowTraffic",isCorrect:!1}],explanation:'The content in the \'hooks\' section of the AppSpec file varies, depending on the compute platform for your deployment.The \'hooks\' section for an EC2/OnPremises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts.The \'hooks\' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment.Therefore, in this scenario a valid structure for the order of hooks that should be specified in the appspec.yml file is:BeforeInstall > AfterInstall > AfterAllowTestTraffic > BeforeAllowTraffic > AfterAllowTrafficCORRECT: "BeforeInstall > AfterInstall > AfterAllowTestTraffic > BeforeAllowTraffic > AfterAllowTraffic" is the correct answer.INCORRECT: "BeforeInstall > AfterInstall > ApplicationStart > ValidateService" is incorrect as this would be valid for Amazon EC2.INCORRECT: "BeforeAllowTraffic > AfterAllowTraffic" is incorrect as this would be valid for AWS Lambda.INCORRECT: "BeforeBlockTraffic > AfterBlockTraffic > BeforeAllowTraffic > AfterAllowTraffic" is incorrect as this is a partial listing of hooks for Amazon EC2 but is incomplete.References: https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfilestructurehooks.html'},{question:"An application writes items to an Amazon DynamoDB table. As the application scales to thousands of instances, calls to the DynamoDB API generate occasional ThrottlingException errors. The application is coded in a language that is incompatible with the AWS SDK. What can be done to prevent the errors from occurring?",answers:[{text:"Add exponential backoff to the application logic",isCorrect:!0},{text:"Send the items to DynamoDB through Amazon Kinesis Data Firehose",isCorrect:!1},{text:"Use Amazon SQS as an API message bus",isCorrect:!1},{text:"Pass API calls through Amazon API Gateway",isCorrect:!1}],explanation:'Implementing error retries and exponential backoff is a good way to resolve this issue. Exponential backoff can improve an application\'s reliability by using progressively longer waits between retries. If you\'re using an AWS SDK, this logic is builtin. If you\'re not using an AWS SDK, consider manually implementing exponential backoff.Additional options for preventing throttling from occurring include:• Distribute read and write operations as evenly as possible across your table. A hot partition can degrade the overall performance of your table. For more information, see Designing Partition Keys to Distribute Your Workload Evenly.• Implement a caching solution. If your workload is mostly read access to static data, then query results can be delivered much faster if the data is in a welldesigned cache rather than in a database. DynamoDB Accelerator(DAX) is a caching service that offers fast inmemory performance for your application. You can also use Amazon ElastiCache.CORRECT: "Add exponential backoff to the application logic" is the correct answer.INCORRECT: "Use Amazon SQS as an API message bus" is incorrect. SQS is used for decoupling (messages, nut not APIs), however for this scenario it would add extra cost and complexity.INCORRECT: "Pass API calls through Amazon API Gateway" is incorrect. For this scenario we don’t want to add an additional layer in when we can simply configure the application to back off and retry.INCORRECT: "Send the items to DynamoDB through Amazon Kinesis Data Firehose" is incorrect as DynamoDB is not a supported destination for Kinesis Data Firehose.References:https://docs.aws.amazon.com/general/latest/gr/apiretries.html https://aws.amazon.com/premiumsupport/knowledgecenter/dynamodbtablethrottled/'},{question:"A Developer needs to return a list of items in a global secondary index from an Amazon DynamoDB table. Which DynamoDB API call can the Developer use in order to consume the LEAST number of read capacity units?",answers:[{text:"Query operation using eventuallyconsistent reads",isCorrect:!0},{text:"Scan operation using stronglyconsistent reads",isCorrect:!1},{text:"Scan operation using eventuallyconsistent reads",isCorrect:!1},{text:"Query operation using stronglyconsistent reads",isCorrect:!1}],explanation:'The Query operation finds items based on primary key values. You can query any table or secondary index that has a composite primary key (a partition key and a sort key).For items up to 4 KB in size, one RCU equals one strongly consistent read request per second or two eventually consistent read requests per second. Therefore, using eventually consistent reads uses fewer RCUs.CORRECT: "Query operation using eventuallyconsistent reads" is the correct answer.INCORRECT: "Query operation using stronglyconsistent reads" is incorrect as stronglyconsistent reads use more RCUs than eventually consistent reads.INCORRECT: "Scan operation using eventuallyconsistent reads" is incorrect. The Scan operation returns one or more items and item attributes by accessing every item in a table or a secondary index and therefore uses more RCUs than a query operation.INCORRECT: "Scan operation using stronglyconsistent reads" is incorrect. The Scan operation returns one or more items and item attributes by accessing every item in a table or a secondary index and therefore uses more RCUs than a query operation.References:https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Query.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html'},{question:"An application needs to read up to 100 items at a time from an Amazon DynamoDB. Each item is up to 100 KB in size and all attributes must be retrieved. What is the BEST way to minimize latency?",answers:[{text:"Use a Query operation with a FilterExpression",isCorrect:!1},{text:"Use BatchGetItem",isCorrect:!0},{text:"Use GetItem and use a projection expression",isCorrect:!1},{text:"Use a Scan operation with pagination",isCorrect:!1}],explanation:'The BatchGetItem operation returns the attributes of one or more items from one or more tables. You identify requested items by primary key.A single operation can retrieve up to 16 MB of data, which can contain as many as 100 items. In order to minimize response latency, BatchGetItem retrieves items in parallel.By default, BatchGetItem performs eventually consistent reads on every table in the request. If you want strongly consistent reads instead, you can set ConsistentRead to true for any or all tables.CORRECT: "Use BatchGetItem" is the correct answer.INCORRECT: "Use GetItem and use a projection expression" is incorrect as this will limit the attributes returned and will retrieve the items sequentially which results in more latency.INCORRECT: "Use a Scan operation with pagination" is incorrect as a Scan operation is the least efficient way to retrieve the data as all items in the table are returned and then filtered. Pagination just breaks the results into pages.INCORRECT: "Use a Query operation with a FilterExpression" is incorrect as this would limit the results that are returned.References: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html'},{question:"An application running on Amazon EC2 is experiencing intermittent technical difficulties. The developer needs to find a solution for tracking the errors that occur in the application logs and setting up a notification when the error rate exceeds a certain threshold. How can this be achieved with the LEAST complexity?",answers:[{text:"Configure Amazon CloudWatch Events to monitor the EC2 instances and configure an SNS topic as a target",isCorrect:!1},{text:"Use CloudWatch Logs to track the number of errors that occur in the application logs and send an SNS notification",isCorrect:!0},{text:"Use CloudTrail to monitor the application log files and send an SNS notification",isCorrect:!1},{text:"Configure the application to send logs to Amazon S3. Use Amazon Kinesis Analytics to analyze the log files and send an SES notification",isCorrect:!1}],explanation:'You can use CloudWatch Logs to monitor applications and systems using log data. For example, CloudWatch Logs can track the number of errors that occur in your application logs and send you a notification whenever the rate of errors exceeds a threshold you specify.CloudWatch Logs uses your log data for monitoring; so, no code changes are required. For example, you can monitor application logs for specific literal terms (such as "NullReferenceException") or count the number of occurrences of a literal term at a particular position in log data (such as "404" status codes in an Apache access log).When the term you are searching for is found, CloudWatch Logs reports the data to a CloudWatch metric that you specify. Log data is encrypted while in transit and while it is at rest.CORRECT: "Use CloudWatch Logs to track the number of errors that occur in the application logs and send an SNS notification" is the correct answer.INCORRECT: "Use CloudTrail to monitor the application log files and send an SNS notification" is incorrect as CloudTrail logs API activity in your account, it does not monitor application logs.INCORRECT: "Configure the application to send logs to Amazon S3. Use Amazon Kinesis Analytics to analyze the log files and send an SES notification" is incorrect. This is a much more complex solution and is not a full solution as it does not include a method of loading the data into Kinesis. Amazon SES is also not suitable for notifications, SNS should be used which can also send emails if required.INCORRECT: "Configure Amazon CloudWatch Events to monitor the EC2 instances and configure an SNS topic as a target" is incorrect as it monitors AWS services for changes in state. You can monitor EC2, but not the application within the EC2 instance.References:https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html'},{question:"A developer has a user account in the Development AWS account. He has been asked to modify resources in a Production AWS account. What is the MOST secure way to provide temporary access to the developer?",answers:[{text:"Use AWS KMS to generate crossaccount customer master keys and use those get shortlived credentials",isCorrect:!1},{text:"Generate an access key on the second account using the root account and share the access keys with the developer for API access",isCorrect:!1},{text:"Create a crossaccount access role, and use sts:AssumeRole API to get shortlived credentials",isCorrect:!0},{text:"Add the user to a group in the second account that has a role attached granting the necessary permissions",isCorrect:!1}],explanation:'This should be implemented using a role in the Production account and a group in the Development account. The developer in the Development account would then be added to the group. The role in the Production account would provide the necessary access and would allow the group in the Development account to assume the role.Therefore, the most secure way to achieve the required access is to use a role in the Production account that the user is able to assume and then the user can request shortlived credentials from the Security Token Service (STS).CORRECT: "Create a crossaccount access role, and use sts:AssumeRole API to get shortlived credentials" is the correct answer.INCORRECT: "Generate an access key on the second account using the root account and share the access keys with the developer for API access" is incorrect as this is highly insecure. You should never share access keys across user accounts, and you should especially not use access keys associated with the root account.INCORRECT: "Add the user to a group in the second account that has a role attached granting the necessary permissions" is incorrect as you cannot add a user to a group in a different AWS account.INCORRECT: "Use AWS KMS to generate crossaccount customer master keys and use those get shortlived credentials" is incorrect as you do not use AWS KMS CMKs for obtaining shortlived credentials from the STS service. CMKs are used for encrypting data.References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_commonscenarios_awsaccounts.html'},{question:'Based on the following AWS CLI command the resulting output, what has happened here?$ aws lambda invoke functionname MyFunction payloadewogICJrZXkxIjogInZhbHVlMSIsCiAgImtleTIiOiAidmFsdWUyIiwKICAia2V5MyI6ICJ2YWx1ZTMiCn0= response.json{"StatusCode": 200}',answers:[{text:"An AWS Lambda function has been invoked synchronously and has not completed successfully",isCorrect:!1},{text:"An AWS Lambda function has been invoked asynchronously and has completed successfully",isCorrect:!1},{text:"An AWS Lambda function has been invoked asynchronously and has not completed successfully",isCorrect:!1},{text:"An AWS Lambda function has been invoked synchronously and has completed successfully",isCorrect:!0}],explanation:'When you invoke a function synchronously, Lambda runs the function and waits for a response. When the function execution ends, Lambda returns the response from the function\'s code with additional data, such as the version of the function that was executed. To invoke a function synchronously with the AWS CLI, use the invoke command.The following diagram shows clients invoking a Lambda function synchronously. Lambda sends the events directly to the function and sends the function\'s response back to the invoker.We know the function has been run synchronously as the invocationtype Event parameter has not been included. Also, the status code 200 indicates a successful execution of a synchronous execution.CORRECT: "An AWS Lambda function has been invoked synchronously and has completed successfully" is the correct answer.INCORRECT: "An AWS Lambda function has been invoked synchronously and has not completed successfully" is incorrect as the status code 200 indicates a successful execution.INCORRECT: "An AWS Lambda function has been invoked asynchronously and has completed successfully" is incorrect as the invocationtype Event has parameter is not included so this is not an asynchronous invocation.INCORRECT: "An AWS Lambda function has been invoked asynchronously and has not completed successfully" is incorrect as the invocationtype Event has parameter is not included so this is not an asynchronous invocation.References: https://docs.aws.amazon.com/lambda/latest/dg/invocationsync.html'},{question:"A Developer needs to run some code using Lambda in response to an event and forward the execution result to another application using a pub/sub notification. How can the Developer accomplish this?",answers:[{text:"Configure a Lambda “on success” destination and route the execution results to Amazon SQS",isCorrect:!1},{text:"Configure a Lambda “on success” destination and route the execution results to Amazon SNS",isCorrect:!0},{text:"Configure a CloudWatch Events alarm the triggers based on Lambda execution success and route the execution results to Amazon SQS",isCorrect:!1},{text:"Configure a CloudWatch Events alarm the triggers based on Lambda execution success and route the execution results to Amazon SNS",isCorrect:!1}],explanation:'With Destinations, you can send asynchronous function execution results to a destination resource without writing code. A function execution result includes version, timestamp, request context, request payload, response context, and response payload. For each execution status (i.e. Success and Failure), you can choose one destination from four options: another Lambda function, an SNS topic, an SQS standard queue, or EventBridge.For this scenario, the code will be run by Lambda and the execution result will then be sent to the SNS topic. The application that is subscribed to the SNS topics will then receive the notification.CORRECT: "Configure a Lambda “on success” destination and route the execution results to Amazon SNS" is the correct answer.INCORRECT: "Configure a CloudWatch Events alarm the triggers based on Lambda execution success and route the execution results to Amazon SNS" is incorrect as CloudWatch Events is used to track changes in the state of AWS resources. To forward execution results from Lambda a destination should be used.INCORRECT: "Configure a Lambda “on success” destination and route the execution results to Amazon SQS" is incorrect as SQS is a message queue not a pub/sub notification service.INCORRECT: "Configure a CloudWatch Events alarm the triggers based on Lambda execution success and route the execution results to Amazon SQS" is incorrect as CloudWatch Events is used to track changes in the state of AWS resources. To forward execution results from Lambda a destination should be used (with an SNS topic).References: https://aws.amazon.com/aboutaws/whatsnew/2019/11/awslambdasupportsdestinationsforasynchronousinvocations/'},{question:"A Developer created an AWS Lambda function for a serverless application. The Lambda function has been executing for several minutes and the Developer cannot find any log data in CloudWatch Logs. What is the MOST likely explanation for this issue?",answers:[{text:"The execution role for the Lambda function is missing permissions to write log data to the CloudWatch Logs",isCorrect:!0},{text:"The Lambda function is missing CloudWatch Logs as a source trigger to send log data",isCorrect:!1},{text:"The Lambda function does not have any explicit log statements for the log data to send it to CloudWatch Logs",isCorrect:!1},{text:"The Lambda function is missing a target CloudWatch Logs group",isCorrect:!1}],explanation:'AWS Lambda automatically monitors Lambda functions on your behalf, reporting metrics through Amazon CloudWatch. To help you troubleshoot failures in a function, Lambda logs all requests handled by your function and also automatically stores logs generated by your code through Amazon CloudWatch Logs.Lambda automatically integrates with CloudWatch Logs and pushes all logs from your code to a CloudWatch Logs group associated with a Lambda function, which is named /aws/lambda/.An AWS Lambda function\'s execution role grants it permission to access AWS services and resources. You provide this role when you create a function, and Lambda assumes the role when your function is invoked. You can create an execution role for development that has permission to send logs to Amazon CloudWatch and upload trace data to AWS XRay.The most likely cause of this issue is that the execution role assigned to the Lambda function does not have the permissions (shown above) to write to CloudWatch Logs.CORRECT: "The execution role for the Lambda function is missing permissions to write log data to the CloudWatch Logs" is the correct answer.INCORRECT: "The Lambda function does not have any explicit log statements for the log data to send it to CloudWatch Logs" is incorrect as this is not required, Lambda automatically logs data to CloudWatch logs and just needs the permissions to do so.INCORRECT: "The Lambda function is missing a target CloudWatch Logs group" is incorrect as the CloudWatch Logs group will be created automatically if the function has sufficient permissions.INCORRECT: "The Lambda function is missing CloudWatch Logs as a source trigger to send log data" is incorrect as CloudWatch Logs is a destination, not a source in this case. However, you do not need to configure CloudWatch Logs as a destination, it is automatic.References: https://docs.aws.amazon.com/lambda/latest/dg/lambdamonitoring.html'},{question:"A company is running a Docker application on Amazon ECS. The application must scale based on user load in the last 15 seconds. How should the Developer instrument the code so that the requirement can be met?",answers:[{text:"Create a standardresolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds",isCorrect:!1},{text:"Create a standardresolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds",isCorrect:!1},{text:"Create a highresolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds",isCorrect:!1},{text:"Create a highresolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds",isCorrect:!0}],explanation:'Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a highresolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.User activity is not a standard CloudWatch metric and as stated above for the resolution we need in this scenario a custom CloudWatch metric is required anyway. Therefore, for this scenario the Developer should create a highresolution custom Amazon CloudWatch metric for user activity data and publish the data every 5 seconds.CORRECT: "Create a highresolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds" is the correct answer.INCORRECT: "Create a highresolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds" is incorrect as the resolution is lower than required which will not provide the granularity required.INCORRECT: "Create a standardresolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds" is incorrect as standard resolution metrics have a granularity of one minute.INCORRECT: "Create a standardresolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds" is incorrect as standard resolution metrics have a granularity of one minute.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html'},{question:"An application runs on Amazon EC2 and generates log files. A Developer needs to centralize the log files so they can be queried and retained. What is the EASIEST way for the Developer to centralize the log files?",answers:[{text:"Create a script that copies the log files to Amazon S3 and use a cron job to run the script on a recurring schedule",isCorrect:!1},{text:"Create a script that uses the AWS SDK to collect and send the log files to Amazon CloudWatch Logs",isCorrect:!1},{text:"Install the Amazon CloudWatch Logs agent and collect the logs from the instances",isCorrect:!0},{text:"Setup a CloudWatch Events rule to trigger an SNS topic when an application log file is generated",isCorrect:!1}],explanation:'You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources.CloudWatch Logs enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service. You can then easily view them, search them for specific error codes or patterns, filter them based on specific fields, or archive them securely for future analysis.To collect logs from Amazon EC2 and onpremises instances it is necessary to install an agent. There are two options: the unified CloudWatch Agent which collects logs and advanced metrics (such as memory usage), or the older CloudWatch Logs agent which only collects logs from Linux servers.CORRECT: "Install the Amazon CloudWatch Logs agent and collect the logs from the instances" is the correct answer.INCORRECT: "Create a script that copies the log files to Amazon S3 and use a cron job to run the script on a recurring schedule" is incorrect as the best place to move the log files to for querying and long term retention would be CloudWatch Logs. It is also easier to use the agent than to create and maintain a script.INCORRECT: "Create a script that uses the AWS SDK to collect and send the log files to Amazon CloudWatch Logs" is incorrect as this is not the easiest way to achieve this outcome. It will be easier to use the CloudWatch Logs agent.INCORRECT: "Setup a CloudWatch Events rule to trigger an SNS topic when an application log file is generated" is incorrect as CloudWatch Events does not collect log files, it monitors state changes in resources.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html'},{question:"You run an adsupported photo sharing website using Amazon S3 to serve photos to visitors of your site. At some point you find out that other sites have been linking to the photos on your site, causing loss to your business. What is an effective method to mitigate this?",answers:[{text:"Block the IPs of the offending websites in Security Groups",isCorrect:!1},{text:"Store photos on an EBS volume of the web server",isCorrect:!1},{text:"Remove public read access and use signed URLs with expiry dates",isCorrect:!0},{text:"Use CloudFront distributions for static content",isCorrect:!1}],explanation:'When Amazon S3 objects are private, only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a presigned URL, using their own security credentials, to grant timelimited permission to download the objects.When you create a presigned URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The presigned URLs are valid only for the specified duration.Anyone who receives the presigned URL can then access the object. In this scenario, the photos can be shared with the owner’s website but not with any other 3rd parties. This will stop other sites from linking to the photos as they will not display anywhere else.CORRECT: "Remove public read access and use signed URLs with expiry dates" is the correct answer.INCORRECT: "Store photos on an EBS volume of the web server" is incorrect as this does not add any more control over content visibility in the website.INCORRECT: "Use CloudFront distributions for static content" is incorrect as this alone will not protect the content. You can also use presigned URLs with CloudFront, but this isn’t mentioned.INCORRECT: "Block the IPs of the offending websites in Security Groups" is incorrect as you can only configure allow rules in security groups so this would be hard to manage.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html'},{question:"A development team require a fully-managed source control service that is compatible with Git. Which service should they use?",answers:[{text:"AWS CodePipeline",isCorrect:!1},{text:"AWS Cloud9",isCorrect:!1},{text:"AWS CodeDeploy",isCorrect:!1},{text:"AWS CodeCommit",isCorrect:!0}],explanation:'AWS CodeCommit is a version control service hosted by Amazon Web Services that you can use to privately store and manage assets (such as documents, source code, and binary files) in the cloud. CodeCommit is a fullymanaged service that hosts secure Gitbased repositories.CORRECT: "AWS CodeCommit" is the correct answer.INCORRECT: "AWS CodeDeploy" is incorrect. CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, onpremises instances, serverless Lambda functions, or Amazon ECS services.INCORRECT: "AWS CodePipeline" is incorrect. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.INCORRECT: "AWS Cloud9" is incorrect. AWS Cloud9 is a cloudbased integrated development environment (IDE) that lets you write, run, and debug your code with just a browser.References: https://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html'},{question:"A Developer has setup an Amazon Kinesis Data Stream with 6 shards to ingest a maximum of 2000 records per second. An AWS Lambda function has been configured to process these records. In which order will these records be processed?",answers:[{text:"Lambda will receive each record in the reverse order it was placed into the stream",isCorrect:!1},{text:"The Developer can select exact order or reverse order using the GetRecords API",isCorrect:!1},{text:"Lambda will receive each record in the exact order it was placed into the shard. There is no guarantee of order across shards",isCorrect:!0},{text:"Lambda will receive each record in the exact order it was placed into the stream",isCorrect:!1}],explanation:'Amazon Kinesis Data Streams (KDS) is a massively scalable and durable realtime data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and locationtracking events. KDS receives data from producers, and the data is stored in shards. Consumers then take the data and process it. In this case the AWS Lambda function is consuming the records from the shards.In this scenario an application will be producing records and placing them in the stream as in step 1 of the image below. The AWS Lambda function will then consume the records (step 2) and will then execute the function by assuming the execution role specified (step 3).A shard is an appendonly log and a unit of streaming capability. A shard contains an ordered sequence of records ordered by arrival time. The order is guaranteed within a shard but not across shards.Therefore, the best answer to this question is that AWS Lambda will receive each record in the exact order it was placed into the shard but there is no guarantee of order across shardsCORRECT: "Lambda will receive each record in the exact order it was placed into the shard. There is no guarantee of order across shards" is the correct answer.INCORRECT: "Lambda will receive each record in the exact order it was placed into the stream " is incorrect as there are multiple shards in the stream and the order of records is not guaranteed across shards.INCORRECT: "Lambda will receive each record in the reverse order it was placed into the stream" is incorrect as the order is guaranteed within a shard.INCORRECT: "The Developer can select exact order or reverse order using the GetRecords API" is incorrect as you cannot choose the order you receive records with the GetRecords API.References:https://aws.amazon.com/kinesis/datastreams/gettingstarted/ https://aws.amazon.com/kinesis/datastreams/faqs/'},{question:"How can a Developer view a summary of proposed changes to an AWS CloudFormation stack without implementing the changes in production?",answers:[{text:"Use a direct update",isCorrect:!1},{text:"Create a StackSet",isCorrect:!1},{text:"Create a Change Set",isCorrect:!0},{text:"Use drift detection",isCorrect:!1}],explanation:'When you need to update a stack, understanding how your changes will affect running resources before you implement them can help you update stacks with confidence. Change sets allow you to preview how proposed changes to a stack might impact your running resources.AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set. You can create and manage change sets using the AWS CloudFormation console, AWS CLI, or AWS CloudFormation API.CORRECT: "Create a Change Set" is the correct answer.INCORRECT: "Create a StackSet" is incorrect as StackSets are used to create, update, or delete stacks across multiple accounts and regions with a single operation.INCORRECT: "Use drift detection" is incorrect as this is used to detect when a configuration deviates from the template configuration.INCORRECT: "Use a direct update" is incorrect as this will directly update the production resources.References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/usingcfnupdatingstackschangesets.html'},{question:"A Developer needs to restrict all users and roles from using a list of API actions within a member account in AWS Organizations. The Developer needs to deny access to a few specific API actions. What is the MOST efficient way to do this?",answers:[{text:"Create a deny list and specify the API actions to deny",isCorrect:!0},{text:"Create an IAM policy that allows only the unrestricted API actions",isCorrect:!1},{text:"Create an allow list and specify the API actions to deny",isCorrect:!1},{text:"Create an IAM policy that denies the API actions for all users and roles",isCorrect:!1}],explanation:'Service control policies (SCPs) are one type of policy that you can use to manage your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines.You can configure the SCPs in your organization to work as either of the following:• A deny list – actions are allowed by default, and you specify what services and actions are prohibited• An allow list – actions are prohibited by default, and you specify what services and actions are allowedAs there are only a few API actions to restrict the most efficient strategy for this scenario is to create a deny list and specify the specific actions that are prohibited.CORRECT: "Create a deny list and specify the API actions to deny" is the correct answer.INCORRECT: "Create an allow list and specify the API actions to deny" is incorrect as with an allow list you specify the API actions to allow.INCORRECT: "Create an IAM policy that denies the API actions for all users and roles" is incorrect as you cannot create deny policies in IAM. IAM policies implicitly deny access unless you explicitly allow permissions.INCORRECT: "Create an IAM policy that allows only the unrestricted API actions" is incorrect. This will not work for administrative users such as the root account (as they have extra permissions) so an SCP must be used.References:https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html https://docs.aws.amazon.com/organizations/latest/userguide/SCP_strategies.html'},{question:"A Developer is creating a banking application that will be used to view financial transactions and statistics. The application requires multifactor authentication to be added to the login protocol. Which service should be used to meet this requirement?",answers:[{text:"Amazon Cognito Identity Pool with MFA",isCorrect:!1},{text:"Amazon Cognito User Pool with MFA",isCorrect:!0},{text:"AWS IAM with MFA",isCorrect:!1},{text:"AWS Directory Service",isCorrect:!1}],explanation:'A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Google, Facebook, Amazon, or Apple, and through SAML identity providers.User pools provide:• Signup and signin services.• A builtin, customizable web UI to sign in users.• Social signin with Facebook, Google, Login with Amazon, and Sign in with Apple, as well as signin with SAML identity providers from your user pool.• User directory management and user profiles.• Security features such as multifactor authentication (MFA), checks for compromised credentials, account takeover protection, and phone and email verification.• Customized workflows and user migration through AWS Lambda triggers.Multifactor authentication (MFA) increases security for your app by adding another authentication method, and not relying solely on username and password. You can choose to use SMS text messages, or timebased onetime (TOTP) passwords as second factors in signing in your users.For this scenario you would want to set the MFA setting to “Required” as the data is highly secure.CORRECT: "Amazon Cognito User Pool with MFA" is the correct answer.INCORRECT: "Amazon Cognito Identity Pool with MFA" is incorrectINCORRECT: "AWS IAM with MFA" is incorrect. With IAM your user accounts are maintained in your AWS account rather than in a Cognito User Pool. For logging into a web or mobile app it is better to create and manage your users in a Cognito User Pool and add MFA to the User Pool for extra security.INCORRECT: "AWS Directory Service" is incorrect as this is a managed Active Directory service. For a web or mobile application using AWS Cognito User Pools is a better solution for storing your user accounts and authenticating to the application.References:https://docs.aws.amazon.com/cognito/latest/developerguide/cognitouseridentitypools.html https://docs.aws.amazon.com/cognito/latest/developerguide/userpoolsettingsmfa.html'},{question:"A Developer is writing an AWS Lambda function that processes records from an Amazon Kinesis Data Stream. The Developer must write the function so that it sends a notice to Administrators if it fails to process a batch of records. How should the Developer write the function?",answers:[{text:"Configure an Amazon SNS topic as an onfailure destination",isCorrect:!0},{text:"Use Amazon CloudWatch Events to send the processed data",isCorrect:!1},{text:"Separate the Lambda handler from the core logic",isCorrect:!1},{text:"Push the failed records to an Amazon SQS queue",isCorrect:!1}],explanation:'With Destinations, you can route asynchronous function results as an execution record to a destination resource without writing additional code. An execution record contains details about the request and response in JSON format including version, timestamp, request context, request payload, response context, and response payload.For each execution status such as Success or Failure you can choose one of four destinations: another Lambda function, SNS, SQS, or EventBridge. Lambda can also be configured to route different execution results to different destinations.In this scenario the Developer can publish the processed data to an Amazon SNS topic by configuring an Amazon SNS topic as an onfailure destination.CORRECT: "Configure an Amazon SNS topic as an onfailure destination" is the correct answer.INCORRECT: "Separate the Lambda handler from the core logic" is incorrect as this will not assist with sending a notification to administrators.INCORRECT: "Use Amazon CloudWatch Events to send the processed data" is incorrect as CloudWatch Events is used for tracking state changes, not forwarding execution resultsINCORRECT: "Push the failed records to an Amazon SQS queue" is incorrect as SQS will not notify the administrators, SNS should be used.References: https://aws.amazon.com/blogs/compute/introducingawslambdadestinations/'},{question:"A Developer wants to find a list of items in a global secondary index from an Amazon DynamoDB table. Which DynamoDB API call can the Developer use in order to consume the LEAST number of read capacity units?",answers:[{text:"Scan operation using eventuallyconsistent reads",isCorrect:!1},{text:"Scan operation using stronglyconsistent reads",isCorrect:!1},{text:"Query operation using eventuallyconsistent reads",isCorrect:!0},{text:"Query operation using stronglyconsistent reads",isCorrect:!1}],explanation:'Amazon DynamoDB provides fast access to items in a table by specifying primary key values. However, many applications might benefit from having one or more secondary (or alternate) keys available, to allow efficient access to data with attributes other than the primary key. To address this, you can create one or more secondary indexes on a table and issue Query or Scan requests against these indexes.A secondary index is a data structure that contains a subset of attributes from a table, along with an alternate key to support Query operations. You can retrieve data from the index using a Query, in much the same way as you use Query with a table. A table can have multiple secondary indexes, which give your applications access to many different query patterns.You can also issue scan operations on a global secondary index however it is less efficient as it will return all items in the index.CORRECT: "Query operation using eventuallyconsistent reads" is the correct answer.INCORRECT: "Query operation using stronglyconsistent reads" is incorrect. Strongly consistent reads require more RCUs and also are not supported on a global secondary index (they are supported on local secondary indexes).INCORRECT: "Scan operation using eventuallyconsistent reads" is incorrect as a scan is less efficient than a query and will therefore use more RCUs.INCORRECT: "Scan operation using stronglyconsistent reads" is incorrect as a scan is less efficient than a query and will therefore use more RCUs.References:https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/SecondaryIndexes.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html'},{question:"An application uses Amazon EC2, and Application Load Balancer and Amazon CloudFront to serve content. The security team have reported malicious activity from a specific range of IP addresses. How can a Developer prevent the application from being targeted by these addresses again?",answers:[{text:"Add a certificate using AWS Certificate Manager (ACM) and encrypt all communications",isCorrect:!1},{text:"Create a security group rule denying the address range and apply it to the EC2 instances",isCorrect:!1},{text:"Disable the Amazon CloudFront distribution and then reenable it",isCorrect:!1},{text:"Add a rule to a Web ACL using AWS WAF that denies the IP address ranges",isCorrect:!0}],explanation:'You use AWS WAF to control how an Amazon CloudFront distribution, an Amazon API Gateway API, or an Application Load Balancer responds to web requests.• Web ACLs – You use a web access control list (ACL) to protect a set of AWS resources. You create a web ACL and define its protection strategy by adding rules. Rules define criteria for inspecting web requests and specify how tohandle requests that match the criteria. You set a default action for the web ACL that indicates whether to block or allow through those requests that pass the rules inspections.• Rules – Each rule contains a statement that defines the inspection criteria, and an action to take if a web request meets the criteria. When a web request meets the criteria, that\'s a match. You can use rules to block matchingrequests or to allow matching requests through. You can also use rules just to count matching requests.• Rules groups – You can use rules individually or in reusable rule groups. AWS Managed Rules and AWS Marketplace sellers provide managed rule groups for your use. You can also define your own rule groups.After you create your web ACL, you can associate it with one or more AWS resources. The resource types that you can protect using AWS WAF web ACLs are Amazon CloudFront distributions, Amazon API Gateway APIs, and Application Load Balancers.CORRECT: "Add a rule to a Web ACL using AWS WAF that denies the IP address ranges" is the correct answer.INCORRECT: "Create a security group rule denying the address range and apply it to the EC2 instances" is incorrect as you cannot add deny rules to security groups.INCORRECT: "Add a certificate using AWS Certificate Manager (ACM) and encrypt all communications" is incorrect as this will not prevent attacks from coming in from the specific IP ranges. This will simply enabled SSL/TLS for communications from clients.INCORRECT: "Disable the Amazon CloudFront distribution and then reenable it" is incorrect as this will do nothing to stop future attacks from occurring.References: https://docs.aws.amazon.com/waf/latest/developerguide/howawswafworks.html'},{question:"A team of developers need to be able to collaborate and synchronize multiple distributed code repositories and leverage a preconfigured continuous delivery toolchain for deploying their projects on AWS. The team also require a centralized project dashboard to monitor application activity. Which AWS service should they use?",answers:[{text:"AWS CodePipeline",isCorrect:!1},{text:"AWS CodeCommit",isCorrect:!1},{text:"AWS Cloud9",isCorrect:!1},{text:"AWS CodeStar",isCorrect:!0}],explanation:'AWS CodeStar enables you to quickly develop, build, and deploy applications on AWS. AWS CodeStar provides a unified user interface, enabling you to easily manage your software development activities in one place. With AWS CodeStar, you can set up your entire continuous delivery toolchain in minutes, allowing you to start releasing code faster. AWS CodeStar makes it easy for your whole team to work together securely, allowing you to easily manage access and add owners, contributors, and viewers to your projects.Each AWS CodeStar project comes with a project management dashboard, including an integrated issue tracking capability powered by Atlassian JIRA Software. With the AWS CodeStar project dashboard, you can easily track progress across your entire software development process, from your backlog of work items to teams’ recent code deployments.CORRECT: "AWS CodeStar" is the correct answer.INCORRECT: "AWS CodePipeline" is incorrect as it does not offer the collaboration and project management dashboard features of CodeStar.INCORRECT: "AWS Cloud9" is incorrect as it is a cloudbased integrated development environment (IDE) that lets you write, run, and debug your code with just a browser.INCORRECT: "AWS CodeCommit" is incorrect. CodeCommit is a fully managed source control service that hosts Gitbased repositories. However, it does not offer the collaboration and project management dashboard features of CodeStar or the preconfigured continuous delivery toolchain.References: https://aws.amazon.com/codestar/features/'},{question:"A Development team are deploying an AWS Lambda function that will be used by a production application. The function code will be updated regularly, and new versions will be published. The development team do not want to modify application code to point to each new version. How can the Development team setup a static ARN that will point to the latest published version?",answers:[{text:"Setup a Route 53 Alias record that points to the published version",isCorrect:!1},{text:"Use an unqualified ARN",isCorrect:!1},{text:"Publish a mutable version and point it to the $LATEST version",isCorrect:!1},{text:"Setup an Alias that will point to the latest version",isCorrect:!0}],explanation:'You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN.This is the best way to setup the Lambda function so you don’t need to modify the application code when a new version is published. Instead, the developer will simply need to update the Alias to point to the new version:As you can see above you can also point to multiple versions and send a percentage of traffic to each. This is great for testing new code.CORRECT: "Setup an Alias that will point to the latest version" is the correct answer.INCORRECT: "Publish a mutable version and point it to the $LATEST version" is incorrect as all published versions are immutable (cannot be modified) and you cannot modify a published version to point to the $LATEST version.INCORRECT: "Use an unqualified ARN" is incorrect as this is an ARN that does not have a version number which means it points to the $LATEST version, not to a published version (as published versions always have version numbers).INCORRECT: "Setup a Route 53 Alias record that points to the published version" is incorrect as you cannot point a Route 53 Alias record to an AWS Lambda function.References: https://docs.aws.amazon.com/lambda/latest/dg/configurationaliases.html'},{question:"A three-tier application is being migrated from an on-premises data center. The application includes an Apache Tomcat web tier, an application tier running on Linux, and a MySQL back end. A Developer must refactor the application to run on the AWS cloud. The cloud-based application must be fault tolerant and elastic. How can the Developer refactor the web tier and application tier? (Select TWO.)",answers:[{text:"Create an Auto Scaling group of EC2 instances for both the web tier and application tier",isCorrect:!0},{text:"Implement an Elastic Load Balancer for the application tier",isCorrect:!1},{text:"Use a multiAZ Amazon RDS database for the back end using the MySQL engine",isCorrect:!1},{text:"Implement an Elastic Load Balancer for both the web tier and the application tier",isCorrect:!0},{text:"Create an Amazon CloudFront distribution for the web tier",isCorrect:!1}],explanation:'The key requirements in this scenario are to add fault tolerances and elasticity to the web tier and application tier. Note that no specific requirements for the back end have been included.To add elasticity to the web and application tiers the Developer should create Auto Scaling groups of EC2 instances. We know that the application tier runs on Linux and the web tier runs on Apache Tomcat (which could be on Linux or Windows).Therefore, these workloads are suitable for an ASG and this will ensure the number of instances dynamically scales out and in based on actual usage.To add fault tolerance to the web and application tiers the Developer should add an Elastic Load Balancer. This will ensure that if the number of EC2 instances are changed by the ASG, the load balancer is able to distribute traffic to them. This also assists with elasticity.CORRECT: "Create an Auto Scaling group of EC2 instances for both the web tier and application tier" is a correct answer.CORRECT: "Implement an Elastic Load Balancer for both the web tier and the application tier" is also a correct answer.INCORRECT: "Create an Amazon CloudFront distribution for the web tier" is incorrect as CloudFront is used for performance reasons, not elasticity or fault tolerance. You would use CloudFront to get content closer to end users around the world.INCORRECT: "Use a multiAZ Amazon RDS database for the back end using the MySQL engine" is incorrect as the question does not ask for fault tolerance of the back end, only the web tier and the application tier.INCORRECT: "Implement an Elastic Load Balancer for the application tier" is incorrect. An Elastic Load Balancer should be implemented for both the web tier and the application tier as that is how we ensure fault tolerance and elasticity for both of those tiers.References:https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/whatisloadbalancing.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/serviceautoscaling.html'},{question:"A developer is creating a new application that will store data in a DynamoDB table. Which APIs can be used to read, write and modify individual items in the table?",answers:[{text:"GetItem, PutItem, DeleteItem",isCorrect:!1},{text:"GetItem, PutItem, UpdateItem",isCorrect:!0},{text:"GetItem, TransactWriteItems, UpdateTable",isCorrect:!1},{text:"BatchGetItem, BatchWriteItem, UpdateItem",isCorrect:!1}],explanation:'The GetItem operation returns a set of attributes for the item with the given primary key. If there is no matching item, GetItem does not return any data and there will be no Item element in the response.PutItem creates a new item or replaces an old item with a new item. If an item that has the same primary key as the new item already exists in the specified table, the new item completely replaces the existing item.UpdateItem edits an existing item\'s attributes or adds a new item to the table if it does not already exist. You can put, delete, or add attribute values. You can also perform a conditional update on an existing item (insert a new attribute namevalue pair if it doesn\'t exist or replace an existing namevalue pair if it has certain expected attribute values).CORRECT: "GetItem, PutItem, UpdateItem" is the correct answer.INCORRECT: "GetItem, TransactWriteItems, UpdateTable" is incorrect as TransactWriteItems is a synchronous write operation that groups up to 25 action requests. In this scenario we are updating individual items.INCORRECT: "GetItem, PutItem, DeleteItem" is incorrect as DeleteItem will delete single items in a table by primary key. We do not want to delete, we want to modify so UpdateItem should be used instead.INCORRECT: "BatchGetItem, BatchWriteItem, UpdateItem" is incorrect as BatchGetItem and BatchGetItem are used when you have multiple items to read/write. In this scenario we are updating individual items.References: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Operations_Amazon_DynamoDB.html'},{question:"A company needs a fully-managed source control service that will work in AWS. The service must ensure that revision control synchronizes multiple distributed repositories by exchanging sets of changes peer-to-peer. All users need to work productively even when not connected to a network. Which source control service should be used?",answers:[{text:"AWS CodeStar",isCorrect:!1},{text:"AWS CodeCommit",isCorrect:!0},{text:"Subversion",isCorrect:!1},{text:"AWS CodeBuild",isCorrect:!1}],explanation:'AWS CodeCommit is a version control service hosted by Amazon Web Services that you can use to privately store and manage assets (such as documents, source code, and binary files) in the cloud.A repository is the fundamental version control object in CodeCommit. It\'s where you securely store code and files for your project. It also stores your project history, from the first commit through the latest changes. You can share your repository with other users so you can work together on a project. If you add AWS tags to repositories, you can set up notifications so that repository users receive email about events (for example, another user commenting on code).You can also change the default settings for your repository, browse its contents, and more. You can create triggers for your repository so that code pushes or other events trigger actions, such as emails or code functions. You can even configure a repository on your local computer (a local repo) to push your changes to more than one repository.CORRECT: "AWS CodeCommit" is the correct answer.INCORRECT: "Subversion" is incorrect as this is not a fully managed source control systemINCORRECT: "AWS CodeBuild" is incorrect as this is a service used for building and testing code.INCORRECT: "AWS CodeStar" is incorrect as this is not a source control system; it integrates with source control systems such as CodeCommit.References: https://aws.amazon.com/codecommit/'},{question:"A monitoring application that keeps track of a large eCommerce website uses Amazon Kinesis for data ingestion. During periods of peak data rates, the Kinesis stream cannot keep up with the incoming data. What step will allow Kinesis data streams to accommodate the traffic during peak hours?",answers:[{text:"Install the Kinesis Producer Library (KPL) for ingesting data into the stream",isCorrect:!1},{text:"Ingest multiple records into the stream in a single call using PutRecords",isCorrect:!1},{text:"Increase the shard count of the stream using UpdateShardCount",isCorrect:!0},{text:"Create an SQS queue and decouple the producers from the Kinesis data stream",isCorrect:!1}],explanation:'The UpdateShardCount API action updates the shard count of the specified stream to the specified number of shards. Updating the shard count is an asynchronous operation. Upon receiving the request, Kinesis Data Streams returns immediately and sets the status of the stream to UPDATING. After the update is complete, Kinesis Data Streams sets the status of the stream back to ACTIVE.Depending on the size of the stream, the scaling action could take a few minutes to complete. You can continue to read and write data to your stream while its status is UPDATING.To update the shard count, Kinesis Data Streams performs splits or merges on individual shards. This can cause shortlived shards to be created, in addition to the final shards. These shortlived shards count towards your total shard limit for your account in the Region.When using this operation, we recommend that you specify a target shard count that is a multiple of 25% (25%, 50%, 75%, 100%). You can specify any target value within your shard limit. However, if you specify a target that isn\'t a multiple of 25%, the scaling action might take longer to complete.This operation has the following default limits. By default, you cannot do the following:• Scale more than ten times per rolling 24hour period per stream• Scale up to more than double your current shard count for a stream• Scale down below half your current shard count for a stream• Scale up to more than 500 shards in a stream• Scale a stream with more than 500 shards down unless the result is less than 500 shards• Scale up to more than the shard limit for your accountNote that the question specifically states that the Kinesis data stream cannot keep up with incoming data. This indicates that the producers are attempting to add records to the stream but there are not enough shards to keep up with demand.Therefore, we need to add additional shards and can do this using the UpdateShardCount API action.CORRECT: "Increase the shard count of the stream using UpdateShardCount" is the correct answer.INCORRECT: "Install the Kinesis Producer Library (KPL) for ingesting data into the stream" is incorrect as that will help the producers to be more efficient and increase write throughput to a Kinesis data stream. However, this will not help as the Kinesis data stream already cannot keep up with the incoming demand.INCORRECT: "Create an SQS queue and decouple the producers from the Kinesis data stream " is incorrect. You cannot decouple a Kinesis producer from a Kinesis data stream using SQS. Kinesis is more than capable of keeping up with demand, it just needs more shards in this case.INCORRECT: "Ingest multiple records into the stream in a single call using PutRecords" is incorrect as the stream is already overloaded, we need more shards, not more data to be written.References: https://docs.aws.amazon.com/kinesis/latest/APIReference/API_UpdateShardCount.html'},{question:"An AWS Lambda function must be connected to an Amazon VPC private subnet that does not have Internet access. The function also connects to an Amazon DynamoDB table. What MUST a Developer do to enable access to the DynamoDB table?",answers:[{text:"Attach an ENI to the DynamoDB table",isCorrect:!1},{text:"Configure a VPC endpoint",isCorrect:!0},{text:"Attach an Internet Gateway",isCorrect:!1},{text:"Create a route table",isCorrect:!1}],explanation:'To connect to AWS services from a private subnet with no internet access, use VPC endpoints. A VPC endpoint for DynamoDB enables resources in a VPC to use their private IP addresses to access DynamoDB with no exposure to the public internet.When you create a VPC endpoint for DynamoDB, any requests to a DynamoDB endpoint within the Region (for example, dynamodb.uswest2.amazonaws.com) are routed to a private DynamoDB endpoint within the Amazon network.CORRECT: "Configure a VPC endpoint" is the correct answer.INCORRECT: "Attach an Internet Gateway" is incorrect as you do not attach these to a private subnet.INCORRECT: "Create a route table" is incorrect as a route table will exist for all subnets and it does not help to route out from a private subnet via the Internet unless an entry for a NAT Gateway or Instance is added.INCORRECT: "Attach an ENI to the DynamoDB table" is incorrect as you do not attach Elastic Network Interfaces to DynamoDB tables.References:https://docs.aws.amazon.com/lambda/latest/dg/troubleshootingnetworking.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpcendpointsdynamodb.html'},{question:"A company provides a large number of services on AWS to customers. The customers connect to one or more services directly and the architecture is becoming complex. How can the architecture be refactored to provide a single interface for the services?",answers:[{text:"AWS XRay",isCorrect:!1},{text:"Amazon API Gateway",isCorrect:!0},{text:"AWS Single Sign On (SSO)",isCorrect:!1},{text:"AWS Cognito",isCorrect:!1}],explanation:'Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door" for applications to access data, business logic, or functionality from your backend services.Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable realtime twoway communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.API Gateway can be used as the single interface for consumers of the services provided by the organization in this scenario. This solution will simplify the architecture.CORRECT: "Amazon API Gateway" is the correct answer.INCORRECT: "AWS XRay" is incorrect. AWS XRay is used for analyzing and debugging applications.INCORRECT: "AWS Cognito" is incorrect. AWS Cognito is used for adding signup, signin and access control to web and mobile apps.INCORRECT: "AWS Single Sign On (SSO)" is incorrect. AWS SSO is used to provide central management of multiple AWS accounts and business applications and to provide single signon to accounts.References: https://aws.amazon.com/apigateway/features/'},{question:"Every time an Amazon EC2 instance is launched, certain metadata about the instance should be recorded in an Amazon DynamoDB table. The data is gathered and written to the table by an AWS Lambda function. What is the MOST efficient method of invoking the Lambda function?",answers:[{text:"Configure detailed monitoring on Amazon EC2 and create an alarm that triggers the Lambda function in initialization",isCorrect:!1},{text:"Create a CloudTrail trail alarm that triggers the Lambda function based on the RunInstances API action",isCorrect:!1},{text:"Create a CloudWatch alarm that triggers the Lambda function based on log streams indicating an EC2 state change in CloudWatch logs",isCorrect:!1},{text:"Create a CloudWatch Event with an event pattern looking for EC2 state changes and a target set to use the Lambda function",isCorrect:!0}],explanation:'Amazon CloudWatch Events delivers a near realtime stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. CloudWatch Events becomes aware of operational changes as they occur. CloudWatch Events responds to these operational changes and takes corrective action as necessary, by sending messages to respond to the environment, activating functions, making changes, and capturing state information.In this scenario the only workable solution is to create a CloudWatch Event with an event pattern looking for EC2 state changes and a target set to use the Lambda function.CORRECT: "Create a CloudWatch Event with an event pattern looking for EC2 state changes and a target set to use the Lambda function" is the correct answer.INCORRECT: "Create a CloudWatch alarm that triggers the Lambda function based on log streams indicating an EC2 state change in CloudWatch logs" is incorrect as Amazon EC2 does not create a log group or log stream by default.INCORRECT: "Create a CloudTrail trail alarm that triggers the Lambda function based on the RunInstances API action" is incorrect as you would need to create a CloudWatch alarm for CloudTrail events (CloudTrail does not have its own alarm feature).INCORRECT: "Configure detailed monitoring on Amazon EC2 and create an alarm that triggers the Lambda function in initialization" is incorrect as you cannot trigger a Lambda function on EC2 instances initialization using detailed monitoring (or the EC2 console).References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html'},{question:"An application scans an Amazon DynamoDB table once per day to produce a report. The scan is performed in nonpeak hours when production usage uses around 50% of the provisioned throughput. How can you MINIMIZE the time it takes to produce the report without affecting production workloads? (Select TWO.)",answers:[{text:"Use a Sequential Scan API operation",isCorrect:!1},{text:"Use the Limit parameter",isCorrect:!0},{text:"Use a Parallel Scan API operation",isCorrect:!0},{text:"Increase read capacity units during the scan operation",isCorrect:!1},{text:"Use pagination to divide results into 1 MB pages",isCorrect:!1}],explanation:'By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data.The larger the table or index being scanned, the more time the Scan takes to complete. In addition, a sequential Scan might not always be able to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table\'s data across multiple physical partitions, a Scan operation can only read one partition at a time. For this reason, the throughput of a Scan is constrained by the maximum throughput of a single partition.To address these issues, the Scan operation can logically divide a table or secondary index into multiple segments, with multiple application workers scanning the segments in parallel. Each worker can be a thread (in programming languages that support multithreading) or an operating system process. To perform a parallel scan, each worker issues its own Scan request with the following parameters:• Segment — A segment to be scanned by a particular worker. Each worker should use a different value for Segment.• TotalSegments — The total number of segments for the parallel scan. This value must be the same as the number of workers that your application will use.The following diagram shows how a multithreaded application performs a parallel Scan with three degrees of parallelism.To make the most of your table’s provisioned throughput, you’ll want to use the Parallel Scan API operation so that your scan is distributed across your table’s partitions. However, you also need to ensure the scan doesn’t consume your table’s provisioned throughput and cause the critical parts of your application to be throttled.To control the amount of data returned per request, use the Limit parameter. This can help prevent situations where one worker consumes all of the provisioned throughput, at the expense of all other workers.Therefore, the best solution to this problem is to use a parallel scan API operation with the Limit parameter.CORRECT: "Use a Parallel Scan API operation " is the correct answer.CORRECT: "Use the Limit parameter" is also a correct answer.INCORRECT: "Use a Sequential Scan API operation" is incorrect as this would take more time and the question requests that we minimize the time it takes to complete the scan.INCORRECT: "Increase read capacity units during the scan operation" is incorrect as this would increase cost and we still need a solution to ensure we maximize usage of available throughput without affecting production workloads.INCORRECT: "Use pagination to divide results into 1 MB pages" is incorrect as this does only divides the results into pages, it does not segment and limit the amount of throughput used.References:https://aws.amazon.com/blogs/developer/ratelimitedscansinamazondynamodb/ https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan.ParallelScan'},{question:"An application uses Amazon Kinesis Data Streams to ingest and process large streams of data records in real time. Amazon EC2 instances consume and process the data using the Amazon Kinesis Client Library (KCL). The application handles the failure scenarios and does not require standby workers. The application reports that a specific shard is receiving more data than expected. To adapt to the changes in the rate of data flow, the “hot” shard is resharded. Assuming that the initial number of shards in the Kinesis data stream is 6, and after resharding the number of shards increased to 8, what is the maximum number of EC2 instances that can be deployed to process data from all the shards?",answers:[{text:"6",isCorrect:!1},{text:"1",isCorrect:!1},{text:"8",isCorrect:!0},{text:"12",isCorrect:!1}],explanation:'Typically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it\'s fine if the number of shards exceeds the number of instances.In this scenario, the number of shards has been increased to 8. Therefore, the maximum number of instances that can be deployed is 8 as the number of instances cannot exceed the number of shards.CORRECT: "8" is the correct answer.INCORRECT: "6" is incorrect as this is not the maximum number of instances that can be deployed to process 8 shards. The maximum number of instances should be the same as the number of shards.INCORRECT: "12" is incorrect as the number of instances exceeds the number of shards. You should ensure that the number of instances does not exceed the number of shardsINCORRECT: "1" is incorrect as this is not the maximum number of instances that can be deployed to process 8 shards. The maximum number of instances should be the same as the number of shards.References: https://docs.aws.amazon.com/streams/latest/dev/kinesisrecordprocessorscaling.html'},{question:"A company manages an application that stores data in an Amazon DynamoDB table. The company need to keep a record of all new changes made to the DynamoDB table in another table within the same AWS region. What is the MOST suitable way to deliver this requirement?",answers:[{text:"Use Amazon DynamoDB streams",isCorrect:!0},{text:"Use CloudWatch events",isCorrect:!1},{text:"Use Amazon CloudTrail",isCorrect:!1},{text:"Use Amazon DynamoDB snapshots",isCorrect:!1}],explanation:'A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the "before" and "after" images of modified items.This is the best way to capture a record of new changes made to the DynamoDB table. Another table can then be populated with this data so the data is stored persistently.CORRECT: "Use Amazon DynamoDB streams" is the correct answer.INCORRECT: "Use CloudWatch events" is incorrect. CloudWatch Events delivers a near realtime stream of system events that describe changes in Amazon Web Services (AWS) resources. However, it does not capture the information that changes in a DynamoDB table so is unsuitable for this purpose.INCORRECT: "Use Amazon CloudTrail" is incorrect as CloudTrail records a history of API calls on your account. It is used for creating an audit trail of events.INCORRECT: "Use Amazon DynamoDB snapshots" is incorrect as snapshots only capture a point in time, they are not used for recording itemlevel changes.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html'},{question:"A Developer is creating an AWS Lambda function that generates a new file each time it runs. Each new file must be checked into an AWS CodeCommit repository hosted in the same AWS account. How should the Developer accomplish this?",answers:[{text:"Use an AWS SDK to instantiate a CodeCommit client. Invoke the put_file method to add the file to the repository",isCorrect:!0},{text:"Upload the new file to an Amazon S3 bucket. Create an AWS Step Function to accept S3 events. In the Step Function, add the new file to the repository",isCorrect:!1},{text:"After the new file is created in Lambda, use cURL to invoke the CodeCommit API. Send the file to the repository",isCorrect:!1},{text:"When the Lambda function starts, use the Git CLI to clone the repository. Check the new file into the cloned repository and push the change",isCorrect:!1}],explanation:'The client can then be used with put_file which adds or updates a file in a branch in an AWS CodeCommit repository, and generates a commit for the addition in the specified branch.The request syntax is as follows:CORRECT: "Use an AWS SDK to instantiate a CodeCommit client. Invoke the put_file method to add the file to the repository" is the correct answer.INCORRECT: "When the Lambda function starts, use the Git CLI to clone the repository. Check the new file into the cloned repository and push the change" is incorrect as there is no need to clone a repository, a file just needs to be added to an existing repository.INCORRECT: "After the new file is created in Lambda, use cURL to invoke the CodeCommit API. Send the file to the repository" is incorrect as a URL cannot be used to invoke a CodeCommit client and upload and check in the file.INCORRECT: "Upload the new file to an Amazon S3 bucket. Create an AWS Step Function to accept S3 events. In the Step Function, add the new file to the repository" is incorrect as Step Functions is not triggered by S3 events.References: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/codecommit.html'},{question:"What does an Amazon SQS delay queue accomplish?",answers:[{text:"Messages are hidden for a configurable amount of time when they are first added to the queue",isCorrect:!0},{text:"The consumer can poll the queue for a configurable amount of time before retrieving a message",isCorrect:!1},{text:"Message cannot be deleted for a configurable amount of time after they are consumed from the queue",isCorrect:!1},{text:"Messages are hidden for a configurable amount of time after they are consumed from the queue",isCorrect:!1}],explanation:'Delay queues let you postpone the delivery of new messages to a queue for a number of seconds, for example, when your consumer application needs additional time to process messages.If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.Therefore, the correct explanation is that with an Amazon SQS Delay Queue messages are hidden for a configurable amount of time when they are first added to the queueCORRECT: "Messages are hidden for a configurable amount of time when they are first added to the queue" is the correct answer.INCORRECT: "Messages are hidden for a configurable amount of time after they are consumed from the queue" is incorrect. They are hidden when they are added to the queue.INCORRECT: "The consumer can poll the queue for a configurable amount of time before retrieving a message" is incorrect. A delay queue simply delays visibility of the message, it does not affect polling behavior.INCORRECT: "Message cannot be deleted for a configurable amount of time after they are consumed from the queue" is incorrect. That is what a visibility timeout achieves.References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsdelayqueues.html'},{question:"A Development team have moved their continuous integration and delivery (CI/CD) pipeline into the AWS Cloud. The team is leveraging AWS CodeCommit for management of source code. The team need to compile their source code, run tests, and produce software packages that are ready for deployment. Which AWS service can deliver these outcomes?",answers:[{text:"AWS CodeBuild",isCorrect:!0},{text:"AWS Cloud9",isCorrect:!1},{text:"AWS CodePipeline",isCorrect:!1},{text:"AWS CodeCommit",isCorrect:!1}],explanation:'AWS CodeBuild is a fully managed build service in the cloud. CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. It provides prepackaged build environments for popular programming languages and build tools such as Apache Maven, Gradle, and more.You can also customize build environments in CodeBuild to use your own build tools. CodeBuild scales automatically to meet peak build requests.CodeBuild provides these benefits:• Fully managed – CodeBuild eliminates the need to set up, patch, update, and manage your own build servers.• On demand – CodeBuild scales on demand to meet your build needs. You pay only for the number of build minutes you consume.• Out of the box – CodeBuild provides preconfigured build environments for the most popular programming languages.All you need to do is point to your build script to start your first build.Therefore, AWS CodeBuild is the best service to use to compile the Development team’s source code, run tests, and produce software packages that are ready for deployment.CORRECT: "AWS CodeBuild" is the correct answer.INCORRECT: "AWS CodeCommit" is incorrect. The team are already using CodeCommit for its correct purpose, which is to manage source code. CodeCommit cannot perform compiling of source code, testing, or package creation.INCORRECT: "AWS CodePipeline" is incorrect. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.INCORRECT: "AWS Cloud9" is incorrect. AWS Cloud9 is a cloudbased integrated development environment (IDE) that lets you write, run, and debug your code with just a browser.References: https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html'}]},{id:"aws-developer-3",title:"AWS Developer Associate (DVA-C02) Practice Test Latest 2025 3",description:"Additional practice questions covering AWS development topics.",questions:[{question:"A developer is completing the configuration for an Amazon ECS cluster. Which task placement strategy will MINIMIZE the number of instances in use?",answers:[{text:"binpack",isCorrect:!0},{text:"random",isCorrect:!1},{text:"spread",isCorrect:!1},{text:"Canary",isCorrect:!1}],explanation:'A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service.Amazon ECS supports the following task placement strategies:binpack place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.random place tasks randomly.spread place tasks evenly based on the specified value. Accepted values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availabilityzone. Service tasks are spread based on the tasks from that service. Standalone tasks are spread based on the tasks from the same task group.To minimize the number of instances in use, the binpack placement strategy is the best choice for this scenario.CORRECT: "binpack" is the correct answer.INCORRECT: "random" is incorrect as random places tasks randomly so this will not minimize the number of instances in use.INCORRECT: "spread" is incorrect as this places tasks evenly.INCORRECT: "Canary" is incorrect as this is a traffic shifting strategy associated with Elastic BeanstalkReferences: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskplacementstrategies.html'},{question:"A serverless application requires a storage location for log files. Which storage solution is the BEST fit?",answers:[{text:"Amazon S3",isCorrect:!0},{text:"Amazon EBS",isCorrect:!1},{text:"Amazon EFS",isCorrect:!1},{text:"Amazon EC2 instance store",isCorrect:!1}],explanation:'Amazon S3 is an objectbased storage system. The serverless application can use the REST API or AWS SDK to write data to an S3 bucket. This is a suitable solution for storing log files from a serverless app at low cost.CORRECT: "Amazon S3" is the correct answer.INCORRECT: "Amazon EBS " is incorrect as this is a blockbased storage solution in which volumes are attached to EC2 instances so it is not suitable for a serverless application.INCORRECT: "Amazon EFS" is incorrect as this is filebased storage solution that is mounted from EC2 instances using the NFS protocol. This is not suitable for a serverless application.INCORRECT: "Amazon EC2 instance store" is incorrect as this is an ephemeral storage volume that is locally attached to EC2 instances from the same physical hardware. It is not suitable for a serverless application.References: https://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html'},{question:"An application will use AWS Lambda and an Amazon RDS database. The Developer needs to secure the database connection string and enable automatic rotation every 30 days. What is the SIMPLEST way to achieve this requirement?",answers:[{text:"Store the connection string in an encrypted Amazon S3 bucket and use a scheduled CloudWatch Event to update the connection string every 30 days",isCorrect:!1},{text:"Store the connection string as an encrypted environment variable in Lambda and create a separate function that rotates the connection string every 30 days",isCorrect:!1},{text:"Store a secret in AWS Secrets Manager and enable automatic rotation every 30 days",isCorrect:!0},{text:"Store a SecureString in Systems Manager Parameter Store and enable automatic rotation every 30 days",isCorrect:!1}],explanation:'AWS Secrets Manager encrypts secrets at rest using encryption keys that you own and store in AWS Key Management Service (KMS). When you retrieve a secret, Secrets Manager decrypts the secret and transmits it securely over TLS to your local environment.With AWS Secrets Manager, you can rotate secrets on a schedule or on demand by using the Secrets Manager console, AWS SDK, or AWS CLI.For example, to rotate a database password, you provide the database type, rotation frequency, and master database credentials when storing the password in Secrets Manager. Secrets Manager natively supports rotating credentials for databases hosted on Amazon RDS and Amazon DocumentDB and clusters hosted on Amazon Redshift.CORRECT: "Store a secret in AWS Secrets Manager and enable automatic rotation every 30 days" is the correct answer.INCORRECT: "Store a SecureString in Systems Manager Parameter Store and enable automatic rotation every 30 days" is incorrect as SSM Parameter Store does not support automatic key rotation.INCORRECT: "Store the connection string as an encrypted environment variable in Lambda and create a separate function that rotates the connection string every 30 days" is incorrect as this is not the simplest solution. In this scenario using AWS Secrets Manager would be easier to implement as it provides native features for rotating the secret.INCORRECT: "Store the connection string in an encrypted Amazon S3 bucket and use a scheduled CloudWatch Event to update the connection string every 30 days" is incorrect. There is no native capability of CloudWatch to update connection strings so you would need some other service such as a Lambda function to execute and rotate the connection string which is missing from this answer.References: https://aws.amazon.com/secretsmanager/features/'},{question:"Fault tolerance needs to be increased for a stateless application that runs on Amazon EC2 instances. The application runs in an Auto Scaling group of EC2 instances in a single subnet behind an Application Load Balancer. How can the application be made more fault tolerant?",answers:[{text:"Add a subnet in another VPC to the ASG and add the same subnet to the ALB",isCorrect:!1},{text:"Add an Elastic IP to each instance and use Amazon Route 53 Alias records to distribute incoming connections",isCorrect:!1},{text:"Add a subnet in another AZ to the ASG and add the same subnet to the ALB",isCorrect:!0},{text:"Add a subnet in another region to the ASG and add the same subnet to the ALB",isCorrect:!1}],explanation:'The application currently resides in a single subnet and that is within a single availability zone. To increase fault tolerance the application instances should be split across subnets that are in different availability zones. This will protect against any faults that occur within a single AZ.To do this, a subnet in another AZ can be added to both the ASG and the ALB. The ASG will automatically launch instances in the new subnet and try and balance the number of instances between these subnets. The ALB will distribute connections across both subnets/AZs.CORRECT: "Add a subnet in another AZ to the ASG and add the same subnet to the ALB" is the correct answer.INCORRECT: "Add a subnet in another region to the ASG and add the same subnet to the ALB" is incorrect as it is not possible to add subnets in different regions to an ASG or ALB.INCORRECT: "Add an Elastic IP to each instance and use Amazon Route 53 Alias records to distribute incoming connections" is incorrect as this will not increase fault tolerance. The instances would still be in a single subnet/AZ.INCORRECT: "Add a subnet in another VPC to the ASG and add the same subnet to the ALB" is incorrect as you cannot add a subnet in another VPC to an ASG or ALB.References:https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscalingbenefits.html https://aws.amazon.com/elasticloadbalancing/'},{question:"A web application is using Amazon Kinesis Data Streams for ingesting IoT data that is then stored before processing for up to 24 hours. How can the Developer implement encryption at rest for data stored in Amazon Kinesis Data Streams?",answers:[{text:"Enable serverside encryption on Kinesis Data Streams with an AWS KMS CMK",isCorrect:!0},{text:"Use the Amazon Kinesis Consumer Library (KCL) to encrypt the data",isCorrect:!1},{text:"Add a certificate and enable SSL/TLS connections to Kinesis Data Streams",isCorrect:!1},{text:"Encrypt the data once it is at rest with an AWS Lambda function",isCorrect:!1}],explanation:'Amazon Kinesis Data Streams (KDS) is a massively scalable and durable realtime data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and locationtracking events.Serverside encryption is a feature in Amazon Kinesis Data Streams that automatically encrypts data before it\'s at rest by using an AWS KMS customer master key (CMK) you specify. Data is encrypted before it\'s written to the Kinesis stream storage layer and decrypted after it’s retrieved from storage. As a result, your data is encrypted at rest within the Kinesis Data Streams service. This allows you to meet strict regulatory requirements and enhance the security of your data.With serverside encryption, your Kinesis stream producers and consumers don\'t need to manage master keys or cryptographic operations. Your data is automatically encrypted as it enters and leaves the Kinesis Data Streams service, so your data at rest is encrypted. AWS KMS provides all the master keys that are used by the serverside encryption feature. AWS KMS makes it easy to use a CMK for Kinesis that is managed by AWS, a userspecified AWS KMS CMK, or a master key imported into the AWS KMS service.Therefore, in this scenario the Developer can enable serverside encryption on Kinesis Data Streams with an AWS KMS CMKCORRECT: "Enable serverside encryption on Kinesis Data Streams with an AWS KMS CMK" is the correct answer.INCORRECT: "Add a certificate and enable SSL/TLS connections to Kinesis Data Streams" is incorrect as SSL/TLS is already used with Kinesis (you don’t need to add a certificate) and this only provides encryption intransit, not encryption at rest.INCORRECT: "Use the Amazon Kinesis Consumer Library (KCL) to encrypt the data" is incorrect. The KCL provides design patterns and code for Amazon Kinesis Data Streams consumer applications. The KCL is not used for adding encryption to the data in a stream.INCORRECT: "Encrypt the data once it is at rest with an AWS Lambda function" is incorrect as this is unnecessary when Kinesis natively supports serverside encryption.References: https://docs.aws.amazon.com/streams/latest/dev/whatissse.html'},{question:"A financial application is hosted on an Auto Scaling group of EC2 instance with an Elastic Load Balancer. A Developer needs to capture information about the IP traffic going to and from network interfaces in the VPC. How can the Developer capture this information?",answers:[{text:"Create a flow log in the VPC and publish data to Amazon S3",isCorrect:!0},{text:"Capture the information directly into Amazon CloudWatch Logs",isCorrect:!1},{text:"Capture the information using a Network ACL",isCorrect:!1},{text:"Create a flow log in the VPC and publish data to Amazon CloudTrail",isCorrect:!1}],explanation:'VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. After you\'ve created a flow log, you can retrieve and view its data in the chosen destination.Flow logs can help you with a number of tasks, such as:• Diagnosing overly restrictive security group rules• Monitoring the traffic that is reaching your instance• Determining the direction of the traffic to and from the network interfacesAs you can see in the image below, you can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored.Therefore, the Developer should create a flow log in the VPC and publish data to Amazon S3. The Developer could also choose CloudWatch Logs as a destination for publishing the data, but this is not presented as an option.CORRECT: "Create a flow log in the VPC and publish data to Amazon S3" is the correct answer.INCORRECT: "Capture the information directly into Amazon CloudWatch Logs" is incorrect as you cannot capture this information directly into CloudWatch Logs. You would need to capture with a flow log and then publish to CloudWatch Logs.INCORRECT: "Capture the information using a Network ACL" is incorrect as you cannot capture data using a Network ACL as it is a subnetlevel firewall.INCORRECT: "Create a flow log in the VPC and publish data to Amazon CloudTrail" is incorrect as you cannot publish data from a flow log to CloudTrail. Amazon CloudTrail captures information about API calls.References: https://docs.aws.amazon.com/vpc/latest/userguide/flowlogs.html'},{question:"To include objects defined by the AWS Serverless Application Model (SAM) in an AWS CloudFormation template, in addition to Resources, what section MUST be included in the document root?",answers:[{text:"Globals",isCorrect:!1},{text:"Properties",isCorrect:!1},{text:"Transform",isCorrect:!0},{text:"Conditions",isCorrect:!1}],explanation:'The primary differences between AWS SAM templates and AWS CloudFormation templates are the following:• Transform declaration. The declaration Transform: AWS::Serverless20161031 is required for AWS SAM templates.This declaration identifies an AWS CloudFormation template as an AWS SAM template.• Globals section. The Globals section is unique to AWS SAM. It defines properties that are common to all your serverless functions and APIs. All the AWS::Serverless::Function, AWS::Serverless::Api,and AWS::Serverless::SimpleTable resources inherit the properties that are defined in the Globals section.• Resources section. In AWS SAM templates the Resources section can contain a combination of AWS CloudFormation resources and AWS SAM resources. Of these three sections, only the Transform section and Resources sections are required; the Globals section is optional.CORRECT: "Transform" is the correct answer.INCORRECT: "Globals" is incorrect as this is not a required section.INCORRECT: "Conditions" is incorrect as this is an optional section.INCORRECT: "Properties" is incorrect as this is not a section in a template, it is used within a resource.References: https://docs.aws.amazon.com/serverlessapplicationmodel/latest/developerguide/samspecificationtemplateanatomy.html'},{question:"An application running on a fleet of EC2 instances use the AWS SDK for Java to copy files into several AWS buckets using access keys stored in environment variables. A Developer has modified the instances to use an assumed IAM role with a more restrictive policy that allows access to only one bucket.However, after applying the change the Developer logs into one of the instances and is still able to write to all buckets. What is the MOST likely explanation for this situation?",answers:[{text:"The AWS CLI is corrupt and needs to be reinstalled",isCorrect:!1},{text:"The AWS credential provider looks for instance profile credentials last",isCorrect:!0},{text:"An IAM inline policy is being used on the IAM role",isCorrect:!1},{text:"An IAM managed policy is being used on the IAM role",isCorrect:!1}],explanation:'When you initialize a new service client without supplying any arguments, the AWS SDK for Java attempts to find AWS credentials by using the default credential provider chain implemented by the DefaultAWSCredentialsProviderChain class. The default credential provider chain looks for credentials in this order:1. Environment variables–AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. The AWS SDK for Java uses the EnvironmentVariableCredentialsProvider class to load these credentials.2. Java system properties–aws.accessKeyId and aws.secretKey. The AWS SDK for Java uses the SystemPropertiesCredentialsProvider to load these credentials.3. The default credential profiles file– typically located at ~/.aws/credentials (location can vary per platform) and shared by many of the AWS SDKs and by the AWS CLI. The AWS SDK for Java uses the ProfileCredentialsProvider toload these credentials.4. Amazon ECS container credentials– loaded from the Amazon ECS if the environment variable AWS_CONTAINER_CREDENTIALS_RELATIVE_URI is set. The AWS SDK for Java uses the ContainerCredentialsProvider to load these credentials. You can specify the IP address for this value.5. Instance profile credentials– used on EC2 instances and delivered through the Amazon EC2 metadata service. The AWS SDK for Java uses the InstanceProfileCredentialsProvider to load these credentials. You can specify the IPaddress for this value.Therefore, the AWS SDK for Java will find the credentials stored in environment variables before it checks for instance provide credentials and will allow access to the extra S3 buckets.NOTE: The Default Credential Provider Chain is very similar for other SDKs and the CLI as well. Check the references below for an article showing the steps for the AWS CLI.CORRECT: "The AWS credential provider looks for instance profile credentials last" is the correct answer.INCORRECT: "An IAM inline policy is being used on the IAM role" is incorrect. If an inline policy was also applied to the role with a less restrictive policy it wouldn’t matter, as the most restrictive policy would be applied.INCORRECT: "An IAM managed policy is being used on the IAM role" is incorrect. Though the managed policies are less restrictive by default (readonly or full access), this is not the most likely cause of the situation as we were told the policy is more restrictive and we know the environments variables have access keys in them which will be used before the policy is checked.INCORRECT: "The AWS CLI is corrupt and needs to be reinstalled" is incorrect. There is a plausible explanation for this situation so no reason to suspect a software bug is to blame.References:https://docs.aws.amazon.com/sdkforjava/v1/developerguide/credentials.html https://docs.aws.amazon.com/cli/latest/userguide/clichapconfigure.html'},{question:"A company is using AWS Lambda for processing small images that are uploaded to Amazon S3. This was working well until a large number of small files (several thousand) were recently uploaded and an error was generated by AWS Lambda (status code 429). What is the MOST likely cause?",answers:[{text:"Lambda cannot process multiple files simultaneously",isCorrect:!1},{text:"The event source mapping has not been configured",isCorrect:!1},{text:"The concurrency execution limit for the account has been exceeded",isCorrect:!0},{text:"Amazon S3 could not handle the sudden burst in traffic",isCorrect:!1}],explanation:'The first time you invoke your function, AWS Lambda creates an instance of the function and runs its handler method to process the event. When the function returns a response, it stays active and waits to process additional events. If you invoke the function again while the first event is being processed, Lambda initializes another instance, and the function processes the two events concurrently. Your functions\' concurrency is the number of instances that serve requests at a given time. For an initial burst of traffic, your functions\' cumulative concurrency in a Region can reach an initial level of between 500 and 3000, which varies per Region. Burst Concurrency Limits: &bull; 3000 - US West (Oregon), US East (N. Virginia), Europe (Ireland). &bull; 1000 - Asia Pacific (Tokyo), Europe (Frankfurt). &bull; 500 - Other Regions. After the initial burst, your functions\' concurrency can scale by an additional 500 instances each minute. This continues until there are enough instances to serve all requests, or until a concurrency limit is reached. The default account limit is up to 1000 executions per second, per region (can be increased). It is therefore most likely that the concurrency execution limit for the account was exceeded. CORRECT: "The concurrency execution limit for the account has been exceeded" is the correct answer. INCORRECT: "Amazon S3 could not handle the sudden burst in traffic" is incorrect as S3 can easily achieve thousands of transactions per second and automatically scales to high request rates. INCORRECT: "Lambda cannot process multiple files simultaneously" is incorrect as Lambda can run multiple executions concurrently as explained above. INCORRECT: "The event source mapping has not been configured" is incorrect as the solution was working well until that large number of files were uploaded. If the event source mapping was not configured it would not have worked at all. References: https://docs.aws.amazon.com/lambda/latest/dg/scaling.html'},{question:"A customer requires a schemaless, key/value database that can be used for storing customer orders. Which type of AWS database is BEST suited to this requirement?",answers:[{text:"Amazon ElastiCache",isCorrect:!1},{text:"Amazon S3",isCorrect:!1},{text:"Amazon RDS",isCorrect:!1},{text:"Amazon DynamoDB",isCorrect:!0}],explanation:'Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It is a nonrelational (schemaless), keyvalue type of database. This is the most suitable solution for this requirement.CORRECT: "Amazon DynamoDB" is the correct answer.INCORRECT: "Amazon RDS" is incorrect as this a relational database that has a schema.INCORRECT: "Amazon ElastiCache" is incorrect as this is a key/value database but it is used to cache the contents of other databases (including DynamoDB and RDS) for better performance for reads.INCORRECT: "Amazon S3" is incorrect as this is an objectbased storage system not a database. It is a key/value store but DynamoDB is a better fit for a customer order database.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html'},{question:"A company needs to ingest several terabytes of data every hour from a large number of distributed sources. The messages are delivered continually 24 hrs a day. Messages must be delivered in real-time for security analysis and live operational dashboards. Which approach will meet these requirements?",answers:[{text:"Use the Amazon S3 API to write messages to an S3 bucket, then process the messages by using Amazon RedShift",isCorrect:!1},{text:"Use AWS Data Pipeline to automate the movement and transformation of data",isCorrect:!1},{text:"Use Amazon Kinesis Data Streams with Kinesis Client Library to ingest and deliver messages",isCorrect:!0},{text:"Send the messages to an Amazon SQS queue, then process the messages by using a fleet of Amazon EC2 instances",isCorrect:!1}],explanation:'You can use Amazon Kinesis Data Streams to collect and process large streams of data records in real time. You can create data processing applications, known as Kinesis Data Streams applications. A typical Kinesis Data Streams application reads data from a data stream as data records.These applications can use the Kinesis Client Library, and they can run on Amazon EC2 instances. You can send the processed records to dashboards, use them to generate alerts, dynamically change pricing and advertising strategies, or send data to a variety of other AWS services.This scenario is an ideal use case for Kinesis Data Streams as large volumes of real time streaming data are being ingested.Therefore, the best approach is to use Amazon Kinesis Data Streams with Kinesis Client Library to ingest and deliver messagesCORRECT: "Use Amazon Kinesis Data Streams with Kinesis Client Library to ingest and deliver messages" is the correct answer.INCORRECT: "Send the messages to an Amazon SQS queue, then process the messages by using a fleet of Amazon EC2 instances" is incorrect as this is not an ideal use case for SQS because SQS is used for decoupling application components, not for ingesting streaming data. It would require more cost (lots of instances to process data) and introduce latency. Also, the message size limitations could be an issue.INCORRECT: "Use the Amazon S3 API to write messages to an S3 bucket, then process the messages by using Amazon RedShift" is incorrect as RedShift does not process messages from S3. RedShift is a data warehouse which is used for analytics.INCORRECT: "Use AWS Data Pipeline to automate the movement and transformation of data" is incorrect as the question is not asking for transformation of data. The scenario calls for a solution for ingesting and processing the real time streaming data for analytics and feeding some data into a system that generates an operational dashboard.References: https://docs.aws.amazon.com/streams/latest/dev/introduction.html'},{question:"A Developer wants to debug an application by searching and filtering log data. The application logs are stored in Amazon CloudWatch Logs. The Developer creates a new metric filter to count exceptions in the application logs. However, no results are returned from the logs. What is the reason that no filtered results are being returned?",answers:[{text:"Metric data points to logs groups can be filtered only after they are exported to an Amazon S3 bucket",isCorrect:!1},{text:"CloudWatch Logs only publishes metric data for events that happen after the filter is created",isCorrect:!0},{text:"The log group for CloudWatch Logs should be first streamed to Amazon Elasticsearch Service before filtering returns the results",isCorrect:!1},{text:"A setup of the Amazon CloudWatch interface VPC endpoint is required for filtering the CloudWatch Logs in the VPC",isCorrect:!1}],explanation:'After the CloudWatch Logs agent begins publishing log data to Amazon CloudWatch, you can begin searching and filtering the log data by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs.CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on. You can use any type of CloudWatch statistic, including percentile statistics, when viewing these metrics or setting alarms.Filters do not retroactively filter data. Filters only publish the metric data points for events that happen after the filter was created. Filtered results return the first 50 lines, which will not be displayed if the timestamp on the filtered results is earlier than the metric creation time.Therefore, the filtered results are not being returned as CloudWatch Logs only publishes metric data for events that happen after the filter is created.CORRECT: "CloudWatch Logs only publishes metric data for events that happen after the filter is created" is the correct answer.INCORRECT: "A setup of the Amazon CloudWatch interface VPC endpoint is required for filtering the CloudWatch Logs in the VPC" is incorrect as a VPC endpoint is not required.INCORRECT: "The log group for CloudWatch Logs should be first streamed to Amazon Elasticsearch Service before filtering returns the results" is incorrect as you do not need to stream the results to Elasticsearch.INCORRECT: "Metric data points to logs groups can be filtered only after they are exported to an Amazon S3 bucket" is incorrect as it is not necessary to export the logs to an S3 bucket.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html'},{question:"A Developer needs to choose the best data store for a new application. The application requires a data store that supports key/value pairs and optimistic locking. Which of the following would provide the MOST suitable solution?",answers:[{text:"Amazon S3",isCorrect:!1},{text:"Amazon DynamoDB",isCorrect:!0},{text:"Amazon RedShift",isCorrect:!1},{text:"Amazon RDS",isCorrect:!1}],explanation:'Amazon DynamoDB is a keyvalue and document database that delivers singledigit millisecond performance at any scale. Optimistic locking is a strategy to ensure that the clientside item that you are updating (or deleting) is the same as the item in Amazon DynamoDB. If you use this strategy, your database writes are protected from being overwritten by the writes of others, and vice versa.With optimistic locking, each item has an attribute that acts as a version number. If you retrieve an item from a table, the application records the version number of that item. You can update the item, but only if the version number on the server side has not changed. If there is a version mismatch, it means that someone else has modified the item before you did.In the diagram below, the application on the left updates an item and increments the version number. Then, the application on the right attempts to update the item but only if the version number is 1.The update attempt fails, because the application has a stale version of the item. Optimistic locking prevents you from accidentally overwriting changes that were made by others. It also prevents others from accidentally overwriting your changes.CORRECT: "Amazon DynamoDB" is the correct answer.INCORRECT: "Amazon RDS" is incorrect as RDS is not a key/value database, nor does it support optimistic locking.INCORRECT: "Amazon RedShift" is incorrect as RedShift is not a key/value database, nor does it support optimistic locking.INCORRECT: "Amazon S3" is incorrect as though it does store objects as key/value pairs it does not support optimistic locking.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html'},{question:"A Development team has deployed several applications running on an Auto Scaling fleet of Amazon EC2 instances. The Operations team have asked for a display that shows a key performance metric for each application on a single screen for monitoring purposes. What steps should a Developer take to deliver this capability using Amazon CloudWatch?",answers:[{text:"Create a custom namespace with a unique metric name for each application",isCorrect:!0},{text:"Create a custom alarm with a unique metric name for each application",isCorrect:!1},{text:"Create a custom dimension with a unique metric name for each application",isCorrect:!1},{text:"Create a custom event with a unique metric name for each application",isCorrect:!1}],explanation:'A namespace is a container for CloudWatch metrics. Metrics in different namespaces are isolated from each other, so that metrics from different applications are not mistakenly aggregated into the same statistics.Therefore, the Developer should create a custom namespace with a unique metric name for each application. This namespace will then allow the metrics for each individual application to be shown in a single view through CloudWatch.CORRECT: "Create a custom namespace with a unique metric name for each application" is the correct answer.INCORRECT: "Create a custom dimension with a unique metric name for each application" is incorrect as a dimension further clarifies what a metric is and what data it stores.INCORRECT: "Create a custom event with a unique metric name for each application" is incorrect as an event is not used to organize metrics for display.INCORRECT: "Create a custom alarm with a unique metric name for each application" is incorrect as alarms are used to trigger actions when a threshold is reached, this is not relevant to organizing metrics for display.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html'},{question:"A mobile application is being developed that will use AWS Lambda, Amazon API Gateway and Amazon DynamoDB. A developer would like to securely authenticate the users of the mobile application and then grant them access to the API. What is the BEST way to achieve this?",answers:[{text:"Create a COGNITO_USER_POOLS authorizer in API Gateway",isCorrect:!0},{text:"Create a COGNITO_IDENTITY_POOLS authorizer in API Gateway",isCorrect:!1},{text:"Create an IAM authorizer in API Gateway",isCorrect:!1},{text:"Create a Lambda authorizer in API Gateway",isCorrect:!1}],explanation:'A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign into your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Google, Facebook, Amazon, or Apple, and through SAML identity providers. Whether your users sign in directly or through a third party, all members of the user pool have a directory profile that you can access through a Software Development Kit (SDK).As an alternative to using IAM roles and policies or Lambda authorizers (formerly known as custom authorizers), you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway.To use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user in to the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request\'s Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn\'t authorized to make the call because the client did not have credentials that could be authorized.CORRECT: "Create a COGNITO_USER_POOLS authorizer in API Gateway" is the correct answer.INCORRECT: "Create a COGNITO_IDENTITY_POOLS authorizer in API Gateway" is incorrect as you should use a Cognito user pool for creating an authorizer in API Gateway.INCORRECT: "Create a Lambda authorizer in API Gateway" is incorrect as this is a mobile application and so the best solution is to use Cognito which is designed for this purpose.INCORRECT: "Create an IAM authorizer in API Gateway" is incorrect as there’s no such thing as an IAM authorizer. You can use IAM roles and policies but then you would need your users to have accounts in IAM. For a mobile application your users are better located in a Cognito user pool.References:https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayintegratewithcognito.html https://docs.aws.amazon.com/cognito/latest/developerguide/cognitouseridentitypools.html'},{question:"A developer needs to implement a caching layer in front of an Amazon RDS database. If the caching layer fails, it is time consuming to repopulate cached data so the solution should be designed for maximum uptime. Which solution is best for this scenario?",answers:[{text:"Implement Amazon ElastiCache Memcached",isCorrect:!1},{text:"Migrate the database to Amazon RedShift",isCorrect:!1},{text:"Implement Amazon DynamoDB DAX",isCorrect:!1},{text:"Implement Amazon ElastiCache Redis",isCorrect:!0}],explanation:'Amazon ElastiCache provides fully managed implementations of two popular inmemory data stores – Redis and Memcached. ElastiCache is a web service that makes it easy to deploy and run Memcached or Redis protocol compliant server nodes in the cloud.The inmemory caching provided by ElastiCache can be used to significantly improve latency and throughput for many readheavy application workloads or computeintensive workloads. It is common to use ElastiCache as a cache in front of databases such as Amazon RDS.The two implementations, Memcached, and Redis, each offer different capabilities and limitations. As you can see from the table below, only Redis supports read replicas and autofailover:The Redis implementation must be used if high availability is required, as is necessary for this scenario. Therefore the correct answer is to use Amazon ElastiCache Redis.CORRECT: "Implement Amazon ElastiCache Redis" is the correct answer.INCORRECT: "Implement Amazon ElastiCache Memcached" is incorrect as Memcached does not offer read replicas or autofailover and therefore cannot provide high availability.INCORRECT: "Migrate the database to Amazon RedShift" is incorrect as RedShift is a data warehouse for use in online analytics processing (OLAP) use cases. It is not suitable to be used as a caching layer.INCORRECT: "Implement Amazon DynamoDB DAX" is incorrect as DAX is used in front of DynamoDB, not Amazon RDS.References: https://aws.amazon.com/elasticache/redisvsmemcached/'},{question:"Messages produced by an application must be pushed to multiple Amazon SQS queues. What is the BEST solution for this requirement?",answers:[{text:"Create and AWS Step Functions state machine that uses multiple Lambda functions to process and push the messages into multiple SQS queues",isCorrect:!1},{text:"Create an Amazon SWF workflow that receives the messages and pushes them to multiple SQS queues",isCorrect:!1},{text:"Publish the messages to an Amazon SNS topic and subscribe each SQS queue to the topic",isCorrect:!0},{text:"Publish the messages to an Amazon SQS queue and configure an AWS Lambda function to duplicate the message into multiple queues",isCorrect:!1}],explanation:'Amazon SNS works closely with Amazon Simple Queue Service (Amazon SQS). Both services provide different benefits for developers. Amazon SNS allows applications to send timecritical messages to multiple subscribers through a “push” mechanism, eliminating the need to periodically check or “poll” for updates.When you subscribe an Amazon SQS queue to an Amazon SNS topic, you can publish a message to the topic and Amazon SNS sends an Amazon SQS message to the subscribed queue. The Amazon SQS message contains the subject and message that were published to the topic along with metadata about the message in a JSON document.CORRECT: "Publish the messages to an Amazon SNS topic and subscribe each SQS queue to the topic" is the correct answer.INCORRECT: "Publish the messages to an Amazon SQS queue and configure an AWS Lambda function to duplicate the message into multiple queues" is incorrect as this seems like an inefficient solution. By using SNS we can eliminate the initial queue and Lambda function.INCORRECT: "Create an Amazon SWF workflow that receives the messages and pushes them to multiple SQS queues" is incorrect as this is not a workable solution. Amazon SWF is not suitable for pushing messages to SQS queues.INCORRECT: Create and AWS Step Functions state machine that uses multiple Lambda functions to process and push the messages into multiple SQS queues"" is incorrect as this is an inefficient solution and there is not mention on how the functions will be invoked with the message dataReferences: https://docs.aws.amazon.com/sns/latest/dg/snssqsassubscriber.html'},{question:"A Development team is creating a microservices application running on Amazon ECS. The release process workflow of the application requires a manual approval step before the code is deployed into the production environment. What is the BEST way to achieve this using AWS CodePipeline?",answers:[{text:"Disable a stage just prior the deployment stage",isCorrect:!1},{text:"Disable the stage transition to allow manual approval",isCorrect:!1},{text:"Use an Amazon SNS notification from the deployment stage",isCorrect:!1},{text:"Use an approval action in a stage before deployment",isCorrect:!0}],explanation:'In AWS CodePipeline, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.If the action is approved, the pipeline execution resumes. If the action is rejected—or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping—the result is the same as an action failing, and the pipeline execution does not continue.In this scenario, the manual approval stage would be placed in the pipeline before the deployment stage that deploys the application update into production:Therefore, the best answer is to use an approval action in a stage before deployment to productionCORRECT: "Use an approval action in a stage before deployment" is the correct answer.INCORRECT: "Use an Amazon SNS notification from the deployment stage" is incorrect as this would send a notification when the actual deployment is already occurring.INCORRECT: "Disable the stage transition to allow manual approval" is incorrect as this requires manual intervention as could be easily missed and allow the deployment to continue.INCORRECT: "Disable a stage just prior the deployment stage" is incorrect as disabling the stage prior would prevent that stage from running, which may be necessary (could be the build / test stage). It is better to use an approval action in a stage in the pipeline before the deployment occursReferences: https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html'},{question:"A company uses an Amazon Simple Queue Service (SQS) Standard queue for an application. An issue has been identified where applications are picking up messages from the queue that are still being processed causing duplication. What can a Developer do to resolve this issue?",answers:[{text:"Increase the ReceiveMessageWaitTimeSeconds API action on the queue",isCorrect:!1},{text:"Create a RedrivePolicy for the queue",isCorrect:!1},{text:"Increase the DelaySeconds API action on the queue",isCorrect:!1},{text:"Increase the VisibilityTimeout API action on the queue",isCorrect:!0}],explanation:'When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn\'t automatically delete the message. Because Amazon SQS is a distributed system, there\'s no guarantee that the consumer actually receives the message (for example, due to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must delete the message from the queue after receiving and processing it. Immediately after a message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds.The maximum is 12 hours.Therefore, the best thing the Developer can do in this situation is to increase the VisibilityTimeout API action on the queueCORRECT: "Increase the VisibilityTimeout API action on the queue" is the correct answer.INCORRECT: "Increase the DelaySeconds API action on the queue" is incorrect as this controls the length of time, in seconds, for which the delivery of all messages in the queue is delayed.INCORRECT: "Increase the ReceiveMessageWaitTimeSeconds API action on the queue" is incorrect as this is the length of time, in seconds, for which a ReceiveMessage action waits for a message to arrive. This is used to configure long polling.INCORRECT: "Create a RedrivePolicy for the queue" is incorrect as this is a string that includes the parameters for the dead letter queue functionality of the source queue as a JSON object.References:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsvisibilitytimeout.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html'},{question:"An application has been instrumented to use the AWS XRay SDK to collect data about the requests the application serves. The Developer has set the user field on segments to a string that identifies the user who sent the request. How can the Developer search for segments associated with specific users?",answers:[{text:"Use a filter expression to search for the user field in the segment annotations",isCorrect:!1},{text:"By using the GetTraceSummaries API with a filter expression",isCorrect:!0},{text:"By using the GetTraceGraph API with a filter expression",isCorrect:!1},{text:"Use a filter expression to search for the user field in the segment metadata",isCorrect:!1}],explanation:'A segment document conveys information about a segment to XRay. A segment document can be up to 64 kB and contain a whole segment with subsegments, a fragment of a segment that indicates that a request is in progress, or a single subsegment that is sent separately. You can send segment documents directly to XRay by using the PutTraceSegments API.Example minimally complete segment:{"name" : "example.com","id" : "70de5b6f19ff9a0a","start_time" : 1.478293361271E9,"trace_id" : "1581cf771a006649127e371903a2de979","end_time" : 1.478293361449E9}A subset of segment fields are indexed by XRay for use with filter expressions. For example, if you set the user field on a segment to a unique identifier, you can search for segments associated with specific users in the XRay console or by using the GetTraceSummaries API.CORRECT: "By using the GetTraceSummaries API with a filter expression" is the correct answer.INCORRECT: "By using the GetTraceGraph API with a filter expression" is incorrect as this API action retrieves a service graph for one or more specific trace IDs.INCORRECT: "Use a filter expression to search for the user field in the segment metadata" is incorrect as the user field is not part of the segment metadata and metadata is not is not indexed for search.INCORRECT: "Use a filter expression to search for the user field in the segment annotations" is incorrect as the user field is not part of the segment annotations.References: https://docs.aws.amazon.com/xray/latest/devguide/xrayapisegmentdocuments.html'},{question:"A Developer has created the code for a Lambda function saved the code in a file named lambda_function.py. He has also created a template that named template.yaml. The following code is included in the template file:AWSTemplateFormatVersion: '20100909'Transform: 'AWS::Serverless20161031'Resources:microservicehttpendpointpython3:Type: 'AWS::Serverless::Function'Properties:Handler: lambda_function.lambda_handlerCodeUri: .What commands can the Developer use to prepare and then deploy this template? (Select TWO.)",answers:[{text:"Run sam build and then sam package",isCorrect:!1},{text:"Run sam package and then sam deploy",isCorrect:!0},{text:"Run aws cloudformation package and then aws cloudformation deploy",isCorrect:!0},{text:"Run aws cloudformation compile and then aws cloudformation deploy",isCorrect:!1},{text:"Run aws serverless package and then aws serverless deploy",isCorrect:!1}],explanation:'The template shown is an AWS SAM template for deploying a serverless application. This can be identified by the template header: Transform: \'AWS::Serverless20161031\'The Developer will need to package and then deploy the template. To do this the source code must be available in the same directory or referenced using the “codeuri” parameter. Then, the Developer can use the “aws cloudformation package” or “sam package” commands to prepare the local artifacts (local paths) that your AWS CloudFormation template references.The command uploads local artifacts, such as source code for an AWS Lambda function or a Swagger file for an AWS API Gateway REST API, to an S3 bucket. The command returns a copy of your template, replacing references to local artifacts with the S3 location where the command uploaded the artifacts.Once that is complete the template can be deployed using the “aws cloudformation deploy” or “sam deploy” commands.Therefore, the developer has two options to prepare and then deploy this package:1. Run aws cloudformation package and then aws cloudformation deploy2. Run sam package and then sam deployCORRECT: "Run aws cloudformation package and then aws cloudformation deploy" is a correct answer.INCORRECT: "Run sam package and then sam deploy" is also a correct answer.INCORRECT: "Run aws cloudformation compile and then aws cloudformation deploy" is incorrect as the “compile” command should be replaced with the “package” command.INCORRECT: "Run sam build and then sam package" is incorrect as the Developer needs to run the “package” command first and then the “deploy” command to actually deploy the function.INCORRECT: "Run aws serverless package and then aws serverless deploy" is incorrect as there is no AWS CLI command named “serverless”.References: https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html'},{question:"A company is migrating a stateful web service into the AWS cloud. The objective is to refactor the application to realize the benefits of cloud computing. How can the Developer leading the project refactor the application to enable more elasticity? (Select TWO.)",answers:[{text:"Use Amazon CloudFormation and the Serverless Application Model",isCorrect:!0},{text:"Use Amazon CloudFront with a Web Application Firewall",isCorrect:!0},{text:"Use an Elastic Load Balancer and Auto Scaling Group",isCorrect:!1},{text:"Store the session state in an Amazon RDS database",isCorrect:!1},{text:"Store the session state in an Amazon DynamoDB table",isCorrect:!1}],explanation:'As this is a stateful application the session data needs to be stored somewhere. Amazon DynamoDB is designed to be used for storing session data and it highly scalable. To add elasticity to the architecture an Amazon Elastic Load Balancer (ELB) and Amazon EC2 Auto Scaling group (ASG) can be used.With this architecture the web service can scale elastically using the ASG and the ELB will distribute traffic to all new instances that the ASG launches. This is a good example of utilizing some of the key benefits of refactoring applications into the AWS cloud.CORRECT: "Use an Elastic Load Balancer and Auto Scaling Group" is a correct answer.CORRECT: "Store the session state in an Amazon DynamoDB table" is also a correct answer.INCORRECT: "Use Amazon CloudFormation and the Serverless Application Model" is incorrect. AWS SAM is used in CloudFormation templates for expressing serverless applications using a simplified syntax. This application is not a serverless application.INCORRECT: "Use Amazon CloudFront with a Web Application Firewall" is incorrect neither protection from web exploits nor improved performance for content delivery are requirements in this scenario.INCORRECT: "Store the session state in an Amazon RDS database" is incorrect as RDS is not suitable for storing session state data. DynamoDB is a better fit for this use case.References: https://docs.aws.amazon.com/awssdkphp/v2/guide/featuredynamodbsessionhandler.html https://digitalcloud.training/certificationtraining/awsdeveloperassociate/awscompute/amazonec2autoscaling/ https://digitalcloud.training/certificationtraining/awsdeveloperassociate/awsdatabase/amazondynamodb/'},{question:"A developer is building a multitier web application that accesses an Amazon RDS MySQL database. The application must use a credentials to connect and these need to be stored securely. The application will take care of secret rotation. Which AWS service represents the LOWEST cost solution for storing credentials?",answers:[{text:"AWS Systems Manager Parameter Store",isCorrect:!0},{text:"AWS IAM with the Security Token Service (STS)",isCorrect:!1},{text:"AWS Secrets Manager",isCorrect:!1},{text:"AWS Key Management Service (KMS)",isCorrect:!1}],explanation:'AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. It is highly scalable, available, and durable.You can store values as plaintext (unencrypted data) or ciphertext (encrypted data). You can then reference values by using the unique name that you specified when you created the parameter.There are no additional charges for using SSM Parameter Store. However, there are limit of 10,000 parameters per accountCORRECT: "AWS Systems Manager Parameter Store" is the correct answer.INCORRECT: "AWS IAM with the Security Token Service (STS)" is incorrect as the application is using credentials to connect, it is not using IAM.INCORRECT: "AWS Secrets Manager" is incorrect as it is not the lowest cost solution as it is a chargeable service. Secrets Manager performs native key rotation; however, this isn’t required in this scenario as the application is handling credential rotation.INCORRECT: "AWS Key Management Service (KMS)" is incorrect as this service is involved with encryption keys, it is not used for storing credentials. You can however encrypt you credentials in SSM using KMS.References: https://docs.aws.amazon.com/systemsmanager/latest/userguide/systemsmanagerparameterstore.html'},{question:"A website is running on a single Amazon EC2 instance. A Developer wants to publish the website on the Internet and is creating an A record on Amazon Route 53 for the website's public DNS name. What type of IP address MUST be assigned to the EC2 instance and used in the A record to ensure ongoing connectivity?",answers:[{text:"Elastic IP address",isCorrect:!0},{text:"Private IP address",isCorrect:!1},{text:"Dynamic IP address",isCorrect:!1},{text:"Public IP address",isCorrect:!1}],explanation:'In Amazon Route 53 when you create an A record you must supply an IP address for the resource to connect to. For a public hosted zone this must be a public IP address.There are three types of IP address that can be assigned to an Amazon EC2 instance:• Public – public address that is assigned automatically to instances in public subnets and reassigned if instance is stopped/started.• Private – private address assigned automatically to all instances.• Elastic IP – public address that is static.To ensure ongoing connectivity the Developer needs to use an Elastic IP address for the EC2 instance and DNS A record as this is the only type of static, public IP address you can assign to an Amazon EC2 instance.CORRECT: "Elastic IP address" is the correct answer.INCORRECT: "Public IP address" is incorrect as though this is a public IP address, it is not static and will change every time the EC2 instance restarts. Therefore, connectivity would be lost until you update the Route 53 A record.INCORRECT: "Dynamic IP address" is incorrect as a dynamic IP address is an IP address that will change over time. For this scenario a static, public address is required.INCORRECT: "Private IP address" is incorrect as a public IP address is required for the public DNS A record.References:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/usinginstanceaddressing.html https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/ResourceRecordTypes.html'},{question:"A developer is creating a serverless application that will use a DynamoDB table. The average item size is 7KB. The application will make 3 strongly consistent reads/sec, and 1 standard write/sec. How many RCUs/WCUs are required?",answers:[{text:"6 RCU and 7 WCU",isCorrect:!0},{text:"12 RCU and 14 WCU",isCorrect:!1},{text:"6 RCU and 14 WCU",isCorrect:!1},{text:"3 RCU and 7 WCU",isCorrect:!1}],explanation:'With provisioned capacity mode, you specify the number of data reads and writes per second that you require for your application.Read capacity unit (RCU):• Each API call to read data from your table is a read request.• Read requests can be strongly consistent, eventually consistent, or transactional.• For items up to 4 KB in size, one RCU can perform one strongly consistent read request per second.• Items larger than 4 KB require additional RCUs.• For items up to 4 KB in size, one RCU can perform two eventually consistent read requests per second.• Transactional read requests require two RCUs to perform one read per second for items up to 4 KB.• For example, a strongly consistent read of an 8 KB item would require two RCUs, an eventually consistent read of an 8 KB item would require one RCU, and a transactional read of an 8 KB item would require four RCUs.Write capacity unit (WCU):• Each API call to write data to your table is a write request.• For items up to 1 KB in size, one WCU can perform one standard write request per second.• Items larger than 1 KB require additional WCUs.• Transactional write requests require two WCUs to perform one write per second for items up to 1 KB.• For example, a standard write request of a 1 KB item would require one WCU, a standard write request of a 3 KB item would require three WCUs, and a transactional write request of a 3 KB item would require six WCUs.To determine the number of RCUs required to handle 3 strongly consistent reads per/second with an average item size of 7KB, perform the following steps:1. Determine the average item size by rounding up the next multiple of 4KB (7KB rounds up to 8KB).2. Determine the RCU per item by dividing the item size by 4KB (8KB/4KB = 2).3. Multiply the value from step 2 with the number of reads required per second (2x3 = 6).To determine the number of WCUs required to handle 1 standard write per/second, simply multiply the average item size by the number of writes required (7x1=7).CORRECT: "6 RCU and 7 WCU" is the correct answer.INCORRECT: "3 RCU and 7 WCU" is incorrect. This would be the correct answer for eventual consistent reads and standard writes.INCORRECT: "6 RCU and 14 WCU" is incorrect. This would be the correct answer for strongly consistent reads and transactional writes.INCORRECT: "12 RCU and 14 WCU" is incorrect. This would be the correct answer for transactional reads and transactional writesReferences: https://aws.amazon.com/dynamodb/pricing/provisioned/'},{question:"A Developer needs to create an instance profile for an Amazon EC2 instance using the AWS CLI. How can this be achieved? (Select THREE.)",answers:[{text:"Run the aws iam addroletoinstanceprofile command",isCorrect:!0},{text:"Run the aws ec2 associateinstanceprofile command",isCorrect:!0},{text:"Run the AssignInstanceProfile API",isCorrect:!1},{text:"Run the AddRoleToInstanceProfile API",isCorrect:!1},{text:"Run the aws iam createinstanceprofile command",isCorrect:!0},{text:"Run the CreateInstanceProfile API",isCorrect:!1}],explanation:'To add a role to an Amazon EC2 instance using the AWS CLI you must first create an instance profile. Then you need to add the role to the instance profile and finally assign the instance profile to the Amazon EC2 instance.The following example commands would achieve this outcome:1. aws iam createinstanceprofile instanceprofilename EXAMPLEPROFILENAME2. aws iam addroletoinstanceprofile instanceprofilename EXAMPLEPROFILENAME rolenameEXAMPLEROLENAME3. aws ec2 associateiaminstanceprofile iaminstanceprofile Name=EXAMPLEPROFILENAME instanceid i012345678910abcdeCORRECT: "Run the aws iam createinstanceprofile command" is a correct answer.CORRECT: "Run the aws iam addroletoinstanceprofile command" is a correct answer.CORRECT: "Run the aws ec2 associateinstanceprofile command" is a correct answer.INCORRECT: "Run the CreateInstanceProfile API" is incorrect as this is an API action, not an AWS CLI command.INCORRECT: "Run the AddRoleToInstanceProfile API" is incorrect as this is an API action, not an AWS CLI command.INCORRECT: "Run the AssignInstanceProfile API" is incorrect as this is an API action, not an AWS CLI command.References: https://aws.amazon.com/premiumsupport/knowledgecenter/attachreplaceec2instanceprofile/'},{question:"An application uses an Auto Scaling group of Amazon EC2 instances, an Application Load Balancer (ALB), and an Amazon Simple Queue Service (SQS) queue. An Amazon CloudFront distribution caches content for global users. A Developer needs to add in transit encryption to the data by configuring end-to-end SSL between the CloudFront Origin and the end users. How can the Developer meet this requirement? (Select TWO.)",answers:[{text:"Configure the Viewer Protocol Policy",isCorrect:!0},{text:"Create an encrypted distribution",isCorrect:!1},{text:"Create an Origin Access Identity (OAI)",isCorrect:!1},{text:"Add a certificate to the Auto Scaling Group",isCorrect:!1},{text:"Configure the Origin Protocol Policy",isCorrect:!0}],explanation:'For web distributions, you can configure CloudFront to require that viewers use HTTPS to request your objects, so that connections are encrypted when CloudFront communicates with viewers. You also can configure CloudFront to use HTTPS to get objects from your origin, so that connections are encrypted when CloudFront communicates with your origin. If you configure CloudFront to require HTTPS both to communicate with viewers and to communicate with your origin, here\'s what happens when CloudFront receives a request for an object:1. A viewer submits an HTTPS request to CloudFront. There\'s some SSL/TLS negotiation here between the viewer and CloudFront. In the end, the viewer submits the request in an encrypted format.2. If the object is in the CloudFront edge cache, CloudFront encrypts the response and returns it to the viewer, and the viewer decrypts it.3. If the object is not in the CloudFront cache, CloudFront performs SSL/TLS negotiation with your origin and, when the negotiation is complete, forwards the request to your origin in an encrypted format.4. Your origin decrypts the request, encrypts the requested object, and returns the object to CloudFront.5. CloudFront decrypts the response, reencrypts it, and forwards the object to the viewer. CloudFront also saves the object in the edge cache so that the object is available the next time it\'s requested.6. The viewer decrypts the response.To enable SSL between the origin and the distribution the Developer can configure the Origin Protocol Policy. Depending on the domain name used (CloudFront default or custom), the steps are different. To enable SSL between the enduser and CloudFront the Viewer Protocol Policy should be configured.CORRECT: "Configure the Origin Protocol Policy" is a correct answer.CORRECT: "Configure the Viewer Protocol Policy" is also a correct answer.INCORRECT: "Create an Origin Access Identity (OAI)" is incorrect as this is a special user used for securing objects in Amazon S3 origins.INCORRECT: "Add a certificate to the Auto Scaling Group" is incorrect as you do not add certificates to an ASG. The certificate should be located on the ALB listener in this scenario.INCORRECT: "Create an encrypted distribution" is incorrect as there’s no such thing as an encrypted distributionReferences:https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/usinghttpsviewerstocloudfront.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/usinghttpscloudfronttocustomorigin.html'},{question:"A developer is planning the deployment of a new version of an application to AWS Elastic Beanstalk. The new version of the application should be deployed only to new EC2 instances. Which deployment methods will meet these requirements? (Select TWO.)",answers:[{text:"Rolling",isCorrect:!1},{text:"Rolling with additional batch",isCorrect:!1},{text:"Immutable",isCorrect:!0},{text:"All at once",isCorrect:!1},{text:"Blue/green",isCorrect:!0}],explanation:'AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments.All at once:• Deploys the new version to all instances simultaneously.Rolling:• Update a few instances at a time (bucket), and then move onto the next bucket once the first bucket is healthy (downtime for 1 bucket at a time).Rolling with additional batch:• Like Rolling but launches new instances in a batch ensuring that there is full availability.Immutable:• Launches new instances in a new ASG and deploys the version update to these instances before swapping traffic to these instances once healthy.• Zero downtime.Blue / Green deployment:• Zero downtime and release facility.• Create a new “stage” environment and deploy updates there.The immutable and blue/green options both provide zero downtime as they will deploy the new version to a new version of the application. These are also the only two options that will ONLY deploy the updates to new EC2 instances.CORRECT: "Immutable" is the correct answer.CORRECT: "Blue/green" is the correct answer.INCORRECT: "Allatonce" is incorrect as this will deploy the updates to existing instances.INCORRECT: "Rolling" is incorrect as this will deploy the updates to existing instances.INCORRECT: "Rolling with additional batch" is incorrect as this will launch new instances but will also update the existing instances as well (which is not allowed according to the requirements).References:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.rollingversiondeploy.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html'},{question:"A company is developing a game for the Android and iOS platforms. The mobile game will securely store user game history and other data locally on the device. The company would like users to be able to use multiple mobile devices and synchronize data between devices. Which service can be used to synchronize the data across mobile devices without the need to create a backend application?",answers:[{text:"AWS Lambda",isCorrect:!1},{text:"Amazon DynamoDB",isCorrect:!1},{text:"Amazon API Gateway",isCorrect:!1},{text:"Amazon Cognito",isCorrect:!0}],explanation:'Amazon Cognito lets you save end user data in datasets containing keyvalue pairs. This data is associated with an Amazon Cognito identity, so that it can be accessed across logins and devices. To sync this data between the Amazon Cognito service and an end user’s devices, invoke the synchronize method. Each dataset can have a maximum size of 1 MB. You can associate up to 20 datasets with an identity.The Amazon Cognito Sync client creates a local cache for the identity data. Your app talks to this local cache when it reads and writes keys. This guarantees that all of your changes made on the device are immediately available on the device, even when you are offline. When the synchronize method is called, changes from the service are pulled to the device, and any local changes are pushed to the service. At this point the changes are available to other devices to synchronize.CORRECT: "Amazon Cognito" is the correct answer.INCORRECT: "AWS Lambda" is incorrect. AWS Lambda provides serverless functions that run your code, it is not used for mobile client data synchronization.INCORRECT: "Amazon API Gateway" is incorrect as API Gateway provides APIs for traffic coming into AWS. It is not used for mobile client data synchronization.INCORRECT: "Amazon DynamoDB" is incorrect as DynamoDB is a NoSQL database. It is not used for mobile client data synchronization.References: https://docs.aws.amazon.com/cognito/latest/developerguide/synchronizingdata.html'},{question:"A web application has been deployed on AWS. A developer is concerned about exposure to common exploits that could affect application availability or compromise security. Which AWS service can protect from these threats?",answers:[{text:"Amazon Cognito",isCorrect:!1},{text:"AWS CloudFront",isCorrect:!1},{text:"AWS CloudHSM",isCorrect:!1},{text:"AWS Web Application Firewall (WAF)",isCorrect:!0}],explanation:'AWS WAF is a web application firewall service that helps protect your web apps from common exploits that could affect app availability, compromise security, or consume excessive resources.AWS WAF helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection and crosssite scripting.AWS WAF gives you control over which traffic to allow or block to your web applications by defining customizable web security rules.CORRECT: "AWS Web Application Firewall (WAF)" is the correct answer.INCORRECT: "AWS CloudFront" is incorrect. CloudFront does provide DDoS attack protection (through AWS Shield), however it is primarily a content delivery network (CDN) so you wouldn’t put it infront of a web application unless you wanted it to cache your content. i.e. its primary use case would not be protection from Internet threats.INCORRECT: "Amazon Cognito" is incorrect as this is a service for providing signup and signin capabilities to mobile applications.INCORRECT: "AWS CloudHSM" is incorrect as this is a service that is used for storing cryptographic keys using a hardware device.References: https://aws.amazon.com/waf/'},{question:"A serverless application is used to process customer information and outputs a JSON file to an Amazon S3 bucket. AWS Lambda is used for processing the data. The data is sensitive and should be encrypted. How can a Developer modify the Lambda function to ensure the data is encrypted before it is uploaded to the S3 bucket?",answers:[{text:"Use the S3 managed key and call the GenerateDataKey API to encrypt the file",isCorrect:!1},{text:"Use the default KMS key for S3 and encrypt the file using the Lambda code",isCorrect:!1},{text:"Use the GenerateDataKey API, then use the data key to encrypt the file using the Lambda code",isCorrect:!0},{text:"Enable serverside encryption on the S3 bucket and create a policy to enforce encryption",isCorrect:!1}],explanation:'The GenerateDataKey API is used with the AWS KMS services and generates a unique symmetric data key. This operation returns a plaintext copy of the data key and a copy that is encrypted under a customer master key (CMK) that you specify. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data.For this scenario we can use GenerateDataKey to obtain an encryption key from KMS that we can then use within the function code to encrypt the file. This ensures that the file is encrypted BEFORE it is uploaded to Amazon S3.CORRECT: "Use the GenerateDataKey API, then use the data key to encrypt the file using the Lambda code" is the correct answer.INCORRECT: "Enable serverside encryption on the S3 bucket and create a policy to enforce encryption" is incorrect. This would not encrypt data before it is uploaded as S3 would only encrypt the data as it is written to storage.INCORRECT: "Use the S3 managed key and call the GenerateDataKey API to encrypt the file" is incorrect as you do not use an encryption key to call KMS. You call KMS with the GenerateDataKey API to obtain an encryption key. Also, the S3 managed key can only be used within the S3 service.INCORRECT: "Use the default KMS key for S3 and encrypt the file using the Lambda code" is incorrect. You cannot use the default KMS key for S3 within the Lambda code as it can only be used within the S3 service.References: https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html'},{question:"A Developer is troubleshooting an issue with a DynamoDB table. The table is used to store order information for a busy online store and uses the order date as the partition key. During busy periods writes to the table are being throttled despite the consumed throughput being well below the provisioned throughput. According to AWS best practices, how can the Developer resolve the issue at the LOWEST cost?",answers:[{text:"Increase the read and write capacity units for the table",isCorrect:!1},{text:"Add a random number suffix to the partition key values",isCorrect:!0},{text:"Use an Amazon SQS queue to buffer the incoming writes",isCorrect:!1},{text:"Add a global secondary index to the table",isCorrect:!1}],explanation:'DynamoDB stores data as groups of attributes, known as items. Items are similar to rows or records in other database systems. DynamoDB stores and retrieves each item based on the primary key value, which must be unique.Items are distributed across 10GB storage units, called partitions (physical storage internal to DynamoDB). Each table has one or more partitions, as shown in the following illustration.DynamoDB uses the partition key’s value as an input to an internal hash function. The output from the hash function determines the partition in which the item is stored. Each item’s location is determined by the hash value of its partition key.All items with the same partition key are stored together, and for composite partition keys, are ordered by the sort key value.DynamoDB splits partitions by sort key if the collection size grows bigger than 10 GB.DynamoDB evenly distributes provisioned throughput—read capacity units (RCUs) and write capacity units (WCUs)—among partitions and automatically supports your access patterns using the throughput you have provisioned. However, if your access pattern exceeds 3000 RCU or 1000 WCU for a single partition key value, your requests might be throttled with a ProvisionedThroughputExceededException error.To avoid request throttling, design your DynamoDB table with the right partition key to meet your access requirements and provide even distribution of data. Recommendations for doing this include the following:• Use high cardinality attributes (e.g. email_id, employee_no, customer_id etc.)• Use composite attributes• Cache popular items• Add random numbers or digits from a predetermined range for writeheavy use casesIn this case there is a hot partition due to the order date being used as the partition key and this is causing writes to be throttled. Therefore, the best solution to ensure the writes are more evenly distributed in this scenario is to add a random number suffix to the partition key values.CORRECT: "Add a random number suffix to the partition key values" is the correct answer.INCORRECT: "Increase the read and write capacity units for the table" is incorrect as this will not solve the hot partition issue and we know that the consumed throughput is lower than provisioned throughput.INCORRECT: "Add a global secondary index to the table" is incorrect as a GSI is used for querying data more efficiently, it will not solve the problem of write performance due to a hot partition.INCORRECT: "Use an Amazon SQS queue to buffer the incoming writes" is incorrect as this is not the lowest cost option. You would need to have producers and consumers of the queue as well as paying for the queue itself.References: https://aws.amazon.com/blogs/database/choosingtherightdynamodbpartitionkey/'},{question:"A company is in the process of migrating an application from a monolithic architecture to a microservicesbased architecture. The developers need to refactor the application so that the many microservices can asynchronously communicate with each other in a decoupled manner. Which AWS services can be used for asynchronous message passing? (Select TWO.)",answers:[{text:"Amazon Kinesis",isCorrect:!1},{text:"Amazon SQS",isCorrect:!0},{text:"Amazon SNS",isCorrect:!0},{text:"AWS Lambda",isCorrect:!1},{text:"Amazon ECS",isCorrect:!1}],explanation:'Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.These services both enable asynchronous message passing in the form of a message bus (SQS) and notifications (SNS).CORRECT: "Amazon SQS" is the correct answer.CORRECT: "Amazon SNS" is also a correct answer.INCORRECT: "Amazon Kinesis" is incorrect. Kinesis is used for streaming data, it is used for realtime analytics, mobile data capture and IoT and similar use cases.INCORRECT: "Amazon ECS" is incorrect. ECS is a service providing Docker containers on Amazon EC2.INCORRECT: "AWS Lambda" is incorrect. AWS Lambda is a compute service that runs functions in response to triggers.References:https://aws.amazon.com/sqs/ https://aws.amazon.com/sns/'},{question:"A company needs a version control system for collaborative software development. The solution must include support for batches of changes across multiple files and parallel branching. Which AWS service will meet these requirements?",answers:[{text:"Amazon S3",isCorrect:!1},{text:"AWS CodePipeline",isCorrect:!1},{text:"AWS CodeCommit",isCorrect:!0},{text:"AWS CodeBuild",isCorrect:!1}],explanation:'AWS CodeCommit is a version control service hosted by Amazon Web Services that you can use to privately store and manage assets (such as documents, source code, and binary files) in the cloud.CodeCommit is optimized for team software development. It manages batches of changes across multiple files, which can occur in parallel with changes made by other developers.CORRECT: "AWS CodeCommit" is the correct answer.INCORRECT: "AWS CodeBuild" is incorrect as it is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy.INCORRECT: "AWS CodePipeline" is incorrect as it is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.INCORRECT: "Amazon S3" is incorrect. Amazon S3 versioning supports the recovery of past versions of files, but it\'s not focused on collaborative file tracking features that software development teams need.References: https://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html'},{question:"A static website is hosted on Amazon S3 using the bucket name of dctlabs.com. Some HTML pages on the site use JavaScript to download images that are located in the bucket https://dctlabsimages.s3.amazonaws.com/. Users have reported that the images are not being displayed. What is the MOST likely cause?",answers:[{text:"The dctlabsimages bucket is not in the same region as the dctlabs.com bucket",isCorrect:!1},{text:"Amazon S3 Transfer Acceleration should be enabled on the dctlabs.com bucket",isCorrect:!1},{text:"Cross Origin Resource Sharing is not enabled on the dctlabs.com bucket",isCorrect:!1},{text:"Cross Origin Resource Sharing is not enabled on the dctlabsimages bucket",isCorrect:!0}],explanation:'Crossorigin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich clientside web applications with Amazon S3 and selectively allow crossorigin access to your Amazon S3 resources.To configure your bucket to allow crossorigin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support for each origin, and other operationspecific information.In this case, you would apply the CORS configuration to the dctlabsimages bucket so that it will allow GET requests from the dctlabs.com origin.CORRECT: "Cross Origin Resource Sharing is not enabled on the dctlabsimages bucket" is the correct answer.INCORRECT: "Cross Origin Resource Sharing is not enabled on the dctlabs.com bucket" is incorrect as in this case the images that are being blocked are located in the dctlabsimages bucket. You need to apply the CORS configuration to the dctlabsimages bucket so it allows requests from the dctlabs.com origin.INCORRECT: "The dctlabsimages bucket is not in the same region as the dctlabs.com bucket" is incorrect as it doesn’t matter what regions the buckets are in.INCORRECT: "Amazon S3 Transfer Acceleration should be enabled on the dctlabs.com bucket" is incorrect as this feature of Amazon S3 is used to speed uploads to S3.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html'},{question:"A company uses continuous integration and continuous delivery (CI/CD) systems. A Developer needs to automate the deployment of a software package to Amazon EC2 instances as well as to onpremises virtual servers. Which AWS service can be used for the software deployment?",answers:[{text:"AWS CodeDeploy",isCorrect:!0},{text:"AWS CloudBuild",isCorrect:!1},{text:"AWS CodePipeline",isCorrect:!1},{text:"AWS Elastic Beanstalk",isCorrect:!1}],explanation:'CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, onpremises instances, serverless Lambda functions, or Amazon ECS services.CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can use CodeDeploy.The image below shows the flow of a typical CodeDeploy inplace deployment.The above deployment could also be directed at onpremises servers. Therefore, the best answer is to use AWS CodeDeploy to deploy the software package to both EC2 instances and onpremises virtual servers.CORRECT: "AWS CodeDeploy" is the correct answer.INCORRECT: "AWS CodePipeline" is incorrect. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. You can use CodeDeploy in a CodePipeline pipeline however it is actually CodeDeploy that deploys the software packages.INCORRECT: "AWS CloudBuild" is incorrect as this is a build tool, not a deployment tool.INCORRECT: "AWS Elastic Beanstalk" is incorrect as you cannot deploy software packages to onpremise virtual servers using Elastic BeanstalkReferences: https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html'},{question:"A highly secured AWS environment has strict policies for granting access to Developers. A Developer requires the ability to use the API to call ec2:StartInstances and ec2:StopInstances. Which element of an IAM policy statement should be used to specify which APIs can be called?",answers:[{text:"Effect",isCorrect:!1},{text:"Action",isCorrect:!0},{text:"Condition",isCorrect:!1},{text:"Resource",isCorrect:!1}],explanation:'The Action element describes the specific action or actions that will be allowed or denied. Statements must include either an Action or NotAction element. Each AWS service has its own set of actions that describe tasks that you can perform with that service.For this scenario, the Action element might include the following JSON”"Action": [ "ec2:StartInstances", "ec2:StopInstances" ]CORRECT: "Action" is the correct answer.INCORRECT: "Effect" is incorrect. The Effect element is required and specifies whether the statement results in an allow or an explicit deny.INCORRECT: "Resource" is incorrect. The Resource element specifies the object or objects that the statement covers.INCORRECT: "Condition" is incorrect. The Condition element (or Condition block) lets you specify conditions for when a policy is in effect.References: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_action.html'},{question:"A company wants to implement authentication for its new REST service using Amazon API Gateway. To authenticate the calls, each request must include HTTP headers with a client ID and user ID. These credentials must be compared to authentication data in an Amazon DynamoDB table. What MUST the company do to implement this authentication in API Gateway?",answers:[{text:"Implement an AWS Lambda authorizer that references the DynamoDB authentication table",isCorrect:!0},{text:"Create a model that requires the credentials, then grant API Gateway access to the authentication table",isCorrect:!1},{text:"Modify the integration requests to require the credentials, then grant API Gateway access to the authentication table",isCorrect:!1},{text:"Implement an Amazon Cognito authorizer that references the DynamoDB authentication table",isCorrect:!1}],explanation:'A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API.A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller\'s identity.When a client makes a request to one of your API\'s methods, API Gateway calls your Lambda authorizer, which takes the caller\'s identity as input and returns an IAM policy as output.There are two types of Lambda authorizers:• A tokenbased Lambda authorizer (also called a TOKEN authorizer) receives the caller\'s identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token.• A request parameterbased Lambda authorizer (also called a REQUEST authorizer) receives the caller\'s identity in a combination of headers, query string parameters, stageVariables, and $context variables.• For WebSocket APIs, only request parameterbased authorizers are supported.In this scenario, the authentication is using headers in the request and therefore the request parameterbased Lambda authorizer should be used.CORRECT: "Implement an AWS Lambda authorizer that references the DynamoDB authentication table" is the correct answer.INCORRECT: "Create a model that requires the credentials, then grant API Gateway access to the authentication table" is incorrect as a model defines the structure of the incoming payload using the JSON Schema.INCORRECT: "Modify the integration requests to require the credentials, then grant API Gateway access to the authentication table" is incorrect as API Gateway will not authorize directly using the table information, an authorizer should be used.INCORRECT: "Implement an Amazon Cognito authorizer that references the DynamoDB authentication table" is incorrect as a Lambda authorizer should be used in this example as the authentication data is being passed in request headers.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayuselambdaauthorizer.html'},{question:"A development team are creating a mobile application that customers will use to receive notifications and special offers. Users will not be required to log in. What is the MOST efficient method to grant users access to AWS resources?",answers:[{text:"Use an IAM SAML 2.0 identity provider to establish trust",isCorrect:!1},{text:"Embed access keys in the application that have limited access to resources",isCorrect:!1},{text:"Use Amazon Cognito Federated Identities and setup authentication using a Cognito User Pool",isCorrect:!1},{text:"Use Amazon Cognito to associate unauthenticated users with an IAM role that has limited access to resources",isCorrect:!0}],explanation:'Amazon Cognito Identity Pools can support unauthenticated identities by providing a unique identifier and AWS credentials for users who do not authenticate with an identity provider. If your application allows users who do not log in, you can enable access for unauthenticated identities.This is the most efficient and secure way to allow unauthenticated access as the process to set it up is simple and the IAM role can be configured with permissions allowing only the access permitted for unauthenticated users.CORRECT: "Use Amazon Cognito to associate unauthenticated users with an IAM role that has limited access to resources" is the correct answer.INCORRECT: "Use an IAM SAML 2.0 identity provider to establish trust" is incorrect as we need to allow unauthenticated users access to the AWS resources, not those who have been authenticated elsewhere (i.e. Active Directory).INCORRECT: "Use Amazon Cognito Federated Identities and setup authentication using a Cognito User Pool" is incorrect as we need to setup unauthenticated access, not authenticated access through a user pool.INCORRECT: "Embed access keys in the application that have limited access to resources" is incorrect. We should try and avoid embedding access keys in application code, it is better to use the builtin features of Amazon Cognito.References: https://docs.aws.amazon.com/cognito/latest/developerguide/identitypools.html'},{question:"A Developer is storing sensitive documents in Amazon S3. The documents must be encrypted at rest and company policy mandates that the encryption keys must be rotated annually. What is the EASIEST way to achieve this?",answers:[{text:"Use AWS KMS with automatic key rotation",isCorrect:!0},{text:"Export a key from AWS KMS to encrypt the data",isCorrect:!1},{text:"Encrypt the data before sending it to Amazon S3",isCorrect:!1},{text:"Import a custom key into AWS KMS with annual rotation enabled",isCorrect:!1}],explanation:'Cryptographic best practices discourage extensive reuse of encryption keys. To create new cryptographic material for your AWS Key Management Service (AWS KMS) customer master keys (CMKs), you can create new CMKs, and then change your applications or aliases to use the new CMKs. Or, you can enable automatic key rotation for an existing customer managed CMK.When you enable automatic key rotation for a customer managed CMK, AWS KMS generates new cryptographic material for the CMK every year. AWS KMS also saves the CMK\'s older cryptographic material in perpetuity so it can be used to decrypt data that it encrypted. AWS KMS does not delete any rotated key material until you delete the CMK.Key rotation changes only the CMK\'s backing key, which is the cryptographic material that is used in encryption operations. The CMK is the same logical resource, regardless of whether or how many times its backing key changes. The properties of the CMK do not change, as shown in the following image.Therefore, the easiest way to meet this requirement is to use AWS KMS with automatic key rotation.CORRECT: "Use AWS KMS with automatic key rotation" is the correct answer.INCORRECT: "Encrypt the data before sending it to Amazon S3" is incorrect as that requires managing your own encryption infrastructure which is not the easiest way to achieve the requirements.INCORRECT: "Import a custom key into AWS KMS with annual rotation enabled" is incorrect as when you import key material into AWS KMS you are still responsible for the key material while allowing KMS to use a copy of it. Therefore, this is not the easiest solution as you must manage the key materials.INCORRECT: "Export a key from AWS KMS to encrypt the data" is incorrect as when you export a data encryption key you are then responsible for using it and managing it.References: https://docs.aws.amazon.com/kms/latest/developerguide/rotatekeys.html'},{question:"An application is being migrated into the cloud. The application is stateless and will run on a fleet of Amazon EC2 instances. The application should scale elastically. How can a Developer ensure that the number of instances available is sufficient for current demand?",answers:[{text:"Create a launch configuration and use Amazon CodeDeploy",isCorrect:!1},{text:"Create a task definition and use an Amazon ECS cluster",isCorrect:!1},{text:"Create a launch configuration and use Amazon EC2 Auto Scaling",isCorrect:!0},{text:"Create a task definition and use an AWS Fargate cluster",isCorrect:!1}],explanation:'Amazon EC2 Auto Scaling helps you maintain application availability and allows you to automatically add or remove EC2 instances according to conditions you define. You can use the fleet management features of EC2 Auto Scaling to maintain the health and availability of your fleet.You can also use the dynamic and predictive scaling features of EC2 Auto Scaling to add or remove EC2 instances. Dynamic scaling responds to changing demand and predictive scaling automatically schedules the right number of EC2 instances based on predicted demand. Dynamic scaling and predictive scaling can be used together to scale faster.A launch configuration is an instance configuration template that an Auto Scaling group uses to launch EC2 instances. When you create a launch configuration, you specify information for the instances. Include the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping. If you\'ve launched an EC2 instance before, you specified the same information in order to launch the instance.You can specify your launch configuration with multiple Auto Scaling groups. However, you can only specify one launch configuration for an Auto Scaling group at a time, and you can\'t modify a launch configuration after you\'ve created it. To change the launch configuration for an Auto Scaling group, you must create a launch configuration and then update your Auto Scaling group with it.Therefore, the Developer should create a launch configuration and use Amazon EC2 Auto Scaling.CORRECT: "Create a launch configuration and use Amazon EC2 Auto Scaling" is the correct answer.INCORRECT: "Create a launch configuration and use Amazon CodeDeploy" is incorrect as CodeDeploy is not used for auto scaling of Amazon EC2 instances.INCORRECT: "Create a task definition and use an Amazon ECS cluster" is incorrect as the migrated application will be running on Amazon EC2 instances, not containers.INCORRECT: "Create a task definition and use an AWS Fargate cluster" is incorrect as the migrated application will be running on Amazon EC2 instances, not containers.References:https://docs.aws.amazon.com/autoscaling/ec2/userguide/whatisamazonec2autoscaling.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html'},{question:"A Developer needs to configure an Elastic Load Balancer that is deployed through AWS Elastic Beanstalk. Where should the Developer place the loadbalancer.config file in the application source bundle?",answers:[{text:"In the bin folder",isCorrect:!1},{text:"In the .ebextensions folder",isCorrect:!0},{text:"In the loadbalancer.config.root",isCorrect:!1},{text:"In the root of the source code",isCorrect:!1}],explanation:'You can add AWS Elastic Beanstalk configuration files (.ebextensions) to your web application\'s source code to configure your environment and customize the AWS resources that it contains.Configuration files are YAML or JSONformatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle.For example, you could include a configuration file for setting the load balancer type into:.ebextensions/loadbalancer.configThis example makes a simple configuration change. It modifies a configuration option to set the type of your environment\'s load balancer to Network Load Balancer:Requirements• Location – Place all of your configuration files in a single folder, named .ebextensions, in the root of your source bundle. Folders starting with a dot can be hidden by file browsers, so make sure that the folder is added when youcreate your source bundle.• Naming – Configuration files must have the .config file extension.• Formatting – Configuration files must conform to YAML or JSON specifications.• Uniqueness – Use each key only once in each configuration file.Therefore, the Developer should place the file in the .ebextensions folder in the application source bundle.CORRECT: "In the .ebextensions folder" is the correct answer.INCORRECT: "In the root of the source code" is incorrect. You need to place .config files in the .ebextensions folder.INCORRECT: "In the bin folder" is incorrect. You need to place .config files in the .ebextensions folder.INCORRECT: "In the loadbalancer.config.root" is incorrect. You need to place .config files in the .ebextensions folder.References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html'},{question:"A Developer is designing a faulttolerant environment where client sessions will be saved. How can the Developer ensure that no sessions are lost if an Amazon EC2 instance fails?",answers:[{text:"Use Elastic Load Balancer connection draining to stop sending requests to failing instances",isCorrect:!1},{text:"Use Amazon DynamoDB to perform scalable session handling",isCorrect:!0},{text:"Use sticky sessions with an Elastic Load Balancer target group",isCorrect:!1},{text:"Use Amazon SQS to save session data",isCorrect:!1}],explanation:'The DynamoDB Session Handler is a custom session handler for PHP that allows developers to use Amazon DynamoDB as a session store. Using DynamoDB for session storage alleviates issues that occur with session handling in a distributed web application by moving sessions off of the local file system and into a shared location. DynamoDB is fast, scalable, easy to setup, and handles replication of your data automatically.CORRECT: "Use Amazon DynamoDB to perform scalable session handling" is the correct answer.INCORRECT: "Use sticky sessions with an Elastic Load Balancer target group" is incorrect as this involves maintaining session state data on the EC2 instances which means that data is lost if an instance fails.INCORRECT: "Use Amazon SQS to save session data" is incorrect as SQS is not used for session data, it is used for application component decoupling.INCORRECT: "Use Elastic Load Balancer connection draining to stop sending requests to failing instances" is incorrect as this does not solve the problem of ensuring the session data is available, the data will be on the failing instance and will be lost.References: https://docs.aws.amazon.com/awssdkphp/v2/guide/featuredynamodbsessionhandler.html'},{question:"A company runs a booking system for a medical practice. The AWS SDK is used to communicate with between several AWS services. Due to compliance requirements, the security department has requested that a record is made of all API calls. How can this requirement be met?",answers:[{text:"Use AWS XRay to trace the API calls and keep a record",isCorrect:!1},{text:"Use Amazon CloudWatch logs to keep a history of API calls",isCorrect:!1},{text:"Use an AWS Lambda to function to continually monitor API calls and log them to an Amazon S3 bucket",isCorrect:!1},{text:"Use Amazon CloudTrail to keep a history of API calls",isCorrect:!0}],explanation:'AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services.This event history simplifies security analysis, resource change tracking, and troubleshooting. In addition, you can use CloudTrail to detect unusual activity in your AWS accounts. These capabilities help simplify operational analysis and troubleshooting.CloudWatch vs CloudTrail:As this scenario requests that a history of API calls are retained (auditing), AWS CloudTrail is the correct solution to use.CORRECT: "Use Amazon CloudTrail to keep a history of API calls" is the correct answer.INCORRECT: "Use Amazon CloudWatch logs to keep a history of API calls" is incorrect as this does not keep a record of API activity. CloudWatch records metrics related to performance.INCORRECT: "Use AWS XRay to trace the API calls and keep a record" is incorrect as XRay does not trace API calls for auditing.INCORRECT: "Use an AWS Lambda to function to continually monitor API calls and log them to an Amazon S3 bucket" is incorrect as this is totally unnecessary when CloudTrail can do this for you.References: https://aws.amazon.com/cloudtrail/'},{question:'The following permissions policy is applied to an IAM user account:{"Version": "20121017","Statement": [{"Effect": "Allow","Action": "sqs:*","Resource": "arn:aws:sqs:*:513246782345:stagingqueue*"}]}Due to this policy, what Amazon SQS actions will the user be able to perform?',answers:[{text:"The user will be able to apply a resourcebased policy to the Amazon SQS queue named “stagingqueue”",isCorrect:!1},{text:"The user will be able to create a queue named “stagingqueue“",isCorrect:!1},{text:"The user will be granted crossaccount access from account number “513246782345” to queue “stagingqueue”",isCorrect:!1},{text:"The user will be able to use all Amazon SQS actions, but only for queues with names begin with the string “stagingqueue“",isCorrect:!0}],explanation:'The policy allows the user to use all Amazon SQS actions, but only with queues whose names are prefixed with the literal string “stagingqueue”. This policy is useful to provide a queue creator the ability to use Amazon SQS actions. Any user who has permissions to create a queue must also have permissions to use other Amazon SQS actions in order to do anything with the created queues.CORRECT: "The user will be able to use all Amazon SQS actions, but only for queues with names begin with the string “stagingqueue“" is the correct answer.INCORRECT: "The user will be able to create a queue named “stagingqueue“" is incorrect as this policy provides the permissions to perform SQS actions on an existing queue.INCORRECT: "The user will be able to apply a resourcebased policy to the Amazon SQS queue named “stagingqueue”" is incorrect as this is a single operation and the permissions policy allows all SQS actions.INCORRECT: "The user will be granted crossaccount access from account number “513246782345” to queue “stagingqueue” is incorrect as this is not a policy for granting crossaccount access. The account number and queue relate to the same account.References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsoverviewofmanagingaccess.html'},{question:"A company uses Amazon SQS to decouple an online application that generates memes. The SQS consumers poll the queue regularly to keep throughput high and this is proving to be costly and resource intensive. A Developer has been asked to review the system and propose changes that can reduce costs and the number of empty responses. What would be the BEST approach to MINIMIZING cost?",answers:[{text:"Set the Imaging queue ReceiveMessageWaitTimeSeconds attribute to 20 seconds",isCorrect:!0},{text:"Set the imaging queue MessageRetentionPeriod attribute to 20 seconds",isCorrect:!1},{text:"Set the DelaySeconds parameter of a message to 20 seconds",isCorrect:!1},{text:"Set the imaging queue visibility Timeout attribute to 20 seconds",isCorrect:!1}],explanation:'The process of consuming messages from a queue depends on whether you use short or long polling. By default, Amazon SQS uses short polling, querying only a subset of its servers (based on a weighted random distribution) to determine whether any messages are available for a response. You can use long polling to reduce your costs while allowing your consumers to receive messages as soon as they arrive in the queue.When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren\'t included in a response).Therefore, the best way to optimize resource usage and reduce the number of empty responses (and cost) is to configure long polling by setting the Imaging queue ReceiveMessageWaitTimeSeconds attribute to 20 seconds.CORRECT: "Set the Imaging queue ReceiveMessageWaitTimeSeconds attribute to 20 seconds" is the correct answer.INCORRECT: "Set the imaging queue visibility Timeout attribute to 20 seconds" is incorrect. This attribute configures message visibility which will not reduce empty responses.INCORRECT: "Set the imaging queue MessageRetentionPeriod attribute to 20 seconds" is incorrect. This attribute sets the length of time, in seconds, for which Amazon SQS retains a message.INCORRECT: "Set the DelaySeconds parameter of a message to 20 seconds" is incorrect. This attribute sets the length of time, in seconds, for which the delivery of all messages in the queue is delayed.References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsshortandlongpolling.html'},{question:"A developer is planning to launch as serverless application composed of AWS Lambda, Amazon API Gateway, and Amazon DynamoDB. What is the EASIEST way to deploy the application using simple syntax?",answers:[{text:"Use AWS Elastic Beanstalk",isCorrect:!1},{text:"Use the Serverless Application Model",isCorrect:!0},{text:"Use AWS CloudFormation",isCorrect:!1},{text:"Use the Serverless Application Repository",isCorrect:!1}],explanation:'The AWS Serverless Application Model (SAM) is an opensource framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax, enabling you to build serverless applications faster.To get started with building SAMbased applications, use the AWS SAM CLI. SAM CLI provides a Lambdalike execution environment that lets you locally build, test, and debug applications defined by SAM templates. You can also use the SAM CLI to deploy your applications to AWS.With the SAM CLI you can package and deploy your source code using two simple commands:• sam package• sam deployAlternatively, you can use:• aws cloudformation package• aws cloudformation deployThe SAM CLI is therefore the easiest way to deploy serverless applications on AWS.CORRECT: "Use the Serverless Application Model" is the correct answer.INCORRECT: "Use the Serverless Application Repository " is incorrect as this is a managed repository for serverless applications.INCORRECT: "Use AWS CloudFormation" is incorrect as this would not be the simplest way to package and deploy this infrastructure. Without using SAM, you would need to build out a much more complex AWS CloudFormation template yourself.INCORRECT: "Use AWS Elastic Beanstalk" is incorrect as Elastic Beanstalk cannot be used to deploy Lambda, API Gateway or DynamoDB.References: https://aws.amazon.com/serverless/sam/'},{question:"A team of Developers have been assigned to a new project. The team will be collaborating on the development and delivery of a new application and need a centralized private repository for managing source code. The repository should support updates from multiple sources. Which AWS service should the development team use?",answers:[{text:"AWS CodePipeline",isCorrect:!1},{text:"AWS CodeCommit",isCorrect:!0},{text:"AWS CodeBuild",isCorrect:!1},{text:"AWS CodeDeploy",isCorrect:!1}],explanation:'CodeCommit is a secure, highly scalable, managed source control service that hosts private Git repositories. CodeCommit eliminates the need for you to manage your own source control system or worry about scaling its infrastructure. You can use CodeCommit to store anything from code to binaries. It supports the standard functionality of Git, so it works seamlessly with your existing Gitbased tools.With CodeCommit, you can:• Benefit from a fully managed service hosted by AWS. CodeCommit provides high service availability and durability and eliminates the administrative overhead of managing your own hardware and software. There is no hardware to provision and scale and no server software to install, configure, and update.• Store your code securely. CodeCommit repositories are encrypted at rest as well as in transit.• Work collaboratively on code. CodeCommit repositories support pull requests, where users can review and comment on each other\'s code changes before merging them to branches; notifications that automatically send emails to users about pull requests and comments; and more.• Easily scale your version control projects. CodeCommit repositories can scale up to meet your development needs.The service can handle repositories with large numbers of files or branches, large file sizes, and lengthy revision histories.• Store anything, anytime. CodeCommit has no limit on the size of your repositories or on the file types you can store.• Integrate with other AWS and thirdparty services. CodeCommit keeps your repositories close to your other production resources in the AWS Cloud, which helps increase the speed and frequency of your development lifecycle.It is integrated with IAM and can be used with other AWS services and in parallel with other repositories. Easily migrate files from other remote repositories. You can migrate to CodeCommit from any Gitbased repository.• Use the Git tools you already know. CodeCommit supports Git commands as well as its own AWS CLI commands and APIs.Therefore, the development team should select AWS CodeCommit as the repository they use for storing code related to the new project.CORRECT: "AWS CodeCommit" is the correct answer.INCORRECT: "AWS CodeBuild" is incorrect. AWS CodeBuild is a fully managed continuous integration (CI) service that compiles source code, runs tests, and produces software packages that are ready to deploy.INCORRECT: "AWS CodeDeploy" is incorrect. CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, onpremises instances, serverless Lambda functions, or Amazon ECS services.INCORRECT: "AWS CodePipeline" is incorrect. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.References: https://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html'},{question:"A customer-facing web application runs on Amazon EC2 with an Application Load Balancer and an Amazon RDS database back end. Recently, the security team noticed some SQL injection attacks and cross-site scripting attacks targeting the web application. Which service can a Developer use to protect against future attacks?",answers:[{text:"Security Groups",isCorrect:!1},{text:"Network ACLs",isCorrect:!1},{text:"AWS WAF",isCorrect:!0},{text:"AWS KMS",isCorrect:!1}],explanation:'AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources.AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or crosssite scripting, and rules that filter out specific traffic patterns you define.CORRECT: "AWS WAF" is the correct answer.INCORRECT: "AWS KMS" is incorrect as this service is used for creating and managing encryption keys.INCORRECT: "Security Groups" is incorrect as they are an instancelevel firewall. They do not have the ability to prevent SQL injection or crosssite scripting attacks.INCORRECT: "Network ACLs" is incorrect as this is a subnetlevel firewall. It doesn’t not have the ability to prevent SQL injection or crosssite scripting attacks.References: https://aws.amazon.com/waf/'},{question:"A team of developers are adding an API layer to a multi-container Docker environment running on AWS Elastic Beanstalk. The clients-submitted method requests should be passed directly to the backend, without modification. Which integration type is MOST suitable for this solution?",answers:[{text:"AWS",isCorrect:!1},{text:"HTTP",isCorrect:!1},{text:"AWS_PROXY",isCorrect:!1},{text:"HTTP_PROXY",isCorrect:!0}],explanation:'You choose an API integration type according to the types of integration endpoint you work with and how you want data to pass to and from the integration endpoint. For a Lambda function, you can have the Lambda proxy integration, or the Lambda custom integration.For an HTTP endpoint, you can have the HTTP proxy integration or the HTTP custom integration. For an AWS service action, you have the AWS integration of the nonproxy type only. API Gateway also supports the mock integration, where API Gateway serves as an integration endpoint to respond to a method request.As this is a Docker deployment running on Elastic Beanstalk the HTTP integration types are applicable. There are two options: HTTP: This type of integration lets an API expose HTTP endpoints in the backend. With the HTTP integration, also known as the HTTP custom integration, you must configure both the integration request and integration response. You must set up necessary data mappings from the method request to the integration request, and from the integration response to the method response.HTTP_PROXY: The HTTP proxy integration allows a client to access the backend HTTP endpoints with a streamlined integration setup on single API method. You do not set the integration request or the integration response. API Gateway passes the incoming request from the client to the HTTP endpoint and passes the outgoing response from the HTTP endpoint to the client. As we can see from the above explanation, the most suitable integration type for this deployment is going to be the HTTP_PROXY.CORRECT: "HTTP_PROXY" is the correct answer.INCORRECT: "HTTP" is incorrect as this is a custom integration that would be used if you need to customize the data mappings.INCORRECT: "AWS" is incorrect as this type of integration lets an API expose AWS service actions.INCORRECT: "AWS_PROXY" is incorrect as this type of integration lets an API method be integrated with the Lambda function invocation action with a flexible, versatile, and streamlined integration setup.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayapiintegrationtypes.html'},{question:"A Developer is creating a microservices architecture for a modern application. The application will run on Docker containers. The Developer requires a serverless service. Which AWS service is MOST suitable?",answers:[{text:"AWS Elastic Beanstalk",isCorrect:!1},{text:"AWS Fargate",isCorrect:!0},{text:"Amazon ECS",isCorrect:!1},{text:"AWS Lambda",isCorrect:!1}],explanation:'AWS Fargate is a serverless compute engine for Docker containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.As you can see in the image above, AWS Fargate is serverless, however, the EC2 launch type requires the management of EC2 instances. Therefore, the most suitable service for the Developer’s requirements is AWS Fargate.CORRECT: "AWS Fargate" is the correct answer.INCORRECT: "Amazon ECS" is incorrect as this is not a serverless service, it requires the launching and management of EC2 instances.INCORRECT: "AWS Elastic Beanstalk" is incorrect as this is also not a serverless service. It uses EC2 instances that are partially managed for you but must be scaled.INCORRECT: "AWS Lambda" is incorrect as though this is a serverless service, it is not used for running Docker containers.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html'},{question:"An application is using Amazon DynamoDB as its data store and needs to be able to read 200 items per second as eventually consistent reads. Each item is 12 KB in size. What value should be set for the table's provisioned throughput for reads?",answers:[{text:"300 Read Capacity Units",isCorrect:!0},{text:"600 Read Capacity Units",isCorrect:!1},{text:"1200 Read Capacity Units",isCorrect:!1},{text:"150 Read Capacity Units",isCorrect:!1}],explanation:'With provisioned capacity mode, you specify the number of data reads and writes per second that you require for your application.Read capacity unit (RCU):• Each API call to read data from your table is a read request.• Read requests can be strongly consistent, eventually consistent, or transactional.• For items up to 4 KB in size, one RCU can perform one strongly consistent read request per second.• Items larger than 4 KB require additional RCUs.• For items up to 4 KB in size, one RCU can perform two eventually consistent read requests per second.• Transactional read requests require two RCUs to perform one read per second for items up to 4 KB.• For example, a strongly consistent read of an 8 KB item would require two RCUs, an eventually consistent read of an 8 KB item would require one RCU, and a transactional read of an 8 KB item would require four RCUs.Write capacity unit (WCU):• Each API call to write data to your table is a write request.• For items up to 1 KB in size, one WCU can perform one standard write request per second.• Items larger than 1 KB require additional WCUs.• Transactional write requests require two WCUs to perform one write per second for items up to 1 KB.• For example, a standard write request of a 1 KB item would require one WCU, a standard write request of a 3 KB item would require three WCUs, and a transactional write request of a 3 KB item would require six WCUs.To determine the number of RCUs required to handle 200 eventually consistent reads per/second with an average item size of 12KB, perform the following steps:1. Determine the average item size by rounding up the next multiple of 4KB (12KB rounds up to 12KB).2. Determine the RCU per item by dividing the item size by 8KB (12KB/8KB = 1.5).3. Multiply the value from step 2 with the number of reads required per second (1.5x200 = 300).CORRECT: "300 Read Capacity Units" is the correct answer.INCORRECT: "600 Read Capacity Units" is incorrect. This would be the value for strongly consistent reads.INCORRECT: "1200 Read Capacity Units" is incorrect. This would be the value for transactional reads.INCORRECT: "150 Read Capacity Units" is incorrect.References: https://aws.amazon.com/dynamodb/pricing/provisioned/'},{question:"A Developer manages a monitoring service for a fleet of IoT sensors in a major city. The monitoring application uses an Amazon Kinesis Data Stream with a group of EC2 instances processing the data. Amazon CloudWatch custom metrics show that the instances a reaching maximum processing capacity and there are insufficient shards in the Data Stream to handle the rate of data flow. What course of action should the Developer take to resolve the performance issues?",answers:[{text:"Increase the EC2 instance size",isCorrect:!1},{text:"Increase the number of EC2 instances to match the number of shards",isCorrect:!1},{text:"Increase the number of open shards",isCorrect:!1},{text:"Increase the EC2 instance size and add shards to the stream",isCorrect:!0}],explanation:'By increasing the instance size and number of shards in the Kinesis stream, the developer can allow the instances to handle more record processors, which are running in parallel within the instance. It also allows the stream to properly accommodate the rate of data being sent in. The data capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the capacities of its shards.Therefore, the best answer is to increase both the EC2 instance size and add shards to the stream.CORRECT: "Increase the EC2 instance size and add shards to the stream" is the correct answer.INCORRECT: "Increase the number of EC2 instances to match the number of shards" is incorrect as you can have an individual instance running multiple KCL workers.INCORRECT: "Increase the EC2 instance size" is incorrect as the Developer would also need to add shards to the stream to increase the capacity of the stream.INCORRECT: "Increase the number of open shards" is incorrect as this does not include increasing the instance size or quantity which is required as they are running at capacity.https://docs.aws.amazon.com/streams/latest/dev/kinesisrecordprocessorscaling.partial.html https://docs.aws.amazon.com/streams/latest/dev/developingconsumerswithkcl.html'},{question:"A Developer is using AWS SAM to create a template for deploying a serverless application. The Developer plans deploy a Lambda function using the template. Which resource type should the Developer specify?",answers:[{text:"AWS::Serverless:API",isCorrect:!1},{text:"AWS::Serverless:Function",isCorrect:!0},{text:"AWS::Serverless::Application",isCorrect:!1},{text:"AWS::Serverless:LayerVersion",isCorrect:!1}],explanation:'A serverless application is a combination of Lambda functions, event sources, and other resources that work together to perform tasks. Note that a serverless application is more than just a Lambda function—it can include additional resources such as APIs, databases, and event source mappings.AWS SAM templates are an extension of AWS CloudFormation templates, with some additional components that make them easier to work with. To create a Lambda function using an AWS SAM template the Developer can use theAWS::Serverless::Function resource type.The AWS::Serverless::Function resource type can be used to Create a Lambda function, IAM execution role, and event source mappings that trigger the function.CORRECT: "AWS::Serverless:Function" is the correct answer.INCORRECT: "AWS::Serverless::Application" is incorrect as this embeds a serverless application from the AWS Serverless Application Repository or from an Amazon S3 bucket as a nested application.INCORRECT: "AWS::Serverless:LayerVersion" is incorrect as this creates a Lambda LayerVersion that contains library or runtime code needed by a Lambda Function.INCORRECT: "AWS::Serverless:API" is incorrect as this creates a collection of Amazon API Gateway resources and methods that can be invoked through HTTPS endpoints.References: https://docs.aws.amazon.com/serverlessapplicationmodel/latest/developerguide/whatissam.html'},{question:"A Developer has updated an AWS Lambda function and published a new version. To ensure the code is working as expected the Developer needs to initially direct a percentage of traffic to the new version and gradually increase this over time. It is important to be able to rollback if there are any issues reported. What is the BEST way the Developer can implement the migration to the new version SAFELY?",answers:[{text:"Use an Amazon Elastic Load Balancer to direct a percentage of traffic to each target group containing the Lambda function versions",isCorrect:!1},{text:"Use an immutable update with a new ASG to deploy the new version in parallel, following testing cutover to the new version",isCorrect:!1},{text:"Create an Amazon Route 53 weighted routing policy pointing to the current and new versions, assign a lower weight to the new version",isCorrect:!1},{text:"Create an Alias, assign the current and new versions and use traffic shifting to assign a percentage of traffic to the new version",isCorrect:!0}],explanation:'You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN.Each alias has a unique ARN. An alias can only point to a function version, not to another alias. You can update an alias to point to a new version of the function. You can also use traffic shifting to direct a percentage of traffic to a specific version as showing in the image below: This is the recommended way to direct traffic to multiple function versions and shift traffic when testing code updated.Therefore, the best answer is to create an Alias, assign the current and new versions and use traffic shifting to assign a percentage of traffic to the new version.CORRECT: "Create an Alias, assign the current and new versions and use traffic shifting to assign a percentage of traffic to the new version" is the correct answer.INCORRECT: "Create an Amazon Route 53 weighted routing policy pointing to the current and new versions, assign a lower weight to the new version" is incorrect. AWS Lambda endpoints are not DNS names that you can route to with Route 53. The best way to route traffic to multiple versions is using an alias.INCORRECT: "Use an immutable update with a new ASG to deploy the new version in parallel, following testing cutover to the new version" is incorrect as immutable updates are associated with Amazon Elastic Beanstalk and this service does not deploy updates to AWS Lambda.INCORRECT: "Use an Amazon Elastic Load Balancer to direct a percentage of traffic to each target group containing the Lambda function versions" is incorrect as this introduces an unnecessary layer (complexity and cost) to the architecture. The best choice is to use an alias instead.References:https://docs.amazonaws.cn/en_us/lambda/latest/dg/configurationaliases.html https://docs.aws.amazon.com/lambda/latest/dg/aliasesintro.html https://docs.aws.amazon.com/lambda/latest/dg/lambdatrafficshiftingusingaliases.html'},{question:"An application uses AWS Lambda to process many files. The Lambda function takes approximately 3 minutes to process each file and does not return any important data. A Developer has written a script that will invoke the function using the AWS CLI. What is the FASTEST way to process all the files?",answers:[{text:"Invoke the Lambda function asynchronously with the invocation type RequestResponse and process the files sequentially",isCorrect:!1},{text:"Invoke the Lambda function synchronously with the invocation type Event and process the files in parallel",isCorrect:!1},{text:"Invoke the Lambda function synchronously with the invocation type RequestResponse and process the files sequentially",isCorrect:!1},{text:"Invoke the Lambda function asynchronously with the invocation type Event and process the files in parallel",isCorrect:!0}],explanation:'You can invoke Lambda functions directly with the Lambda console, the Lambda API, the AWS SDK, the AWS CLI, and AWS toolkits.You can also configure other AWS services to invoke your function, or you can configure Lambda to read from a stream or queue and invoke your function.When you invoke a function, you can choose to invoke it synchronously or asynchronously.• Synchronous invocation:o You wait for the function to process the event and return a response.o To invoke a function synchronously with the AWS CLI, use the invoke command.o The Invocationtype can be used to specify a value of “RequestResponse”. This instructs AWS to execute your Lambda function and wait for the function to complete.• Asynchronous invocation:o When you invoke a function asynchronously, you don’t wait for a response from the function code.o For asynchronous invocation, Lambda handles retries and can send invocation records to a destination.o To invoke a function asynchronously, set the invocation type parameter to Event.The fastest way to process all the files is to use asynchronous invocation and process the files in parallel. To do this you should specify the invocation type of EventCORRECT: "Invoke the Lambda function asynchronously with the invocation type Event and process the files in parallel" is the correct answer.INCORRECT: "Invoke the Lambda function synchronously with the invocation type Event and process the files in parallel" is incorrect as the invocation type for a synchronous invocation should be RequestResponse.INCORRECT: "Invoke the Lambda function synchronously with the invocation type RequestResponse and process the files sequentially" is incorrect as this is not the fastest way of processing the files as Lambda will wait for completion of once file before moving on to the next one.INCORRECT: "Invoke the Lambda function asynchronously with the invocation type RequestResponse and process the files sequentially" is incorrect as the invocation type RequestResponse is used for synchronous invocations.References: https://aws.amazon.com/blogs/architecture/understandingthedifferentwaystoinvokelambdafunctions/'},{question:"A development team have deployed a new application and users have reported some performance issues. The developers need to enable monitoring for specific metrics with a data granularity of one second. How can this be achieved?",answers:[{text:"Create custom metrics and enable detailed monitoring",isCorrect:!1},{text:"Do nothing, CloudWatch uses standard resolution metrics by default",isCorrect:!1},{text:"Create custom metrics and configure them as standard resolution",isCorrect:!1},{text:"Create custom metrics and configure them as high resolution",isCorrect:!0}],explanation:'You can publish your own metrics to CloudWatch using the AWS CLI or an API. You can view statistical graphs of your published metrics with the AWS Management Console.CloudWatch stores data about a metric as a series of data points. Each data point has an associated time stamp. You can even publish an aggregated set of data points called a statistic set. Each metric is one of the following:• Standard resolution, with data having a oneminute granularity• High resolution, with data at a granularity of one second Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a highresolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of60 seconds. Highresolution metrics can give you more immediate insight into your application\'s subminute activity. Keep in mind that every PutMetricData call for a custom metric is charged, so calling PutMetricData more often on a highresolution metric can lead to higher charges.Therefore, the best action to take is to Create custom metrics and configure them as high resolution. This will ensure that granularity can be down to 1 second.CORRECT: "Create custom metrics and configure them as high resolution" is the correct answer.INCORRECT: "Do nothing, CloudWatch uses standard resolution metrics by default" is incorrect as standard resolution has a granularity of oneminute.INCORRECT: "Create custom metrics and configure them as standard resolution" is incorrect as standard resolution has a granularity of oneminute.INCORRECT: "Create custom metrics and enable detailed monitoring" is incorrect as detailed monitoring has a granularity of oneminute.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html'},{question:"A team of Developers are building a continuous integration and delivery pipeline using AWS Developer Tools. Which services should they use for running tests against source code and installing compiled code on their AWS resources? (Select TWO.)",answers:[{text:"AWS CodeBuild for running tests against source code",isCorrect:!0},{text:"AWS Cloud9 for running tests against source code",isCorrect:!1},{text:"AWS CodeDeploy for installing compiled code on their AWS resources",isCorrect:!0},{text:"AWS CodeCommit for installing compiled code on their AWS resources",isCorrect:!1},{text:"AWS CodePipeline for running tests against source code",isCorrect:!1}],explanation:'AWS CodeBuild is a fully managed build service in the cloud. CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. It provides prepackaged build environments for popular programming languages and build tools such as Apache Maven, Gradle, and more.CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, onpremises instances, serverless Lambda functions, or Amazon ECS services.CORRECT: "AWS CodeBuild for running tests against source code" is a correct answer.CORRECT: "AWS CodeDeploy for installing compiled code on their AWS resources" is also a correct answer.INCORRECT: "AWS CodePipeline for running tests against source code" is incorrect. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. This service works with the other Developer Tools to create a pipeline.INCORRECT: "AWS CodeCommit for installing compiled code on their AWS resources" is incorrect as AWS CodeCommit is a fullymanaged source control service that hosts secure Gitbased repositories.INCORRECT: "AWS Cloud9 for running tests against source code" is incorrect as AWS Cloud9 is a cloudbased integrated development environment (IDE) that lets you write, run, and debug your code with just a browser.References:https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html'},{question:"An application writes items to an Amazon DynamoDB table. As the application scales to thousands of instances, calls to the DynamoDB API generate occasional ThrottlingException errors. The application is coded in a language incompatible with the AWS SDK. How should the error be handled?",answers:[{text:"Pass API calls through Amazon API Gateway",isCorrect:!1},{text:"Add exponential backoff to the application logic",isCorrect:!0},{text:"Send the items to DynamoDB through Amazon Kinesis Data Firehose",isCorrect:!1},{text:"Use Amazon SQS as an API message bus",isCorrect:!1}],explanation:'Exponential backoff can improve an application\'s reliability by using progressively longer waits between retries. When using the AWS SDK, this logic is builtin. However, in this case the application is incompatible with the AWS SDK so it is necessary to manually implement exponential backoff.CORRECT: "Add exponential backoff to the application logic" is the correct answer.INCORRECT: "Use Amazon SQS as an API message bus" is incorrect as SQS requires instances or functions to pick up and process the messages and put them in the DynamoDB table. This is unnecessary cost and complexity and will not improve performance.INCORRECT: "Pass API calls through Amazon API Gateway" is incorrect as this is not a suitable method of throttling the application. Exponential backoff logic in the application is a better solution.INCORRECT: "Send the items to DynamoDB through Amazon Kinesis Data Firehose" is incorrect as DynamoDB is not a destination for Kinesis Data Firehose.References: https://aws.amazon.com/premiumsupport/knowledgecenter/dynamodbtablethrottled/'},{question:"A Developer has completed some code updates and needs to deploy the updates to an Amazon Elastic Beanstalk environment. Due to the criticality of the application, the ability to quickly roll back must be prioritized of any other considerations. Which deployment policy should the Developer choose?",answers:[{text:"Rolling",isCorrect:!1},{text:"Immutable",isCorrect:!0},{text:"Rolling with additional batch",isCorrect:!1},{text:"All at once",isCorrect:!1}],explanation:'AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments.Each deployment policy has advantages and disadvantages and it’s important to select the best policy to use for each situation.The following tables summarizes the different deployment policies:The “immutable” policy will create a new ASG with a whole new set of instances and deploy the updates there.Immutable:• Launches new instances in a new ASG and deploys the version update to these instances before swapping traffic to these instances once healthy.• Zero downtime.• New code is deployed to new instances using an ASG.• High cost as double the number of instances running during updates.• Longest deployment.• Quick rollback in case of failures.• Great for production environments.For this scenario a quick rollback must be prioritized over all other considerations. Therefore, the best choice is “immutable”.This deployment policy is the most expensive and longest (duration) option. However, you can roll back quickly and safely as the original instances are all available and unmodified.CORRECT: "Immutable" is the correct answer.INCORRECT: "Rolling" is incorrect as this policy requires manual redeployment if there are any issues caused by the update.INCORRECT: "Rolling with additional batch" is incorrect as this policy requires manual redeployment if there are any issues caused by the update.INCORRECT: "All at once" is incorrect as this takes the entire environment down at once and requires manual redeployment if there are any issues caused by the update.References:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.rollingversiondeploy.html'},{question:"A developer is preparing the resources for creating a multicontainer Docker environment on AWS Elastic Beanstalk. How can the developer define the Docker containers?",answers:[{text:"Define the containers in the Dockerrun.aws.json file in JSON format and save at the root of the source directory",isCorrect:!0},{text:"Create a buildspec.yml file and save it at the root of the source directory",isCorrect:!1},{text:"Define the containers in the Dockerrun.aws.json file in YAML format and save at the root of the source directory",isCorrect:!1},{text:"Create a Docker.config file and save it in the .ebextensions folder at the root of the source directory",isCorrect:!1}],explanation:'You can launch a cluster of multicontainer instances in a singleinstance or autoscaling Elastic Beanstalk environment using the Elastic Beanstalk console. The single container and multicontainer Docker platforms for Elastic Beanstalk support the use of Docker images stored in a public or private online image repository. You specify images by name in the Dockerrun.aws.json file and save it in the root of your source directory.CORRECT: "Define the containers in the Dockerrun.aws.json file in JSON format and save at the root of the source directory" is the correct answer.INCORRECT: "Create a Docker.config file and save it in the .ebextensions folder at the root of the source directory" is incorrect as the you need to create a Dockerrun.aws.json file, not a Dokcer.config file and it should be saved at the root of the source directory not in the .ebextensions folder.INCORRECT: "Define the containers in the Dockerrun.aws.json file in YAML format and save at the root of the source directory" is incorrect because the contents of the file should be in JSON format, not YAML format.INCORRECT: "Create a buildspec.yml file and save it at the root of the source directory" is incorrect as the buildspec.yml file is used with AWS CodeBuild, not Elastic Beanstalk.References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_v2config.html'},{question:"An application is instrumented to generate traces using AWS XRay and generates a large amount of trace data. A Developer would like to use filter expressions to filter the results to specific key-value pairs added to custom subsegments. How should the Developer add the key-value pairs to the custom subsegments?",answers:[{text:"Add metadata to the custom subsegments",isCorrect:!1},{text:"Add the keyvalue pairs to the Trace ID",isCorrect:!1},{text:"Setup sampling for the custom subsegments",isCorrect:!1},{text:"Add annotations to the custom subsegments",isCorrect:!0}],explanation:'You can record additional information about requests, the environment, or your application with annotations and metadata. You can add annotations and metadata to the segments that the XRay SDK creates, or to custom subsegments that you create.Annotations are keyvalue pairs with string, number, or Boolean values. Annotations are indexed for use with filter expressions.Use annotations to record data that you want to use to group traces in the console, or when calling the GetTraceSummaries API.Metadata are keyvalue pairs that can have values of any type, including objects and lists, but are not indexed for use with filter expressions. Use metadata to record additional data that you want stored in the trace but don\'t need to use with search.Annotations can be used with filter expressions, so this is the best solution for this requirement. The Developer can add annotations to the custom subsegments and will then be able to use filter expressions to filter the results in AWS XRay.CORRECT: "Add annotations to the custom subsegments" is the correct answer.INCORRECT: "Add metadata to the custom subsegments" is incorrect as though you can add metadata to custom subsegments it is not indexed and cannot be used with filters.INCORRECT: "Add the keyvalue pairs to the Trace ID" is incorrect as this is not something you can do.INCORRECT: "Setup sampling for the custom subsegments " is incorrect as this is a mechanism used by XRay to send only statistically significant data samples to the API.References: https://docs.aws.amazon.com/xray/latest/devguide/xraysdkjavasegment.html'},{question:"A company is creating an application that will require users to access AWS services and allow them to reset their own passwords. Which of the following would allow the company to manage users and authorization while allowing users to reset their own passwords?",answers:[{text:"Amazon Cognito identity pools and AWS IAM",isCorrect:!1},{text:"Amazon Cognito user pools and AWS KMS",isCorrect:!1},{text:"Amazon Cognito identity pools and AWS STS",isCorrect:!1},{text:"Amazon Cognito user pools and identity pools",isCorrect:!0}],explanation:'There are two key requirements in this scenario. Firstly the company wants to manage user accounts using a system that allows users to reset their own passwords. The company also wants to authorize users to access AWS services.The first requirement is provided by an Amazon Cognito User Pool. With a Cognito user pool you can add signup and signin to mobile and web apps and it also offers a user directory so user accounts can be created directly within the user pool. Users also have the ability to reset their passwords.To access AWS services you need a Cognito Identity Pool. An identity pool can be used with a user pool and enables a user to obtain temporary limitedprivilege credentials to access AWS services.Therefore, the best answer is to use Amazon Cognito user pools and identity pools.CORRECT: "Amazon Cognito user pools and identity pools" is the correct answer.INCORRECT: "Amazon Cognito identity pools and AWS STS" is incorrect as there is no user directory in this solution. A Cognito user pool is required.INCORRECT: "Amazon Cognito identity pools and AWS IAM" is incorrect as a Cognito user pool should be used as the directory source for creating and managing users. IAM is used for accounts that are used to administer AWS services, not for application user access.INCORRECT: "Amazon Cognito user pools and AWS KMS" is incorrect as KMS is used for encryption, not for authentication to AWS services.References: https://docs.aws.amazon.com/cognito/latest/developerguide/cognitouseridentitypools.html'},{question:"A Developer is designing a cloud native application. The application will use several AWS Lambda functions that will process items that the functions read from an event source. Which AWS services are supported for Lambda event source mappings? (Select THREE.)",answers:[{text:"Amazon Kinesis",isCorrect:!0},{text:"Another Lambda function",isCorrect:!1},{text:"Amazon DynamoDB",isCorrect:!0},{text:"Amazon Simple Notification Service (SNS)",isCorrect:!1},{text:"Amazon Simple Storage Service (S3)",isCorrect:!1},{text:"Amazon Simple Queue Service (SQS)",isCorrect:!0}],explanation:'An event source mapping is an AWS Lambda resource that reads from an event source and invokes a Lambda function. You can use event source mappings to process items from a stream or queue in services that don\'t invoke Lambda functions directly. Lambda provides event source mappings for the following services.Services That Lambda Reads Events From• Amazon Kinesis• Amazon DynamoDB• Amazon Simple Queue ServiceAn event source mapping uses permissions in the function\'s execution role to read and manage items in the event source.Permissions, event structure, settings, and polling behavior vary by event source.CORRECT: "Amazon Kinesis, Amazon DynamoDB, and Amazon Simple Queue Service (SQS)" are the correct answers.INCORRECT: "Amazon Simple Notification Service (SNS)" is incorrect as SNS should be used as destination for asynchronous invocation.INCORRECT: "Amazon Simple Storage Service (S3)" is incorrect as Lambda does not read from Amazon S3, you must configure the event notification on the S3 side.INCORRECT: "Another Lambda function" is incorrect as another function should be invoked asynchronously.References: https://docs.aws.amazon.com/lambda/latest/dg/invocationeventsourcemapping.html'},{question:"AWS CodeBuild builds code for an application, creates a Docker image, pushes the image to Amazon Elastic Container Registry (ECR), and tags the image with a unique identifier. If the Developers already have AWS CLI configured on their workstations, how can the Docker images be pulled to the workstations?",answers:[{text:"Run the following: docker pull REPOSITORY URI : TAG",isCorrect:!1},{text:"Run the output of the following: aws ecr getdownloadurlforlayer, and then run docker pull REPOSITORY URI : TAG",isCorrect:!1},{text:"Run the output of the following: aws ecr getlogin, and then run docker pull REPOSITORY URI : TAG",isCorrect:!0},{text:"Run the following: aws ecr getlogin, and then run: docker pull REPOSITORY URI : TAG",isCorrect:!1}],explanation:'If you would like to run a Docker image that is available in Amazon ECR, you can pull it to your local environment with the docker pull command. You can do this from either your default registry or from a registry associated with another AWS account.Docker CLI does not support standard AWS authentication methods, so client authentication must be handled so that ECR knows who is requesting to push or pull an image. To do this you can issue the aws ecr getlogin or aws ecr getloginpassword (AWS CLI v2) and then use the output to login using docker login and then issue a docker pull command specifying the image name using registry/repository[:tag].CORRECT: "Run the output of the following: aws ecr getlogin, and then run docker pull REPOSITORY URI : TAG" is the correct answer.INCORRECT: "Run the following: docker pull REPOSITORY URI : TAG" is incorrect as the Developers first need to authenticate before they can pull the image.INCORRECT: "Run the following: aws ecr getlogin, and then run: docker pull REPOSITORY URI : TAG" is incorrect. TheDevelopers need to not just run the login command but run the output of the login command which contains the authentication token required to log in.INCORRECT: "Run the output of the following: aws ecr getdownloadurlforlayer, and then run docker pull REPOSITORY URI : TAG" is incorrect as this command retrieves a presigned Amazon S3 download URL corresponding to an image layer.References:https://docs.aws.amazon.com/AmazonECR/latest/userguide/Registries.html#registry_auth https://docs.aws.amazon.com/AmazonECR/latest/userguide/dockerpullecrimage.html'}]},{id:"aws-developer-4",title:"AWS Developer Associate (DVA-C02) Practice Test Latest 2025 4",description:"Additional practice questions covering AWS development topics.",questions:[{question:"A Development team are creating a new REST API that uses Amazon API Gateway and AWS Lambda. To support testing there need to be different versions of the service. What is the BEST way to provide multiple versions of the REST API?",answers:[{text:"Create an API Gateway resource policy to isolate versions and provide context to the Lambda functions",isCorrect:!1},{text:"Deploy an HTTP Proxy integration and configure the proxy with API versions",isCorrect:!1},{text:"Deploy the API versions as unique stages with unique endpoints and use stage variables to provide further context",isCorrect:!0},{text:"Create an AWS Lambda authorizer to route API clients to the correct API version",isCorrect:!1}],explanation:'A stage is a named reference to a deployment, which is a snapshot of the API. You use a Stage to manage and optimize a particular deployment. For example, you can set up stage settings to enable caching, customize request throttling, configure logging, define stage variables or attach a canary release for testing. APIs are deployed to stages:Stage variables are namevalue pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates.With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints. For example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, http://example.com).In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage, and calls a different web host (for example, beta.example.com). Similarly, stage variables can be used to specify a different AWS Lambda function name for each stage in your API.Therefore, for this scenario the Developers can deploy the API versions as unique stages with unique endpoints and use stage variables to provide further context such as connections to different backend services.CORRECT: "Deploy the API versions as unique stages with unique endpoints and use stage variables to provide further context" is the correct answer.INCORRECT: "Create an API Gateway resource policy to isolate versions and provide context to the Lambda functions" is incorrect. API Gateway resource policies are JSON policy documents that you attach to an API to control whether a specified principal (typically, an IAM user or role) can invoke the API.INCORRECT: "Create an AWS Lambda authorizer to route API clients to the correct API version" is incorrect. A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API. This is not used for routing API clients to different versions.INCORRECT: "Deploy an HTTP Proxy integration and configure the proxy with API versions" is incorrect. The HTTP proxy integration allows a client to access the backend HTTP endpoints with a streamlined integration setup on a single API method. This is not used for providing multiple versions of the API, use stages and stage variables instead.References:https://docs.aws.amazon.com/apigateway/latest/developerguide/setupstages.html https://docs.aws.amazon.com/apigateway/latest/developerguide/stagevariables.html'},{question:"A Developer is creating a serverless application that includes Amazon API Gateway, Amazon DynamoDB and AWS Lambda. The Developer will use AWS CloudFormation to deploy the application and is creating a template. Which tool should the Developer use to define simplified syntax for expressing serverless resources?",answers:[{text:"AWS Serverless Application Model",isCorrect:!0},{text:"OpenAPI Swagger Specification",isCorrect:!1},{text:"AWS Step Functions State Machine",isCorrect:!1},{text:"A CloudFormation Serverless Plugin",isCorrect:!1}],explanation:'The AWS Serverless Application Model (AWS SAM) is an opensource framework that you can use to build serverless applications on AWS. A serverless application is a combination of Lambda functions, event sources, and other resources that work together to perform tasks.Note: A serverless application is more than just a Lambda function—it can include additional resources such as APIs, databases, and event source mappings.You can use AWS SAM to define your serverless applications. AWS SAM consists of the following components:• AWS SAM template specification. You use this specification to define your serverless application. It provides you with a simple and clean syntax to describe the functions, APIs, permissions, configurations, and events that make up aserverless application.• AWS SAM command line interface (AWS SAM CLI). You use this tool to build serverless applications that are defined by AWS SAM templates.CORRECT: "AWS Serverless Application Model" is the correct answer.INCORRECT: "AWS Step Functions State Machine" is incorrect. AWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. It does not provide a simplified syntax for expressing serverless resources in a CloudFormation template.INCORRECT: "OpenAPI Swagger Specification" is incorrect. Swagger is an opensource software framework backed by a large ecosystem of tools that helps developers design, build, document, and consume RESTful web services.INCORRECT: "A CloudFormation Serverless Plugin" is incorrect as this does not exist.References: https://docs.aws.amazon.com/serverlessapplicationmodel/latest/developerguide/whatissam.html'},{question:"A batch job runs every 24 hours and writes around 1 million items into a DynamoDB table each day. The batch job completes quickly, and the items are processed within 2 hours and are no longer needed. What's the MOST efficient way to provide an empty table each day?",answers:[{text:"Use the BatchUpdateItem API with expressions",isCorrect:!1},{text:"Issue an AWS CLI aws dynamodb deleteitem command with a wildcard",isCorrect:!1},{text:"Use the BatchWriteItem API with a DeleteRequest",isCorrect:!1},{text:"Delete the entire table and recreate it each day",isCorrect:!0}],explanation:'With this scenario we have a table that has a large number of items quickly written to it on a recurring schedule. These items are no longer of use after they have been processed (within 2 hours) so from that point on until the next job the table is not being used. The items need to be deleted and we need to choose the most efficient (think cost as well as operations) way of doing this.Any delete operation will consume RCUs to scan/query the table and WCUs to delete the items. It will be much cheaper and simpler to just delete the table and recreate it again ahead of the next batch job. This can easily be automated through the API.CORRECT: "Delete the entire table and recreate it each day" is the correct answer.INCORRECT: "Use the BatchUpdateItem API with expressions" is incorrect as this API does not exist.INCORRECT: "Issue an AWS CLI aws dynamodb deleteitem command with a wildcard" is incorrect as this operation deletes data from a table one item at a time, which is highly inefficient. You also must specify the item\'s primary key values; you cannot use a wildcard.INCORRECT: "Use the BatchWriteItem API with a DeleteRequest" is incorrect as this is an inefficient way to solve this challenge.References: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchWriteItem.html https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/SQLtoNoSQL.DeleteData.html'},{question:"An AWS Lambda function has been connected to a VPC to access an application running a private subnet. The Lambda function also pulls data from an Internetbased service and is no longer able to connect to the Internet. How can this be rectified?",answers:[{text:"Add a NAT Gateway to a public subnet and specify a route in the private subnet",isCorrect:!0},{text:"Connect the Lambda function to an Internet Gateway",isCorrect:!1},{text:"Connect an AWS VPN to Lambda to connect to the Internet",isCorrect:!1},{text:"Add an Elastic IP to the Lambda function",isCorrect:!1}],explanation:'To enable connectivity to an application in a private subnet and the Internet you must first allow the function to connect to the private subnet (which has already been done).Lambda needs the following VPC configuration information so that it can connect to the VPC:• Private subnet ID.• Security Group ID (with required access).Lambda uses this information to setup an Elastic Network Interface (ENI) using an available IP address from your private subnet. Next you need to add a NAT Gateway for Internet access (no public IP). The NAT Gateway should be connected to a public subnet and a route needs to be added to the private subnet.CORRECT: "Add a NAT Gateway to the private subnet" is the correct answer.INCORRECT: "Connect the Lambda function to an Internet Gateway" is incorrect. Though by using a NAT Gateway you are effectively establishing routing to an Internet Gateway, you cannot actually connect Lambda to an Internet Gateway.INCORRECT: "Connect an AWS VPN to Lambda to connect to the Internet" is incorrect as you cannot connect an AWS VPN to a Lambda function.INCORRECT: "Add an Elastic IP to the Lambda function" is incorrect as you cannot add an Elastic IP to a Lambda function.References: https://docs.aws.amazon.com/lambda/latest/dg/configurationvpc.html'},{question:"A Developer is using AWS SAM to create a template for deploying a serverless application. The Developer plans to leverage an application from the AWS Serverless Application Repository in the template as a nested application. Which resource type should the Developer specify?",answers:[{text:"AWS::Serverless:SimpleTable",isCorrect:!1},{text:"AWS::Serverless:HttpApi",isCorrect:!1},{text:"AWS::Serverless:Function",isCorrect:!1},{text:"AWS::Serverless::Application",isCorrect:!0}],explanation:'A serverless application can include one or more nested applications. You can deploy a nested application as a standalone artifact or as a component of a larger application.As serverless architectures grow, common patterns emerge in which the same components are defined in multiple application templates. You can now separate out common patterns as dedicated applications, and then nest them as part of new or existing application templates. With nested applications, you can stay more focused on the business logic that\'s unique to yourapplication.To define a nested application in your serverless application, use the AWS::Serverless::Application resource type.CORRECT: "AWS::Serverless::Application" is the correct answer.INCORRECT: "AWS::Serverless:Function" is incorrect as this is used to define a serverless Lamdba function.INCORRECT: "AWS::Serverless:HttpApi" is incorrect as this is used to define an API Gateway HTTP API.INCORRECT: "AWS::Serverless:SimpleTable" is incorrect as this is used to define a DynamoDB table.References: https://docs.aws.amazon.com/serverlessapplicationmodel/latest/developerguide/serverlesssamtemplatenestedapplications.html'},{question:"A Developer has added a Global Secondary Index (GSI) to an existing Amazon DynamoDB table. The GSI is used mainly for read operations whereas the primary table is extremely writeintensive. Recently, the Developer has noticed throttling occurring under heavy write activity on the primary table. However, the write capacity units on the primary table are not fully utilized. What is the best explanation for why the writes are being throttled on the primary table?",answers:[{text:"The Developer should have added an LSI instead of a GSI",isCorrect:!1},{text:"There are insufficient read capacity units on the primary table",isCorrect:!1},{text:"There are insufficient write capacity units on the primary table",isCorrect:!1},{text:"The write capacity units on the GSI are under provisioned",isCorrect:!0}],explanation:'Some applications might need to perform many kinds of queries, using a variety of different attributes as query criteria. To support these requirements, you can create one or more global secondary indexes and issue Query requests against these indexes in Amazon DynamoDB.When items from a primary table are written to the GSI they consume write capacity units. It is essential to ensure the GSI has sufficient WCUs (typically, at least as many as the primary table). If writes are throttled on the GSI, the main table will be throttled (even if there’s enough WCUs on the main table). LSIs do not cause any special throttling considerations.In this scenario, it is likely that the Developer assumed that the GSI would need fewer WCUs as it is more readintensive and neglected to factor in the WCUs required for writing data into the GSI. Therefore, the most likely explanation is that the write capacity units on the GSI are under provisionedCORRECT: "The write capacity units on the GSI are under provisioned" is the correct answer.INCORRECT: "There are insufficient read capacity units on the primary table" is incorrect as the table is being throttled due to writes, not reads.INCORRECT: "The Developer should have added an LSI instead of a GSI" is incorrect as a GSI has specific advantages and there was likely good reason for adding a GSI. Also, you cannot add an LSI to an existing table.INCORRECT: "There are insufficient write capacity units on the primary table" is incorrect as the question states that the WCUs are underutilized.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html'},{question:"There are multiple AWS accounts across multiple regions managed by a company. The operations team require a single operational dashboard that displays some key performance metrics from these accounts and regions. What is the SIMPLEST solution?",answers:[{text:"Create an Amazon CloudTrail trail that applies to all regions and deliver the logs to a single Amazon S3 bucket. Create a dashboard using the data in the bucket",isCorrect:!1},{text:"Create an AWS Lambda function that collects metrics from each account and region and pushes the metrics to the account where the dashboard has been created",isCorrect:!1},{text:"Create an Amazon CloudWatch crossaccount crossregion dashboard",isCorrect:!0},{text:"Create an Amazon CloudWatch dashboard in one account and region and import the data from the other accounts and regions",isCorrect:!1}],explanation:'You can create crossaccount crossRegion dashboards, which summarize your CloudWatch data from multiple AWS accounts and multiple Regions into one dashboard. From this highlevel dashboard you can get a view of your entire application, and also drill down into more specific dashboards without having to log in and out of accounts or switch Regions.You can create crossaccount crossRegion dashboards in the AWS Management Console and programmatically.CORRECT: "Create an Amazon CloudWatch crossaccount crossregion dashboard" is the correct answer.INCORRECT: "Create an Amazon CloudWatch dashboard in one account and region and import the data from the other accounts and regions" is incorrect as this is more complex and unnecessary.INCORRECT: "Create an AWS Lambda function that collects metrics from each account and region and pushes the metrics to the account where the dashboard has been created" is incorrect as this is not a simple solution.INCORRECT: "Create an Amazon CloudTrail trail that applies to all regions and deliver the logs to a single Amazon S3 bucket.Create a dashboard using the data in the bucket" is incorrect as CloudTrail logs API activity, not performance metrics.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_xaxr_dashboard.html'},{question:"A Developer is creating a serverless application. The application looks up information about a customer using a separate Lambda function for each item such as address and phone number. The Developer has created branches in AWS Step Functions for each lookup function. How can the Developer optimize the performance, so the lookups complete faster?",answers:[{text:"Use a Map state to iterate over all the items.",isCorrect:!1},{text:"Use a Parallel state to iterate over all the branches parallel.",isCorrect:!0},{text:"Use a Wait state to reduce the wait time for function execution.",isCorrect:!1},{text:"Use a Choice state to lookup the specific information required.",isCorrect:!1}],explanation:'The Parallel state ("Type": "Parallel") can be used to create parallel branches of execution in your AWS Step Functions state machine. This will improve the performance of the application by ensuring that all information lookups occur in parallel.CORRECT: "Use a Parallel state to iterate over all the branches parallel" is the correct answer.INCORRECT: "Use a Choice state to lookup the specific information required" is incorrect. This is used to add additional logic but is not required and is unlikely to improve performance.INCORRECT: "Use a Wait state to reduce the wait time for function execution" is incorrect. The Wait state delays the state machine from continuing for a specified time.INCORRECT: "Use a Map state to iterate over all the items" is incorrect. The Map state executes the same steps for multiple entries of an array in the state input.References: https://docs.aws.amazon.com/stepfunctions/latest/dg/amazonstateslanguageparallelstate.html'},{question:"An application running on Amazon EC2 generates a large number of small files (1KB each) containing personally identifiable information that must be converted to ciphertext. The data will be stored on a proprietary network-attached file system. What is the SAFEST way to encrypt the data using AWS KMS?",answers:[{text:"Encrypt the data directly with a customer managed customer master key",isCorrect:!0},{text:"Create a data encryption key from a customer master key and encrypt the data with the customer master key",isCorrect:!1},{text:"Create a data encryption key from a customer master key and encrypt the data with the data encryption key",isCorrect:!1},{text:"Encrypt the data directly with an AWS managed customer master key",isCorrect:!1}],explanation:'With AWS KMS you can encrypt files directly with a customer master key (CMK). A CMK can encrypt up to 4KB (4096 bytes) of data in a single encrypt, decrypt, or reencrypt operation. As CMKs cannot be exported from KMS this is a very safe way to encrypt small amounts of data.Customer managed CMKs are CMKs in your AWS account that you create, own, and manage. You have full control over these CMKs, including establishing and maintaining their key policies, IAM policies, and grants, enabling and disabling them, rotating their cryptographic material, adding tags, creating aliases that refer to the CMK, and scheduling the CMKs for deletion.AWS managed CMKs are CMKs in your account that are created, managed, and used on your behalf by an AWS service that is integrated with AWS KMS. Some AWS services support only an AWS managed CMK. In this example the Amazon EC2 instance is saving files on a proprietary networkattached file system and this will not have support for AWS managed CMKs.Data keys are encryption keys that you can use to encrypt data, including large amounts of data and other data encryption keys. You can use AWS KMS CMKs to generate, encrypt, and decrypt data keys. However, AWS KMS does not store, manage, or track your data keys, or perform cryptographic operations with data keys. You must use and manage data keys outside of AWS KMS – this is potentially less secure as you need to manage the security of these keys.CORRECT: "Encrypt the data directly with a customer managed customer master key" is the correct answer.INCORRECT: "Create a data encryption key from a customer master key and encrypt the data with the data encryption key" is incorrect as this is not the most secure option here as you need to secure the data encryption key outside of KMS. It is also unwarranted as you can use a CMK directly to encrypt files up to 4KB in size.INCORRECT: "Create a data encryption key from a customer master key and encrypt the data with the customer master key" is incorrect as the creation of the data encryption key is of no use here. It does not necessarily pose a security risk as the data key hasn’t been used (and you can use the CMK to encrypt the data), however this is not the correct process to follow.INCORRECT: "Encrypt the data directly with an AWS managed customer master key" is incorrect as the networkattached file system is proprietary and therefore will not be supported by AWS managed CMKs.References: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html'},{question:"A Developer is creating a DynamoDB table for storing transaction logs. The table has 10 write capacity units (WCUs). The Developer needs to configure the read capacity units (RCUs) for the table in order to MAXIMIZE the number of requestsallowed per second. Which of the following configurations should the Developer use?",answers:[{text:"Strongly consistent reads of 5 RCUs reading items that are 4 KB in size",isCorrect:!1},{text:"Eventually consistent reads of 5 RCUs reading items that are 4 KB in size",isCorrect:!1},{text:"Strongly consistent reads of 15 RCUs reading items that are 1KB in size",isCorrect:!1},{text:"Eventually consistent reads of 15 RCUs reading items that are 1 KB in size",isCorrect:!0}],explanation:'A read capacity unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. For example, suppose that you create a table with 10 provisioned read capacity units. This allows you to perform 10 strongly consistent reads per second, or 20 eventually consistent reads per second, for items up to 4 KB.Reading an item larger than 4 KB consumes more read capacity units. For example, a strongly consistent read of an item that is 8 KB (4 KB \xd7 2) consumes 2 read capacity units. An eventually consistent read on that same item consumes only 1 read capacity unit.Item sizes for reads are rounded up to the next 4 KB multiple. For example, reading a 3,500byte item consumes the same throughput as reading a 4 KB item. Therefore, the smaller (1 KB) items in this scenario would consume the same number of RCUs as the 4 KB items. Also, we know that eventually consistent reads consume half the RCUs of strongly consistent reads.The following bullets provide the read throughput for each configuration:• Eventually consistent, 15 RCUs, 1 KB item = 30 items read per second.• Strongly consistent, 15 RCUs, 1 KB item = 15 items read per second.• Eventually consistent, 5 RCUs, 4 KB item = 10 items read per second.• Strongly consistent, 5 RCUs, 4 KB item = 5 items read per second.Therefore, the Developer should choose the option to enable eventually consistent reads of 15 RCUs reading items that are 1 KB in size as this will result in the highest number of items read per second.CORRECT: "Eventually consistent reads of 15 RCUs reading items that are 1 KB in size" is the correct answer.INCORRECT: "Eventually consistent reads of 5 RCUs reading items that are 4 KB in size" is incorrect as described above.INCORRECT: "Strongly consistent reads of 5 RCUs reading items that are 4 KB in size" is incorrect as described above.INCORRECT: "Strongly consistent reads of 15 RCUs reading items that are 1KB in size" is incorrect as described above.References: https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/ProvisionedThroughput.html'},{question:"A developer is making updates to the code for a Lambda function. The developer is keen to test the code updates by directing a small amount of traffic to a new version. How can this BEST be achieved?",answers:[{text:"Create a new function using the new code and update the application to split requests between the new functions",isCorrect:!1},{text:"Create an alias that points to both the new and previous versions of the function code and assign a weighting for sending a portion of traffic to the new version",isCorrect:!0},{text:"Create two versions of the function code. Configure the application to direct a subset of requests to the new version",isCorrect:!1},{text:"Create an API using API Gateway and use stage variables to point to different versions of the Lambda function",isCorrect:!1}],explanation:'You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN. You can point an alias to multiple versions of your function code and then assign a weighting to direct certain amounts of traffic to each version. This enables a blue/green style of deployment and means it’s easy to roll back to the older version by simplyupdating the weighting if issues occur.CORRECT: "Create an alias that points to both the new and previous versions of the function code and assign a weighting for sending a portion of traffic to the new version" is the correct answer.INCORRECT: "Create two versions of the function code. Configure the application to direct a subset of requests to the new version" is incorrect as this would entail using application logic to direct traffic to the different versions. This is not the best way to solve this problem as Lambda aliases are a better solution.INCORRECT: "Create an API using API Gateway and use stage variables to point to different versions of the Lambda function" is incorrect. Stage variables are namevalue pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates. You can use stage variables to point to different Lambda ARNs and associate these with different stages of your API, however this is not a good solution for this scenario.INCORRECT: "Create a new function using the new code and update the application to split requests between the new functions" is incorrect as this would entail using application logic to direct traffic to the different versions. This is not the best way to solve this problem as Lambda aliases are a better solution.References:https://docs.aws.amazon.com/lambda/latest/dg/configurationaliases.html https://docs.aws.amazon.com/lambda/latest/dg/configurationversions.html'},{question:"An Amazon DynamoDB table has been created using provisioned capacity. A manager needs to understand whether the DynamoDB table is costeffective. How can the manager query how much provisioned capacity is actually being used?",answers:[{text:"Monitor the ConsumedReadCapacityUnits and ConsumedWriteCapacityUnits over a specified time period",isCorrect:!0},{text:"Monitor the ReadThrottleEvents and WriteThrottleEvents metrics for the table",isCorrect:!1},{text:"Use Amazon CloudTrail and monitor the DescribeLimits API action",isCorrect:!1},{text:"Use AWS XRay to instrument the DynamoDB table and monitor subsegments",isCorrect:!1}],explanation:'You can monitor Amazon DynamoDB using CloudWatch, which collects and processes raw data from DynamoDB into readable, near realtime metrics. These statistics are retained for a period of time, so that you can access historical information for a better perspective on how your web application or service is performing. By default, DynamoDB metric data is sent toCloudWatch automatically.To determine how much of the provisioned capacity is being used you can monitor ConsumedReadCapacityUnits or ConsumedWriteCapacityUnits over the specified time period.CORRECT: "Monitor the ConsumedReadCapacityUnits and ConsumedWriteCapacityUnits over a specified time period" is the correct answer.INCORRECT: "Monitor the ReadThrottleEvents and WriteThrottleEvents metrics for the table" is incorrect as these metrics are used to determine which requests exceed the provisioned throughput limits of a table.INCORRECT: "Use Amazon CloudTrail and monitor the DescribeLimits API action" is incorrect as CloudTrail records API actions, not performance metrics.INCORRECT: "Use AWS XRay to instrument the DynamoDB table and monitor subsegments" is incorrect. DynamoDB does not directly integrate with XRay but you can record information in subsegments for downstream requests. This is not, however, a method for monitoring provisioned capacity utilization.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/monitoringcloudwatch.html https://digitalcloud.training/certificationtraining/awsdeveloperassociate/awsmanagementandgovernance/amazoncloudwatch/'},{question:"A development team manage a hightraffic eCommerce site with dynamic pricing that is updated in realtime. There have been incidents where multiple updates occur simultaneously and cause an original editor's updates to be overwritten. How can the developers ensure that overwriting does not occur?",answers:[{text:"Use conditional writes",isCorrect:!0},{text:"Use atomic counters",isCorrect:!1},{text:"Use batch operations",isCorrect:!1},{text:"Use concurrent writes",isCorrect:!1}],explanation:'By default, the DynamoDB write operations (PutItem, UpdateItem, DeleteItem) are unconditional: Each operation overwrites an existing item that has the specified primary key.DynamoDB optionally supports conditional writes for these operations. A conditional write succeeds only if the item attributes meet one or more expected conditions. Otherwise, it returns an error. Conditional writes are helpful in many situations. For example, you might want a PutItem operation to succeed only if there is not already an item with the same primary key. Or you could prevent an UpdateItem operation from modifying an item if one of its attributes has a certain value. Conditional writes can be idempotent if the conditional check is on the same attribute that is being updated. This means that DynamoDB performs a given write request only if certain attribute values in the item match what you expect them to be at the time of the request.For example, suppose that you issue an UpdateItem request to increase the Price of an item by 3, but only if the Price is currently 20. After you send the request, but before you get the results back, a network error occurs, and you don\'t know whether the request was successful. Because this conditional write is idempotent, you can retry the same UpdateItem request, and DynamoDB updates the item only if the Price is currently 20.The following example shows how to use the conditionexpression parameter to achieve a conditional write with idempotence:aws dynamodb updateitem \\tablename ProductCatalog \\key \'{"Id":{"N":"1"}}\' \\updateexpression "SET Price = :newval" \\conditionexpression "Price = :currval" \\expressionattributevalues file://expressionattributevalues.jsonFor this scenario, conditional writes with idempotence will mean that each writer can check the current price and update the price only if the price matches that price. If the price is updated by another writer before the write is made, it will fail as the item price has changed and will not reflect the expected price.CORRECT: "Use conditional writes" is the correct answer.INCORRECT: "Use concurrent writes" is incorrect as writing concurrently to the same items is exactly what we want to avoid.INCORRECT: "Use atomic counters" is incorrect. An atomic counter is a numeric attribute that is incremented, unconditionally, without interfering with other write requests. This is used for cases such as tracking visitors to a website. This does not prevent recent updated from being overwritten.INCORRECT: "Use batch operations" is incorrect. Batch operations can reduce the number of network round trips from your application to DynamoDB. However, this does not solve the problem of preventing recent updates from being overwritten.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ConditionExpressions.html'},{question:"A developer needs to add signup and signin capabilities for a mobile app. The solution should integrate with social identity providers (IdPs) and SAML IdPs. Which service should the developer use?",answers:[{text:"AWS Cognito identity pool",isCorrect:!1},{text:"AWS Cognito user pool",isCorrect:!0},{text:"API Gateway with a Lambda authorizer",isCorrect:!1},{text:"AWS IAM and STS",isCorrect:!1}],explanation:'User pools are for authentication (identify verification). With a user pool, your app users can sign in through the user pool or federate through a thirdparty identity provider (IdP).Identity pools are for authorization (access control). You can use identity pools to create unique identities for users and give them access to other AWS services.User pool use cases:Use a user pool when you need to:• Design signup and signin webpages for your app.• Access and manage user data.• Track user device, location, and IP address, and adapt to signin requests of different risk levels.• Use a custom authentication flow for your app.Identity pool use cases:Use an identity pool when you need to:• Give your users access to AWS resources, such as an Amazon Simple Storage Service (Amazon S3) bucket or anAmazon DynamoDB table.• Generate temporary AWS credentials for unauthenticated users.Therefore, a user pool is the correct service to use as in this case we are not granting access to AWS services, just providing signup and signin capabilities for a mobile app.CORRECT: "AWS Cognito user pool" is the correct answer.INCORRECT: "AWS Cognito identity pool" is incorrect as an identity pool is used when you need to provide access to AWS resources (see explanation above).INCORRECT: "API Gateway with a Lambda authorizer" is incorrect as AWS Cognito is the best solution for providing signup and signin for mobile apps and also integrates with the 3rd party IdPs.INCORRECT: "AWS IAM and STS" is incorrect as AWS Cognito is the best solution for providing signup and signin for mobile apps and also integrates with the 3rd party IdPs.References:https://aws.amazon.com/premiumsupport/knowledgecenter/cognitouserpoolsidentitypools/ https://docs.aws.amazon.com/cognito/latest/developerguide/cognitouserpoolsidentityfederation.html'},{question:"An application reads data from Amazon S3 and makes 55,000 read requests per second. A Developer must design the storage solution to ensure the performance requirements are met cost-effectively. How can the storage be optimized to meet these requirements?",answers:[{text:"Create at least 10 prefixes and split the files across the prefixes.",isCorrect:!0},{text:"Move the files to Amazon EFS. Index the files with S3 metadata.",isCorrect:!1},{text:"Create at least 10 S3 buckets and split the files across the buckets.",isCorrect:!1},{text:"Move the files to Amazon DynamoDB. Index the files with S3 metadata.",isCorrect:!1}],explanation:'To avoid throttling in Amazon S3 you must ensure you do not exceed certain limits on a perprefix basis. You can send 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in an Amazon S3 bucket. There are no limits to the number of prefixes that you can have in your bucket.In this case the Developer would need to split the files across at least 10 prefixes in a single Amazon S3 bucket. The application should then read the files across the prefixes in parallel.CORRECT: "Create at least 10 prefixes and split the files across the prefixes" is the correct answer.INCORRECT: "Create at least 10 S3 buckets and split the files across the buckets" is incorrect. Performance is improved based on splitting reads across prefixes, not buckets.INCORRECT: "Move the files to Amazon EFS. Index the files with S3 metadata" is incorrect. This is not costeffective.INCORRECT: "Move the files to Amazon DynamoDB. Index the files with S3 metadata" is incorrect. This is not costeffective.References: https://aws.amazon.com/premiumsupport/knowledgecenter/s3requestlimitavoidthrottling/'},{question:"A Developer needs to be notified by email for all new object creation events in a specific Amazon S3 bucket. Amazon SNS will be used for sending the messages. How can the Developer enable these notifications?",answers:[{text:"Create an event notification for all s3:ObjectRemoved:Delete API calls",isCorrect:!1},{text:"Create an event notification for all s3:ObjectCreated:Put API calls",isCorrect:!1},{text:"Create an event notification for all s3:ObjectRestore:Post API calls",isCorrect:!1},{text:"Create an event notification for all s3:ObjectCreated:* API calls",isCorrect:!0}],explanation:'The Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. You store this configuration in the notification subresource that is associated with a bucket. Currently, Amazon S3 can publish notifications for the following events: &bull; New object created events &mdash; Amazon S3 supports multiple APIs to create objects. You can request notification when only a specific API is used (for example, s3:ObjectCreated:Put), or you can use a wildcard (for example, s3:ObjectCreated:*) to request notification when an object is created regardless of the API used. &bull; Object removal events &mdash; Amazon S3 supports deletes of versioned and unversioned objects. For information about object versioning, see Object Versioning and Using Versioning. &bull; Restore object events &mdash; Amazon S3 supports the restoration of objects archived to the S3 Glacier storage class. You request to be notified of object restoration completion by using s3:ObjectRestore:Completed. You use s3:ObjectRestore:Post to request notification of the initiation of a restore. &bull; Reduced Redundancy Storage (RRS) object lost events &mdash; Amazon S3 sends a notification message when it detects that an object of the RRS storage class has been lost. &bull; Replication events &mdash; Amazon S3 sends event notifications for replication configurations that have S3 Replication Time Control (S3 RTC) enabled. It sends these notifications when an object fails replication, when an object exceeds the 15minute threshold, when an object is replicated after the 15minute threshold, and when an object is no longer tracked by replication metrics. It publishes a second event when that object replicates to the destination Region. Therefore, the Developer should create an event notification for all s3:ObjectCreated:* API calls as this will capture all new object creation events. CORRECT: "Create an event notification for all s3:ObjectCreated:* API calls" is the correct answer. INCORRECT: "Create an event notification for all s3:ObjectCreated:Put API calls" is incorrect as this will not capture all new object creation events (e.g. POST or COPY). The wildcard should be used instead. INCORRECT: "Create an event notification for all s3:ObjectRemoved:Delete API calls" is incorrect as this is used for object deletions. INCORRECT: "Create an event notification for all s3:ObjectRestore:Post API calls" is incorrect as this is used for restore events from Amazon S3 Glacier archives. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html https://aws.amazon.com/sns/faqs/'},{question:"A company runs a popular website behind an Amazon CloudFront distribution that uses an Application Load Balancer as the origin. The Developer wants to set up custom HTTP responses to 404 errors for content that has been removed from the origin that redirects the users to another page. The Developer wants to use an AWS Lambda@Edge function that is associated with the current CloudFront distribution to accomplish this goal. The solution must use a minimum amount of resources. Which CloudFront event type should the Developer use to invoke the Lambda@Edge function that contains the redirect logic?",answers:[{text:"Viewer response",isCorrect:!1},{text:"Origin request",isCorrect:!1},{text:"Origin response",isCorrect:!0},{text:"Viewer request",isCorrect:!1}],explanation:'When CloudFront receives an HTTP response from the origin server, if there is an originresponse trigger associated with the cache behavior, you can modify the HTTP response to override what was returned from the origin.Some common scenarios for updating HTTP responses include the following:• Changing the status to set an HTTP 200 status code and creating static body content to return to the viewer when an origin returns an error status code (4xx or 5xx)• Changing the status to set an HTTP 301 or HTTP 302 status code, to redirect the user to another website when an origin returns an error status code (4xx or 5xx)You can also replace the HTTP responses in viewer and origin request events. However, in this case it is the error response being returned from the origin that must be modified when a 404 error is encountered for a page that has been removed.CORRECT: "Origin response" is the correct answer.INCORRECT: "Origin request" is incorrect as explained above.INCORRECT: "Viewer response" is incorrect as explained above.INCORRECT: "Viewer request" is incorrect as explained above.References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambdaupdatinghttpresponses.html'},{question:"A Developer is creating an application that will process some data and generate an image file from it. The application will use an AWS Lambda function which will require 150 MB of temporary storage while executing. The temporary files will not be needed after the function execution is completed. What is the best location for the Developer to store the files?",answers:[{text:"Store the files in Amazon S3 and use a lifecycle policy to delete the files automatically",isCorrect:!1},{text:"Store the files in an Amazon Instance Store and delete the files when the execution completes",isCorrect:!1},{text:"Store the files in an Amazon EFS filesystem and delete the files when the execution completes",isCorrect:!1},{text:"Store the files in the /tmp directory and delete the files when the execution completes",isCorrect:!0}],explanation:'The /tmp directory can be used for storing temporary files within the execution context. This can be used for storing static assets that can be used by subsequent invocations of the function. If the assets must be deleted before the function is invoked again the function code should take care of deleting them.There is a limit of 512 MB storage space in the /tmp directory, but this is more than adequate for this scenario.CORRECT: "Store the files in the /tmp directory and delete the files when the execution completes" is the correct answer.INCORRECT: "Store the files in Amazon S3 and use a lifecycle policy to delete the files automatically" is incorrect. The /tmp directory within the execution context has enough space for these files and this will reduce latency, cost, and execution time.INCORRECT: "Store the files in an Amazon Instance Store and delete the files when the execution completes" is incorrect. Instance stores are ephemeral storage attached to Ec2 instances, they cannot be used except by EC2 instances for temporary storage.INCORRECT: "Store the files in an Amazon EFS filesystem and delete the files when the execution completes" is incorrect. This is another option that would increase cost, complexity and latency. It is better to use the /tmp directory.References: https://docs.aws.amazon.com/lambda/latest/dg/bestpractices.html'},{question:"A Developer is creating a database solution using an Amazon ElastiCache caching layer. The solution must provide strong consistency to ensure that updates to product data are consistent between the backend database and the ElastiCache cache. Low latency performance is required for all items in the database. Which cache writing policy will satisfy these requirements?",answers:[{text:"Add a short duration TTL value to each write.",isCorrect:!1},{text:"Invalidate the cache for each database write.",isCorrect:!1},{text:"Use a writethrough caching strategy.",isCorrect:!0},{text:"Use a lazyloading caching strategy.",isCorrect:!1}],explanation:'The writethrough strategy adds data or updates data in the cache whenever data is written to the database. The advantages of writethrough are as follows:• Data in the cache is never stale. Because the data in the cache is updated every time it\'s written to the database, the data in the cache is always current.• Write penalty vs. read penalty. Every write involves two trips:1. A write to the cache2. A write to the databaseWhich adds latency to the process. That said, end users are generally more tolerant of latency when updating data than when retrieving data. There is an inherent sense that updates are more work and thus take longer.CORRECT: "Use a writethrough caching strategy" is the correct answer.INCORRECT: "Use a lazyloading caching strategy" is incorrect. Lazy loading is a caching strategy that loads data into the cache only when necessary. This will not ensure strong consistency between the database and the cache.INCORRECT: "Add a short duration TTL value to each write" is incorrect. A TTL specifies the number of seconds until the key expires. This will not ensure strong consistency between the database and the cache.INCORRECT: "Invalidate the cache for each database write" is incorrect. This will allow the cache to be updated when an item is next read but will not ensure the best performance for all items in the database.References: https://docs.aws.amazon.com/AmazonElastiCache/latest/memug/Strategies.html'},{question:"An AWS Lambda functions downloads a 50MB from an object storage system each time it is invoked. The download delays the function completion and causes intermittent timeouts which is slowing down the application. How can the application be refactored to resolve the timeout?",answers:[{text:"Increase the timeout of the function",isCorrect:!1},{text:"Increase the concurrency allocation of the function",isCorrect:!1},{text:"Increase the memory allocation of the function",isCorrect:!1},{text:"Store the file in the /tmp directory of the execution context and reuse it on subsequent invocations",isCorrect:!0}],explanation:'You can use the /tmp directory if the function needs to download a large file or disk space for operations. The maximum size is 512 MB. The content is frozen within the execution context so multiple invocations can use the data.Therefore, the download will occur once, and then subsequent invocations will use the file from the /tmp directory. This requires minimal refactoring and is the best way of resolving these issues.CORRECT: "Store the file in the /tmp directory of the execution context and reuse it on subsequent invocations" is the correct answer.INCORRECT: "Increase the memory allocation of the function" is incorrect as this will not resolve the issue of needing to download the file for each invocation. Adding memory results in more CPU being allocated which can reduce processing time but the problem still remains.INCORRECT: "Increase the timeout of the function" is incorrect as this does not resolve the main issue. The download will still need to occur for each invocation and therefore the application will continue to be affected by poor performance.INCORRECT: "Increase the concurrency allocation of the function" is incorrect as concurrency is not the issue here. The issue that needs to be resolved is to remove the requirement to download the large file for each invocation.References: https://aws.amazon.com/lambda/features/'},{question:"An application will be hosted on the AWS Cloud. Developers will be using an Agile software development methodology with regular updates deployed through a continuous integration and delivery (CI/CD) model. Which AWS service can assist theDevelopers with automating the build, test, and deploy phases of the release process every time there is a code change?",answers:[{text:"AWS CodePipeline",isCorrect:!0},{text:"AWS CloudFormation",isCorrect:!1},{text:"AWS Elastic Beanstalk",isCorrect:!1},{text:"AWS CodeBuild",isCorrect:!1}],explanation:'AWS CodePipeline is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. You can quickly model and configure the different stages of a software release process. CodePipeline automates the steps required to release your software changes continuously.Specifically, you can:• Automate your release processes: CodePipeline fully automates your release process from end to end, starting from your source repository through build, test, and deployment. You can prevent changes from moving through a pipeline by including a manual approval action in any stage except a Source stage. You can release when you want, in the way you want, on the systems of your choice, across one instance or multiple instances.• Establish a consistent release process: Define a consistent set of steps for every code change. CodePipeline runs each stage of your release according to your criteria.• Speed up delivery while improving quality: You can automate your release process to allow your developers to test and release code incrementally and speed up the release of new features to your customers.• Use your favorite tools: You can incorporate your existing source, build, and deployment tools into your pipeline.• View progress at a glance: You can review realtime status of your pipelines, check the details of any alerts, retry failed actions, view details about the source revisions used in the latest pipeline execution in each stage, and manually rerun any pipeline.• View pipeline history details: You can view details about executions of a pipeline, including start and end times, run duration, and execution IDs.Therefore, AWS CodePipeline is the perfect tool for the Developer’s requirements.CORRECT: "AWS CodePipeline" is the correct answer.INCORRECT: "AWS CloudFormation" is incorrect as CloudFormation is not triggered by changes in a source code repository. You must create change sets for deploying updates.INCORRECT: "AWS Elastic Beanstalk" is incorrect as this is a platform service that can be used to deploy code to managed runtimes such as Nodejs. It does not update automatically based on changes to source code. You must update that environment when you need to release new code.INCORRECT: "AWS CodeBuild" is incorrect as CodeBuild is used for compiling code, running unit tests and creating the deployment package. It does not manage the deployment of the code.References: https://docs.aws.amazon.com/codepipeline/latest/userguide/welcomewhatcanIdo.html'},{question:"A Developer is deploying an application using Docker containers running on the Amazon Elastic Container Service (ECS). The Developer is testing application latency and wants to capture trace information between the microservices. Which solution will meet these requirements?",answers:[{text:"Install the AWS XRay daemon locally on an Amazon EC2 instance and instrument the Amazon ECS microservices using the XRay SDK.",isCorrect:!1},{text:"Create a Docker image that runs the XRay daemon, upload it to a Docker image repository, and then deploy it to the Amazon ECS cluster.",isCorrect:!0},{text:"Install the AWS XRay daemon on each of the Amazon ECS instances.",isCorrect:!1},{text:"Install the Amazon CloudWatch agent on the container image. Use the CloudWatch SDK to publish custom metrics from each of the microservices.",isCorrect:!1}],explanation:'In Amazon ECS, create a Docker image that runs the XRay daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster. You can use port mappings and network mode settings in your task definition file to allow your application to communicate with the daemon container.CORRECT: "Create a Docker image that runs the XRay daemon, upload it to a Docker image repository, and then deploy it to the Amazon ECS cluster." is the correct answer.INCORRECT: "Install the Amazon CloudWatch agent on the container image. Use the CloudWatch SDK to publish custom metrics from each of the microservices" is incorrect. The CloudWatch agent does not capture trace information between Docker containers.INCORRECT: "Install the AWS XRay daemon on each of the Amazon ECS instances" is incorrect. The XRay daemon must be installed on the Docker containers, not the ECS hosts.INCORRECT: "Install the AWS XRay daemon locally on an Amazon EC2 instance and instrument the Amazon ECS microservices using the XRay SDK" is incorrect. You cannot trace Docker microservices from an Amazon EC2 instance.References: https://docs.aws.amazon.com/xray/latest/devguide/xraydaemonecs.html'},{question:"A customer requires a relational database for a transactional workload. Which type of AWS database is BEST suited to this requirement?",answers:[{text:"Amazon DynamoDB",isCorrect:!1},{text:"Amazon RedShift",isCorrect:!1},{text:"Amazon RDS",isCorrect:!0},{text:"Amazon ElastiCache",isCorrect:!1}],explanation:'Amazon Relational Database Service (Amazon RDS) is a managed service that makes it easy to set up, operate, and scale a relational database in the cloud. RDS is an Online Transaction Processing (OLTP) type of database. The primary use case is a transactional database (rather than analytical) and it is best for structured, relational data store requirements.CORRECT: "Amazon RDS" is the correct answer.INCORRECT: "Amazon RedShift" is incorrect as though this is a relational database it is best suited for analytics workloads rather than transactional workloads.INCORRECT: "Amazon DynamoDB" is incorrect as this is nonrelational database.INCORRECT: "Amazon ElastiCache" is incorrect as this is a key/value database used for caching databases (including AmazonRDS).References: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html'},{question:"A developer is creating an Auto Scaling group of Amazon EC2 instances. The developer needs to publish a custom metric to Amazon CloudWatch. Which method would be the MOST secure way to authenticate a CloudWatch PUT request?",answers:[{text:"Create an IAM user with the PutMetricData permission and modify the Auto Scaling launch configuration to inject the user credentials into the instance user data",isCorrect:!1},{text:"Create an IAM role with the PutMetricData permission and modify the Amazon EC2 instances to use that role",isCorrect:!1},{text:"Create an IAM role with the PutMetricData permission and create a new Auto Scaling launch configuration to launch instances using that role",isCorrect:!0},{text:"Modify the CloudWatch metric policies to allow the PutMetricData permission to instances from the Auto Scaling group",isCorrect:!1}],explanation:'The most secure configuration to authenticate the request is to create an IAM role with a permissions policy that only provides the minimum permissions requires (least privilege). This IAM role should have a customermanaged permissions policy applied with the PutMetricData allowed. The PutMetricData API publishes metric data points to Amazon CloudWatch. CloudWatch associates the data points with the specified metric. If the specified metric does not exist, CloudWatch creates the metric. When CloudWatch creates a metric, it can take up to fifteen minutes for the metric to appear in calls to ListMetrics.The following images shows a permissions policy being created with the PutMetricData permission:CORRECT: "Create an IAM role with the PutMetricData permission and create a new Auto Scaling launch configuration to launch instances using that role" is the correct answerINCORRECT: "Modify the CloudWatch metric policies to allow the PutMetricData permission to instances from the Auto Scaling group" is incorrect as this is not possible. You should instead grant the permissions through a permissions policy and attach that to a role that the EC2 instances can assume.INCORRECT: "Create an IAM user with the PutMetricData permission and modify the Auto Scaling launch configuration to inject the user credentials into the instance user data" is incorrect. You cannot “inject user credentials” using a launch configuration. Instead, you can attach an IAM role which allows the instance to assume the role and take on the privileges allowed through any permissions policies that are associated with that role.INCORRECT: "Create an IAM role with the PutMetricData permission and modify the Amazon EC2 instances to use that role" is incorrect as you should create a new launch configuration for the Auto Scaling group rather than updating the instances manually.References:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iamrolesforamazonec2.html https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_PutMetricData.html'},{question:"A Developer has deployed an application that runs on an Auto Scaling group of Amazon EC2 instances. The application data is stored in an Amazon DynamoDB table and records are constantly updated by all instances. An instance sometimes retrieves old data. The Developer wants to correct this by making sure the reads are strongly consistent.",answers:[{text:"Use the GetShardIterator command",isCorrect:!1},{text:"Set consistency to strong when calling UpdateTable",isCorrect:!1},{text:"Create a new DynamoDB Accelerator (DAX) table",isCorrect:!1},{text:"Set ConsistentRead to true when calling GetItem",isCorrect:!0}],explanation:'When you request a strongly consistent read, DynamoDB returns a response with the most uptodate data, reflecting the updates from all prior write operations that were successful. The GetItem operation returns a set of attributes for the item with the given primary key. If there is no matching item, GetItem does not return any data and there will be no Item element in the response. GetItem provides an eventually consistent read by default. If your application requires a strongly consistent read, set ConsistentRead to true. Although a strongly consistent read might take more time than an eventually consistent read, it always returns the last updated value. Therefore, the Developer should set ConsistentRead to true when calling GetItem. CORRECT: "Set ConsistentRead to true when calling GetItem" is the correct answer. INCORRECT: "Create a new DynamoDB Accelerator (DAX) table" is incorrect as DAX is not used to enable strongly consistent reads. DAX is used for improving read performance as it caches data in an inmemory cache. INCORRECT: "Set consistency to strong when calling UpdateTable" is incorrect as you cannot use this API action to configure consistency at a table level. INCORRECT: "Use the GetShardIterator command" is incorrect as this is not related to DynamoDB, it is related to Amazon Kinesis. References: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_GetItem.html'},{question:"An Amazon EC2 instance requires permissions to read and write data in an Amazon S3 bucket. A Developer is creating an IAM role that will be assumed by the EC2 instance. When creating the role using the AWS CLI createrole command, which policy must be added to allow the instance to assume the role?",answers:[{text:"Trust policy",isCorrect:!0},{text:"Inline policy",isCorrect:!1},{text:"Bucket policy",isCorrect:!1},{text:"Managed policy",isCorrect:!1}],explanation:'The IAM role must have a policy attached that provides the permissions necessary to read and write data in the S3 bucket. Additionally, a trust policy must be attached. This policy defines which principals can assume the role, and under which conditions. This is sometimes referred to as a resourcebased policy for the IAM role.CORRECT: "Trust policy" is the correct answer.INCORRECT: "Bucket policy" is incorrect. A bucket policy is applied to a bucket, it does not allow the EC2 instance to assume the IAM role.INCORRECT: "Inline policy" is incorrect. Inline policies are permissions policies, not trust policies, that are applied directly to principals.INCORRECT: "Managed policy" is incorrect. A managed policy is a permissions policy managed by AWS.References: https://aws.amazon.com/blogs/security/howtousetrustpolicieswithiamroles/'},{question:"Customers who use a REST API have reported performance issues. A Developer needs to measure the time between when API Gateway receives a request from a client and when it returns a response to the client. Which metric should the Developer monitor?",answers:[{text:"5XXError",isCorrect:!1},{text:"CacheHitCount",isCorrect:!1},{text:"IntegrationLatency",isCorrect:!1},{text:"Latency",isCorrect:!0}],explanation:'The Latency metric measures the time between when API Gateway receives a request from a client and when it returns a response to the client. The latency includes the integration latency and other API Gateway overhead.igital Cloud TrainingCORRECT: "Latency" is the correct answer.INCORRECT: "IntegrationLatency" is incorrect. This measures the time between when API Gateway relays a request to the backend and when it receives a response from the backend.INCORRECT: "CacheHitCount" is incorrect. This measures the number of requests served from the API cache in a given period.INCORRECT: "5XXError" is incorrect. This measures the number of serverside errors captured in a given period.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaymetricsanddimensions.html'},{question:"A new application will be deployed using AWS CodeDeploy to Amazon Elastic Container Service (ECS). What must be supplied to CodeDeploy to specify the ECS service to deploy?",answers:[{text:"The AppSpec file",isCorrect:!0},{text:"The Template file",isCorrect:!1},{text:"The Policy file",isCorrect:!1},{text:"The BuildSpec file",isCorrect:!1}],explanation:'The application specification file (AppSpec file) is a YAMLformatted or JSONformatted file used by CodeDeploy to manage a deployment.If your application uses the Amazon ECS compute platform, the AppSpec file is named appspec.yaml. It is used by CodeDeploy to determine:• Your Amazon ECS task definition file. This is specified with its ARN in the TaskDefinition instruction in the AppSpec file.• The container and port in your replacement task set where your Application Load Balancer or Network Load Balancer reroutes traffic during a deployment. This is specified with the LoadBalancerInfo instruction in the AppSpec file.• Optional information about your Amazon ECS service, such the platform version on which it runs, its subnets, and its security groups.• Optional Lambda functions to run during hooks that correspond with lifecycle events during an Amazon ECS deployment. For more information, see AppSpec \'hooks\' Section for an Amazon ECS Deployment.CORRECT: "The AppSpec file" is the correct answer.INCORRECT: "The BuildSpec file" is incorrect as this is a file type that is used with AWS CodeBuild.INCORRECT: "The Template file" is incorrect as this is something that is used with AWS CloudFormation and AWS SAM.INCORRECT: "The Policy file" is incorrect as a policy is typically referring to an IAM permissions policy document.References: https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfile.html#appspecreferenceecs'},{question:"An application searches a DynamoDB table to return items based on primary key attributes. A developer noticed some ProvisionedThroughputExceeded exceptions being generated by DynamoDB. How can the application be optimized to reduce the load on DynamoDB and use the LEAST amount of RCU?",answers:[{text:"Modify the application to issue query API calls with eventual consistency reads",isCorrect:!0},{text:"Modify the application to issue scan API calls with eventual consistency reads",isCorrect:!1},{text:"Modify the application to issue query API calls with strong consistency reads",isCorrect:!1},{text:"Modify the application to issue scan API calls with strong consistency reads",isCorrect:!1}],explanation:'In general, Scan operations are less efficient than other operations in DynamoDB. A Scan operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set.If possible, you should avoid using a Scan operation on a large table or index with a filter that removes many results. Also, as a table or index grows, the Scan operation slows. The Scan operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation. For faster response times, design your tables and indexes so that your applications can use Query instead of Scan. (For tables, you can also consider using the GetItem and BatchGetItem APIs.)Additionally, eventual consistency consumes fewer RCUs than strong consistency. Therefore, the application should be refactored to use query APIs with eventual consistency.CORRECT: "Modify the application to issue query API calls with eventual consistency reads" is the correct answer.INCORRECT: "Modify the application to issue scan API calls with strong eventual reads" is incorrect as the Scan API is less efficient as it will return all items in the table.INCORRECT: "Modify the application to issue query API calls with strong consistency reads" is incorrect as strong consistency reads will consume more RCUs.INCORRECT: "Modify the application to issue scan API calls with strong consistency reads" is incorrect as the Scan API is less efficient as it will return all items in the table and strong consistency reads will use more RCUs.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bpqueryscan.html'},{question:"A Developer is configuring an Amazon ECS Service with Auto Scaling. The tasks should scale based on user load in the previous 20 seconds. How can the Developer enable the scaling?",answers:[{text:"Create a standardresolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds",isCorrect:!1},{text:"Create a highresolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds",isCorrect:!1},{text:"Create a standardresolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds",isCorrect:!1},{text:"Create a highresolution custom Amazon CloudWatch metric for user activity data, then publish data every 10 seconds",isCorrect:!0}],explanation:'Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a highresolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of60 seconds.User activity is not a standard CloudWatch metric and as stated above for the resolution we need in this scenario a custom CloudWatch metric is required anyway. Therefore, for this scenario the Developer should create a highresolution custom Amazon CloudWatch metric for user activity data and publish the data every 10 seconds.CORRECT: "Create a highresolution custom Amazon CloudWatch metric for user activity data, then publish data every 10 seconds" is the correct answer.INCORRECT: "Create a highresolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds" is incorrect as the resolution is higher than required which will cost more. We need the resolution to be 20 seconds so that means publishing in 10 second intervals with 2 data points. At 5 second intervals there would be 4 data points which will incur additional costs.INCORRECT: "Create a standardresolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds" is incorrect as standard resolution metrics have a granularity of one minute.INCORRECT: "Create a standardresolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds" is incorrect as standard resolution metrics have a granularity of one minute.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html'},{question:"A Developer has created an Amazon S3 bucket and uploaded some objects that will be used for a publicly available static website. What steps MUST be performed to configure the bucket as a static website? (Select TWO.)",answers:[{text:"Enable public access and grant everyone the s3:GetObject permissions",isCorrect:!0},{text:"Upload an index document and enter the name of the index document when enabling static website hosting",isCorrect:!0},{text:"Upload an index and error document and enter the name of the index and error documents when enabling static website hosting",isCorrect:!1},{text:"Create an object access control list (ACL) granting READ permissions to the AllUsers group",isCorrect:!1},{text:"Upload a certificate from AWS Certificate Manager",isCorrect:!1}],explanation:'You can use Amazon S3 to host a static website. On a static website, individual webpages include static content. They might also contain clientside scripts.To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document.When you enable static website hosting for your bucket, you enter the name of the index document (for example, index.html). After you enable static website hosting for your bucket, you upload an HTML file with the index document name to your bucket. Note that an error document is optional.To provide permissions, it is necessary to disable “block public access” settings and then create a bucket policy that grants everyone the s3:GetObject permission. For example:{"Version": "20121017","Statement": [{"Sid": "PublicReadGetObject","Effect": "Allow","Principal": "*","Action": ["s3:GetObject"],"Resource": ["arn:aws:s3:::example.com/*"]}]}CORRECT: "Upload an index document and enter the name of the index document when enabling static website hosting" is a correct answer.CORRECT: "Enable public access and grant everyone the s3:GetObject permissions" is also a correct answer.INCORRECT: "Upload an index and error document and enter the name of the index and error documents when enabling static website hosting" is incorrect as the error document is optional and the question specifically asks for the steps that MUST be completed.INCORRECT: "Create an object access control list (ACL) granting READ permissions to the AllUsers group" is incorrect. This may be necessary if the bucket objects are not owned by the bucket owner but the question states that the Developer created thebucket and uploaded the objects and so must be the object owner.INCORRECT: "Upload a certificate from AWS Certificate Manager" is incorrect as this is not supported or necessary for staticwebsites on Amazon S3.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html'},{question:"A company has deployed a REST API using Amazon API Gateway with a Lambda authorizer. The company needs to log who has accessed the API and how the caller accessed the API. They also require logs that include errors and execution traces for the Lambda authorizer. Which combination of actions should the Developer take to meet these requirements? (Select TWO.)",answers:[{text:"Create an API Gateway usage plan.",isCorrect:!1},{text:"Enable API Gateway access logs.",isCorrect:!0},{text:"Enable API Gateway execution logging.",isCorrect:!0},{text:"Enable detailed logging in Amazon CloudWatch.",isCorrect:!1},{text:"Enable server access logging.",isCorrect:!1}],explanation:'There are two types of API logging in CloudWatch: execution logging and access logging. In execution logging, API Gateway manages the CloudWatch Logs. The process includes creating log groups and log streams, and reporting to the log streams any caller\'s requests and responses.The logged data includes errors or execution traces (such as request or response parameter values or payloads), data used by Lambda authorizers, whether API keys are required, whether usage plans are enabled, and so on.In access logging, you, as an API Developer, want to log who has accessed your API and how the caller accessed the API. You can create your own log group or choose an existing log group that could be managed by API Gateway.CORRECT: "Enable API Gateway execution logging" is a correct answer.CORRECT: "Enable API Gateway access logs" is also a correct answer.INCORRECT: "Enable detailed logging in Amazon CloudWatch" is incorrect. Detailed logging does not provide the requested information.INCORRECT: "Create an API Gateway usage plan" is incorrect. This will not enable logging.INCORRECT: "Enable server access logging" is incorrect. This is a type of logging that applies to Amazon S3 buckets.References: https://docs.aws.amazon.com/apigateway/latest/Developerguide/setuplogging.html'},{question:"A static website that serves a collection of images runs from an Amazon S3 bucket in the useast1 region. The website is gaining in popularity and a is now being viewed around the world. How can a Developer improve the performance of the website for global users?",answers:[{text:"Use Amazon CloudFront to cache the website content",isCorrect:!0},{text:"Use Amazon S3 Transfer Acceleration to improve the performance of the website",isCorrect:!1},{text:"Use cross region replication to replicate the bucket to several global regions",isCorrect:!1},{text:"Use Amazon ElastiCache to cache the website content",isCorrect:!1}],explanation:'CloudFront is a web service that gives businesses and web application developers an easy and costeffective way to distribute content with low latency and high data transfer speeds. CloudFront is a good choice for distribution of frequently accessed static content that benefits from edge delivery—like popular website images, videos, media files or software downloads.CORRECT: "Use Amazon CloudFront to cache the website content" is the correct answer.INCORRECT: "Use Amazon ElastiCache to cache the website content" is incorrect as ElastiCache is used for caching the contents of databases, not S3 buckets.INCORRECT: "Use cross region replication to replicate the bucket to several global regions" is incorrect as though this would get the content closer to users it would not provide a mechanism for connecting to those copies. This could be achieved using Route 53 latency based routing however it would be easier to use CloudFront.INCORRECT: "Use Amazon S3 Transfer Acceleration to improve the performance of the website" is incorrect as this service is used for improving the performance of uploads to Amazon S3.References:https://aws.amazon.com/blogs/networkingandcontentdelivery/amazons3amazoncloudfrontamatchmadeinthecloud/ https://aws.amazon.com/premiumsupport/knowledgecenter/cloudfrontservestaticwebsite/'},{question:"A company runs a popular online game on premises. The application stores players' results in an inmemory database. The application is being migrated to AWS and the company needs to ensure there is no reduction in performance. Which database would be MOST suitable?",answers:[{text:"Amazon Elastic Beanstalk",isCorrect:!1},{text:"Amazon DynamoDB",isCorrect:!1},{text:"Amazon RDS",isCorrect:!1},{text:"Amazon ElastiCache",isCorrect:!0}],explanation:'ElastiCache is a fully managed, low latency, inmemory data store that supports either Memcached or Redis. With ElastiCache, management tasks such as provisioning, setup, patching, configuration, monitoring, backup, and failure recovery are taken care of, so you can focus on application development.Amazon ElastiCache is a popular choice for realtime use cases like Caching, Session Stores, Gaming, Geospatial Services, RealTime Analytics, and Queuing. For this scenario, the company is currently running an inmemory database and needs to ensure similar performance, so this is an ideal use case for ElastiCache.CORRECT: "Amazon ElastiCache" is the correct answer.INCORRECT: "Amazon RDS" is incorrect as RDS is not an inmemory database so the performance may not be as good as ElastiCache.INCORRECT: "Amazon DynamoDB" is incorrect as this is not an inmemory database. DynamoDB does offer great performance but if you need an inmemory cache you must use DynamoDB Accelerator (DAX).INCORRECT: "Amazon Elastic Beanstalk" is incorrect as this is not a database service at all. You can launch databases such as RDS through Elastic Beanstalk, however EB itself is a platform service responsible for launching and managing the resources.References:https://aws.amazon.com/blogs/database/buildingarealtimegamingleaderboardwithamazonelasticacheforredis/ https://aws.amazon.com/elasticache/features/'},{question:"A company needs to provide additional security for their APIs deployed on Amazon API Gateway. They would like to be able to authenticate their customers with a token. What is the SAFEST way to do this?",answers:[{text:"Use AWS Single Signon to authenticate the customers",isCorrect:!1},{text:"Create an API Gateway Lambda authorizer",isCorrect:!0},{text:"Create an Amazon Cognito identity pool",isCorrect:!1},{text:"Setup usage plans and distribute API keys to the customers",isCorrect:!1}],explanation:'A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API.A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller\'s identity.When a client makes a request to one of your API\'s methods, API Gateway calls your Lambda authorizer, which takes the caller\'s identity as input and returns an IAM policy as output.There are two types of Lambda authorizers:• A tokenbased Lambda authorizer (also called a TOKEN authorizer) receives the caller\'s identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token.• A request parameterbased Lambda authorizer (also called a REQUEST authorizer) receives the caller\'s identity in a combination of headers, query string parameters, stageVariables, and $context variables.For this scenario, a Lambda authorizer is the most secure method available. It can also be used with usage plans and AWS recommend that you don’t rely only on API keys, so a Lambda authorizer is a better solution.CORRECT: "Create an API Gateway Lambda authorizer" is the correct answer.INCORRECT: "Setup usage plans and distribute API keys to the customers" is incorrect as this is not the most secure (safest) option. AWS recommend that you don\'t rely on API keys as your only means of authentication and authorization for your APIs.INCORRECT: "Create an Amazon Cognito identity pool" is incorrect. You can create an authorizer in API Gateway that uses Cognito user pools, but not identity pools.INCORRECT: "Use AWS Single Signon to authenticate the customers" is incorrect. This is used to centrally access multiple AWS accounts and business applications from one place.References:https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayuselambdaauthorizer.html https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayapiusageplans.html https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayintegratewithcognito.html'},{question:"A Developer is creating a design for an application that will include Docker containers on Amazon ECS with the EC2 launch type. The Developer needs to control the placement of tasks onto groups of container instances organized by availability zone and instance type. Which Amazon ECS feature provides expressions that can be used to group container instances by the relevant attributes?",answers:[{text:"Task Placement Strategy",isCorrect:!1},{text:"Task Placement Constraints",isCorrect:!1},{text:"Cluster Query Language",isCorrect:!0},{text:"Task Group",isCorrect:!1}],explanation:'Cluster queries are expressions that enable you to group objects. For example, you can group container instances by attributes such as Availability Zone, instance type, or custom metadata.After you have defined a group of container instances, you can customize Amazon ECS to place tasks on container instances based on group. You can also apply a group filter when listing container instances.As an example, the following cluster query expressions selects instances with the specified instance type: attribute:ecs.instancetype == t2.smallThe following cluster query expressions selects instances in the useast1a or useast1b availability zones: attribute:ecs.availabilityzone in [useast1a, useast1b]The Developer should therefore use the cluster query language to generate expressions that can group the container instances by instance type and availability zone.CORRECT: "Cluster Query Language" is the correct answer.INCORRECT: "Task Group" is incorrect as this is just a set of related tasks with the same task group name.INCORRECT: "Task Placement Constraints" is incorrect. A task placement constraint is a rule that is considered during task placement. Task placement constraints can be specified when either running a task or creating a new service.INCORRECT: "Task Placement Strategy" is incorrect. A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/clusterquerylanguage.html'},{question:"The source code for an application is stored in a file named index.js that is in a folder along with a template file that includes the following code:AWSTemplateFormatVersion: '20100909'Transform: 'AWS::Serverless20161031'Resources:LambdaFunctionWithAPI:Type: AWS::Serverless::FunctionProperties:Handler: index.handlerRuntime: nodejs12.xWhat does a Developer need to do to prepare the template so it can be deployed using an AWS CLI command?",answers:[{text:"Run the aws cloudformation compile command to base64 encode and embed the source file into a modified CloudFormation template",isCorrect:!1},{text:"Run the aws serverless createpackage command to embed the source file directly into the existing CloudFormation template",isCorrect:!1},{text:"Run the aws cloudformation package command to upload the source code to an Amazon S3 bucket and produce a modified CloudFormation template",isCorrect:!0},{text:"Run the aws lambda zip command to package the source file together with the CloudFormation template and deploy the resulting zip archive",isCorrect:!1}],explanation:'The template shown is an AWS SAM template for deploying a serverless application. This can be identified by the template header: Transform: \'AWS::Serverless20161031\'The Developer will need to package and then deploy the template. To do this the source code must be available in the same directory or referenced using the “codeuri” parameter. Then, the Developer can use the “aws cloudformation package” or “sam package” commands to prepare the local artifacts (local paths) that your AWS CloudFormation template references.The command uploads local artifacts, such as source code for an AWS Lambda function or a Swagger file for an AWS API Gateway REST API, to an S3 bucket. The command returns a copy of your template, replacing references to local artifacts with the S3 location where the command uploaded the artifacts.Once that is complete the template can be deployed using the “aws cloudformation deploy” or “sam deploy” commands.Therefore, the next step in this scenario is for the Developer to run the “aws cloudformation” package command to upload the source code to an Amazon S3 bucket and produce a modified CloudFormation template. An example of this command is provided below:aws cloudformation package templatefile /path_to_template/template.json s3bucket bucketname outputtemplatefilepackagedtemplate.jsonCORRECT: "Run the aws cloudformation package command to upload the source code to an Amazon S3 bucket and produce a modified CloudFormation template" is the correct answer.INCORRECT: "Run the aws cloudformation compile command to base64 encode and embed the source file into a modified CloudFormation template" is incorrect as the Developer should run the “aws cloudformation package” command.INCORRECT: "Run the aws lambda zip command to package the source file together with the CloudFormation template and deploy the resulting zip archive" is incorrect as the Developer should run the “aws cloudformation package” command which will automatically copy the relevant files to Amazon S3.INCORRECT: "Run the aws serverless createpackage command to embed the source file directly into the existing CloudFormation template" is incorrect as the Developer has the choice to run either “aws cloudformation package” or “sam package”, but not “aws serverless createpackage”.References: https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html'},{question:"A company uses Amazon DynamoDB to store sensitive data that must be encrypted. The company security policy mandates that data must be encrypted before it is submitted to DynamoDB. How can a Developer meet these requirements?",answers:[{text:"Use the DynamoDB Encryption Client to enable endtoend protection using clientside encryption.",isCorrect:!0},{text:"Use AWS Certificate Manager (ACM) to create one certificate for each DynamoDB table.",isCorrect:!1},{text:"Use the UpdateTable operation to switch to a customer managed customer master key (CMK).",isCorrect:!1},{text:"Use the UpdateTable operation to switch to an AWS managed customer master key (CMK).",isCorrect:!1}],explanation:'In addition to encryption at rest, which is a serverside encryption feature, AWS provides the Amazon DynamoDB Encryption Client. This clientside encryption library enables you to protect your table data before submitting it to DynamoDB. With serverside encryption, your data is encrypted in transit over an HTTPS connection, decrypted at the DynamoDB endpoint, and then reencrypted before being stored in DynamoDB. Clientside encryption provides endtoend protection for your data from its source to storage in DynamoDB. CORRECT: "Use the DynamoDB Encryption Client to enable endtoend protection using clientside encryption" is the correct answer. INCORRECT: "Use the UpdateTable operation to switch to a customer managed customer master key (CMK)" is incorrect. This will not ensure data is encrypted before it is submitted to DynamoDB; to meet this requirement, clientside encryption must be used. INCORRECT: "Use the UpdateTable operation to switch to an AWS managed customer master key (CMK)" is incorrect. is will not ensure data is encrypted before it is submitted to DynamoDB; to meet this requirement, clientside encryption must be used. INCORRECT: "Use AWS Certificate Manager (ACM) to create one certificate for each DynamoDB table" is incorrect. ACM is used to create SSL/TLS certificates and you cannot attach these to a DynamoDB table. References: https://docs.aws.amazon.com/kms/latest/Developerguide/servicesdynamodb.html'},{question:"An Amazon RDS database that stores product information for an online eCommerce marketplace is experiencing heavy demand. An increase in read requests is causing the database performance to be impacted and is affecting database writes. What is the best way to offload the read traffic from the database with MINIMAL code changes and cost?",answers:[{text:"Change the RDS database instance type to an instance with more CPU/RAM",isCorrect:!1},{text:"Create an ElastiCache Memcached cluster and modify the application to send read requests to the cluster",isCorrect:!1},{text:"Create an RDS Read Replica and modify the application to send read requests to the replica",isCorrect:!0},{text:"Create an RDS Multi AZ DB and modify the application to send read requests to the standby DB",isCorrect:!1}],explanation:'Amazon RDS read replicas are used for offloading reads from the primary database instance. Read replicas provide a readonly copy of the database. In this scenario this represents the simplest way of achieving the required outcome. The application will need to be modified to point to the read replica for all read requests. This requires some code changes, but they are minimal.CORRECT: "Create an RDS Read Replica and modify the application to send read requests to the replica" is the correct answer.INCORRECT: "Change the RDS database instance type to an instance with more CPU/RAM" is incorrect as this is not a way of “offloading the read traffic from the database”. This is an example of scaling vertically, rather than scaling horizontally.INCORRECT: "Create an RDS Multi AZ DB and modify the application to send read requests to the standby DB" is incorrect as we are trying to simply offload read traffic which is a use case for a read replica. However, it is possible for some database engines (MySQL and MariaDB) to combine multiAZ and read replicas.INCORRECT: "Create an ElastiCache Memcached cluster and modify the application to send read requests to the cluster" is incorrect as this would require more code changes and higher cost. For this use case an RDS read replica will be simpler and cheaper.References: https://aws.amazon.com/rds/features/readreplicas/'},{question:"A developer is building a web application that will be hosted on Amazon EC2 instances. The EC2 instances will store configuration data in an Amazon S3 bucket. What is the SAFEST way to allow the EC2 instances to access the S3 bucket?",answers:[{text:"Store an access key and secret ID that has the necessary permissions on the EC2 instances",isCorrect:!1},{text:"Use the AWS SDK and authenticate with a user account that has the necessary permissions on the EC2 instances",isCorrect:!1},{text:"Create an IAM Role with an AWS managed policy attached that has the necessary permissions and attach the role to the EC2 instances",isCorrect:!1},{text:"Create an IAM Role with a customermanaged policy attached that has the necessary permissions and attach the role to the EC2 instances",isCorrect:!0}],explanation:'Applications that run on an EC2 instance must include AWS credentials in their AWS API requests. You could have your developers store AWS credentials directly within the EC2 instance and allow applications in that instance to use those credentials. But developers would then have to manage the credentials and ensure that they securely pass the credentials to each instance and update each EC2 instance when it\'s time to rotate the credentials. That\'s a lot of additional work.Instead, you can and should use an IAM role to manage temporary credentials for applications that run on an EC2 instance. When you use a role, you don\'t have to distribute longterm credentials (such as a user name and password or access keys) to an EC2 instance. Instead, the role supplies temporary permissions that applications can use when they make calls to other AWS resources. When you launch an EC2 instance, you specify an IAM role to associate with the instance. Applications that run on the instance can then use the rolesupplied temporary credentials to sign API requests. There are two answers that would work in this scenario. In one a customermanaged policy is used and in the other an AWS managed policy is used. The customermanaged policy is more secure in this situation as it can be locked down with more granularity to ensure the EC2 instances can only read and write to the specific bucket.With an AWS managed policy, you must choose from read only or full access and full access would provide more access than is required:CORRECT: "Create an IAM Role with a customermanaged policy attached that has the necessary permissions and attach the role to the EC2 instances" is the correct answer.INCORRECT: "Store an access key and secret ID that has the necessary permissions on the EC2 instances" is incorrect as storing access keys on the EC2 instances is insecure and cumbersome to manage.INCORRECT: "Create an IAM Role with an AWS managed policy attached that has the necessary permissions and attach the role to the EC2 instances" is incorrect as the AWS managed policy would provide more privileges than required.INCORRECT: "Use the AWS SDK and authenticate with a user account that has the necessary permissions on the EC2 instances " is incorrect as you cannot authenticate through the AWS SDK using a user account on an EC2 instance.References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switchroleec2.html'},{question:"An application is running on an Amazon EC2 Linux instance. The instance needs to make AWS API calls to several AWS services. What is the MOST secure way to provide access to the AWS services with MINIMAL management overhead?",answers:[{text:"Store the credentials in the ~/.aws/credentials file",isCorrect:!1},{text:"Use EC2 instance profiles",isCorrect:!0},{text:"Use AWS KMS to store and retrieve credentials",isCorrect:!1},{text:"Store the credentials in AWS CloudHSM",isCorrect:!1}],explanation:'An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. Using an instance profile you can attach an IAM Role to an EC2 instance that the instance can then assume in order to gain access to AWS services.CORRECT: "Use EC2 instance profiles" is the correct answer.INCORRECT: "Use AWS KMS to store and retrieve credentials" is incorrect as KMS is used to manage encryption keys.INCORRECT: "Store the credentials in AWS CloudHSM" is incorrect as CloudHSM is also used to manage encryption keys. It is similar to KMS but uses a dedicated hardware device that is not multitenant.INCORRECT: "Store the credentials in the ~/.aws/credentials file" is incorrect as this is not the most secure option. The credentials file is associated with the AWS CLI and used for passing credentials in the form of an access key ID and secret access key when making programmatic requests from the command line.References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switchroleec2_instanceprofiles.html'},{question:"A developer has deployed a serverless application with AWS Lambda. The function must make remote calls to external endpoints. Which configuration element in Lambda can be used store the connection strings related to the external endpoints?",answers:[{text:"Aliases",isCorrect:!1},{text:"Versions",isCorrect:!1},{text:"Tags",isCorrect:!1},{text:"Environment variables",isCorrect:!0}],explanation:'Use environment variables to pass environmentspecific settings to your code. For example, you can have two functions with the same code but different configuration. One function connects to a test database, and the other connects to a production database. In this situation, you use environment variables to tell the function the hostname and other connection details for the database. You might also set an environment variable to configure your test environment to use more verbose logging or more detailed tracing.Therefore, using environment variables is the correct place to store the connection strings associated with the external endpoints.CORRECT: "Environment variables" is the correct answer.INCORRECT: "Aliases" is incorrect. A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN.INCORRECT: "Tags" is incorrect. Tags are keyvalue pairs that you attach to AWS resources to better organize them.INCORRECT: "Versions" is incorrect. You can use versions to manage the deployment of your AWS Lambda functions.References: https://docs.aws.amazon.com/lambda/latest/dg/configurationenvvars.html'},{question:"A Development team would like to migrate their existing application code from a GitHub repository to AWS CodeCommit. What needs to be created before they can migrate a cloned repository to CodeCommit over HTTPS?",answers:[{text:"A public and private SSH key file",isCorrect:!1},{text:"A GitHub secure authentication token",isCorrect:!1},{text:"An Amazon EC2 IAM role with CodeCommit permissions",isCorrect:!1},{text:"A set of credentials generated from IAM",isCorrect:!0}],explanation:'The simplest way to set up connections to AWS CodeCommit repositories is to configure Git credentials for CodeCommit in the IAM console, and then use those credentials for HTTPS connections.You can also use these same credentials with any thirdparty tool or individual development environment (IDE) that supports HTTPS authentication using a static user name and password. For examples, see For Connections from Development Tools.CORRECT: "A set of credentials generated from IAM" is the correct answer.INCORRECT: "A GitHub secure authentication token" is incorrect as this is not how you authenticated to CodeCommit.INCORRECT: "A public and private SSH key file" is incorrect as that is required for accessing CodeCommit using SSH.INCORRECT: "An Amazon EC2 IAM role with CodeCommit permissions" is incorrect as that would be used to provide access to administer CodeCommit. However, the question is asking how to authenticate a Git client to CodeCommit using HTTPS.References: https://docs.aws.amazon.com/codecommit/latest/userguide/settingupgc.html'},{question:"An eCommerce application uses an Amazon RDS database with Amazon ElastiCache in front. Stock volume data is updated dynamically in listings as sales are made. Customers have complained that occasionally the stock volume data is incorrect, and they end up purchasing items that are out of stock. A Developer has checked the front end and indeed some items display the incorrect stock count. What could be causing this issue?",answers:[{text:"The Amazon RDS database has insufficient IOPS provisioned for its EBS volumes.",isCorrect:!1},{text:"The stock volume data is being retrieved using a writethrough ElastiCache cluster.",isCorrect:!1},{text:"The Amazon RDS database is deployed as MultiAZ and the standby is inconsistent.",isCorrect:!1},{text:"The cache is not being invalidated when the stock volume data is changed.",isCorrect:!0}],explanation:'Amazon ElastiCache is being used to cache data from the Amazon RDS database to improve performance when performing queries. In this case the cache has stale stock volume data stored and is returning this information when customers are purchasing items.The resolution is to ensure that the cache is invalidated whenever the stock volume data is changed. This can be done in the application layer.CORRECT: "The cache is not being invalidated when the stock volume data is changed" is the correct answer.INCORRECT: "The stock volume data is being retrieved using a writethrough ElastiCache cluster" is incorrect. If this was the case the data would not be stale.INCORRECT: "The Amazon RDS database is deployed as MultiAZ and the standby is inconsistent" is incorrect. MultiAZ standbys are not used for reading data and the replication is synchronous so it would not be inconsistent.INCORRECT: "The Amazon RDS database has insufficient IOPS provisioned for its EBS volumes" is incorrect. This is not the issue here; the stale data is being retrieved from the ElastiCache database.References: https://docs.aws.amazon.com/AmazonElastiCache/latest/redug/Strategies.html'},{question:"A legacy service has an XMLbased SOAP interface. The Developer wants to expose the functionality of the service to external clients with the Amazon API Gateway. Which technique will accomplish this?",answers:[{text:"Create a RESTful API with the API Gateway; pass the incoming JSON to the SOAP interface through an Application Load Balancer",isCorrect:!1},{text:"Create a RESTful API with the API Gateway; pass the incoming XML to the SOAP interface through an Application Load Balancer",isCorrect:!1},{text:"Create a RESTful API with the API Gateway; transform the incoming XML into a valid message for the SOAP interface using mapping templates",isCorrect:!1},{text:"Create a RESTful API with the API Gateway; transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates",isCorrect:!0}],explanation:'Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services as well as data stored in the AWS Cloud.In API Gateway, an API\'s method request can take a payload in a different format from the corresponding integration request payload, as required in the backend. Similarly, the backend may return an integration response payload different from the method response payload, as expected by the frontend.API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response.If an existing legacy service returns XMLstyle data, you can use the API Gateway to transform the output to JSON as part of your modernization effort. The API Gateway can be configured to transform the output of legacy services from XML to JSON, allowing them to make a move that is seamless and nondisruptive. The transformation is specified using JSONSchema.Therefore, the technique the Developer should use is to create a RESTful API with the API Gateway and transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates.CORRECT: "Create a RESTful API with the API Gateway; transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates" is the correct answer.INCORRECT: "Create a RESTful API with the API Gateway; pass the incoming JSON to the SOAP interface through an Application Load Balancer" is incorrect as we don’t need an ALB to do this, we can use a mapping template within the API Gateway which will be more costefficient.INCORRECT: "Create a RESTful API with the API Gateway; pass the incoming XML to the SOAP interface through an Application Load Balancer" is incorrect as the incoming data will be JSON, not XML as the Developer needs to publish a modern application interface. A mapping template should also be used in place of the ALB.INCORRECT: "Create a RESTful API with the API Gateway; transform the incoming XML into a valid message for the SOAP interface using mapping templates" is incorrect as the incoming data will be JSON, not XML as the Developer needs to publish a modern application interface.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/modelsmappings.html'},{question:"A company uses an Amazon S3 bucket to store a large number of sensitive files relating to eCommerce transactions. The company has a policy that states that all data written to the S3 bucket must be encrypted. How can a Developer ensure compliance with this policy?",answers:[{text:"Create an Amazon CloudWatch alarm that notifies an administrator if unencrypted objects are uploaded to the S3 bucket",isCorrect:!1},{text:"Enable ServerSide Encryption with Amazon S3Managed Keys (SSES3) on the Amazon S3 bucket",isCorrect:!1},{text:"Create an S3 bucket policy that denies any S3 Put request that does not include the xamzserversideencryption",isCorrect:!0},{text:"Create a bucket policy that denies the S3 PutObject request with the attribute xamzacl having values public read, publicreadwrite, or authenticatedread",isCorrect:!1}],explanation:'To encrypt an object at the time of upload, you need to add a header called xamzserversideencryption to the request to tell S3 to encrypt the object using SSEC, SSES3, or SSEKMS. The following code example shows a Put request using SSES3.Enabling encryption on an S3 bucket does not enforce encryption however, so it is still necessary to take extra steps to force compliance with the policy. As the message in the image below states, bucket policies are applied before encryption settings so PUT requests without encryption information can be rejected by a bucket policy: Therefore, we need to create an S3 bucket policy that denies any S3 Put request that do not include the xamzserversideencryption header. There are two possible values for the xamzserversideencryption header: AES256, which tells S3 to use S3managed keys, and aws:kms, which tells S3 to use AWS KMS–managed keys.CORRECT: "Create an S3 bucket policy that denies any S3 Put request that does not include the xamzserversideencryption" is the correct answer.INCORRECT: "Create a bucket policy that denies the S3 PutObject request with the attribute xamzacl having values publicread, publicreadwrite, or authenticatedread" is incorrect. This policy means that authenticated users cannot upload objects to the bucket if the objects have public permissions.INCORRECT: "Enable ServerSide Encryption with Amazon S3Managed Keys (SSES3) on the Amazon S3 bucket" is incorrect as this will enable default encryption but will not enforce encryption on the S3 bucket. You do still need to enable default encryption on the bucket, but this alone will not enforce encryption.INCORRECT: "Create an Amazon CloudWatch alarm that notifies an administrator if unencrypted objects are uploaded to the S3 bucket" is incorrect. This is operationally difficult to manage and only notifies, it does not prevent.References: https://aws.amazon.com/blogs/security/howtopreventuploadsofunencryptedobjectstoamazons3/'},{question:"An application exports documents to an Amazon S3 bucket. The data must be encrypted at rest and company policy mandates that encryption keys must be rotated annually. How can this be achieved automatically and with the LEAST effort?",answers:[{text:"Encrypt the data within the application before writing to S3",isCorrect:!1},{text:"Import a custom key into AWS KMS and configure automatic rotation",isCorrect:!1},{text:"Use AWS KMS keys with automatic rotation enabled",isCorrect:!0},{text:"Configure automatic rotation with AWS Secrets Manager",isCorrect:!1}],explanation:'With AWS KMS you can choose to have AWS KMS automatically rotate CMKs every year, provided that those keys were generated within AWS KMS HSMs. Automatic key rotation is not supported for imported keys, asymmetric keys, or keys generated in an AWS CloudHSM cluster using the AWS KMS custom key store feature.If you choose to import keys to AWS KMS or asymmetric keys or use a custom key store, you can manually rotate them by creating a new CMK and mapping an existing key alias from the old CMK to the new CMK.If you choose to have AWS KMS automatically rotate keys, you don’t have to reencrypt your data. AWS KMS automatically keeps previous versions of keys to use for decryption of data encrypted under an old version of a key. All new encryption requests against a key in AWS KMS are encrypted under the newest version of the key.CORRECT: "Use AWS KMS keys with automatic rotation enabled" is the correct answer.INCORRECT: "Import a custom key into AWS KMS and configure automatic rotation" is incorrect as per the explanation above KMS will not automatically rotate imported encryption keys (it can automatically rotate imported CMKs though).INCORRECT: "Encrypt the data within the application before writing to S3" is incorrect as this is both an incomplete solution (where would the encryption keys come from) and would also likely require more maintenance and management overhead.INCORRECT: "Configure automatic rotation with AWS Secrets Manager" is incorrect as Secrets Manager is used for rotating credentials, not encryption keys.References: https://aws.amazon.com/kms/faqs/'},{question:"A Developer is creating a REST service using Amazon API Gateway with AWS Lambda integration. The service adds data to a spreadsheet and the data is sent as query string parameters in the method request. How should the Developer convert the query string parameters to arguments for the Lambda function?",answers:[{text:"Enable request validation",isCorrect:!1},{text:"Change the integration type",isCorrect:!1},{text:"Create a mapping template",isCorrect:!0},{text:"Include the Amazon Resource Name (ARN) of the Lambda function",isCorrect:!1}],explanation:'Standard API Gateway parameter and response code mapping templates allow you to map parameters onetoone and map a family of integration response status codes (matched by a regular expression) to a single response status code.Mapping template overrides provides you with the flexibility to perform manytoone parameter mappings; override parameters after standard API Gateway mappings have been applied; conditionally map parameters based on body content or other parameter values; programmatically create new parameters on the fly; and override status codes returned by yourintegration endpoint.Any type of request parameter, response header, or response status code may be overridden.Following are example uses for a mapping template override:• To create a new header (or overwrite an existing header) as a concatenation of two parameters• To override the response code to a success or failure code based on the contents of the body• To conditionally remap a parameter based on its contents or the contents of some other parameter• To iterate over the contents of a json body and remap key value pairs to headers or query stringsTherefore, the Developer can convert the query string parameters by creating a mapping template.CORRECT: "Create a mapping template" is the correct answer.INCORRECT: "Enable request validation" is incorrect as this is used to configure API Gateway to perform basic validation of an API request before proceeding with the integration request.INCORRECT: "Include the Amazon Resource Name (ARN) of the Lambda function" is incorrect as that doesn’t assist with converting the query string parameters.INCORRECT: "Change the integration type" is incorrect as to perform a conversion the Lambda integration does not need to have a different integration type such as Lambda proxy.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayoverriderequestresponseparameters.html'},{question:"A developer is designing a web application that will run on Amazon EC2 Linux instances using an Auto Scaling Group. The application should scale based on a threshold for the number of users concurrently using the application. How should the Auto Scaling Group be configured to scale out?",answers:[{text:"Create a custom Amazon CloudWatch metric for concurrent users",isCorrect:!0},{text:"Create a custom Amazon CloudWatch metric for memory usage",isCorrect:!1},{text:"Use a target tracking scaling policy",isCorrect:!1},{text:"Use the Amazon CloudWatch metric “NetworkIn”",isCorrect:!1}],explanation:'You can create a custom CloudWatch metric for your EC2 Linux instance statistics by creating a script through the AWS Command Line Interface (AWS CLI). Then, you can monitor that metric by pushing it to CloudWatch. In this scenario you could then monitor the number of users currently logged in.CORRECT: "Create a custom Amazon CloudWatch metric for concurrent users" is the correct answer.INCORRECT: "Use the Amazon CloudWatch metric “NetworkIn”" is incorrect as this will only shows statistics for the number of inbound connections, not the number of concurrent users.INCORRECT: "Use a target tracking scaling policy" is incorrect as this is used to maintain a certain number of instances based on a target utilization.INCORRECT: "Create a custom Amazon CloudWatch metric for memory usage" is incorrect as memory usage does not tell us how many users are logged in.References: https://aws.amazon.com/premiumsupport/knowledgecenter/cloudwatchcustommetrics/'},{question:"A Developer has completed some code updates and needs to deploy the updates to an Amazon Elastic Beanstalk environment. The environment includes twelve Amazon EC2 instances and there can be no reduction in application performance and availability during the update. Which deployment policy is the most cost-effective choice to suit these requirements?",answers:[{text:"Rolling",isCorrect:!1},{text:"All at once",isCorrect:!1},{text:"Immutable",isCorrect:!1},{text:"Rolling with additional batch",isCorrect:!0}],explanation:'AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments. Each deployment policy has advantages and disadvantages and it’s important to select the best policy to use for each situation.The following tables summarizes the different deployment policies:The “rolling with additional batch” policy will add an additional batch of instances, updates those instances, then move onto the next batch.Rolling with additional batch:• Like Rolling but launches new instances in a batch ensuring that there is full availability.• Application is running at capacity.• Can set the bucket size.• Application is running both versions simultaneously.• Small additional cost.• Additional batch is removed at the end of the deployment.• Longer deployment.• Good for production environments.For this scenario there can be no reduction in application performance and availability during the update. The question also asks for the most costeffective choice.Therefore, the “rolling with additional batch” is the best choice as it will ensure fully availability of the application but minimize cost as the additional batch size can be kept small.CORRECT: "Rolling with additional batch" is the correct answer.INCORRECT: "Rolling" is incorrect as this will result in a reduction in capacity as there is no additional batch of instances introduced to the environment. This is a better choice if speed is required and a reduction in capacity of a batch size is acceptable.INCORRECT: "All at once" is incorrect as this will take the application down and cause a complete outage of the application during the update.INCORRECT: "Immutable" is incorrect as this is the most expensive option as it doubles capacity with a whole new set of instances attached to a new ASG.References:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.rollingversiondeploy.html'},{question:"A Developer implemented a static website hosted in Amazon S3 that makes web service requests hosted in Amazon API Gateway and AWS Lambda. The site is showing an error that reads:“No 'AccessControlAllowOrigin' header is present on the requested resource. Origin 'null' is therefore not allowed access.”What should the Developer do to resolve this issue?",answers:[{text:"Add the AccessControlRequestMethod header to the request",isCorrect:!1},{text:"Enable crossorigin resource sharing (CORS) for the method in API Gateway",isCorrect:!0},{text:"Enable crossorigin resource sharing (CORS) on the S3 bucket",isCorrect:!1},{text:"Add the AccessControlRequestHeaders header to the request",isCorrect:!1}],explanation:'Crossorigin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. In this scenario the S3 bucket is the requestor and is requesting access to resources served by Amazon API Gateway and AWS Lambda. Therefore, the CORS configuration must be enabled on the requested endpoint which is the method in API Gateway.CORRECT: "Enable crossorigin resource sharing (CORS) for the method in API Gateway" is the correct answer.INCORRECT: "Enable crossorigin resource sharing (CORS) on the S3 bucket" is incorrect as CORS must be enabled on the requested endpoint which is API Gateway, not S3.INCORRECT: "Add the AccessControlRequestMethod header to the request" is incorrect as this is a request header value that asks permission to use a specific HTTP method.INCORRECT: "Add the AccessControlRequestHeaders header to the request" is incorrect as this notifies a server what headers will be sent in a request.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html'},{question:"A development team is migrating data from various file shares to AWS from onpremises. The data will be migrated into a single Amazon S3 bucket. What is the SIMPLEST method to ensure the data is encrypted at rest in the S3 bucket?",answers:[{text:"Ensure all requests use the xamzserversideencryptioncustomerkey header",isCorrect:!1},{text:"Use SSL to transmit the data over the Internet",isCorrect:!1},{text:"Ensure all requests use the xamzserversideencryption header",isCorrect:!1},{text:"Enable default encryption when creating the bucket",isCorrect:!0}],explanation:'Amazon S3 default encryption provides a way to set the default encryption behavior for an S3 bucket. You can set default encryption on a bucket so that all new objects are encrypted when they are stored in the bucket. The objects are encrypted using serverside encryption with either Amazon S3managed keys (SSES3) or customer master keys (CMKs) stored in AWS Key Management Service (AWS KMS). CORRECT: "Enable default encryption when creating the bucket" is the correct answer. INCORRECT: "Use SSL to transmit the data over the Internet" is incorrect as this only deals with encrypting the data whilst it is being transmitted, it does not provide encryption at rest. INCORRECT: "Ensure all requests use the xamzserversideencryptioncustomerkey header" is incorrect as it is unnecessary to use customerprovided keys. This is used with clientside encryption which is more complex to manage and is not required in this scenario. INCORRECT: "Ensure all requests use the xamzserversideencryption header" is incorrect as though this has the required effect of ensuring all data is encrypted, it is not the simplest method. In this scenario there is a team migrating data from different file shares which increases the risk of human error where a team member may neglect to add the header to the API call. Using default encryption on the bucket is a simpler solution. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/bucketencryption.html'},{question:"A company recently migrated a multitier application to AWS. The web tier runs on an Auto Scaling group of Amazon EC2 instances and the database tier uses Amazon DynamoDB. The database tier requires extremely high performance and most requests are repeated read requests. What service can be used to scale the database tier for BEST performance?",answers:[{text:"Amazon SQS",isCorrect:!1},{text:"Amazon CloudFront",isCorrect:!1},{text:"Amazon DynamoDB Accelerator (DAX)",isCorrect:!0},{text:"Amazon ElastiCache",isCorrect:!1}],explanation:'Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, inmemory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add inmemory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management.How it works:• DAX is a writethrough caching service – this means the data is written to the cache as well as the back end store atthe same time.• Allows you to point your DynamoDB API calls at the DAX cluster and if the item is in the cache (cache hit), DAX returnsthe result to the application.• If the item requested is not in the cache (cache miss) then DAX performs an Eventually Consistent GetItem operationsagainst DynamoDB• Retrieval of data from DAX reduces the read load on DynamoDB tables.• This may result in being able to reduce the provisioned read capacity on the table. DynamoDB DAX is the correct solution for best performance for a readheavy workload.CORRECT: "Amazon DynamoDB Accelerator (DAX)" is the correct answer.INCORRECT: "Amazon CloudFront" is incorrect. CloudFront is a content delivery network (CDN) that is used for serving static assets from a cache around the globe. It is used to get content closer to end users for better performance. However, it cannot cache DynamoDB read requests.INCORRECT: "Amazon ElastiCache" is incorrect. ElastiCache is an inmemory database that can be used for caching read requests to a backend database. However, ElastiCache is not the correct choice to put in front of DynamoDB (better for RDS), you should choose DAX instead.INCORRECT: "Amazon SQS" is incorrect. An SQS queue is used for decoupling application components, this will not assist with improving the performance of the DynamoDB database.References: https://aws.amazon.com/dynamodb/dax/'},{question:"An application is using Amazon DynamoDB as its data store and needs to be able to read 100 items per second as strongly consistent reads. Each item is 5 KB in size. What value should be set for the table's provisioned throughput for reads?",answers:[{text:"250 Read Capacity Units",isCorrect:!1},{text:"500 Read Capacity Units",isCorrect:!1},{text:"50 Read Capacity Units",isCorrect:!1},{text:"200 Read Capacity Units",isCorrect:!0}],explanation:'With provisioned capacity mode, you specify the number of data reads and writes per second that you require for your application.Read capacity unit (RCU):• Each API call to read data from your table is a read request.• Read requests can be strongly consistent, eventually consistent, or transactional.• For items up to 4 KB in size, one RCU can perform one strongly consistent read request per second.• Items larger than 4 KB require additional RCUs.• For items up to 4 KB in size, one RCU can perform two eventually consistent read requests per second.• Transactional read requests require two RCUs to perform one read per second for items up to 4 KB.• For example, a strongly consistent read of an 8 KB item would require two RCUs, an eventually consistent read of an 8 KB item would require one RCU, and a transactional read of an 8 KB item would require four RCUs.Write capacity unit (WCU):• Each API call to write data to your table is a write request.• For items up to 1 KB in size, one WCU can perform one standard write request per second.• Items larger than 1 KB require additional WCUs.• Transactional write requests require two WCUs to perform one write per second for items up to 1 KB.• For example, a standard write request of a 1 KB item would require one WCU, a standard write request of a 3 KB item would require three WCUs, and a transactional write request of a 3 KB item would require six WCUs.To determine the number of RCUs required to handle 100 strongly consistent reads per/second with an average item size of 5KB, perform the following steps:1. Determine the average item size by rounding up the next multiple of 4KB (5KB rounds up to 8KB).2. Determine the RCU per item by dividing the item size by 4KB (8KB/4KB = 2).3. Multiply the value from step 2 with the number of reads required per second (2x100 = 200).CORRECT: "200 Read Capacity Units" is the correct answer.INCORRECT: "50 Read Capacity Units" is incorrect.INCORRECT: "250 Read Capacity Units" is incorrect.INCORRECT: "500 Read Capacity Units" is incorrect.References: https://aws.amazon.com/dynamodb/pricing/provisioned/'},{question:"A Developer has created an Amazon Cognito user pool and configured a domain for it. The Developer wants to add signup and sign in pages to an app with a company logo. What should the Developer do to meet these requirements?",answers:[{text:"Upload the company logo to an Amazon S3 bucket. Specify the S3 object path in the app client settings in Amazon Cognito.",isCorrect:!1},{text:"Create a REST API using Amazon API Gateway and add a Cognito authorizer. Upload the company logo to a stage in the API.",isCorrect:!1},{text:"Customize the Amazon Cognito hosted web UI and add the company logo.",isCorrect:!0},{text:"Create a custom login page that includes the company logo and upload it to Amazon Cognito. Specify the login page in the app client settings.",isCorrect:!1}],explanation:'When you create a user pool in Amazon Cognito and then configure a domain for it, Amazon Cognito automatically provisions a hosted web UI to let you add signup and signin pages to your app. You can add a custom logo or customize the CSS for the hosted web UI.CORRECT: "Customize the Amazon Cognito hosted web UI and add the company logo" is the correct answer.INCORRECT: "Create a REST API using Amazon API Gateway and add a Cognito authorizer. Upload the company logo to a stage in the API" is incorrect. There is no need to add a REST API to this solution.INCORRECT: "Upload the company logo to an Amazon S3 bucket. Specify the S3 object path in the app client settings in Amazon Cognito" is incorrect. This is not required as the hosted web UI can be used.INCORRECT: "Create a custom login page that includes the company logo and upload it to Amazon Cognito. Specify the login page in the app client settings" is incorrect. This is not required as the hosted web UI can be used.References: https://aws.amazon.com/premiumsupport/knowledgecenter/cognitohostedwebui/'},{question:"AWS CodeBuild builds code for an application, creates the Docker image, pushes the image to Amazon Elastic Container Registry (Amazon ECR), and tags the image with a unique identifier. If the Developers already have AWS CLI configured on their workstations, how can the Docker images be pulled to the workstations?",answers:[{text:"Run the output of the following: aws ecr getlogin and then run: docker pull REPOSITORY URI : TAG",isCorrect:!0},{text:"Run the following: docker pull REPOSITORY URI : TAG",isCorrect:!1},{text:"Run the following: aws ecr getlogin and then run: docker pull REPOSITORY URI : TAG",isCorrect:!1},{text:"Run the output of the following: aws ecr getdownloadurlforlayer and then run: docker pull REPOSITORY URI : TAG",isCorrect:!1}],explanation:'If you would like to run a Docker image that is available in Amazon ECR, you can pull it to your local environment with the docker pull command. You can do this from either your default registry or from a registry associated with another AWS account.Docker CLI does not support standard AWS authentication methods, so client authentication must be handled so that ECR knows who is requesting to push or pull an image. To do this you can issue the aws ecr getlogin or aws ecr getloginpassword (AWS CLI v2) and then use the output to login using docker login and then issue a docker pull command specifying the repository and image with the REPOSITORY URI : TAG format.CORRECT: "Run the output of the following: aws ecr getlogin and then run: docker pull REPOSITORY URI : TAG" is the correct answer.INCORRECT: "Run the following: docker pull REPOSITORY URI : TAG" is incorrect as you need to authenticate first.INCORRECT: "Run the following: aws ecr getlogin and then run: docker pull REPOSITORY URI : TAG" is incorrect as you need to run the output of the “aws ecr getlogin” command before you can issue a “docker pull” command.INCORRECT: "Run the output of the following: aws ecr getdownloadurlforlayer and then run: docker pull REPOSITORY URI :TAG" is incorrect as the first command is incorrect. You need to run the output of the “aws ecr getlogin” command instead.References: https://docs.aws.amazon.com/AmazonECR/latest/userguide/dockerpullecrimage.html'},{question:"A Developer is using AWS SAM to create a template for deploying a serverless application. The Developer plans deploy an AWS Lambda function and an Amazon DynamoDB table using the template. Which resource types should the Developer specify? (Select TWO.)",answers:[{text:"AWS::Serverless:Function",isCorrect:!0},{text:"AWS::Serverless:API",isCorrect:!1},{text:"AWS::Serverless::Application",isCorrect:!1},{text:"AWS::Serverless:LayerVersion",isCorrect:!1},{text:"AWS::Serverless::SimpleTable",isCorrect:!0}],explanation:'A serverless application is a combination of Lambda functions, event sources, and other resources that work together to perform tasks. Note that a serverless application is more than just a Lambda function—it can include additional resources such as APIs, databases, and event source mappings.AWS SAM templates are an extension of AWS CloudFormation templates, with some additional components that make them easier to work with.To create a Lambda function using an AWS SAM template the Developer can use the AWS::Serverless::Function resource type.The AWS::Serverless::Function resource type can be used to Create a Lambda function, IAM execution role, and event source mappings that trigger the function.To create a DynamoDB table using an AWS SAM template the Developer can use the AWS::Serverless::SimpleTable resource type which creates a DynamoDB table with a single attribute primary key. It is useful when data only needs to be accessed via a primary key.CORRECT: "AWS::Serverless:Function" is a correct answer.CORRECT: "AWS::Serverless:SimpleTable" is also a correct answer.INCORRECT: "AWS::Serverless::Application" is incorrect as this embeds a serverless application from the AWS Serverless Application Repository or from an Amazon S3 bucket as a nested application.INCORRECT: "AWS::Serverless:LayerVersion" is incorrect as this creates a Lambda LayerVersion that contains library or runtime code needed by a Lambda Function.INCORRECT: "AWS::Serverless:API" is incorrect as this creates a collection of Amazon API Gateway resources and methods that can be invoked through HTTPS endpoints.References: https://docs.aws.amazon.com/serverlessapplicationmodel/latest/developerguide/whatissam.html'},{question:"A developer needs use the attribute of an Amazon S3 object that uniquely identifies the object in a bucket. Which of the following represents an Object Key?",answers:[{text:"s3://dctlabs/Development/Projects.xls",isCorrect:!1},{text:"Project=Blue",isCorrect:!1},{text:"Development/Projects.xls",isCorrect:!0},{text:"arn:aws:s3:::dctlabs",isCorrect:!1}],explanation:'When you create an object, you specify the key name, which uniquely identifies the object in the bucket. For example, in the Amazon S3 console, when you highlight a bucket, a list of objects in your bucket appears. These names are the object keys.The name for a key is a sequence of Unicode characters whose UTF8 encoding is at most 1024 bytes long.The Amazon S3 data model is a flat structure: you create a bucket, and the bucket stores objects. There is no hierarchy of subbuckets or subfolders. However, you can infer logical hierarchy using key name prefixes and delimiters as the Amazon S3 console does. The Amazon S3 console supports a concept of folders. Suppose that your bucket (admincreated) has four objectswith the following object keys:• Development/Projects.xls• Finance/statement1.pdf• Private/taxdocument.pdf• s3dg.pdfThe console uses the key name prefixes (Development/, Finance/, and Private/) and delimiter (\'/\') to present a folder structure as shown.CORRECT: "Development/Projects.xls" is the correct answer.INCORRECT: "s3://dctlabs/Development/Projects.xls" is incorrect as this is the full path to a file including the bucket name and object key.INCORRECT: "Project=Blue" is incorrect as this is an example of an object tag. You can use object tagging to categorize storage. Each tag is a keyvalue pair.INCORRECT: "arn:aws:s3:::dctlabs" is incorrect as this is the Amazon Resource Name (ARN) of a bucket.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html'},{question:"The development team is working on an API that will be served from Amazon API Gateway. The API will serve three environments PROD, DEV, and TEST and requires a cache size of 250GB. What is the MOST costefficient deployment strategy?",answers:[{text:"Create a single API Gateway with three stages and enable the cache for all environments",isCorrect:!1},{text:"Create a single API Gateway with three deployments and configure a global cache of 250GB",isCorrect:!1},{text:"Create three API Gateways, one for each environment and enable the cache for the DEV and TEST environments only when required",isCorrect:!1},{text:"Create a single API Gateway with three stages and enable the cache for the DEV and TEST environments only when required",isCorrect:!0}],explanation:'You can enable API caching in Amazon API Gateway to cache your endpoint\'s responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API.Caching is enabled for a stage. When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified timetolive (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint.The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled. In this scenario we are asked to choose the most costefficient solution. Therefore, the best answer is to use a single API Gateway with three stages and, as caching is enabled per stage, we can choose to save cost by only enabling the cache on DEV and TEST when we need to perform tests relating to that functionality.CORRECT: "Create a single API Gateway with three stages and enable the cache for the DEV and TEST environments only when required" is the correct answer.INCORRECT: "Create three API Gateways, one for each environment and enable the cache for the DEV and TEST environments only when required" is incorrect. It is unnecessary to create separate API Gateways. This will increase complexity. Instead we can choose to use stages for the different environments.INCORRECT: "Create a single API Gateway with three stages and enable the cache for all environments" is incorrect as this would not be the most costefficient option.INCORRECT: "Create a single API Gateway with three deployments and configure a global cache of 250GB" is incorrect. When you deploy you API, you do so to a stage. Caching is enabled at the stage level, not globally.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaycaching.html'},{question:"A Developer is building a WebSocket API using Amazon API Gateway. The payload sent to this API is JSON that includes an action key which can have multiple values. The Developer must integrate with different routes based on the value of the action key of the incoming JSON payload. How can the Developer accomplish this task with the LEAST amount of configuration?",answers:[{text:"Create a mapping template to map the action key to an integration request.",isCorrect:!1},{text:"Set the value of the route selection expression to $request.body.action.",isCorrect:!0},{text:"Create a separate stage for each possible value of the action key.",isCorrect:!1},{text:"Set the value of the route selection expression to $default.",isCorrect:!1}],explanation:'In your WebSocket API, incoming JSON messages are directed to backend integrations based on routes that you configure. (NonJSON messages are directed to a $default route that you configure.)A route includes a route key, which is the value that is expected once a route selection expression is evaluated.The routeSelectionExpression is an attribute defined at the API level. It specifies a JSON property that is expected to be present in the message payload.For example, if your JSON messages contain an action property and you want to perform different actions based on this property, your route selection expression might be ${request.body.action}. Your routing table would specify which action to perform by matching the value of the action property against the custom route key values that you have defined in the table.CORRECT: "Set the value of the route selection expression to $request.body.action" is the correct answer.INCORRECT: "Create a separate stage for each possible value of the action key" is incorrect. There is no need to create separate stages, the action key can be used for routing as described above.INCORRECT: "Create a mapping template to map the action key to an integration request" is incorrect. Mapping templates are not used for routing to different integrations, they are used for transforming data.INCORRECT: "Set the value of the route selection expression to $default" is incorrect. The $default route is used for routing nonJSON messages.References: https://docs.aws.amazon.com/apigateway/latest/Developerguide/websocketapideveloproutes.html'},{question:"A Developer is creating an AWS Lambda function that will process data from an Amazon Kinesis data stream. The function is expected to be invoked 50 times per second and take 100 seconds to complete each request. What MUST the Developer do to ensure the functions runs without errors?",answers:[{text:"No action is required as AWS Lambda can easily accommodate this requirement",isCorrect:!1},{text:"Increase the concurrency limit for the function",isCorrect:!1},{text:"Contact AWS and request to increase the limit for concurrent executions",isCorrect:!0},{text:"Implement exponential backoff in the function code",isCorrect:!1}],explanation:'Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function\'s concurrency.Concurrency is subject to a Regional limit that is shared by all functions in a Region. For an initial burst of traffic, your functions\' cumulative concurrency in a Region can reach an initial level of between 500 and 3000, which varies per Region:• 3000 – US West (Oregon), US East (N. Virginia), Europe (Ireland)• 1000 – Asia Pacific (Tokyo), Europe (Frankfurt)• 500 – Other RegionsAfter the initial burst, your functions\' concurrency can scale by an additional 500 instances each minute. This continues until there are enough instances to serve all requests, or until a concurrency limit is reached. When requests come in faster than your function can scale, or when your function is at maximum concurrency, additional requests fail with a throttling error (429 status code).The function continues to scale until the account\'s concurrency limit for the function\'s Region is reached. The function catches up to demand, requests subside, and unused instances of the function are stopped after being idle for some time. Unused instances are frozen while they\'re waiting for requests and don\'t incur any charges.The regional concurrency limit starts at 1,000. You can increase the limit by submitting a request in the Support Center console. Calculating concurrency requirements for this scenarioTo calculate the concurrency requirements for this scenario, simply multiply the invocation requests per second (50) with the average execution time in seconds (100). This calculation is 50 x 100 = 5,000.Therefore, 5,000 concurrent executions is over the default limit and the Developer will need to request in the AWS Support Center console.CORRECT: "Contact AWS and request to increase the limit for concurrent executions" is the correct answer.INCORRECT: "No action is required as AWS Lambda can easily accommodate this requirement" is incorrect as by default the AWS account will be limited. Lambda can easily scale to this level of demand however the account limits must first be increased.INCORRECT: "Increase the concurrency limit for the function" is incorrect as the default account limit of 1,000 concurrent executions will mean you can only assign up to 900 executions to the function (100 must be left unreserved). This is insufficient for this requirement to the account limit must be increased.INCORRECT: "Implement exponential backoff in the function code" is incorrect. Exponential backoff means configuring the application to wait longer between API calls, slowing the demand. However, this is not a good resolution to this issue as it will have negative effects on the application. The correct choice is to raise the account limits so the function can concurrently execute according to its requirements.References: https://docs.aws.amazon.com/lambda/latest/dg/invocationscaling.html'},{question:"A Developer is creating a DynamoDB table for storing application logs. The table has 5 write capacity units (WCUs). The Developer needs to configure the read capacity units (RCUs) for the table. Which of the following configurations represents themost efficient use of throughput?",answers:[{text:"Strongly consistent reads of 15 RCUs reading items that are 1KB in size",isCorrect:!1},{text:"Strongly consistent reads of 5 RCUs reading items that are 4 KB in size",isCorrect:!1},{text:"Eventually consistent reads of 15 RCUs reading items that are 1 KB in size",isCorrect:!1},{text:"Eventually consistent reads of 5 RCUs reading items that are 4 KB in size",isCorrect:!0}],explanation:'In this scenario the Developer needs to maximize efficiency of RCUs. Therefore, the Developer will need to consider the item size and consistency model to determine the most efficient usage of RCUs.Item size/consistency model: we know that both 1 KB items and 4 KB items consume the same number of RCUs as a read capacity unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size.The following bullets provide the read throughput for each configuration:• Eventually consistent, 15 RCUs, 1 KB item = 30 items/s = 2 items per RCU• Strongly consistent, 15 RCUs, 1 KB item = 15 items/s = 1 item per RCU• Eventually consistent, 5 RCUs, 4 KB item = 10 items/s = 2 items per RCU• Strongly consistent, 5 RCUs, 4 KB item = 5 items/s = 1 item per RCUFrom the above we can see that 4 KB items with eventually consistent reads is the most efficient option. Therefore, the Developer should choose the option “Eventually consistent reads of 5 RCUs reading items that are 4 KB in size”. This will achieve 2x 4 KB items per RCU.CORRECT: "Eventually consistent reads of 5 RCUs reading items that are 4 KB in size" is the correct answer.INCORRECT: "Eventually consistent reads of 15 RCUs reading items that are 1 KB in size" is incorrect as described above.INCORRECT: "Strongly consistent reads of 5 RCUs reading items that are 4 KB in size" is incorrect as described above.INCORRECT: "Strongly consistent reads of 15 RCUs reading items that are 1KB in size" is incorrect as described above.References: https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/ProvisionedThroughput.html'},{question:"A company is running a web application on Amazon EC2 behind an Elastic Load Balancer (ELB). The company is concerned about the security of the web application and would like to secure the application with SSL certificates. The solution should not have any performance impact on the EC2 instances. What steps should be taken to secure the web application? (Select TWO.)",answers:[{text:"Add an SSL certificate to the Elastic Load Balancer",isCorrect:!0},{text:"Configure ServerSide Encryption with KMS managed keys",isCorrect:!1},{text:"Install SSL certificates on the EC2 instances",isCorrect:!1},{text:"Configure the Elastic Load Balancer with SSL passthrough",isCorrect:!1},{text:"Configure the Elastic Load Balancer for SSL termination",isCorrect:!0}],explanation:'The requirements clearly state that we cannot impact the performance of the EC2 instances at all. Therefore, we will not be able to add certificates to the EC2 instances as that would place a burden on the CPU when encrypting and decrypting data. We are therefore left with configuring SSL on the Elastic Load Balancer itself. For this we need to add an SSL certificate to the ELB and then configure the ELB for SSL termination.You can create an HTTPS listener, which uses encrypted connections (also known as SSL offload). This feature enables traffic encryption between your load balancer and the clients that initiate SSL or TLS sessions.To use an HTTPS listener, you must deploy at least one SSL/TLS server certificate on your load balancer. The load balancer uses a server certificate to terminate the frontend connection and then decrypt requests from clients before sending them to the targets.This is the most secure solution we can created without adding any performance impact to the EC2 instances.CORRECT: "Add an SSL certificate to the Elastic Load Balancer" is a correct answer.CORRECT: "Configure the Elastic Load Balancer for SSL termination" is also a correct answer.INCORRECT: "Configure the Elastic Load Balancer with SSL passthrough" is incorrect as this would be used to forward encrypted packets directly to the EC2 instance for termination but we do not want to add SSL certificates to the EC2 instances due to the extra processing required.INCORRECT: "Install SSL certificates on the EC2 instances" is incorrect as we do not want to add SSL certificates to the EC2 instances due to the extra processing required.INCORRECT: "Configure ServerSide Encryption with KMS managed keys" is incorrect as this applies to Amazon S3, not ELB.References: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/createhttpslistener.html'},{question:"An application uses Amazon API Gateway, an AWS Lambda function and a DynamoDB table. The developer requires that another Lambda function is triggered when an item lifecycle activity occurs in the DynamoDB table. How can this be achieved?",answers:[{text:"Enable a DynamoDB stream and trigger the Lambda function synchronously from the stream",isCorrect:!0},{text:"Enable a DynamoDB stream and trigger the Lambda function asynchronously from the stream",isCorrect:!1},{text:"Configure an Amazon CloudTrail API alarm that sends a message to an Amazon SQS queue. Configure the Lambda function to poll the queue and invoke the function synchronously",isCorrect:!1},{text:"Configure an Amazon CloudWatch alarm that sends an Amazon SNS notification. Trigger the Lambda function asynchronously from the SNS notification",isCorrect:!1}],explanation:'Amazon DynamoDB is integrated with AWS Lambda so that you can create triggers—pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.If you enable DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with an AWS Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table\'s stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.CORRECT: "Enable a DynamoDB stream and trigger the Lambda function synchronously from the stream" is the correct answer.INCORRECT: "Enable a DynamoDB stream and trigger the Lambda function asynchronously from the stream" is incorrect as the invocation should be synchronous.INCORRECT: "Configure an Amazon CloudWatch alarm that sends an Amazon SNS notification. Trigger the Lambda function asynchronously from the SNS notification" is incorrect as you cannot configure a CloudWatch alarm that notifies based on item lifecycle events. It is better to use DynamoDB streams and integrate Lambda.INCORRECT: "Configure an Amazon CloudTrail API alarm that sends a message to an Amazon SQS queue. Configure the Lambda function to poll the queue and invoke the function synchronously" is incorrect. There is no such alarm that notifies from Amazon CloudTrail relating to item lifecycle events.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html'},{question:"A company runs an application on a fleet of web servers running on Amazon EC2 instances. The web servers are behind an Elastic Load Balancer (ELB) and use an Amazon DynamoDB table for storing session state. A Developer has been asked to implement a mechanism for automatically deleting session state data that is older than 24 hours. What is the SIMPLEST solution to this requirement?",answers:[{text:"Add an attribute with the expiration time; enable the Time To Live feature based on that attribute",isCorrect:!0},{text:"Add an attribute with the expiration time; name the attribute ItemExpiration",isCorrect:!1},{text:"Write a script that deletes old records; schedule the scripts as a cron job on an Amazon EC2 instance",isCorrect:!1},{text:"Each day, create a new table to hold session data; delete the previous day's table",isCorrect:!1}],explanation:'Time to Live (TTL) for Amazon DynamoDB lets you define when items in a table expire so that they can be automatically deleted from the database. With TTL enabled on a table, you can set a timestamp for deletion on a peritem basis, allowing you to limit storage usage to only those records that are relevant.TTL is useful if you have continuously accumulating data that loses relevance after a specific time period (for example, session data, event logs, usage patterns, and other temporary data). If you have sensitive data that must be retained only for a certain amount of time according to contractual or regulatory obligations, TTL helps you ensure that it is removed promptly and as scheduled.When Time to Live (TTL) is enabled on a table in Amazon DynamoDB, a background job checks the TTL attribute of items to determine whether they are expired.DynamoDB compares the current time, in epoch time format, to the value stored in the userdefined Number attribute of an item. If the attribute’s value is in the epoch time format, is less than the current time, and is not older than 5 years, the item is deleted.Processing takes place automatically, in the background, and doesn\'t affect read or write traffic to the table. In addition, deletes performed via TTL are not counted towards capacity units or request units. TTL deletes are available at no additional cost.For this requirement, the Developer must add an attribute to each item with the expiration time in epoch format and then enable the Time To Live (TTL) feature based on that attribute.CORRECT: "Add an attribute with the expiration time; enable the Time To Live feature based on that attribute" is the correct answer.INCORRECT: "Each day, create a new table to hold session data; delete the previous day\'s table" is incorrect. This solution would delete some data that is not 24 hours old as it would have to run at a specific time.INCORRECT: "Write a script that deletes old records; schedule the scripts as a cron job on an Amazon EC2 instance" is incorrect. This is not an elegant solution and would also cost more as it requires RCUs/WCUs to delete the items.INCORRECT: "Add an attribute with the expiration time; name the attribute ItemExpiration" is incorrect as this is not a complete solution. You also need to enable the TTL feature on the table.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworksttl.html'}]},{id:"aws-developer-5",title:"AWS Developer Associate (DVA-C02) Practice Test Latest 2025 5",description:"Additional practice questions covering AWS development topics.",questions:[{question:"A Developer at a company is working on a CloudFormation template to set up resources. Resources will be defined using code and provisioned based on certain conditions defined in the Conditions section. Which section of a CloudFormation template cannot be associated with Condition?",answers:[{text:"Resources",isCorrect:!1},{text:"Parameters",isCorrect:!0},{text:"Conditions",isCorrect:!1},{text:"Outputs",isCorrect:!1}],explanation:"Correct option:ParametersParameters enable you to input custom values to your CloudFormation template each time you create or update a stack. Please see this note to understand how to define a parameter in a template: via https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameterssectionstructure.html The optional Conditions section contains statements that define the circumstances under which entities are created or configured. For example, you can create a condition and then associate it with a resource or output so that AWS CloudFormation only creates the resource or output if the condition is true.You might use conditions when you want to reuse a template that can create resources in different contexts, such as a test environment versus a production environment. In your template, you can add an EnvironmentType input parameter, which accepts either prod or test as inputs. For the production environment, you might include Amazon EC2 instances with certain capabilities; however, for the test environment, you want to use reduced capabilities to save money.Conditions cannot be used within the Parameters section. After you define all your conditions, you can associate them with resources and resource properties only in the Resources and Outputs sections of a template.Please review this note for more details: via https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditionssectionstructure.html Please visit https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameterssectionstructure.html for more information on the parameter structure.Incorrect options:Resources Resources section describes the resources that you want to provision in your AWS CloudFormation stacks. You can associate conditions with the resources that you want to conditionally create.Conditions You actually define conditions in this section of the CloudFormation templateOutputs The optional Outputs section declares output values that you can import into other stacks (to create crossstack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name for a stack to make the bucket easier to find. You can associate conditions with the outputs that you want to conditionally create.References:https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameterssectionstructure.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditionssectionstructure.htm"},{question:"Your company has configured AWS Organizations to manage multiple AWS accounts. Within each AWS account, there are many CloudFormation scripts running. Your manager has requested that each script output the account number of the account the script was executed in. Which Pseudo parameter will you use to get this information?",answers:[{text:"AWS::Region",isCorrect:!1},{text:"AWS::AccountId",isCorrect:!0},{text:"AWS::StackName",isCorrect:!1},{text:"AWS::NoValue",isCorrect:!1}],explanation:'Correct option:AWS::AccountIdUsing CloudFormation, you can create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and AWS CloudFormation takes care of provisioning and configuring those resources for you.Pseudo parameters are parameters that are predefined by AWS CloudFormation. You do not declare them in your template. Use them the same way as you would a parameter, as the argument for the Ref function.AWS::AccountId returns the AWS account ID of the account in which the stack is being created.Incorrect options:AWS::NoValue This removes the corresponding resource property when specified as a return value in the Fn::If intrinsic function.AWS::Region Returns a string representing the AWS Region in which the encompassing resource is being created, such as uswest2.AWS::StackName Returns the name of the stack as specified with the aws cloudformation createstack command, such as "teststack".Reference:https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudoparameterreference.html'},{question:"A Development team wants to run their container workloads on Amazon ECS. Each application container needs to share data with another container to collect logs and metrics. What should the Development team do to meet these requirements?",answers:[{text:"Create a single pod specification. Include both containers in the specification. Mount a persistent volume to both containers",isCorrect:!1},{text:"Create two pod specifications. Make one to include the application container and the other to include the other container. Link the two pods together",isCorrect:!1},{text:"Create two task definitions. Make one to include the application container and the other to include the other container. Mount a shared volume between the two tasks",isCorrect:!1},{text:"Create one task definition. Specify both containers in the definition. Mount a shared volume between those two containers",isCorrect:!0}],explanation:'Amazon ECS tasks support Docker volumes. To use data volumes, you must specify the volume and mount point configurations in your task definition. Docker volumes are supported for the EC2 launch type only.To configure a Docker volume, in the task definition volumes section, define a data volume with name and DockerVolumeConfiguration values. In the containerDefinitions section, define multiple containers with mountPoints values that reference the name of the defined volume and the containerPath value to mount the volume at on the container.The containers should both be specified in the same task definition. Therefore, the Development team should create one task definition, specify both containers in the definition and then mount a shared volume between those two containersCORRECT: "Create one task definition. Specify both containers in the definition. Mount a shared volume between those two containers" is the correct answer.INCORRECT: "Create two pod specifications. Make one to include the application container and the other to include the other container. Link the two pods together" is incorrect as pods are a concept associated with the Elastic Kubernetes Service (EKS).INCORRECT: "Create two task definitions. Make one to include the application container and the other to include the other container. Mount a shared volume between the two tasks" is incorrect as a single task definition should be created with both containers.INCORRECT: "Create a single pod specification. Include both containers in the specification. Mount a persistent volume to both containers" is incorrect as pods are a concept associated with the Elastic Kubernetes Service (EKS).References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/dockervolumes.html'},{question:"A company has created a set of APIs using Amazon API Gateway and exposed them to partner companies. The APIs have caching enabled for all stages. The partners require a method of invalidating the cache that they can build into their applications. What can the partners use to invalidate the API cache?",answers:[{text:"They can use the query string parameter INVALIDATE_CACHE",isCorrect:!1},{text:"They can pass the HTTP header CacheControl: maxage=0",isCorrect:!0},{text:"They can invoke an AWS API endpoint which invalidates the cache",isCorrect:!1},{text:"They must wait for the TTL to expire",isCorrect:!1}],explanation:'You can enable API caching in Amazon API Gateway to cache your endpoint\'s responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API.When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified timetolive (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600seconds. TTL=0 means caching is disabled.A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests.The client must send a request that contains the CacheControl: maxage=0 header.The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint.To grant permission for a client, attach a policy of the following format to an IAM execution role for the user.This policy allows the API Gateway execution service to invalidate the cache for requests on the specified resource (orresources).Therefore, as described above the solution is to get the partners to pass the HTTP header CacheControl: maxage=0.CORRECT: "They can pass the HTTP header CacheControl: maxage=0" is the correct answer.INCORRECT: "They can use the query string parameter INVALIDATE_CACHE" is incorrect. This is not a valid method of invalidating the cache with API Gateway.INCORRECT: "They must wait for the TTL to expire" is incorrect as this is not true, you do not need to wait as you can pass the HTTP header CacheControl: maxage=0 whenever you need to in order to invalidate the cache.INCORRECT: "They can invoke an AWS API endpoint which invalidates the cache" is incorrect. This is not a valid method of invalidating the cache with API Gateway.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaycaching.html'},{question:"A Developer is building a three-tier web application that must be able to handle a minimum of 10,000 requests per minute. The requirements state that the web tier should be completely stateless while the application maintains session state data for users. How can the session state data be maintained externally, whilst keeping latency at the LOWEST possible value?",answers:[{text:"Create an Amazon RedShift instance, then implement session handling at the application level to leverage a database inside the RedShift database instance for session data storage",isCorrect:!1},{text:"Implement a shared Amazon EFS file system solution across the underlying Amazon EC2 instances, then implement session handling at the application level to leverage the EFS file system for session data storage",isCorrect:!1},{text:"Create an Amazon ElastiCache Redis cluster, then implement session handling at the application level to leverage the cluster for session data storage",isCorrect:!0},{text:"Create an Amazon DynamoDB table, then implement session handling at the application level to leverage the table for session data storage",isCorrect:!1}],explanation:'It is common to use key/value stores for storing session state data. The two options presented in the answers are Amazon DynamoDB and Amazon ElastiCache Redis. Of these two, ElastiCache will provide the lowest latency as it is an inmemorydatabase.Therefore, the best answer is to create an Amazon ElastiCache Redis cluster, then implement session handling at the application level to leverage the cluster for session data storage.CORRECT: "Create an Amazon ElastiCache Redis cluster, then implement session handling at the application level to leverage the cluster for session data storage" is the correct answer.INCORRECT: "Create an Amazon DynamoDB table, then implement session handling at the application level to leverage the table for session data storage" is incorrect as though this is a good solution for storing session state data, the latency will not be as low as with ElastiCache.INCORRECT: "Create an Amazon RedShift instance, then implement session handling at the application level to leverage a database inside the RedShift database instance for session data storage" is incorrect. RedShift is a data warehouse that is used for OLAP use cases, not for storing session state data.INCORRECT: "Implement a shared Amazon EFS file system solution across the underlying Amazon EC2 instances, then implement session handling at the application level to leverage the EFS file system for session data storage" is incorrect. For session state data a key/value store such as DynamoDB or ElastiCache will provide better performance.References: https://aws.amazon.com/caching/sessionmanagement/'},{question:"An AWS Lambda function authenticates to an external website using a regularly rotated username and password. The credentials need to be stored securely and must not be stored in the function code. What combination of AWS services can be used to achieve this requirement? (Select TWO.)",answers:[{text:"Amazon GuardDuty",isCorrect:!1},{text:"AWS Key Management Store (KMS)",isCorrect:!0},{text:"AWS Systems Manager Parameter Store",isCorrect:!0},{text:"AWS Certificate Manager (ACM)",isCorrect:!1},{text:"AWS Artifact",isCorrect:!1}],explanation:'With AWS Systems Manager Parameter Store, you can create secure string parameters, which are parameters that have a plaintext parameter name and an encrypted parameter value. Parameter Store uses AWS KMS to encrypt and decrypt the parameter values of secure string parameters.With Parameter Store you can create, store, and manage data as parameters with values. You can create a parameter in Parameter Store and use it in multiple applications and services subject to policies and permissions that you design. When you need to change a parameter value, you change one instance, rather than managing errorprone changes to numerous sources.Parameter Store supports a hierarchical structure for parameter names, so you can qualify a parameter for specific uses.To manage sensitive data, you can create secure string parameters. Parameter Store uses AWS KMS customer master keys (CMKs) to encrypt the parameter values of secure string parameters when you create or change them. It also uses CMKs to decrypt the parameter values when you access them. You can use the AWS managed CMK that Parameter Store creates for your account or specify your own customer managed CMK.Therefore, you can use a combination of AWS Systems Manager Parameter Store and AWS Key Management Store to store the credentials securely. These keys can be then be referenced in the Lambda function code or through environment variables.NOTE: Systems Manager Parameter Store does not natively perform rotation of credentials so this must be done in the application. AWS Secrets Manager does perform credential rotation however it is not an answer option for this question.CORRECT: "AWS Systems Manager Parameter Store" is a correct answer.CORRECT: "AWS Key Management Store (KMS)" is also a correct answer.INCORRECT: "AWS Certificate Manager (ACM)" is incorrect as this service is used to issue SSL/TLS certificates not encryption keys.INCORRECT: "AWS Artifact" is incorrect as this is a service to view compliance information about the AWS platformINCORRECT: "Amazon GuardDuty" is incorrect. Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads.References: https://docs.aws.amazon.com/kms/latest/developerguide/servicesparameterstore.html'},{question:"A critical application runs on an Amazon EC2 instance. A Developer has configured a custom Amazon CloudWatch metric that monitors application availability with a data granularity of 1 second. The Developer must be notified within 30 seconds if the application experiences any issues. What should the Developer do to meet this requirement?",answers:[{text:"Use Amazon CloudWatch Logs Insights and trigger an Amazon Eventbridge rule to send a notification.",isCorrect:!1},{text:"Specify an Amazon SNS topic for alarms when issuing the putmetricdata AWS CLI command.",isCorrect:!1},{text:"Configure a highresolution CloudWatch alarm and use Amazon SNS to send the alert.",isCorrect:!0},{text:"Use a default CloudWatch metric, configure an alarm, and use Amazon SNS to send the alert.",isCorrect:!1}],explanation:'If you set an alarm on a highresolution metric, you can specify a highresolution alarm with a period of 10 seconds or 30 seconds, or you can set a regular alarm with a period of any multiple of 60 seconds. There is a higher charge for highresolution alarms.Amazon SNS can then be used to send notifications based on the CloudWatch alarm.CORRECT: "Configure a highresolution CloudWatch alarm and use Amazon SNS to send the alert" is the correct answer.INCORRECT: "Specify an Amazon SNS topic for alarms when issuing the putmetricdata AWS CLI command" is incorrect. You cannot specify an SNS topic with this CLI command.INCORRECT: "Use Amazon CloudWatch Logs Insights and trigger an Amazon Eventbridge rule to send a notification" is incorrect.Logs Insights cannot be used for alarms or alerting based on custom CloudWatch metrics.INCORRECT: "Use a default CloudWatch metric, configure an alarm, and use Amazon SNS to send the alert" is incorrect. There is no default metric that would monitor the application uptime and the resolution would be lower.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html#highresolutionalarms'},{question:"When running a Rolling deployment in Elastic Beanstalk environment, only two batches completed the deployment successfully, while rest of the batches failed to deploy the updated version. Following this, the development team terminated the instances from the failed deployment. What will be the status of these failed instances post termination?",answers:[{text:"Elastic Beanstalk will replace the failed instances with instances running the application version from the oldest successful deployment",isCorrect:!1},{text:"Elastic Beanstalk will not replace the failed instances",isCorrect:!1},{text:"Elastic Beanstalk will replace the failed instances after the application version to be installed is manually chosen from AWS Console",isCorrect:!1},{text:"Elastic Beanstalk will replace the failed instances with instances running the application version from the most recent successful deployment",isCorrect:!0}],explanation:"Correct option:Elastic Beanstalk will replace them with instances running the application version from the most recent successful deploymentWhen processing a batch, Elastic Beanstalk detaches all instances in the batch from the load balancer, deploys the new application version, and then reattaches the instances. If you enable connection draining, Elastic Beanstalk drains existing connections from the Amazon EC2 instances in each batch before beginning the deployment.If a deployment fails after one or more batches completed successfully, the completed batches run the new version of your application while any pending batches continue to run the old version. You can identify the version running on the instances in your environment on the health page in the console. This page displays the deployment ID of the most recent deployment that was executed on each instance in your environment. If you terminate instances from the failed deployment, Elastic Beanstalk replaces them with instances running the application version from the most recent successful deployment.Incorrect options:Elastic Beanstalk will not replace the failed instancesElastic Beanstalk will replace the failed instances with instances running the application version from the oldest successful deploymentElastic Beanstalk will replace the failed instances after the application version to be installed is manually chosen from AWS ConsoleThese three options contradict the explanation provided above, so these options are incorrect.Reference:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.rollingversiondeploy.html"},{question:"A Lambda function is taking a long time to complete. The Developer has discovered that inadequate compute capacity is being allocated to the function. How can the Developer ensure that more compute capacity is allocated to the function?",answers:[{text:"Increase the reserved concurrency",isCorrect:!1},{text:"Increase the maximum execution time",isCorrect:!1},{text:"Allocate more memory to the function",isCorrect:!0},{text:"Use an instance type with more CPU",isCorrect:!1}],explanation:'You can allocate memory between 128 MB and 3,008 MB in 64MB increments. AWS Lambda allocates CPU power linearly in proportion to the amount of memory configured. At 1,792 MB, a function has the equivalent of one full vCPU (one vCPUsecond of credits per second).Therefore, the way provide more compute capacity to this function is to allocate more memory.CORRECT: "Allocate more memory to the function" is the correct answer.INCORRECT: "Use an instance type with more CPU" is incorrect as Lambda is a serverless service and you cannot choose an instance type for your function.INCORRECT: "Increase the maximum execution time" is incorrect as the function is not timing out, it’s just taking longer than expected due to having insufficient compute allocated.INCORRECT: "Increase the reserved concurrency" is incorrect as this would enable more invocations to run in parallel but would not add more CPU to each function execution.References: https://docs.aws.amazon.com/lambda/latest/dg/configurationconsole.html'},{question:"An application onpremises uses Linux servers and a relational database using PostgreSQL. The company will be migrating the application to AWS and require a managed service that will take care of capacity provisioning, load balancing, and autoscaling. Which combination of services should the Developer use? (Select TWO.)",answers:[{text:"Amazon RDS with PostrgreSQL",isCorrect:!0},{text:"Amazon EC2 with Auto Scaling",isCorrect:!1},{text:"AWS Elastic Beanstalk",isCorrect:!0},{text:"AWS Lambda with CloudWatch Events",isCorrect:!1},{text:"Amazon EC2 with PostgreSQL",isCorrect:!1}],explanation:'The company require a managed service therefore the Developer should choose to use Elastic Beanstalk for the compute layer and Amazon RDS with the PostgreSQL engine for the database layer.AWS Elastic Beanstalk will handle all capacity provisioning, load balancing, and autoscaling for the web frontend and Amazon RDS provides pushbutton scaling for the backend.CORRECT: "AWS Elastic Beanstalk" is a correct answer.CORRECT: "Amazon RDS with PostrgreSQL" is also a correct answer.INCORRECT: "Amazon EC2 with Auto Scaling" is incorrect as though these services will be used to provide the automatic scalability required for the solution, they still need to be managed. The questions asks for a managed solution and Elastic Beanstalk will manage this for you. Also, there is no mention of a load balancer so connections cannot be distributed toinstances.INCORRECT: "Amazon EC2 with PostgreSQL" is incorrect as the question asks for a managed service and therefore the database should be run on Amazon RDS.INCORRECT: "AWS Lambda with CloudWatch Events" is incorrect as there is no mention of refactoring application code to run on AWS Lambda.References:https://aws.amazon.com/elasticbeanstalk/ https://aws.amazon.com/rds/postgresql/'},{question:'A Developer has created a task definition that includes the following JSON code:"placementStrategy": [{"field": "attribute:ecs.availabilityzone","type": "spread"},{"field": "instanceId","type": "spread"}]What is the effect of this task placement strategy?',answers:[{text:"It distributes tasks evenly across Availability Zones and then distributes tasks evenly across the instances within each Availability Zone",isCorrect:!0},{text:"It distributes tasks evenly across Availability Zones and then bin packs tasks based on memory within each Availability Zone",isCorrect:!1},{text:"It distributes tasks evenly across Availability Zones and then distributes tasks randomly across instances within each Availability Zone",isCorrect:!1},{text:"It distributes tasks evenly across Availability Zones and then distributes tasks evenly across distinct instances within each Availability Zone",isCorrect:!1}],explanation:'A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service.Amazon ECS supports the following task placement strategies:binpackPlace tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.randomPlace tasks randomly.spreadPlace tasks evenly based on the specified value. Accepted values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availabilityzone.You can specify task placement strategies with the following actions: CreateService, UpdateService, and RunTask. You can also use multiple strategies together as in the example JSON code provided with the question.CORRECT: "It distributes tasks evenly across Availability Zones and then distributes tasks evenly across the instances within each Availability Zone" is the correct answer.INCORRECT: "It distributes tasks evenly across Availability Zones and then bin packs tasks based on memory within each Availability Zone" is incorrect as it does not use the binpack strategy.INCORRECT: "It distributes tasks evenly across Availability Zones and then distributes tasks evenly across distinct instances within each Availability Zone" is incorrect as it does not spread tasks across distinct instances (use a task placement constraint).INCORRECT: "It distributes tasks evenly across Availability Zones and then distributes tasks randomly across instances within each Availability Zone" is incorrect as it does not use the random strategy.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskplacementstrategies.html'},{question:"A Developer is developing a web application and will maintain separate sets of resources for the alpha, beta, and release stages. Each version runs on Amazon EC2 and uses an Elastic Load Balancer. How can the Developer create a single page to view and manage all of the resources?",answers:[{text:"Create an AWS Elastic Beanstalk environment for each stage",isCorrect:!1},{text:"Create a single AWS CodeDeploy deployment",isCorrect:!1},{text:"Deploy all resources using a single Amazon CloudFormation stack",isCorrect:!1},{text:"Create a resource group",isCorrect:!0}],explanation:'In AWS, a resource is an entity that you can work with. Examples include an Amazon EC2 instance, an AWS CloudFormation stack, or an Amazon S3 bucket. If you work with multiple resources, you might find it useful to manage them as a group rather than move from one AWS service to another for each task.By default, the AWS Management Console is organized by AWS service. But with Resource Groups, you can create a custom console that organizes and consolidates information based on criteria specified in tags, or the resources in an AWS CloudFormation stack. The following list describes some of the cases in which resource grouping can help organize yourresources.• An application that has different phases, such as development, staging, and production.• Projects managed by multiple departments or individuals.• A set of AWS resources that you use together for a common project or that you want to manage or monitor as a group.• A set of resources related to applications that run on a specific platform, such as Android or iOS.CORRECT: "Create a resource group" is the correct answer.INCORRECT: "Deploy all resources using a single Amazon CloudFormation stack" is incorrect as this would not be a best practice as it is better to create separate stacks to manage deployment separately.INCORRECT: "Create an AWS Elastic Beanstalk environment for each stage" is incorrect. It’s fine to create separate environments for each stage, however this won’t create a single view to view and manage all resources.INCORRECT: "Create a single AWS CodeDeploy deployment" is incorrect as each stage should be created in a separate deployment.References: https://docs.aws.amazon.com/ARG/latest/userguide/welcome.html'},{question:"A Developer has made an update to an application. The application serves users around the world and uses Amazon CloudFront for caching content closer to users. It has been reported that after deploying the application updates, users are not able to see the latest changes. How can the Developer resolve this issue?",answers:[{text:"Invalidate all the application objects from the edge caches",isCorrect:!0},{text:"Disable forwarding of query strings and request headers from the CloudFront distribution configuration",isCorrect:!1},{text:"Disable the CloudFront distribution and enable it again to update all the edge locations",isCorrect:!1},{text:"Remove the origin from the CloudFront configuration and add it again",isCorrect:!1}],explanation:'If you need to remove a file from CloudFront edge caches before it expires, you can do one of the following:• Invalidate the file from edge caches. The next time a viewer requests the file, CloudFront returns to the origin to fetch the latest version of the file.• Use file versioning to serve a different version of the file that has a different name. For more information, see Updating Existing Files Using Versioned File Names.In this case, the best option available is to invalidate all the application objects from the edge caches. This will result in the new objects being cached next time a request is made for them.CORRECT: "Invalidate all the application objects from the edge caches" is the correct answer.INCORRECT: "Remove the origin from the CloudFront configuration and add it again" is incorrect as this is going to cause all objects to be removed and then recached which is overkill and will cost more.INCORRECT: "Disable forwarding of query strings and request headers from the CloudFront distribution configuration" is incorrect as this is not a way to invalidate objects in Amazon CloudFront.INCORRECT: "Disable the CloudFront distribution and enable it again to update all the edge locations" is incorrect as this will not cause the objects to expire, they will expire whenever their expiration date occurs and must be invalidated to make this happen sooner.References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html'},{question:"A company is setting up a Lambda function that will process events from a DynamoDB stream. The Lambda function has been created and a stream has been enabled. What else needs to be done for this solution to work?",answers:[{text:"An eventsource mapping must be created on the DynamoDB side to associate the DynamoDB stream with the Lambda function",isCorrect:!1},{text:"An alarm should be created in CloudWatch that sends a notification to Lambda when a new entry is added to the DynamoDB stream",isCorrect:!1},{text:"An eventsource mapping must be created on the Lambda side to associate the DynamoDB stream with the Lambda function",isCorrect:!0},{text:"Update the CloudFormation template to map the DynamoDB stream to the Lambda function",isCorrect:!1}],explanation:'An event source mapping is an AWS Lambda resource that reads from an event source and invokes a Lambda function. You can use event source mappings to process items from a stream or queue in services that don\'t invoke Lambda functions directly.Lambda provides event source mappings for the following services.Services That Lambda Reads Events From• Amazon Kinesis• Amazon DynamoDB• Amazon Simple Queue ServiceAn event source mapping uses permissions in the function\'s execution role to read and manage items in the event source.Permissions, event structure, settings, and polling behavior vary by event source.The configuration of the event source mapping for streambased services (DynamoDB, Kinesis), and Amazon SQS, is made on the Lambda side.Note: for other services, such as Amazon S3 and SNS, the function is invoked asynchronously and the configuration is made on the source (S3/SNS) rather than Lambda.CORRECT: "An eventsource mapping must be created on the Lambda side to associate the DynamoDB stream with the Lambda function" is the correct answer.INCORRECT: "An alarm should be created in CloudWatch that sends a notification to Lambda when a new entry is added to the DynamoDB stream" is incorrect as you should use an eventsource mapping between Lambda and DynamoDB instead.INCORRECT: "An eventsource mapping must be created on the DynamoDB side to associate the DynamoDB stream with the Lambda function" is incorrect because for streambased services that don’t invoke Lambda functions directly, the configuration should be made on the Lambda side.INCORRECT: "Update the CloudFormation template to map the DynamoDB stream to the Lambda function" is incorrect as CloudFormation may not even be used in this scenario (it wasn’t mentioned) and wouldn’t continuously send events from DynamoDB streams to Lambda either.References: https://docs.aws.amazon.com/lambda/latest/dg/invocationeventsourcemapping.html'},{question:"You are a developer in a manufacturing company that has several servers onsite. The company decides to move new development to the cloud using serverless technology. You decide to use the AWS Serverless Application Model (AWS SAM) and work with an AWS SAM template file to represent your serverless architecture. Which of the following is NOT a valid serverless resource type?",answers:[{text:"AWS::Serverless::Api",isCorrect:!1},{text:"AWS::Serverless::SimpleTable",isCorrect:!1},{text:"AWS::Serverless::Function",isCorrect:!1},{text:"AWS::Serverless::UserPool",isCorrect:!0}],explanation:"Correct option:AWS::Serverless::UserPoolThe AWS Serverless Application Model (SAM) is an opensource framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML.SAM supports the following resource types:AWS::Serverless::ApiAWS::Serverless::ApplicationAWS::Serverless::FunctionAWS::Serverless::HttpApiAWS::Serverless::LayerVersionAWS::Serverless::SimpleTableAWS::Serverless::StateMachineUserPool applies to the Cognito service which is used for authentication for mobile app and web. There is no resource named UserPool in the Serverless Application Model.Incorrect options:AWS::Serverless::Function This resource creates a Lambda function, IAM execution role, and event source mappings that trigger the function.AWS::Serverless::Api This creates a collection of Amazon API Gateway resources and methods that can be invoked through HTTPS endpoints. It is useful for advanced use cases where you want full control and flexibility when you configure your APIs.AWS::Serverless::SimpleTable This creates a DynamoDB table with a single attribute primary key. It is useful when data only needs to be accessed via a primary key.Reference:https://docs.aws.amazon.com/serverlessapplicationmodel/latest/developerguide/samspecificationresourcesandproperties.html"},{question:"An application component writes thousands of itemlevel changes to a DynamoDB table per day. The developer requires that a record is maintained of the items before they were modified. What MUST the developer do to retain this information? (Select TWO.)",answers:[{text:"Use an AWS Lambda function to extract the item records from the notification and write to an S3 bucket",isCorrect:!1},{text:"Set the StreamViewType to OLD_IMAGE",isCorrect:!0},{text:"Create a CloudWatch alarm that sends a notification when an item is modified",isCorrect:!1},{text:"Set the StreamViewType to NEW_AND_OLD_IMAGES",isCorrect:!1},{text:"Enable DynamoDB Streams for the table",isCorrect:!0}],explanation:'DynamoDB Streams captures a timeordered sequence of itemlevel modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in nearreal time.You can also use the CreateTable or UpdateTable API operations to enable or modify a stream.The StreamSpecification parameter determines how the stream is configured:StreamEnabled — Specifies whether a stream is enabled (true) or disabled (false) for the table.StreamViewType — Specifies the information that will be written to the stream whenever data in the table is modified:• KEYS_ONLY — Only the key attributes of the modified item.• NEW_IMAGE — The entire item, as it appears after it was modified.• OLD_IMAGE — The entire item, as it appeared before it was modified.• NEW_AND_OLD_IMAGES — Both the new and the old images of the item.In this scenario, we only need to keep a copy of the items before they were modified. Therefore, the solution is to enable DynamoDB streams and set the StreamViewType to OLD_IMAGES.CORRECT: "Enable DynamoDB Streams for the table" is the correct answer.CORRECT: "Set the StreamViewType to OLD_IMAGE" is the correct answer.INCORRECT: "Create a CloudWatch alarm that sends a notification when an item is modified" is incorrect as DynamoDB streams is the best way to capture a timeordered sequence of itemlevel modifications in a DynamoDB table.INCORRECT: "Set the StreamViewType to NEW_AND_OLD_IMAGES" is incorrect as we only need to keep a record of the items before they were modified. This setting would place a record in the stream that includes the item before and after modification.INCORRECT: "Use an AWS Lambda function to extract the item records from the notification and write to an S3 bucket" is incorrect. There is no requirement to write the updates to S3 and if you did want to do this with Lambda you would need to extract the information from the stream, not a notification.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html'},{question:"You have created a Java application that uses RDS for its main data storage and ElastiCache for user session storage. The application needs to be deployed using Elastic Beanstalk and every new deployment should allow the application servers to reuse the RDS database. On the other hand, user session data stored in ElastiCache can be recreated for every deployment. Which of the following configurations will allow you to achieve this? (Select two)",answers:[{text:"RDS database defined in .ebextensions/",isCorrect:!1},{text:"ElastiCache bundled with the application source code",isCorrect:!1},{text:"ElastiCache database defined externally and referenced through environment variables",isCorrect:!1},{text:"RDS database defined externally and referenced through environment variables",isCorrect:!0},{text:"ElastiCache defined in .ebextensions/",isCorrect:!0}],explanation:"Correct option:ElastiCache defined in .ebextensions/ Any resources created as part of your .ebextensions is part of your Elastic Beanstalk template and will get deleted if the environment is terminated.via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.htmlRDS database defined externally and referenced through environment variables To decouple your database instance from your environment, you can run a database instance in Amazon RDS and configure your application to connect to it on launch. This enables you to connect multiple environments to a database, terminate an environment without affecting the database, and perform seamless updates with bluegreen deployments. To allow the Amazon EC2 instances in your environment to connect to an outside database, you can configure the environment's Auto Scaling group with an additional security group.Using Elastic Beanstalk with Amazon RDS: via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html Incorrect options:ElastiCache bundled with the application source code ElastiCache is an AWS service and cannot be bundled with the source code.RDS database defined in .ebextensions/ The lifetime of the RDS instance gets tied to the lifetime of the Elastic Beanstalk environment, so this option is incorrect.ElastiCache database defined externally and referenced through environment variables For the given usecase, the client is fine with losing user session data and hence defining it in .ebextensions/ is more appropriate.References:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customizeenvironmentresourceselasticache.html"},{question:"Your global organization has an IT infrastructure that is deployed using CloudFormation on AWS Cloud. One employee, in useast1 Region, has created a stack 'Application1' and made an exported output with the name 'ELBDNSName'. Another employee has created a stack for a different application 'Application2' in useast2 Region and also exported an output with the name 'ELBDNSName'. The first employee wanted to deploy the CloudFormation stack 'Application1' in useast2, but it got an error. What is the cause of the error?",answers:[{text:"Output Values in CloudFormation must have unique names across all Regions",isCorrect:!1},{text:"Exported Output Values in CloudFormation must have unique names within a single Region",isCorrect:!0},{text:"Output Values in CloudFormation must have unique names within a single Region",isCorrect:!1},{text:"Exported Output Values in CloudFormation must have unique names across all Regions",isCorrect:!1}],explanation:'Correct option:"Exported Output Values in CloudFormation must have unique names within a single Region"Using CloudFormation, you can create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and AWS CloudFormation takes care of provisioning and configuring those resources for you.A CloudFormation template has an optional Outputs section which declares output values that you can import into other stacks (to create crossstack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name for a stack to make the bucket easier to find.You can use the Export Output Values to export the name of the resource output for a crossstack reference. For each AWS account, export names must be unique within a region. In this case, we would have a conflict within useast2.via https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputssectionstructure.htmlIncorrect options:"Output Values in CloudFormation must have unique names across all Regions""Exported Output Values in CloudFormation must have unique names across all Regions""Output Values in CloudFormation must have unique names within a single Region"These three options contradict the explanation provided earlier, hence these options are incorrect.Reference:https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputssectionstructure.html'},{question:"Change management procedures at an organization require that a log is kept recording activity within AWS accounts. The activity that must be recorded includes API activity related to creating, modifying or deleting AWS resources. Which AWS service should be used to record this information?",answers:[{text:"Amazon CloudWatch",isCorrect:!1},{text:"AWS OpsWorks",isCorrect:!1},{text:"Amazon CloudTrail",isCorrect:!0},{text:"AWS XRay",isCorrect:!1}],explanation:'AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWSManagement Console, AWS SDKs, command line tools, and other AWS services.This event history simplifies security analysis, resource change tracking, and troubleshooting. In addition, you can use CloudTrail to detect unusual activity in your AWS accounts. These capabilities help simplify operational analysis and troubleshooting.Therefore, Amazon CloudTrail is the most suitable service for the requirements in this scenario.CORRECT: "Amazon CloudTrail" is the correct answer.INCORRECT: "Amazon CloudWatch" is incorrect as this service is used for performance monitoring, not recording API actions.INCORRECT: "AWS XRay" is incorrect as this is used for tracing application activity for performance and operational statistics.INCORRECT: "AWS OpsWorks" is incorrect as this is a configuration management service that helps you build and operate highly dynamic applications, and propagate changes instantly.References: https://aws.amazon.com/cloudtrail/features/'},{question:"A developer has deployed an application on an Amazon EC2 instance in a private subnet within a VPC. The subnet does not have Internet connectivity. The developer would like to write application logs to an Amazon S3 bucket. What MUST be configured to enable connectivity?",answers:[{text:"A VPC endpoint should be provisioned for S3",isCorrect:!0},{text:"A bucket policy needs to be added specifying the principles that are allowed to write data to the bucket",isCorrect:!1},{text:"An IAM role must be added to the instance that has permissions to write to the S3 bucket",isCorrect:!1},{text:"A VPN should be established to enable private connectivity to S3",isCorrect:!1}],explanation:'Please note that the question specifically asks how to enable connectivity so this is not about permissions. When using a private subnet with no Internet connectivity there are only two options available for connecting to Amazon S3 (which remember, is a service with a public endpoint, it’s not in your VPC).The first option is to enable Internet connectivity through either a NAT Gateway or a NAT Instance. However, there is no answer offering either of these as a solution. The other option is to enable a VPC endpoint for S3.The specific type of VPC endpoint to S3 is a Gateway Endpoint. EC2 instances running in private subnets of a VPC can use the endpoint to enable controlled access to S3 buckets, objects, and API functions that are in the same region as the VPC. You can then use an S3 bucket policy to indicate which VPCs and which VPC Endpoints have access to your S3 buckets.Therefore, the only answer that presents a solution to this challenge is to provision an VPC endpoint for S3.CORRECT: "A VPC endpoint should be provisioned for S3" is the correct answer.INCORRECT: "An IAM role must be added to the instance that has permissions to write to the S3 bucket" is incorrect. You do need to do this, but the question is asking about connectivity, not permissions.INCORRECT: "A bucket policy needs to be added specifying the principles that are allowed to write data to the bucket" is incorrect. You may choose to use a bucket policy to enable permissions but the question is asking about connectivity, not permissions.INCORRECT: "A VPN should be established to enable private connectivity to S3" is incorrect. You can create a VPN to establish an encrypted tunnel into a VPC from a location outside of AWS. However, you cannot create a VPN connection from a subnet within a VPC to Amazon S3.References: https://docs.aws.amazon.com/vpc/latest/userguide/vpcendpointss3.html'},{question:"A serverless application uses an AWS Lambda function to process Amazon S3 events. The Lambda function executes 20 times per second and takes 20 seconds to complete each execution. How many concurrent executions will the Lambda function require?",answers:[{text:"40",isCorrect:!1},{text:"20",isCorrect:!1},{text:"400",isCorrect:!0},{text:"5",isCorrect:!1}],explanation:'Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function\'s concurrency.To calculate the concurrency requirements for the Lambda function simply multiply the number of executions per second (20) by the time it takes to complete the execution (20).Therefore, for this scenario the calculation is 20 x 20 = 400.CORRECT: "400" is the correct answer.INCORRECT: "5" is incorrect. Please use the formula above to calculate concurrency requirements.INCORRECT: "40" is incorrect. Please use the formula above to calculate concurrency requirements.INCORRECT: "20" is incorrect. Please use the formula above to calculate concurrency requirements.References: https://docs.aws.amazon.com/lambda/latest/dg/invocationscaling.html'},{question:"A Developer is launching an application on Amazon ECS. The application should scale tasks automatically based on load and incoming connections must be spread across the containers. How should the Developer configure the ECS cluster?",answers:[{text:"Create an ECS Service with Auto Scaling and attach an Elastic Load Balancer",isCorrect:!0},{text:"Create a capacity provider and configure cluster auto scaling",isCorrect:!1},{text:"Write statements using the Cluster Query Language to scale the Docker containers",isCorrect:!1},{text:"Create an ECS Task Definition that uses Auto Scaling and Elastic Load Balancing",isCorrect:!1}],explanation:'Automatic scaling is the ability to increase or decrease the desired count of tasks in your Amazon ECS service automatically. Amazon ECS leverages the Application Auto Scaling service to provide this functionality.Amazon ECS publishes CloudWatch metrics with your service’s average CPU and memory usage. You can use these and other CloudWatch metrics to scale out your service (add more tasks) to deal with high demand at peak times, and to scale in your service (run fewer tasks) to reduce costs during periods of low utilization.Amazon ECS services support the Application Load Balancer, Network Load Balancer, and Classic Load Balancer load balancer types. Application Load Balancers are used to route HTTP/HTTPS (or Layer 7) traffic. Network Load Balancers and Classic Load Balancers are used to route TCP (or Layer 4) traffic.Therefore, the Developer should create an ECS Service with Auto Scaling and attach an Elastic Load Balancer.CORRECT: "Create an ECS Service with Auto Scaling and attach an Elastic Load Balancer" is the correct answer.INCORRECT: "Create an ECS Task Definition that uses Auto Scaling and Elastic Load Balancing" is incorrect as the Developer needs to configure auto scaling and load balancing in a service, not a task definition.INCORRECT: "Create a capacity provider and configure cluster auto scaling " is incorrect as this is used to scale the cluster container instances, not the number of tasks.INCORRECT: "Write statements using the Cluster Query Language to scale the Docker containers" is incorrect as cluster queries are expressions that enable you to group objects.References:https://docs.aws.amazon.com/AmazonECS/latest/developerguide/serviceautoscaling.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/serviceloadbalancing.html'},{question:"A Developer is building an application that will store data relating to financial transactions in multiple DynamoDB tables. The Developer needs to ensure the transactions provide atomicity, isolation, and durability (ACID) and that changes are committed following an all-or-nothing paradigm. What write API should be used for the DynamoDB table?",answers:[{text:"Standard",isCorrect:!1},{text:"Strongly consistent",isCorrect:!1},{text:"Transactional",isCorrect:!0},{text:"Eventually consistent",isCorrect:!1}],explanation:'Amazon DynamoDB transactions simplify the developer experience of making coordinated, allornothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, helping you to maintain data correctness in your applications.You can use the DynamoDB transactional read and write APIs to manage complex business workflows that require adding, updating, or deleting multiple items as a single, allornothing operation. For example, a video game developer can ensure that players’ profiles are updated correctly when they exchange items in a game or make ingame purchases.With the transaction write API, you can group multiple Put, Update, Delete, and ConditionCheck actions. You can then submit the actions as a single TransactWriteItems operation that either succeeds or fails as a unit. The same is true for multiple Get actions, which you can group and submit as a single TransactGetItems operation.There is no additional cost to enable transactions for your DynamoDB tables. You pay only for the reads or writes that are part of your transaction. DynamoDB performs two underlying reads or writes of every item in the transaction: one to prepare the transaction and one to commit the transaction. These two underlying read/write operations are visible in your AmazonCloudWatch metrics.CORRECT: "Transactional" is the correct answer.INCORRECT: "Standard" is incorrect as this will not provide the ACID / allor nothing transactional writes that are required for this solution.INCORRECT: "Strongly consistent" is incorrect as this applies to reads only, not writes.INCORRECT: "Eventually consistent" is incorrect as this applies to reads only, not writes.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html'},{question:"A company manages a web application that is deployed on AWS Elastic Beanstalk. A Developer has been instructed to update to a new version of the application code. There is no tolerance for downtime if the update fails and rollback should be fast. What is the SAFEST deployment method to use?",answers:[{text:"Amazon Elastic Kubernetes Service (EKS)",isCorrect:!1},{text:"AWS Lambda",isCorrect:!1},{text:"Amazon ECS with Fargate launch type",isCorrect:!0},{text:"Amazon ECS with EC2 launch type",isCorrect:!1}],explanation:'AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improvessecurity through application isolation by design.As you can see with the EC2 launch type you must manage the infrastructure layer (Amazon EC2 instances), whereas with Amazon Fargate you do not. Therefore, for this scenario the Fargate launch type should be used.CORRECT: "Amazon ECS with Fargate launch type" is the correct answer.INCORRECT: "Amazon ECS with EC2 launch type" is incorrect as the EC2 launch type requires more platform overhead as you must manage Amazon EC2 instances.INCORRECT: "Amazon Elastic Kubernetes Service (EKS)" is incorrect as this would require more management overhead (unless used with Fargate).INCORRECT: "AWS Lambda" is incorrect as this is not a service that can be used to run Docker containers.References: https://aws.amazon.com/fargate/'},{question:"An organization has hosted its EC2 instances in two AZs. AZ1 has two instances and AZ2 has 8 instances. The Elastic Load Balancer managing the instances in the two AZs has crosszone load balancing enabled in its configuration. What percentage traffic will each of the instances in AZ1 receive?",answers:[{text:"20",isCorrect:!1},{text:"25",isCorrect:!1},{text:"15",isCorrect:!1},{text:"10",isCorrect:!0}],explanation:"Correct option:A load balancer accepts incoming traffic from clients and routes requests to its registered targets (such as EC2 instances) in one or more Availability Zones.The nodes for a load balancer distribute requests from clients to registered targets. When crosszone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. When crosszone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone. With Application Load Balancers, crosszone load balancing is always enabled.10 When crosszone load balancing is enabled, each of the 10 targets receives 10% of the traffic. This is because each load balancer node can route its 50% of the client traffic to all 10 targets (present in both AZs).CrossZone Load Balancing Overview: via https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/howelasticloadbalancingworks.html Incorrect options:25 If crosszone load balancing is disabled, each of the two targets in AZ1 will receive 25% of the traffic. Because the load balancer is only able to send to the targets registered in AZ1 (AZ2 instances are not accessible for load balancer on AZ1)20 Invalid option, given only as a distractor.15 Invalid option, given only as a distractor.Reference:https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/howelasticloadbalancingworks.html"},{question:"A Developer has completed some code updates and needs to deploy the updates to an Amazon Elastic Beanstalk environment. The update must be deployed in the fastest possible time and application downtime is acceptable. Which deployment policy should the Developer choose?",answers:[{text:"Rolling with additional batch",isCorrect:!1},{text:"Immutable",isCorrect:!1},{text:"All at once",isCorrect:!0},{text:"Rolling",isCorrect:!1}],explanation:'AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments.Each deployment policy has advantages and disadvantages and it’s important to select the best policy to use for each situation.The “all at once” policy will deploy the update in the fastest time but will incur downtime.All at once:• Deploys the new version to all instances simultaneously.• All of your instances are out of service while the deployment takes place.• Fastest deployment.• Good for quick iterations in development environment.• You will experience an outage while the deployment is taking place – not ideal for missioncritical systems.• If the update fails, you need to roll back the changes by redeploying the original version to all of your instances.• No additional cost.For this scenario downtime is acceptable and deploying in the fastest possible time is required so the “all at once” policy is the best choice.CORRECT: "All at once" is the correct answer.INCORRECT: "Rolling" is incorrect as this takes longer than “all at once”. This is a better choice if speed is required but downtime is not acceptable.INCORRECT: "Rolling with additional batch" is incorrect if you require no reduction in capacity as it adds an additional batch of instances to the deployment.INCORRECT: "Immutable" is incorrect as this takes a long time to complete. This is good if you cannot sustain application downtime and need to be able to quickly and easily roll back if issues occur.References:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.rollingversiondeploy.html'},{question:"An application deployed on AWS Elastic Beanstalk experienced increased error rates during deployments of new application versions, resulting in service degradation for users. The Development team believes that this is because of the reduction in capacity during the deployment steps. The team would like to change the deployment policy configuration of the environment to an option that maintains full capacity during deployment while using the existing instances. Which deployment policy will meet these requirements while using the existing instances?",answers:[{text:"Rolling with additional batch",isCorrect:!0},{text:"All at once",isCorrect:!1},{text:"Rolling",isCorrect:!1},{text:"Immutable",isCorrect:!1}],explanation:"AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments.All at once:• Deploys the new version to all instances simultaneously.Rolling:• Update a few instances at a time (bucket), and then move onto the next bucket once the first bucket is healthy(downtime for 1 bucket at a time).Rolling with additional batch:• Like Rolling but launches new instances in a batch ensuring that there is full availability.Immutable:• Launches new instances in a new ASG and deploys the version update to these instances before swapping traffic tothese instances once healthy.• Zero downtime.Blue / Green deployment:• Zero downtime and release facility.• Create a new “stage” environment and deploy updates there.The rolling with additional batch launches a new batch to ensure capacity is not reduced and then updates the existing instances. Therefore, this is the best option to use for these requirements.CORRECT: “Rolling with additional batch” is the correct answer.INCORRECT: “Rolling” is incorrect as this will only use the existing instances without introducing an extra batch and therefore this will reduce the capacity of the application while the updates are taking place.INCORRECT: “All at once” is incorrect as this will run the updates on all instances at the same time causing a total outage.INCORRECT: “Immutable” is incorrect as this installs the updates on new instances, not existing instances.References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.rollingversiondeploy.html"},{question:"A DynamoDB table is being used to store session information for users of an online game. A developer has noticed that the table size has increased considerably and much of the data is not required after a gaming session is completed. What is the MOST cost-effective approach to reducing the size of the table?",answers:[{text:"Create an AWS Lambda function that purges stale items from the table daily",isCorrect:!1},{text:"Use the batchwriteitem API to delete the data",isCorrect:!1},{text:"Enable a Time To Live (TTL) on the table and add a timestamp attribute on new items",isCorrect:!0},{text:"Use the deleteitem API to delete the data",isCorrect:!1}],explanation:'Time to Live (TTL) for Amazon DynamoDB lets you define when items in a table expire so that they can be automatically deleted from the database. With TTL enabled on a table, you can set a timestamp for deletion on a peritem basis, allowing you to limit storage usage to only those records that are relevant.TTL is useful if you have continuously accumulating data that loses relevance after a specific time period (for example, session data, event logs, usage patterns, and other temporary data). If you have sensitive data that must be retained only for a certain amount of time according to contractual or regulatory obligations, TTL helps you ensure that it is removed promptly and asscheduled.Therefore, using a TTL is the best solution as it will automatically purge items after their useful lifetime.CORRECT: "Enable a Time To Live (TTL) on the table and add a timestamp attribute on new items" is the correct answer.INCORRECT: "Use the batchwriteitem API to delete the data" is incorrect as this would use RCUs and WCUs to remove the data.INCORRECT: "Create an AWS Lambda function that purges stale items from the table daily" is incorrect as this would also require reading/writing to the table so it would require RCUs/WCUs.INCORRECT: "Use the deleteitem API to delete the data" is incorrect is incorrect as this would use RCUs and WCUs to remove the data.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworksttl.html'},{question:"A multinational company has just moved to AWS Cloud and it has configured forecastbased AWS Budgets alerts for cost management. However, no alerts have been received even though the account and the budgets have been created almost three weeks ago. What could be the issue with the AWS Budgets configuration?",answers:[{text:"Budget forecast has been created from an account that does not have enough privileges",isCorrect:!1},{text:"AWS requires approximately 5 weeks of usage data to generate budget forecasts",isCorrect:!0},{text:"Amazon CloudWatch could be down and hence alerts are not being sent",isCorrect:!1},{text:"Account has to be part of AWS Organizations to receive AWS Budgets alerts",isCorrect:!1}],explanation:"Correct option:AWS Budgets lets customers set custom budgets and receive alerts if their costs or usage exceed (or are forecasted to exceed) their budgeted amount.AWS requires approximately 5 weeks of usage data to generate budget forecasts AWS requires approximately 5 weeks of usage data to generate budget forecasts. If you set a budget to alert based on a forecasted amount, this budget alert isn't triggered until you have enough historical usage information.Incorrect options:Budget forecast has been created from an account that does not have enough privileges This is an incorrect statement. If the user account does not have enough privileges, the user will not be able to create the budget at all.Amazon CloudWatch could be down and hence alerts are not being sent Amazon CloudWatch is fully managed by AWS, this option has been added as a distractor.Account has to be part of AWS Organizations to receive AWS Budget alerts This is an incorrect statement. Standalone accounts too can create budgets and being part of an Organization is not mandatory to use AWS Budgets.Reference:https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgetsbestpractices.html"},{question:"A developer is preparing to deploy a Docker container to Amazon ECS using CodeDeploy. The developer has defined the deployment actions in a file. What should the developer name the file?",answers:[{text:"buildspec.yml",isCorrect:!1},{text:"appspec.yml",isCorrect:!0},{text:"cron.yml",isCorrect:!1},{text:"appspec.json",isCorrect:!1}],explanation:'The application specification file (AppSpec file) is a YAMLformatted or JSONformatted file used by CodeDeploy to manage a deployment. The AppSpec file defines the deployment actions you want AWS CodeDeploy to execute.The name of the AppSpec file for an EC2/OnPremises deployment must be appspec.yml. The name of the AppSpec file for an Amazon ECS or AWS Lambda deployment must be appspec.yaml.Therefore, as this is an ECS deployment the file name must be appspec.yaml.CORRECT: "appspec.yml" is the correct answer.INCORRECT: "buildspec.yml" is incorrect as this is the file name you should use for the file that defines the build instructions for AWS CodeBuild.INCORRECT: "cron.yml" is incorrect. This is a file you can use with Elastic Beanstalk if you want to deploy a worker application that processes periodic background tasks.INCORRECT: "appspec.json" is incorrect as the file extension for ECS or Lambda deployments should be .yml not .json.References: https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfile.html'},{question:"A utilities company needs to ensure that documents uploaded by customers through a web portal are securely stored in Amazon S3 with encryption at rest. The company does not want to manage the security infrastructure inhouse. However, the company still needs maintain control over its encryption keys due to industry regulations. Which encryption strategy should a Developer use to meet these requirements?",answers:[{text:"Serverside encryption with customerprovided encryption keys (SSEC)",isCorrect:!0},{text:"Clientside encryption",isCorrect:!1},{text:"Serverside encryption with Amazon S3 managed keys (SSES3)",isCorrect:!1},{text:"Serverside encryption with AWS KMS managed keys (SSEKMS)",isCorrect:!1}],explanation:'Serverside encryption is about protecting data at rest. Serverside encryption encrypts only the object data, not object metadata. Using serverside encryption with customerprovided encryption keys (SSEC) allows you to set your own encryption keys.With the encryption key you provide as part of your request, Amazon S3 manages the encryption as it writes to disks and decryption when you access your objects. Therefore, you don\'t need to maintain any code to perform data encryption and decryption. The only thing you do is manage the encryption keys you provide.Therefore, SSEC is the best choice as AWS will manage all encryption and decryption operations whilst the company get to supply keys that they can manage.CORRECT: "Serverside encryption with customerprovided encryption keys (SSEC)" is the correct answer.INCORRECT: "Serverside encryption with Amazon S3 managed keys (SSES3)" is incorrect as with this option AWS manage the keys in S3.INCORRECT: "Serverside encryption with AWS KMS managed keys (SSEKMS)" is incorrect as with this option the keys are managed by AWS KMS.INCORRECT: "Clientside encryption" is incorrect as with this option all encryption and decryption is handled by the company (client) which is not desired in this scenario.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html'},{question:"You're a developer working on a large scale order processing application. After developing the features, you commit your code to AWS CodeCommit and begin building the project with AWS CodeBuild before it gets deployed to the server. The build is taking too long and the error points to an issue resolving dependencies from a thirdparty. You would like to prevent a build running this long in the future for similar underlying reasons. Which of the following options represents the best solution to address this usecase?",answers:[{text:"Enable CodeBuild timeouts",isCorrect:!0},{text:"Use AWS Lambda",isCorrect:!1},{text:"Use AWS CloudWatch Events",isCorrect:!1},{text:"Use VPC Flow Logs",isCorrect:!1}],explanation:"Correct option:Enable CodeBuild timeoutsA build represents a set of actions performed by AWS CodeBuild to create output artifacts (for example, a JAR file) based on a set of input artifacts (for example, a collection of Java class files).The following rules apply when you run multiple builds:When possible, builds run concurrently. The maximum number of concurrently running builds can vary.Builds are queued if the number of concurrently running builds reaches its limit. The maximum number of builds in a queue is five times the concurrent build limit.A build in a queue that does not start after the number of minutes specified in its time out value is removed from the queue. The default timeout value is eight hours. You can override the build queue timeout with a value between five minutes and eight hours when you run your build.By setting the timeout configuration, the build process will automatically terminate post the expiry of the configured timeout.Incorrect options:Use AWS Lambda AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Lambda cannot be used to impact the code build process.Use AWS CloudWatch Events Amazon CloudWatch allows you to monitor AWS cloud resources and the applications you run on AWS. Metrics are provided automatically for a number of AWS products and services. CloudWatch is good for monitoring and viewing logs. CloudWatch cannot be used to impact the code build process.Use VPC Flow Logs VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC but not for code compiling configuration. VPC Flow Logs cannot be used to impact the code build process.Reference:https://docs.aws.amazon.com/codebuild/latest/userguide/buildsworking.html"},{question:"A team of Developers need to deploy a website for a development environment. The team do not want to manage the infrastructure and just need to upload Node.js code to the instances. Which AWS service should Developers do?",answers:[{text:"Create an AWS Lambda package",isCorrect:!1},{text:"Launch an Auto Scaling group of Amazon EC2 instances",isCorrect:!1},{text:"Create an AWS CloudFormation template",isCorrect:!1},{text:"Create an AWS Elastic Beanstalk environment",isCorrect:!0}],explanation:'The Developers do not want to manage the infrastructure so the best AWS service for them to use to create a website for a development environment is AWS Elastic Beanstalk. This will allow the Developers to simply upload their Node.js code to Elastic Beanstalk and it will handle the provisioning and management of the underlying infrastructure. AWS Elastic Beanstalk can be used to quickly deploy and manage applications in the AWS Cloud. Developers upload applications and Elastic Beanstalk handles the deployment details of capacity provisioning, load balancing, autoscaling, and application health monitoring. AWS Elastic Beanstalk leverages Elastic Load Balancing and Auto Scaling to automatically scale your application in and out based on your application’s specific needs.CORRECT: "Create an AWS Elastic Beanstalk environment" is the correct answer.INCORRECT: "Create an AWS CloudFormation template" is incorrect as though you can use CloudFormation to deploy the infrastructure, it will not be managed for you.INCORRECT: "Create an AWS Lambda package" is incorrect as the Developers are deploying a website and Lambda is not a website. It is possible to use a Lambda function for a website however this would require a frontend component such as REST API.INCORRECT: "Launch an Auto Scaling group of Amazon EC2 instances" is incorrect as this would not provide a managed solution.References: https://aws.amazon.com/elasticbeanstalk/details/'},{question:"A developer is testing Amazon Simple Queue Service (SQS) queues in a development environment. The queue along with all its contents has to be deleted after testing. Which SQS API should be used for this requirement?",answers:[{text:"PurgeQueue",isCorrect:!1},{text:"DeleteQueue",isCorrect:!0},{text:"RemoveQueue",isCorrect:!1},{text:"RemovePermission",isCorrect:!1}],explanation:"Correct option:DeleteQueue Deletes the queue specified by the QueueUrl, regardless of the queue's contents. When you delete a queue, any messages in the queue are no longer available.When you delete a queue, the deletion process takes up to 60 seconds. Requests you send involving that queue during the 60 seconds might succeed. For example, a SendMessage request might succeed, but after 60 seconds the queue and the message you sent no longer exist.When you delete a queue, you must wait at least 60 seconds before creating a queue with the same name.Incorrect options:PurgeQueue Deletes the messages in a queue specified by the QueueURL parameter. When you use the PurgeQueue action, you can't retrieve any messages deleted from a queue. The queue however remains.RemoveQueue This is an invalid option, given only as a distractor.RemovePermission Revokes any permissions in the queue policy that matches the specified Label parameter.Reference:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_RemovePermission.html"},{question:"A company has a large Amazon DynamoDB table which they scan periodically so they can analyze several attributes. The scans are consuming a lot of provisioned throughput. What technique can a Developer use to minimize the impact of the scan on the table's provisioned throughput?",answers:[{text:"Define a range key on the table",isCorrect:!1},{text:"Set a smaller page size for the scan",isCorrect:!0},{text:"Use parallel scans",isCorrect:!1},{text:"Prewarm the table by updating all items",isCorrect:!1}],explanation:'In general, Scan operations are less efficient than other operations in DynamoDB. A Scan operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set.If possible, you should avoid using a Scan operation on a large table or index with a filter that removes many results. Also, as a table or index grows, the Scan operation slows. The Scan operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation.Instead of using a large Scan operation, you can use the following techniques to minimize the impact of a scan on a table\'s provisioned throughput.• Reduce page sizeBecause a Scan operation reads an entire page (by default, 1 MB), you can reduce the impact of the scan operation by setting a smaller page size. The Scan operation provides a Limit parameter that you can use to set the page size for your request.Each Query or Scan request that has a smaller page size uses fewer read operations and creates a "pause" between each request.• Isolate scan operationsDynamoDB is designed for easy scalability. As a result, an application can create tables for distinct purposes, possibly even duplicating content across several tables. You want to perform scans on a table that is not taking "missioncritical" traffic. Some applications handle this load by rotating traffic hourly between two tables—one for critical traffic, and one for bookkeeping.Other applications can do this by performing every write on two tables: a "missioncritical" table, and a "shadow" table. Therefore, the best option to reduce the impact of the scan on the table\'s provisioned throughput is to set a smaller page size for the scan.CORRECT: "Set a smaller page size for the scan" is the correct answer.INCORRECT: "Use parallel scans" is incorrect as this will return results faster but place more burden on the table’s provisioned throughput.INCORRECT: "Define a range key on the table" is incorrect. A range key is a composite key that includes the hash key and another attribute. This is of limited use in this scenario as the table is being scanned to analyze multiple attributes.INCORRECT: "Prewarm the table by updating all items" is incorrect as updating all items would incur significant costs in terms of provisioned throughput and would not be advantageous.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bpqueryscan.html'},{question:"An application is deployed using AWS Elastic Beanstalk and uses a Classic Load Balancer (CLB). A developer is performing a blue/green migration to change to an Application Load Balancer (ALB). After deployment, the developer has noticed that customers connecting to the ALB need to reauthenticate every time they connect. Normally they would only authenticate once and then be able to reconnect without reauthenticating for several hours. How can the developer resolve this issue?",answers:[{text:"Enable Sticky Sessions on the target group",isCorrect:!0},{text:"Add a new SSL certificate to the ALBs listener",isCorrect:!1},{text:"Enable IAM authentication on the ALBs listener",isCorrect:!1},{text:'Change the load balancing algorithm on the target group to &ldquo;least outstanding requests"',isCorrect:!1}],explanation:'Sticky sessions are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information in order to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies. In this case, it is likely that the clients authenticate to the backend instance and when they are reconnecting without sticky sessions enabled they may be load balanced to a different instance and need to authenticate again. The most obvious first step in troubleshooting this issue is to enable sticky sessions on the target group. CORRECT: "Enable Sticky Sessions on the target group" is the correct answer. INCORRECT: "Enable IAM authentication on the ALBs listener" is incorrect as you cannot enable &ldquo;IAM authentication&rdquo; on a listener. INCORRECT: "Add a new SSL certificate to the ALBs listener" is incorrect as this is not related to authentication. INCORRECT: "Change the load balancing algorithm on the target group to &ldquo;least outstanding requests)" is incorrect as this does not prevent the customer from being load balanced to a different instance, which is what is most likely to resolve this issue. References: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/loadbalancertargetgroups.html#stickysessions'},{question:"A Developer has been tasked by a client to create an application. The client has provided the following requirements for the application:• Performance efficiency of seconds with up to a minute of latency• Data storage requirements will be up to thousands of terabytes• Permessage sizes may vary between 100 KB and 100 MB• Data can be stored as key/value stores supporting eventual consistencyWhat is the MOST cost-effective AWS service to meet these requirements?",answers:[{text:"Amazon S3",isCorrect:!0},{text:"Amazon RDS (with a MySQL engine)",isCorrect:!1},{text:"Amazon DynamoDB",isCorrect:!1},{text:"Amazon ElastiCache",isCorrect:!1}],explanation:'The question is looking for a costeffective solution. Multiple options can support the latency and scalability requirements. Amazon RDS is not a key/value store so that rules that option out. Of the remaining options ElastiCache would be expensive and DynamoDB only supports a maximum item size of 400 KB. Therefore, the best option is Amazon S3 which delivers all of therequirements.CORRECT: "Amazon S3" is the correct answer.INCORRECT: "Amazon DynamoDB" is incorrect as it supports a maximum item size of 400 KB and the messages will be up to 100 MB.INCORRECT: "Amazon RDS (with a MySQL engine)" is incorrect as it is not a key/value store.INCORRECT: "Amazon ElastiCache" is incorrect as it is an inmemory database and would be the most expensive solution.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html'},{question:"A Developer is writing a serverless application that will process data uploaded to a file share. The Developer has created an AWS Lambda function and requires the function to be invoked every 15 minutes to process the data. What is an automated and serverless way to trigger the function?",answers:[{text:"Deploy an Amazon EC2 instance based on Linux, and edit it’s /etc/crontab file by adding a command to periodically invoke the Lambda function",isCorrect:!1},{text:"Create an Amazon CloudWatch Events rule that triggers on a regular schedule to invoke the Lambda function",isCorrect:!0},{text:"Configure an environment variable named PERIOD for the Lambda function. Set the value at 600",isCorrect:!1},{text:"Create an Amazon SNS topic that has a subscription to the Lambda function with a 600second timer",isCorrect:!1}],explanation:'Amazon CloudWatch Events help you to respond to state changes in your AWS resources. When your resources change state, they automatically send events into an event stream. You can create rules that match selected events in the stream and route them to your AWS Lambda function to take action.You can create a Lambda function and direct AWS Lambda to execute it on a regular schedule. You can specify a fixed rate (for example, execute a Lambda function every hour or 15 minutes), or you can specify a Cron expression.Therefore, the Developer should create an Amazon CloudWatch Events rule that triggers on a regular schedule to invoke the Lambda function. This is a serverless and automated solution.CORRECT: "Create an Amazon CloudWatch Events rule that triggers on a regular schedule to invoke the Lambda function" is the correct answer.INCORRECT: "Deploy an Amazon EC2 instance based on Linux, and edit it’s /etc/crontab file by adding a command to periodically invoke the Lambda function" is incorrect as EC2 is not a serverless solution.INCORRECT: "Configure an environment variable named PERIOD for the Lambda function. Set the value at 600" is incorrect as you cannot cause a Lambda function to execute based on a value in an environment variable.INCORRECT: "Create an Amazon SNS topic that has a subscription to the Lambda function with a 600second timer" is incorrect as SNS does not run on a timer, CloudWatch Events should be used instead.References: https://docs.aws.amazon.com/lambda/latest/dg/servicescloudwatchevents.html'},{question:"A company has three different environments: Development, QA, and Production. The company wants to deploy its code first in the Development environment, then QA, and then Production. Which AWS service can be used to meet this requirement?",answers:[{text:"Use AWS Data Pipeline to create multiple data pipeline provisions to deploy the application",isCorrect:!1},{text:"Use AWS CodeCommit to create multiple repositories to deploy the application",isCorrect:!1},{text:"Use AWS CodeDeploy to create multiple deployment groups",isCorrect:!0},{text:"Use AWS CodeBuild to create, configure, and deploy multiple build application projects",isCorrect:!1}],explanation:'You can specify one or more deployment groups for a CodeDeploy application. Each application deployment uses one of its deployment groups. The deployment group contains settings and configurations used during the deployment.You can associate more than one deployment group with an application in CodeDeploy. This makes it possible to deploy an application revision to different sets of instances at different times. For example, you might use one deployment group to deploy an application revision to a set of instances tagged Test where you ensure the quality of the code.Next, you deploy the same application revision to a deployment group with instances tagged Staging for additional verification.Finally, when you are ready to release the latest application to customers, you deploy to a deployment group that includes instances tagged Production.Therefore, using AWS CodeDeploy to create multiple deployment groups can be used to meet the requirementCORRECT: "Use AWS CodeDeploy to create multiple deployment groups" is the correct answer.INCORRECT: "Use AWS CodeCommit to create multiple repositories to deploy the application" is incorrect as the requirement is to deploy the same code to separate environments in a staged manner. Therefore, having multiple code repositories is not useful.INCORRECT: "Use AWS CodeBuild to create, configure, and deploy multiple build application projects" is incorrect as the requirement is not to build the application, it is to deploy the application.INCORRECT: "Use AWS Data Pipeline to create multiple data pipeline provisions to deploy the application" is incorrect as Data Pipeline is a service used for data migration, not deploying updates to applications.References: https://docs.aws.amazon.com/codedeploy/latest/userguide/deploymentgroups.html'},{question:"A Developer is designing a faulttolerant application that will use Amazon EC2 instances and an Elastic Load Balancer. The Developer needs to ensure that if an EC2 instance fails session data is not lost. How can this be achieved?",answers:[{text:"Use Amazon SQS to save session data",isCorrect:!1},{text:"Use Amazon DynamoDB to perform scalable session handling",isCorrect:!0},{text:"Use an EC2 Auto Scaling group to automatically launch new instances",isCorrect:!1},{text:"Enable Sticky Sessions on the Elastic Load Balancer",isCorrect:!1}],explanation:'For this scenario the key requirement is to ensure the data is not lost. Therefore, the data must be stored in a durable data store outside of the EC2 instances. Amazon DynamoDB is a suitable solution for storing session data. DynamoDB has a session handling capability for multiple languages as in the below example for PHP:“The DynamoDB Session Handler is a custom session handler for PHP that allows developers to use Amazon DynamoDB as a session store. Using DynamoDB for session storage alleviates issues that occur with session handling in a distributed web application by moving sessions off of the local file system and into a shared location. DynamoDB is fast, scalable, easy to setup, and handles replication of your data automatically.”Therefore, the best answer is to use DynamoDB to store the session data.CORRECT: "Use Amazon DynamoDB to perform scalable session handling" is the correct answer.INCORRECT: "Enable Sticky Sessions on the Elastic Load Balancer" is incorrect. Sticky sessions attempts to direct a user that has reconnected to the application to the same EC2 instance that they connected to previously. However, this does not ensure that the session data is going to be available.INCORRECT: "Use an EC2 Auto Scaling group to automatically launch new instances" is incorrect as this does not provide a solution for storing the session data.INCORRECT: "Use Amazon SQS to save session data" is incorrect as Amazon SQS is not suitable for storing session data.References: https://docs.aws.amazon.com/awssdkphp/v2/guide/featuredynamodbsessionhandler.html'},{question:"A CloudFormation stack needs to be deployed in several regions and requires a different Amazon Machine Image (AMI) in each region. Which AWS CloudFormation template key can be used to specify the correct AMI for each region?",answers:[{text:"Parameters",isCorrect:!1},{text:"Mappings",isCorrect:!0},{text:"Resources",isCorrect:!1},{text:"Outputs",isCorrect:!1}],explanation:'The optional Mappings section matches a key to a corresponding set of named values. For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. You use the Fn::FindInMap intrinsic function to retrieve values in a map.CORRECT: "Mappings" is the correct answer.INCORRECT: "Outputs" is incorrect. The optional Outputs section declares output values that you can import into other stacks (to create crossstack references), return in response (to describe stack calls), or view on the AWS CloudFormation console.INCORRECT: "Parameters" is incorrect. Parameters enable you to input custom values to your template each time you create or update a stack.INCORRECT: "Resources" is incorrect. The required Resources section declares the AWS resources that you want to include in the stack, such as an Amazon EC2 instance or an Amazon S3 bucket.References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappingssectionstructure.html'},{question:"A company runs an ecommerce website that uses Amazon DynamoDB where pricing for items is dynamically updated in real-time. At any given time, multiple updates may occur simultaneously for pricing information on a particular product. This is causing the original editor's changes to be overwritten without a proper review process. Which DynamoDB write option should be selected to prevent this overwriting?",answers:[{text:"Concurrent writes",isCorrect:!1},{text:"Conditional writes",isCorrect:!0},{text:"Batch writes",isCorrect:!1},{text:"Atomic writes",isCorrect:!1}],explanation:'By default, the DynamoDB write operations (PutItem, UpdateItem, DeleteItem) are unconditional: Each operation overwrites an existing item that has the specified primary key.DynamoDB optionally supports conditional writes for these operations. A conditional write succeeds only if the item attributes meet one or more expected conditions. Otherwise, it returns an error. Conditional writes are helpful in many situations. For example, you might want a PutItem operation to succeed only if there is not already an item with the same primary key. Or you could prevent an UpdateItem operation from modifying an item if one of its attributes has a certain value. Conditional writes are helpful in cases where multiple users attempt to modify the same item. Consider the following diagram, in which two users (Alice and Bob) are working with the same item from a DynamoDB table.Therefore, conditional writes are should be used to prevent the overwriting that has been occurring.CORRECT: "Conditional writes" is the correct answer.INCORRECT: "Concurrent writes" is incorrect is not a feature of DynamoDB. If concurrent writes occur this could lead to the very issues that conditional writes can be used to resolve.INCORRECT: "Atomic writes" is incorrect. Atomic reads and writes are something that can be performed using DynamoDB transactions using conditional writes.INCORRECT: "Batch writes" is incorrect as this is just a way of making multiple put or delete API operations in a single batch operation.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate'},{question:"A developer is planning to use a Lambda function to process incoming requests from an Application Load Balancer (ALB). How can this be achieved?",answers:[{text:"Configure an eventsource mapping between the ALB and the Lambda function",isCorrect:!1},{text:"Setup an API in front of the ALB using API Gateway and use an integration request to map the request to the Lambda function",isCorrect:!1},{text:"Create a target group and register the Lambda function using the AWS CLI",isCorrect:!0},{text:"Create an Auto Scaling Group (ASG) and register the Lambda function in the launch configuration",isCorrect:!1}],explanation:'You can register your Lambda functions as targets and configure a listener rule to forward requests to the target group for your Lambda function. When the load balancer forwards the request to a target group with a Lambda function as a target, it invokes your Lambda function and passes the content of the request to the Lambda function, in JSON format.You need to create a target group, which is used in request routing, and register a Lambda function to the target group. If the request content matches a listener rule with an action to forward it to this target group, the load balancer invokes the registered Lambda function.CORRECT: "Create a target group and register the Lambda function using the AWS CLI" is the correct answer.INCORRECT: "Create an Auto Scaling Group (ASG) and register the Lambda function in the launch configuration" is incorrect as launch configurations and ASGs are used for launching Amazon EC2 instances, you cannot use an ASG with a Lambda function.INCORRECT: "Setup an API in front of the ALB using API Gateway and use an integration request to map the request to the Lambda function" is incorrect as it is not a common design pattern to map an API Gateway API to a Lambda function when using an ALB. Though technically possible, typically you would choose to put API Gateway or an ALB in front of your application, not both.INCORRECT: "Configure an eventsource mapping between the ALB and the Lambda function" is incorrect as you cannot configure an eventsource mapping between and ALB and a Lambda function.References: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/lambdafunctions.html'},{question:"A Developer requires a multithreaded inmemory cache to place in front of an Amazon RDS database. Which caching solution should the Developer choose?",answers:[{text:"Amazon ElastiCache Redis",isCorrect:!1},{text:"Amazon DynamoDB DAX",isCorrect:!1},{text:"Amazon ElastiCache Memcached",isCorrect:!0},{text:"Amazon RedShift",isCorrect:!1}],explanation:'Amazon ElastiCache is a fully managed implementation of two popular inmemory data stores – Redis and Memcached. The inmemory caching provided by ElastiCache can be used to significantly improve latency and throughput for many readheavy application workloads or computeintensive workloads.There are two types of engine you can choose from: Memcached, Redis:MEMCACHED• Simplest model and can run large nodes.• Can be scaled in and out and cache objects such as DBs.• Widely adopted memory object caching system.• Multithreaded.REDIS• Opensource inmemory keyvalue store.• Supports more complex data structures: sorted sets and lists.• Supports master / slave replication and multiAZ for crossAZ redundancy.• Support automatic failover and backup/restore.As the Developer requires a multithreaded cache, the best choice is Memcached.CORRECT: "Amazon ElastiCache Memcached" is the correct answer.INCORRECT: "Amazon ElastiCache Redis" is incorrect as Redis it not multithreaded.INCORRECT: "Amazon DynamoDB DAX" is incorrect as this is more suitable for use with an Amazon DynamoDB table.INCORRECT: "Amazon RedShift" is incorrect as this is not an inmemory caching engine, it is a data warehouse.References: https://aws.amazon.com/elasticache/'},{question:"A Developer attempted to run an AWS CodeBuild project, and received an error. The error stated that the length of all environment variables exceeds the limit for the combined maximum of characters. What is the recommended solution?",answers:[{text:"Use Amazon Cognito to store keyvalue pairs for large numbers of environment variables",isCorrect:!1},{text:"Update the settings for the build project to use an Amazon S3 bucket for large numbers of environment variables",isCorrect:!1},{text:"Add the export LC_ALL=”en_US.utf8” command to the pre_build section to ensure POSIX localization",isCorrect:!1},{text:"Use AWS Systems Manager Parameter Store to store large numbers of environment variables",isCorrect:!0}],explanation:'In this case the build is using environment variables that are too large for AWS CodeBuild. CodeBuild can raise errors when the length of all environment variables (all names and values added together) reach a combined maximum of around 5,500 characters.The recommended solution is to use Amazon EC2 Systems Manager Parameter Store to store large environment variables and then retrieve them from your buildspec file. Amazon EC2 Systems Manager Parameter Store can store an individual environment variable (name and value added together) that is a combined 4,096 characters or less.CORRECT: "Use AWS Systems Manager Parameter Store to store large numbers of environment variables" is the correct answer.INCORRECT: "Add the export LC_ALL=”en_US.utf8” command to the pre_build section to ensure POSIX localization" is incorrect as this is used to set the locale and will not affect the limits that have been reached.INCORRECT: "Use Amazon Cognito to store keyvalue pairs for large numbers of environment variables" is incorrect as Cognito is used for authentication and authorization and is not suitable for this purpose.INCORRECT: "Update the settings for the build project to use an Amazon S3 bucket for large numbers of environment variables" is incorrect as Systems Manager Parameter Store is designed for this purpose and is a better fit.References: https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html#troubleshootinglargeenvvars'},{question:"A team of Developers require readonly access to an Amazon DynamoDB table. The Developers have been added to a group. What should an administrator do to provide the team with access whilst following the principal of least privilege?",answers:[{text:"Create a customer managed policy with read/write access to DynamoDB for all resources. Attach the policy to the group",isCorrect:!1},{text:"Assign the AmazonDynamoDBReadOnlyAccess AWS managed policy to the group",isCorrect:!1},{text:"Assign the AWSLambdaDynamoDBExecutionRole AWS managed policy to the group",isCorrect:!1},{text:"Create a customer managed policy with read only access to DynamoDB and specify the ARN of the table for the “Resource” element. Attach the policy to the group",isCorrect:!0}],explanation:'The key requirement is to provide readonly access to the team for a specific DynamoDB table. Therefore, the AWS managed policy cannot be used as it will provide access to all DynamoDB tables in the account which does not follow the principal of least privilege.Therefore, a customer managed policy should be created that provides readonly access and specifies the ARN of the table. For instance, the resource element might include the following ARN:arn:aws:dynamodb:uswest1:515148227241:table/exampletableThis will lock down access to the specific DynamoDB table, following the principal of least privilege.CORRECT: "Create a customer managed policy with read only access to DynamoDB and specify the ARN of the table for the“Resource” element. Attach the policy to the group" is the correct answer.INCORRECT: "Assign the AmazonDynamoDBReadOnlyAccess AWS managed policy to the group" is incorrect as this will provide readonly access to all DynamoDB tables in the account.INCORRECT: "Assign the AWSLambdaDynamoDBExecutionRole AWS managed policy to the group" is incorrect as this is a role used with AWS Lambda.INCORRECT: "Create a customer managed policy with read/write access to DynamoDB for all resources. Attach the policy to the group" is incorrect as readonly access should be provided, not read/write.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/usingidentitybasedpolicies.html'},{question:"A Developer will be launching several Docker containers on a new Amazon ECS cluster using the EC2 Launch Type. The containers will all run a web service on port 80. What is the EASIEST way the Developer can configure the task definition to ensure the web services run correctly and there are no port conflicts on the host instances?",answers:[{text:"Specify port 80 for the container port and a unique port number for the host port",isCorrect:!1},{text:"Specify a unique port number for the container port and port 80 for the host port",isCorrect:!1},{text:"Leave both the container port and host port configuration blank",isCorrect:!1},{text:"Specify port 80 for the container port and port 0 for the host port",isCorrect:!0}],explanation:'Port mappings allow containers to access ports on the host container instance to send or receive traffic. Port mappings are specified as part of the container definition. The container port is the port number on the container that is bound to the userspecified or automatically assigned host port. The host port is the port number on the container instance to reserve for your container.As we cannot have multiple services bound to the same host port, we need to ensure that each container port mapping uses a different host port. The easiest way to do this is to set the host port number to 0 and ECS will automatically assign an available port. We also need to assign port 80 to the container port so that the web service is able to run.CORRECT: "Specify port 80 for the container port and port 0 for the host port" is the correct answer.INCORRECT: "Specify port 80 for the container port and a unique port number for the host port" is incorrect as this is more difficult to manage as you have to manually assign the port number.INCORRECT: "Specify a unique port number for the container port and port 80 for the host port" is incorrect as the web service on the container needs to run on pot 80 and you can only bind one container to port 80 on the host so this would not allow more than one container to work.INCORRECT: "Leave both the container port and host port configuration blank" is incorrect as this would mean that ECS would dynamically assign both the container and host port. As the web service must run on port 80 this would not work correctly.References: https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PortMapping.html'},{question:"A global ecommerce company wants to perform geographic load testing of its order processing API. The company must deploy resources to multiple AWS Regions to support the load testing of the API. How can the company address these requirements without additional application code?",answers:[{text:"Set up an AWS CloudFormation template that defines the load test resources. Leverage the AWS CLI createstackset command to create a stack set in the desired Regions",isCorrect:!0},{text:"Set up an AWS CloudFormation template that defines the load test resources. Develop regionspecific Lambda functions to create a stack from the AWS CloudFormation template in each Region when the respective function is invoked",isCorrect:!1},{text:"Set up an AWS Organizations template that defines the load test resources across the organization. Leverage the AWS CLI createstackset command to create a stack set in the desired Regions",isCorrect:!1},{text:"Set up an AWS Cloud Development Kit (CDK) ToolKit that defines the load test resources. Leverage the CDK CLI to create a stack from the template in each Region",isCorrect:!1}],explanation:"Correct option:Set up an AWS CloudFormation template that defines the load test resources. Leverage the AWS CLI createstackset command to create a stack set in the desired RegionsAWS CloudFormation StackSets extends the capability of stacks by enabling you to create, update, or delete stacks across multiple accounts and AWS Regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified AWS Regions.via https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/whatiscfnstacksets.html A stack set lets you create stacks in AWS accounts across regions by using a single CloudFormation template. A stack set's CloudFormation template defines all the resources in each stack. As you create the stack set, specify the template to use, in addition to any parameters and capabilities that the template requires.After you've defined a stack set, you can create, update, or delete stacks in the target accounts and AWS Regions you specify.via https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksetsconcepts.html Incorrect options:Set up an AWS CloudFormation template that defines the load test resources. Develop regionspecific Lambda functions to create a stack from the AWS CloudFormation template in each Region when the respective function is invoked If you do not use a stack set, then you need to define the CloudFormation templates in each region as well as develop lambda functions in each region to create a stack from the corresponding CloudFormation template. This is unnecessary bloat that can be avoided by simply using the CloudFormation StackSets.Set up an AWS Cloud Development Kit (CDK) ToolKit that defines the load test resources. Leverage the CDK CLI to create a stack from the template in each Region AWS CDK is a framework for defining cloud infrastructure in code and provisioning it through AWS CloudFormation. The CDK Toolkit again poses regional limitations and is not the right fit for the given use case.Set up an AWS Organizations template that defines the load test resources across the organization. Leverage the AWS CLI createstackset command to create a stack set in the desired Regions This option acts as a distractor. AWS Organizations cannot be used to create templates for provisioning AWS infrastructure. AWS Organizations is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage.References:https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksetsconcepts.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/whatiscfnstacksets.html"},{question:"ECS Fargate container tasks are usually spread across Availability Zones (AZs) and the underlying workloads need persistent crossAZ shared access to the data volumes configured for the container tasks. Which of the following solutions is the best choice for these workloads?",answers:[{text:"Bind mounts",isCorrect:!1},{text:"AWS Gateway Storage volumes",isCorrect:!1},{text:"Docker volumes",isCorrect:!1},{text:"Amazon EFS volumes",isCorrect:!0}],explanation:"Correct option:Amazon EFS volumes EFS volumes provide a simple, scalable, and persistent file storage for use with your Amazon ECS tasks. With Amazon EFS, storage capacity is elastic, growing and shrinking automatically as you add and remove files. Your applications can have the storage they need, when they need it. Amazon EFS volumes are supported for tasks hosted on Fargate or Amazon EC2 instances.You can use Amazon EFS file systems with Amazon ECS to export file system data across your fleet of container instances. That way, your tasks have access to the same persistent storage, no matter the instance on which they land. However, you must configure your container instance AMI to mount the Amazon EFS file system before the Docker daemon starts. Also, your task definitions must reference volume mounts on the container instance to use the file system.Incorrect options:Docker volumes A Dockermanaged volume that is created under /var/lib/docker/volumes on the host Amazon EC2 instance. Docker volume drivers (also referred to as plugins) are used to integrate the volumes with external storage systems, such as Amazon EBS. The builtin local volume driver or a thirdparty volume driver can be used. Docker volumes are only supported when running tasks on Amazon EC2 instances.Bind mounts A file or directory on the host, such as an Amazon EC2 instance or AWS Fargate, is mounted into a container. Bind mount host volumes are supported for tasks hosted on Fargate or Amazon EC2 instances. Bind mounts provide temporary storage, and hence these are a wrong choice for this use case.AWS Storage Gateway volumes This is an incorrect choice, given only as a distractor.Reference:https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_data_volumes.html https://aws.amazon.com/blogs/containers/amazonecsavailabilitybestpractices/"},{question:"A Developer needs to scan a full DynamoDB 50GB table within nonpeak hours. About half of the strongly consistent RCUs are typically used during nonpeak hours and the scan duration must be minimized. How can the Developer optimize the scan execution time without impacting production workloads?",answers:[{text:"Use parallel scans while limiting the rate",isCorrect:!0},{text:"Increase the RCUs during the scan operation",isCorrect:!1},{text:"Change to eventually consistent RCUs during the scan operation",isCorrect:!1},{text:"Use sequential scans",isCorrect:!1}],explanation:'Performing a scan on a table consumes a lot of RCUs. A Scan operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set. To reduce the amount of RCUs used by the scan so it doesn’t affect production workloads whilst minimizing the execution time, there are a couple of recommendations the Developer can follow.Firstly, the Limit parameter can be used to reduce the page size. The Scan operation provides a Limit parameter that you can use to set the page size for your request. Each Query or Scan request that has a smaller page size uses fewer read operations and creates a "pause" between each request.Secondly, the Developer can configure parallel scans. With parallel scans the Developer can maximize usage of the available throughput and have the scans distributed across the table’s partitions.A parallel scan can be the right choice if the following conditions are met:• The table size is 20 GB or larger.• The table\'s provisioned read throughput is not being fully used.• Sequential Scan operations are too slow.Therefore, to optimize the scan operation the Developer should use parallel scans while limiting the rate as this will ensure that the scan operation does not affect the performance of production workloads and still have it complete in the minimum time.CORRECT: "Use parallel scans while limiting the rate" is the correct answer.INCORRECT: "Use sequential scans" is incorrect as this is slower than parallel scans and the Developer needs to minimize scan execution time.INCORRECT: "Increase the RCUs during the scan operation" is incorrect as the table is only using half of the RCUs during nonpeak hours so there are RCUs available. You could increase RCUs and perform the scan faster, but this would be more expensive. The better solution is to use parallel scans with the limit parameter.INCORRECT: "Change to eventually consistent RCUs during the scan operation" is incorrect as this does not provide a solution for preventing impact to the production workloads. The limit parameter should be used to ensure the tables RCUs are not fully used.References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bpqueryscan.html#QueryAndScanGuidelines.ParallelScan'},{question:"A serverless application uses an Amazon API Gateway and AWS Lambda. The application processes data submitted in a form by users of the application and certain data must be stored and available to subsequent function calls. What is the BEST solution for storing this data?",answers:[{text:"Store the data in an Amazon Kinesis Data Stream",isCorrect:!1},{text:"Store the data in the /tmp directory",isCorrect:!1},{text:"Store the data in an Amazon SQS queue",isCorrect:!1},{text:"Store the data in an Amazon DynamoDB table",isCorrect:!0}],explanation:'AWS Lambda is a stateless compute service and so you cannot store session data in AWS Lambda itself. You can store a limited amount of information (up to 512 MB) in the /tmp directory. This information is preserved if the function is reused (i.e. the execution context is reused). However, it is not guaranteed that the execution context will be reused so the data could bedestroyed.The /tmp should only be used for data that can be regenerated or for operations that require a local filesystem, but not as a permanent storage solution. It is ideal for setting up database connections that will be needed across invocations of the function as the connection is made once and preserved across invocations.Amazon DynamoDB is a good solution for this scenario as it is a lowlatency NoSQL database that is often used for storing session state data. Amazon S3 would also be a good fit for this scenario but is not offered as an option.With both Amazon DynamoDB and Amazon S3 you can store data longterm and it is available for multiple invocations of your function as well as being available from multiple invocations simultaneously.CORRECT: "Store the data in an Amazon DynamoDB table" is the correct answer.INCORRECT: "Store the data in an Amazon Kinesis Data Stream" is incorrect as this service is used for streaming data. It is not used for sessionstore use cases.INCORRECT: "Store the data in the /tmp directory" is incorrect as any data stored in the /tmp may not be available for subsequent calls to your function. The /tmp directory content remains when the execution context is frozen, providing transient cache that can be used for multiple invocations. However, it is not guaranteed that the execution context will bereused so the data could be lost.INCORRECT: "Store the data in an Amazon SQS queue" is incorrect as a message queue is not used for longterm storage of data.References: https://aws.amazon.com/dynamodb/'},{question:"A company is deploying an onpremise application server that will connect to several AWS services. What is the BEST way to provide the application server with permissions to authenticate to AWS services?",answers:[{text:"Create an IAM user and generate a key pair. Use the key pair in API calls to AWS services",isCorrect:!1},{text:"Create an IAM role with the necessary permissions and assign it to the application server",isCorrect:!1},{text:"Create an IAM group with the necessary permissions and add the onpremise application server to the group",isCorrect:!1},{text:"Create an IAM user and generate access keys. Create a credentials file on the application server",isCorrect:!0}],explanation:'Access keys are longterm credentials for an IAM user or the AWS account root user. You can use access keys to sign programmatic requests to the AWS CLI or AWS API (directly or using the AWS SDK).Access keys are stored in one of the locations on a client that needs to make authenticated API calls to AWS services:• Linux: ~/.aws/credentials• Windows: %UserProfle%\\.aws\\credentialsIn this scenario the application server is running onpremises. Therefore, you cannot assign an IAM role (which would be the preferable solution for Amazon EC2 instances). In this case it is therefore better to use access keys.CORRECT: "Create an IAM user and generate access keys. Create a credentials file on the application server" is the correct answer.INCORRECT: "Create an IAM role with the necessary permissions and assign it to the application server" is incorrect. This is an onpremises server so it is not possible to use an IAM role. If it was an EC2 instance, this would be the preferred (best practice)option.INCORRECT: "Create an IAM group with the necessary permissions and add the onpremise application server to the group" is incorrect. You cannot add a server to an IAM group. You put IAM users into groups and assign permissions to them using a policy.INCORRECT: "Create an IAM user and generate a key pair. Use the key pair in API calls to AWS services" is incorrect as key pairs are used for SSH access to Amazon EC2 instances. You cannot use them in API calls to AWS services.References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_accesskeys.html'},{question:"A Developer is creating an application and would like add AWS XRay to trace user requests endtoend through the software stack. The Developer has implemented the changes and tested the application and the traces are successfully sent to XRay. The Developer then deployed the application on an Amazon EC2 instance, and noticed that the traces are not being sent to XRay. What is the most likely cause of this issue? (Select TWO.)",answers:[{text:"The XRay API is not installed on the EC2 instance",isCorrect:!1},{text:"The XRay segments are being queued",isCorrect:!1},{text:"The traces are reaching XRay, but the Developer does not have permission to view the records",isCorrect:!1},{text:"The instance’s instance profile role does not have permission to upload trace data to XRay",isCorrect:!0},{text:"The XRay daemon is not installed on the EC2 instance.",isCorrect:!0}],explanation:'AWS XRay is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization. For any traced request to your application, you can see detailed information not only about the request and response, but also about calls that yourapplication makes to downstream AWS resources, microservices, databases and HTTP web APIs.You can run the XRay daemon on the following operating systems on Amazon EC2:• Amazon Linux• Ubuntu• Windows Server (2012 R2 and newer)The XRay daemon must be running on the EC2 instance in order to collect data. You can use a user data script to run the daemon automatically when you launch the instance. The XRay daemon uses the AWS SDK to upload trace data to XRay, and it needs AWS credentials with permission to do that.On Amazon EC2, the daemon uses the instance\'s instance profile role automatically. The IAM role or user that the daemon\'s credentials belong to must have permission to write data to the service on your behalf.• To use the daemon on Amazon EC2, create a new instance profile role or add the managed policy to an existing one.• To use the daemon on Elastic Beanstalk, add the managed policy to the Elastic Beanstalk default instance profile role.• To run the daemon locally, create an IAM user and save its access keys on your computer.Therefore, the most likely cause of the issues being experienced in this scenario is that the instance’s instance profile role does not have permission to upload trace data to XRay or the XRay daemon is not running on the EC2 instance.CORRECT: "The instance’s instance profile role does not have permission to upload trace data to XRay" is the correct answer.CORRECT: "The XRay daemon is not installed on the EC2 instance." is also a correct answer.INCORRECT: "The XRay API is not installed on the EC2 instance " is incorrect as you do not install the XRay API, you run the XRay daemon. The API will always be accessible using the XRay endpoint.INCORRECT: "The traces are reaching XRay, but the Developer does not have permission to view the records" is incorrect as the developer previously viewed data in XRay so clearly has permissions.INCORRECT: "The XRay segments are being queued" is incorrect. The XRay daemon is responsible for relaying trace data to XRay. However, it will not queue data for an extended period of time so this is unlikely to be a cause of this issue.References:https://docs.aws.amazon.com/xray/latest/devguide/xraydaemonec2.html https://docs.aws.amazon.com/xray/latest/devguide/xraydaemon.html https://docs.aws.amazon.com/xray/latest/devguide/xraydaemon.html#xraydaemonpermissions'},{question:"A company is deploying an Amazon Kinesis Data Streams application that will collect streaming data from a gaming application. Consumers will run on Amazon EC2 instances. In this architecture, what can be deployed on consumers to act as an intermediary between the record processing logic and Kinesis Data Streams and instantiate a record processor for each shard?",answers:[{text:"AWS CLI",isCorrect:!1},{text:"Amazon Kinesis Client Library (KCL)",isCorrect:!0},{text:"Amazon Kinesis CLI",isCorrect:!1},{text:"Amazon Kinesis API",isCorrect:!1}],explanation:'The Kinesis Client Library (KCL) helps you consume and process data from a Kinesis data stream. This type of application is also referred to as a consumer. The KCL takes care of many of the complex tasks associated with distributed computing, such as load balancing across multiple instances, responding to instance failures, checkpointing processed records, and reacting to resharding. The KCL enables you to focus on writing recordprocessing logic.The KCL is different from the Kinesis Data Streams API that is available in the AWS SDKs. The Kinesis Data Streams API helps you manage many aspects of Kinesis Data Streams (including creating streams, resharding, and putting and getting records). The KCL provides a layer of abstraction specifically for processing data in a consumer role.Therefore, the correct answer is to use the Kinesis Client Library.CORRECT: "Amazon Kinesis Client Library (KCL)" is the correct answer.INCORRECT: "Amazon Kinesis API" is incorrect. You can work with Kinesis Data Streams directly from your consumers using the API but this is does not deploy an intermediary component as required.INCORRECT: "AWS CLI" is incorrect. The AWS CLI can be used to work directly with the Kinesis API but this does not deploy an intermediary component as required.INCORRECT: "Amazon Kinesis CLI" is incorrect as this does not exist. The AWS CLI has commands for working with Kinesis.References: https://docs.aws.amazon.com/streams/latest/dev/developingconsumerswithkcl.html'},{question:"An AWS Lambda function has been packaged for deployment to multiple environments including development, test, and production. The Lambda function uses an Amazon RDS MySQL database for storing data. Each environment has a different RDS MySQL database. How can a Developer configure the Lambda function package to ensure the correct database connection string is used for each environment?",answers:[{text:"Use layers for storing the database connection strings",isCorrect:!1},{text:"Use a separate function for development and production",isCorrect:!1},{text:"Include the resources in the function code",isCorrect:!1},{text:"Use environment variables for the database connection strings",isCorrect:!0}],explanation:'You can use environment variables to store secrets securely and adjust your function\'s behavior without updating code. An environment variable is a pair of strings that are stored in a function\'s versionspecific configuration.Use environment variables to pass environmentspecific settings to your code. For example, you can have two functions with the same code but different configuration. One function connects to a test database, and the other connects to a production database.In this situation, you use environment variables to tell the function the hostname and other connection details for the database. You might also set an environment variable to configure your test environment to use more verbose logging or more detailed tracing.You set environment variables on the unpublished version of your function by specifying a key and value. When you publish a version, the environment variables are locked for that version along with other versionspecific configuration.It is possible to create separate versions of a function with different environment variables referencing the relevant database connection strings. Therefore, using environment variables is the best way to ensure the environmentspecific database connection strings are available in a single deployment package.CORRECT: "Use environment variables for the database connection strings" is the correct answer.INCORRECT: "Use a separate function for development and production" is incorrect as there’s a single deployment package that must contain the connection strings for multiple environments. Therefore, using environment variables is necessary.INCORRECT: "Include the resources in the function code" is incorrect. It would not be secure to include the database connection strings in the function code. With environment variables the password string can be encrypted using KMS which is much more secure.INCORRECT: "Use layers for storing the database connection strings" is incorrect. Layers are used for adding external libraries to your functions.References: https://docs.aws.amazon.com/lambda/latest/dg/configurationenvvars.html'},{question:"A company runs a legacy application that uses an XMLbased SOAP interface. The company needs to expose the functionality of the service to external customers and plans to use Amazon API Gateway. How can a Developer configure the integration?",answers:[{text:"Create a SOAP API using Amazon API Gateway. Transform the incoming JSON into a valid XML message for the SOAP interface using AWS Lambda.",isCorrect:!0},{text:"Create a RESTful API using Amazon API Gateway. Pass the incoming JSON to the SOAP interface through an Application Load Balancer.",isCorrect:!1},{text:"Create a RESTful API using Amazon API Gateway. Transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates.",isCorrect:!1},{text:"Create a SOAP API using Amazon API Gateway. Pass the incoming JSON to the SOAP interface through a Network Load Balancer.",isCorrect:!1}],explanation:'In API Gateway, an API\'s method request can take a payload in a different format from the corresponding integration request payload, as required in the backend. Similarly, the backend may return an integration response payload different from the method response payload, as expected by the frontend.API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response.CORRECT: "Create a RESTful API using Amazon API Gateway. Transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates" is the correct answer.INCORRECT: "Create a RESTful API using Amazon API Gateway. Pass the incoming JSON to the SOAP interface through an Application Load Balancer" is incorrect. The API Gateway cannot process the XML SOAP data and cannot pass it through an ALB.INCORRECT: "Create a SOAP API using Amazon API Gateway. Transform the incoming JSON into a valid XML message for the SOAP interface using AWS Lambda" is incorrect. API Gateway does not support SOAP APIs.INCORRECT: "Create a SOAP API using Amazon API Gateway. Pass the incoming JSON to the SOAP interface through a Network Load Balancer" is incorrect. API Gateway does not support SOAP APIs.References: https://docs.aws.amazon.com/apigateway/latest/Developerguide/requestresponsedatamappings.html'},{question:"An IT company is configuring Auto Scaling for its Amazon EC2 instances spread across different AZs and Regions. Which of the following scenarios are NOT correct about EC2 Auto Scaling? (Select two)",answers:[{text:"An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same Region",isCorrect:!1},{text:"Auto Scaling groups that span across multiple Regions need to be enabled for all the Regions specified",isCorrect:!0},{text:"Amazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group",isCorrect:!1},{text:"An Auto Scaling group can contain EC2 instances in only one Availability Zone of a Region",isCorrect:!0},{text:"For Auto Scaling groups in a VPC, the EC2 instances are launched in subnets",isCorrect:!1}],explanation:"Correct options:Auto Scaling groups that span across multiple Regions need to be enabled for all the Regions specified This is not valid for Auto Scaling groups. Auto Scaling groups cannot span across multiple Regions.An Auto Scaling group can contain EC2 instances in only one Availability Zone of a Region This is not valid for Auto Scaling groups. An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same Region.Amazon EC2 Auto Scaling Overview: via https://docs.aws.amazon.com/autoscaling/ec2/userguide/whatisamazonec2autoscaling.html Incorrect options:An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same Region This is a valid statement. Auto Scaling groups can span across the availability Zones of a Region.Amazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group When one Availability Zone becomes unhealthy or unavailable, Auto Scaling launches new instances in an unaffected Availability Zone. When the unhealthy Availability Zone returns to a healthy state, Auto Scaling automatically redistributes the application instances evenly across all of the designated Availability Zones.For Auto Scaling groups in a VPC, the EC2 instances are launched in subnets For Auto Scaling groups in a VPC, the EC2 instances are launched in subnets. Customers can select the subnets for your EC2 instances when you create or update the Auto Scaling group.References:https://docs.aws.amazon.com/autoscaling/ec2/userguide/whatisamazonec2autoscaling.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscalingbenefits.html"},{question:"An application runs on a fleet of Amazon EC2 instances in an Auto Scaling group. The application stores data in an Amazon DynamoDB table and all instances make updates to the table. When querying data, EC2 instances sometimes retrieve stale data. The Developer needs to update the application to ensure the most uptodate data is retrieved for all queries. How can the Developer accomplish this?",answers:[{text:"Use the UpdateGlobalTable API to create a global secondary index.",isCorrect:!1},{text:"Set the ConsistentRead parameter to true when calling GetItem.",isCorrect:!0},{text:"Cache the database writes using Amazon DynamoDB Accelerator.",isCorrect:!1},{text:"Use the TransactWriteItems API when issuing PutItem actions.",isCorrect:!1}],explanation:'DynamoDB supports eventually consistent and strongly consistent reads. When using eventually consistent reads the response might not reflect the results of a recently completed write operation. The response might include some stale data.When using strongly consistent reads DynamoDB returns a response with the most uptodate data, reflecting the updates from all prior write operations that were successful.DynamoDB uses eventually consistent reads unless you specify otherwise. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation.CORRECT: "Set the ConsistentRead parameter to true when calling GetItem" is the correct answer.INCORRECT: "Cache the database writes using Amazon DynamoDB Accelerator" is incorrect. DynamoDB DAX caches items from DynamoDB to improve read performance but will not ensure the latest data is retrieved.INCORRECT: "Use the TransactWriteItems API when issuing PutItem actions" is incorrect. This operation is used to group transactions in an allornothing update.INCORRECT: "Use the UpdateGlobalTable API to create a global secondary index" is incorrect. A GSI does not assist in any way in this solution.References: https://docs.aws.amazon.com/amazondynamodb/latest/Developerguide/HowItWorks.ReadConsistency.html'},{question:"A developer has been asked to create a web application to be deployed on EC2 instances. The developer just wants to focus on writing application code without worrying about server provisioning, configuration and deployment. As a Developer Associate, which AWS service would you recommend for the given use-case?",answers:[{text:"CloudFormation",isCorrect:!1},{text:"Serverless Application Model",isCorrect:!1},{text:"CodeDeploy",isCorrect:!1},{text:"Elastic Beanstalk",isCorrect:!0}],explanation:"Correct option:Elastic BeanstalkAWS Elastic Beanstalk provides an environment to easily deploy and run applications in the cloud. It is integrated with developer tools and provides a onestop experience for you to manage the lifecycle of your applications.AWS Elastic Beanstalk lets you manage all of the resources that run your application as environments where each environment runs only a single application version at a time. When an environment is being created, Elastic Beanstalk provisions all the required resources needed to run the application version. You don't need to worry about server provisioning, configuration, and deployment as that's taken care of by Beanstalk.Benefits of Elastic Beanstalk: via https://aws.amazon.com/elasticbeanstalk/ Incorrect options:CloudFormation AWS CloudFormation is a service that gives developers and businesses an easy way to create a collection of related AWS and thirdparty resources and provision them in an orderly and predictable fashion. With CloudFormation, you still need to create a template to specify the type of resources you need, hence this option is not correct.How CloudFormation Works: via https://aws.amazon.com/cloudformation/ CodeDeploy AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your onpremises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. It can deploy an application to an instance but it cannot provision the instance.Serverless Application Model The AWS Serverless Application Model (AWS SAM) is an opensource framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML. As the web application needs to be deployed on EC2 instances, so this option is ruled out.References:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html https://aws.amazon.com/cloudformation/"},{question:"A Developer has used a third-party tool to build, bundle, and package a software package on-premises. The software package is stored in a local file system and must be deployed to Amazon EC2 instances. How can the application be deployed onto the EC2 instances?",answers:[{text:"Use AWS CodeBuild to commit the package and automatically deploy the software package.",isCorrect:!1},{text:"Use AWS CodeDeploy and point it to the local file system to deploy the software package.",isCorrect:!1},{text:"Create a repository using AWS CodeCommit to automatically trigger a deployment to the EC2 instances.",isCorrect:!1},{text:"Upload the bundle to an Amazon S3 bucket and specify the S3 location when doing a deployment using AWS CodeDeploy.",isCorrect:!0}],explanation:'AWS CodeDeploy can deploy software packages using an archive that has been uploaded to an Amazon S3 bucket. The archive file will typically be a .zip file containing the code and files required to deploy the software package.CORRECT: "Upload the bundle to an Amazon S3 bucket and specify the S3 location when doing a deployment using AWS CodeDeploy" is the correct answer.INCORRECT: "Use AWS CodeDeploy and point it to the local file system to deploy the software package" is incorrect. You cannot point CodeDeploy to a local file system running onpremises.INCORRECT: "Create a repository using AWS CodeCommit to automatically trigger a deployment to the EC2 instances" is incorrect. CodeCommit is a source control system. In this case the source code has already been package using a thirdparty tool.INCORRECT: "Use AWS CodeBuild to commit the package and automatically deploy the software package" is incorrect. CodeBuild does not commit packages (CodeCommit does) or deploy the software. It is a build service. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/tutorialswindowsuploadapplication.html'},{question:"Amazon Simple Queue Service (SQS) has a set of APIs for various actions supported by the service. As a developer associate, which of the following would you identify as correct regarding the CreateQueue API? (Select two)",answers:[{text:"Queue tags are case insensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag",isCorrect:!1},{text:"The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using MessageRetentionPeriod attribute",isCorrect:!1},{text:"You can't change the queue type after you create it",isCorrect:!0},{text:"The visibility timeout value for the queue is in seconds, which defaults to 30 seconds",isCorrect:!0},{text:"The deadletter queue of a FIFO queue must also be a FIFO queue. Whereas, the deadletter queue of a standard queue can be a standard queue or a FIFO queue",isCorrect:!1}],explanation:"Correct options:You can't change the queue type after you create it You can't change the queue type after you create it and you can't convert an existing standard queue into a FIFO queue. You must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue.The visibility timeout value for the queue is in seconds, which defaults to 30 seconds The visibility timeout for the queue is in seconds. Valid values are: An integer from 0 to 43,200 (12 hours), the Default value is 30.Incorrect options:The deadletter queue of a FIFO queue must also be a FIFO queue. Whereas, the deadletter queue of a standard queue can be a standard queue or a FIFO queue The deadletter queue of a FIFO queue must also be a FIFO queue. Similarly, the deadletter queue of a standard queue must also be a standard queue.The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using MessageRetentionPeriod attribute The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using DelaySeconds attribute. MessageRetentionPeriod attribute controls the length of time, in seconds, for which Amazon SQS retains a message.Queue tags are case insensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag Queue tags are casesensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag. To be able to tag a queue on creation, you must have the sqs:CreateQueue and sqs:TagQueue permissions.Reference:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_CreateQueue.htmll"},{question:"To reduce the cost of API actions performed on an Amazon SQS queue, a Developer has decided to implement long polling. Which of the following modifications should the Developer make to the API actions?",answers:[{text:"Set the SetQueueAttributes with a MessageRetentionPeriod of 60",isCorrect:!1},{text:"Set the ReceiveMessage API with a VisibilityTimeout of 30",isCorrect:!1},{text:"Set the SetQueueAttributes API with a DelaySeconds of 20",isCorrect:!1},{text:"Set the ReceiveMessage API with a WaitTimeSeconds of 20",isCorrect:!0}],explanation:'The process of consuming messages from a queue depends on whether you use short or long polling. By default, Amazon SQS uses short polling, querying only a subset of its servers (based on a weighted random distribution) to determine whether any messages are available for a response. You can use long polling to reduce your costs while allowing your consumers to receive messages as soon as they arrive in the queue.When you consume messages from a queue using short polling, Amazon SQS samples a subset of its servers (based on a weighted random distribution) and returns messages from only those servers. Thus, a particular ReceiveMessage request might not return all of your messages. However, if you have fewer than 1,000 messages in your queue, a subsequent request will return your messages. If you keep consuming from your queues, Amazon SQS samples all of its servers, and you receive all of your messages.The following diagram shows the shortpolling behavior of messages returned from a standard queue after one of your system components makes a receive request. Amazon SQS samples several of its servers (in gray) and returns messages A, C, D, and B from these servers. Message E isn\'t returned for this request but is returned for a subsequent request.When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren\'t included in a response).Long polling occurs when the WaitTimeSeconds parameter of a ReceiveMessage request is set to a value greater than 0 in one of two ways:• The ReceiveMessage call sets WaitTimeSeconds to a value greater than 0.• The ReceiveMessage call doesn’t set WaitTimeSeconds, but the queue attribute ReceiveMessageWaitTimeSeconds is set to a value greater than 0.Therefore, the Developer should set the ReceiveMessage API with a WaitTimeSeconds of 20.CORRECT: "Set the ReceiveMessage API with a WaitTimeSeconds of 20" is the correct answer.INCORRECT: "Set the SetQueueAttributes API with a DelaySeconds of 20" is incorrect as this would be used to configure a delay queue where the delivery of messages in the queue is delayed.INCORRECT: "Set the ReceiveMessage API with a VisibilityTimeout of 30" is incorrect as this would configure the visibility timeout which is the length of time a message that has been received is invisible.INCORRECT: "Set the SetQueueAttributes with a MessageRetentionPeriod of 60" is incorrect as this would configure how long messages are retained in the queue.References:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsshortandlongpolling.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html'},{question:"An application is hosted by a 3rd party and exposed at yourapp.3rdparty.com. You would like to have your users access your application using www.mydomain.com, which you own and manage under Route 53. What Route 53 record should you create?",answers:[{text:"Create an A record",isCorrect:!1},{text:"Create a PTR record",isCorrect:!1},{text:"Create an Alias Record",isCorrect:!1},{text:"Create a CNAME record",isCorrect:!0}],explanation:"Correct option:Create a CNAME recordA CNAME record maps DNS queries for the name of the current record, such as acme.example.com, to another domain (example.com or example.net) or subdomain (acme.example.com or zenith.example.org).CNAME records can be used to map one domain name to another. Although you should keep in mind that the DNS protocol does not allow you to create a CNAME record for the top node of a DNS namespace, also known as the zone apex. For example, if you register the DNS name example.com, the zone apex is example.com. You cannot create a CNAME record for example.com, but you can create CNAME records for www.example.com, newproduct.example.com, and so on.Please review the major differences between CNAME and Alias Records: via https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resourcerecordsetschoosingaliasnonalias.htmlIncorrect options:Create an A record Used to point a domain or subdomain to an IP address. 'A record' cannot be used to map one domain name to another.Create a PTR record A Pointer (PTR) record resolves an IP address to a fullyqualified domain name (FQDN) as an opposite to what A record does. PTR records are also called Reverse DNS records. 'PTR record' cannot be used to map one domain name to another.Create an Alias Record Alias records let you route traffic to selected AWS resources, such as CloudFront distributions and Amazon S3 buckets. They also let you route traffic from one record in a hosted zone to another record. 3rd party websites do not qualify for these as we have no control over those. 'Alias record' cannot be used to map one domain name to another.Reference:https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resourcerecordsetschoosingaliasnonalias.htmll"},{question:"An organization developed an application that uses a set of APIs that are being served through Amazon API Gateway. The API calls must be authenticated based on OpenID identity providers such as Amazon, Google, or Facebook. The APIs should allow access based on a custom authorization model. Which is the simplest and MOST secure design to use to build an authentication and authorization model for the APIs?",answers:[{text:"Use Amazon DynamoDB to store user credentials and have the application retrieve temporary credentials from AWS STS. Make API calls by passing user credentials to the APIs for authentication and authorization",isCorrect:!1},{text:"Use Amazon ElastiCache to store user credentials and pass them to the APIs for authentication and authorization",isCorrect:!1},{text:"Use Amazon Cognito user pools and a custom authorizer to authenticate and authorize users based on JSON Web Tokens",isCorrect:!0},{text:"Build an OpenID token broker with Amazon and Facebook. Users will authenticate with these identify providers and pass the JSON Web Token to the API to authenticate each API call",isCorrect:!1}],explanation:'With Amazon Cognito User Pools your app users can sign in either directly through a user pool or federate through a thirdparty identity provider (IdP). The user pool manages the overhead of handling the tokens that are returned from social signin through Facebook, Google, Amazon, and Apple, and from OpenID Connect (OIDC) and SAML IdPs.After successful authentication, Amazon Cognito returns user pool tokens to your app. You can use the tokens to grant your users access to your own serverside resources, or to the Amazon API Gateway. Or, you can exchange them for AWS credentials to access other AWS services.The ID token is a JSON Web Token (JWT) that contains claims about the identity of the authenticated user such as name, email, and phone_number. You can use this identity information inside your application. The ID token can also be used to authenticate users against your resource servers or server applications.CORRECT: "Use Amazon Cognito user pools and a custom authorizer to authenticate and authorize users based on JSON Web Tokens" is the correct answer.INCORRECT: "Use Amazon ElastiCache to store user credentials and pass them to the APIs for authentication and authorization" is incorrect. This option does not provide a solution for authenticating based on Open ID providers and is not secure as there is no mechanism mentioned for ensuring the secrecy of the credentials.INCORRECT: "Use Amazon DynamoDB to store user credentials and have the application retrieve temporary credentials from AWS STS. Make API calls by passing user credentials to the APIs for authentication and authorization" is incorrect. This option also does not solve the requirement of integrating with Open ID providers and also suffers from the same security concerns as the option above.INCORRECT: "Build an OpenID token broker with Amazon and Facebook. Users will authenticate with these identify providers and pass the JSON Web Token to the API to authenticate each API call" is incorrect. This may be a workable and secure solution however it is definitely not the simplest as it would require significant custom development.References:https://docs.aws.amazon.com/cognito/latest/developerguide/authentication.html https://docs.aws.amazon.com/cognito/latest/developerguide/amazoncognitouserpoolsusingtokenswithidentityproviders.html'},{question:"A website is deployed in several AWS regions. A Developer needs to direct global users to the website that provides the best performance. How can the Developer achieve this?",answers:[{text:"Create Alias records in AWS Route 53 and direct the traffic to an Elastic Load Balancer",isCorrect:!1},{text:"Create CNAME records in AWS Route 53 and direct traffic to Amazon CloudFront",isCorrect:!1},{text:"Create A records in AWS Route 53 and use a latencybased routing policy",isCorrect:!0},{text:"Create A records in AWS Route 53 and use a weighted routing policy",isCorrect:!1}],explanation:'If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency.To use latencybased routing, you create latency records for your resources in multiple AWS Regions. When Route 53 receives a DNS query for your domain or subdomain (example.com or acme.example.com), it determines which AWS Regions you\'ve created latency records for, determines which region gives the user the lowest latency, and then selects a latency record for that region. Route 53 responds with the value from the selected record, such as the IP address for a web server.CORRECT: "Create A records in AWS Route 53 and use a latencybased routing policy" is the correct answer.INCORRECT: "Create Alias records in AWS Route 53 and direct the traffic to an Elastic Load Balancer" is incorrect as an ELB is within a single region. In this case the Developer needs to direct traffic to different regions.INCORRECT: "Create A records in AWS Route 53 and use a weighted routing policy" is incorrect as weighting is used to send more traffic to one region other another, not to direct for best performance.INCORRECT: "Create CNAME records in AWS Route 53 and direct traffic to Amazon CloudFront" is incorrect as this does not direct traffic to different regions for best performance which is what the questions asks for.References: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routingpolicy.html#routingpolicylatency'},{question:"An application uses an Amazon DynamoDB table that is 50 GB in size and provisioned with 10,000 read capacity units (RCUs) per second. The table must be scanned during nonpeak hours when normal traffic consumes around 5,000 RCUs. The Developer must scan the whole table in the shortest possible time whilst ensuring the normal workload is not affected. How would the Developer optimize this scan cost-effectively?",answers:[{text:"Increase read capacity units during the scan operation.",isCorrect:!1},{text:"Use sequential scans and set the ConsistentRead parameter to false.",isCorrect:!1},{text:"Use sequential scans and apply a FilterExpression.",isCorrect:!1},{text:"Use the Parallel Scan API operation and limit the rate.",isCorrect:!0}],explanation:'To make the most of the table’s provisioned throughput, the Developer can use the Parallel Scan API operation so that the scan is distributed across the table’s partitions. This will help to optimize the scan to complete in the fastest possible time. However, the Developer will also need to apply rate limiting to ensure that the scan does not affect normal workloads.CORRECT: "Use the Parallel Scan API operation and limit the rate" is the correct answer.INCORRECT: "Use sequential scans and apply a FilterExpression" is incorrect. A FilterExpression is a string that contains conditions that DynamoDB applies after the Scan operation, but before the data is returned to you. This will not assist with speeding up the scan or preventing it from affecting normal workloads.INCORRECT: "Increase read capacity units during the scan operation" is incorrect. There are already more RCUs provisioned than are needed during the nonpeak hours. The key here is to use what is available for costeffectiveness whilst ensuing normal workloads are not affected.INCORRECT: "Use sequential scans and set the ConsistentRead parameter to false" is incorrect. This setting would turn off consistent reads making the scan eventually consistent. This will not satisfy the requirements of the question.References: https://aws.amazon.com/blogs/Developer/ratelimitedscansinamazondynamodb/'},{question:"A company needs to store sensitive documents on Amazon S3. The documents should be encrypted in transit using SSL/TLS and then be encrypted for storage at the destination. The company do not want to manage any of the encryption infrastructure or customer master keys and require the most cost-effective solution. What is the MOST suitable option to encrypt the data?",answers:[{text:"Clientside encryption with Amazon S3 managed keys",isCorrect:!1},{text:"ServerSide Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSEKMS) using customer managed CMKs",isCorrect:!1},{text:"ServerSide Encryption with Amazon S3Managed Keys (SSES3)",isCorrect:!0},{text:"ServerSide Encryption with CustomerProvided Keys (SSEC)",isCorrect:!1}],explanation:'Serverside encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects.As you can see in the image above, there are three options for serverside encryption:• ServerSide Encryption with Amazon S3Managed Keys (SSES3) – the data is encrypted by Amazon S3 using keys that are managed through S3• ServerSide Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSEKMS) – this options uses CMKs managed in AWS KMS. There are additional benefits such as auditing and permissions associated with the CMKs but also additional charges• ServerSide Encryption with CustomerProvided Keys (SSEC) – you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects. The most suitable option for the requirements in this scenario is to use ServerSide Encryption with Amazon S3Managed Keys (SSES3) as the company do not want to manage CMKs and require a simple solution.CORRECT: "ServerSide Encryption with Amazon S3Managed Keys (SSES3)" is the correct answer.INCORRECT: "ServerSide Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSEKMS) using customer managed CMKs" is incorrect as the company do not want to manage CMKs and they need the most costeffective option and this does add additional costs.INCORRECT: "ServerSide Encryption with CustomerProvided Keys (SSEC)" is incorrect as with this option the customer must manage the keys or use keys managed in AWS KMS (which adds cost and complexity).INCORRECT: "Clientside encryption with Amazon S3 managed keys" is incorrect as you cannot use Amazon S3 managed keys for clientside encryption and the encryption does not need to take place clientside for this solution.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/servsideencryption.html'},{question:"A CloudFormation template is going to be used by a global team to deploy infrastructure in several regions around the world. Which section of the template file can be used to set values based on a region?",answers:[{text:"Mappings",isCorrect:!0},{text:"Parameters",isCorrect:!1},{text:"Metadata",isCorrect:!1},{text:"Conditions",isCorrect:!1}],explanation:'The optional Mappings section matches a key to a corresponding set of named values. For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. You use the Fn::FindInMap intrinsic function to retrieve values in a map.CORRECT: "Mappings" is the correct answer.INCORRECT: "Metadata" is incorrect. You can use the optional Metadata section to include arbitrary JSON or YAML objects that provide details about the template.INCORRECT: "Parameters" is incorrect. Parameters enable you to input custom values to your template each time you create or update a stack.INCORRECT: "Conditions" is incorrect. The optional Conditions section contains statements that define the circumstances under which entities are created or configured. For example, you can create a condition and then associate it with a resource or output so that AWS CloudFormation only creates the resource or output if the condition is true.References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/templateanatomy.html'},{question:"A temporary Developer needs to be provided with access to specific resources for a one week period. Which element of an IAM policy statement can be used to allow access only on or before a specific date?",answers:[{text:"Condition",isCorrect:!0},{text:"NotResource",isCorrect:!1},{text:"Version",isCorrect:!1},{text:"Action",isCorrect:!1}],explanation:'The Condition element (or Condition block) lets you specify conditions for when a policy is in effect. The Condition element is optional. In the Condition element, you build expressions in which you use condition operators (equal, less than, etc.) to match the condition keys and values in the policy against keys and values in the request context.For example, in this scenario the following condition statement could be used:"Condition": {"DateLessThanEquals" : {"aws:CurrentTime" : "20200418T12:00:00Z"}}CORRECT: "Condition" is the correct answer.INCORRECT: "NotResource" is incorrect. NotResource is an advanced policy element that explicitly matches every resource except those specified.INCORRECT: "Action" is incorrect. The Action element describes the specific action or actions that will be allowed or denied.INCORRECT: "Version" is incorrect. The Version policy element is used within a policy and defines the version of the policy language.References:https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition.html https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition_operators.html'},{question:"A company has implemented AWS CodePipeline to automate its release pipelines. The Development team is writing an AWS Lambda function that will send notifications for state changes of each of the actions in the stages. Which steps must be taken to associate the Lambda function with the event source?",answers:[{text:"Create an Amazon CloudWatch Events rule that uses CodePipeline as an event source",isCorrect:!0},{text:"Create an Amazon CloudWatch alarm that monitors status changes in CodePipeline and triggers the Lambda function",isCorrect:!1},{text:"Create a trigger that invokes the Lambda function from the Lambda console by selecting CodePipeline as the event source",isCorrect:!1},{text:"Create an event trigger and specify the Lambda function from the CodePipeline console",isCorrect:!1}],explanation:'Amazon CloudWatch Events help you to respond to state changes in your AWS resources. When your resources change state, they automatically send events into an event stream. You can create rules that match selected events in the stream and route them to your AWS Lambda function to take action.AWS CodePipeline can be configured as an event source in CloudWatch Events and can then send notifications using as service such as Amazon SNS.Therefore, the best answer is to create an Amazon CloudWatch Events rule that uses CodePipeline as an event source.CORRECT: "Create an Amazon CloudWatch Events rule that uses CodePipeline as an event source" is the correct answer.INCORRECT: "Create a trigger that invokes the Lambda function from the Lambda console by selecting CodePipeline as the event source" is incorrect as CodePipeline cannot be configured as a trigger for Lambda.INCORRECT: "Create an event trigger and specify the Lambda function from the CodePipeline console" is incorrect as CodePipeline cannot be configured as a trigger for Lambda.INCORRECT: "Create an Amazon CloudWatch alarm that monitors status changes in CodePipeline and triggers the Lambda function" is incorrect as CloudWatch Events is used for monitoring state changes.References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html'},{question:"A Development team is involved with migrating an on-premises MySQL database to Amazon RDS. The database usage is very read-heavy. The Development team wants refactor the application code to achieve optimum read performance for queries. How can this objective be met?",answers:[{text:"Add a connection string to use a read replica on an Amazon EC2 instance",isCorrect:!1},{text:"Add database retries to the code and vertically scale the Amazon RDS database",isCorrect:!1},{text:"Use Amazon RDS with a multiAZ deployment",isCorrect:!1},{text:"Add a connection string to use an Amazon RDS read replica for read queries",isCorrect:!0}],explanation:'Amazon RDS uses the MariaDB, MySQL, Oracle, and PostgreSQL DB engines\' builtin replication functionality to create a special type of DB instance called a Read Replica from a source DB instance. Updates made to the source DB instance are asynchronously copied to the Read Replica.You can reduce the load on your source DB instance by routing read queries from your applications to the Read Replica. Using Read Replicas, you can elastically scale out beyond the capacity constraints of a single DB instance for readheavy database workloads.In the image below a primary Amazon RDS database server allows reads and writes while a Read Replica can be used for running readonly workloads such as BI/reporting. This reduces the load on the primary database.It is necessary to add logic to your code to direct read traffic to the Read Replica and write traffic to the primary database.Therefore, in this scenario the Development team will need to “Add a connection string to use an Amazon RDS read replica for read queries”.CORRECT: "Add a connection string to use an Amazon RDS read replica for read queries" is the correct answer.INCORRECT: "Add database retries to the code and vertically scale the Amazon RDS database" is incorrect as this is not a good way to scale reads as you will likely hit a ceiling at some point in terms of cost or instance type. Scaling reads can be better implemented with horizontal scaling using a Read Replica.INCORRECT: "Use Amazon RDS with a multiAZ deployment" is incorrect as this creates a standby copy of the database in another AZ that can be failed over to in a failure scenario. This is used for DR not (at least not primarily) used for scaling performance. It is possible for certain RDS engines to use a multiAZ standby as a read replica however the requirements in thissolution do not warrant this configuration.INCORRECT: "Add a connection string to use a read replica on an Amazon EC2 instance" is incorrect as Read Replicas are something you create on Amazon RDS, not on an EC2 instance.References: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html'},{question:"A gaming company wants to store information about all the games that the company has released. Each game has a name, version number, and category (such as sports, puzzles, strategy, etc). The game information also can include additional properties about the supported platforms and technical specifications. This additional information is inconsistent across games. You have been hired as an AWS Certified Developer Associate to build a solution that addresses the following use cases: For a given name and version number, get all details about the game that has that name and version number. For a given name, get all details about all games that have that name. For a given category, get all details about all games in that category. What will you recommend as the most efficient solution?",answers:[{text:"Set up an Amazon DynamoDB table with a primary key that consists of the category as the partition key and the version number as the sort key. Create a global secondary index that has the name as the partition key",isCorrect:!1},{text:"Set up an Amazon RDS MySQL instance having a games table that contains columns for name, version number, and category. Configure the name column as the primary key",isCorrect:!1},{text:"Set up an Amazon DynamoDB table with a primary key that consists of the name as the partition key and the version number as the sort key. Create a global secondary index that has the category as the partition key and the name as the sort key",isCorrect:!0},{text:"Permanently store the name, version number, and category information about the games in an Amazon Elasticache for Memcached instance",isCorrect:!1}],explanation:"Correct option:Set up an Amazon DynamoDB table with a primary key that consists of the name as the partition key and the version number as the sort key. Create a global secondary index that has the category as the partition key and the name as the sort keyWhen you create a DynamoDB table, in addition to the table name, you must specify the primary key of the table. The primary key uniquely identifies each item in the table, so that no two items can have the same key. You can create one or more secondary indexes on a table. A secondary index lets you query the data in the table using an alternate key, in addition to queries against the primary key. DynamoDB doesn't require that you use indexes, but they give your applications more flexibility when querying your data.via https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html You should note that a global secondary index (GSI) contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table. The Global secondary indexes allow you to perform queries on attributes that are not part of the table's primary key.via https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html Incorrect options:Set up an Amazon DynamoDB table with a primary key that consists of the category as the partition key and the version number as the sort key. Create a global secondary index that has the name as the partition key The DynamoDB table for this option has the primary key and GSI that do not solve for the condition \"For a given name and version number, get all details about the game that has that name and version number\". This option does not allow for efficient querying of a specific game by its name and version number as you need multiple queries which would be less efficient than the single query allowed by the correct option.Set up an Amazon RDS MySQL instance having a games table that contains columns for name, version number, and category. Configure the name column as the primary key This option is not the right fit as it does not allow you to efficiently query on the version number and category columns.Permanently store the name, version number, and category information about the games in an Amazon Elasticache for Memcached instance You cannot use Elasticache for Memcached to permanently store values meant to be persisted in a database (relational or NoSQL). Elasticache is a caching layer. So this option is incorrect.References:https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html https://aws.amazon.com/premiumsupport/knowledgecenter/primarykeydynamodbtable/"},{question:"An ecommerce web application that shares session state on-premises is being migrated to AWS. The application must be fault tolerant, natively highly scalable, and any service interruption should not affect the user experience. What is the best option to store the session state?",answers:[{text:"Enable session stickiness using elastic load balancers",isCorrect:!1},{text:"Store the session state in Amazon CloudFront",isCorrect:!1},{text:"Store the session state in Amazon ElastiCache",isCorrect:!0},{text:"Store the session state in Amazon S3",isCorrect:!1}],explanation:'There are various ways to manage user sessions including storing those sessions locally to the node responding to the HTTP request or designating a layer in your architecture which can store those sessions in a scalable and robust manner. Common approaches used include utilizing Sticky sessions or using a Distributed Cache for your session management.In this scenario, a distributed cache is suitable for storing session state data. ElastiCache can perform this role and with the Redis engine replication is also supported. Therefore, the solution is faulttolerant and natively highly scalable.CORRECT: "Store the session state in Amazon ElastiCache" is the correct answer.INCORRECT: "Store the session state in Amazon CloudFront" is incorrect as CloudFront is not suitable for storing session state data, it is used for caching content for better global performance.INCORRECT: "Store the session state in Amazon S3" is incorrect as though you can store session data in Amazon S3 and replicate the data to another bucket, this would result in a service interruption if the S3 bucket was not accessible.INCORRECT: "Enable session stickiness using elastic load balancers" is incorrect as this feature directs sessions from a specific client to a specific EC2 instances. Therefore, if the instance fails the user must be redirected to another EC2 instance and the session state data would be lost.References: https://aws.amazon.com/caching/sessionmanagement/'},{question:"A company manages a web application that is deployed on AWS Elastic Beanstalk. A Developer has been instructed to update to a new version of the application code. There is no tolerance for downtime if the update fails and rollback should be fast. What is the SAFEST deployment method to use?",answers:[{text:"Immutable",isCorrect:!0},{text:"All at once",isCorrect:!1},{text:"Rolling",isCorrect:!1},{text:"Rolling with Additional Batch",isCorrect:!1}],explanation:'AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, and Immutable) and options that let you configure batch size and health check behavior during deployments.For this scenario we need to ensure that no downtime occurs if the update fails and there is a quick way to rollback. All policies except for Immutable and Blue/Green require manual redeployment of the previous version of the code which will take time and result in downtime. The blue/green option is not actually an Elastic Beanstalk policy but it is a method you can use, however it is not offered as an answer choiceTherefore, the best deployment policy to use for this scenario is the Immutable deployment policy.CORRECT: "Immutable" is the correct answer.INCORRECT: "All at once" is incorrect as it causes complete downtime and manual redeployment in the case of failure.INCORRECT: "Rolling" is incorrect because it requires manual redeployment in the case of failure.INCORRECT: "Rolling with Additional Batch" is incorrect because it requires manual redeployment in the case of failure.References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html'},{question:"An application that runs on an Amazon EC2 instance needs to access and make API calls to multiple AWS services. What is the MOST secure way to provide access to the AWS services with MINIMAL management overhead?",answers:[{text:"Use AWS root user to make requests to the application",isCorrect:!1},{text:"Use EC2 instance profiles",isCorrect:!0},{text:"Use AWS KMS to store and retrieve credentials",isCorrect:!1},{text:"Store and retrieve credentials from AWS CodeCommit",isCorrect:!1}],explanation:'An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. This is a secure way to authorize and EC2 instance to access AWS services.Instance profiles are created automatically if you use the console to add a role to an instance. You can also create instance profiles using the AWS CLI or API and assign roles to them.CORRECT: "Use EC2 instance profiles" is the correct answer.INCORRECT: "Use AWS KMS to store and retrieve credentials" is incorrect as KMS is used for encrypting data, not storing credentials.INCORRECT: "Use AWS root user to make requests to the application " is incorrect as this is not a secure way to access services as the root user has full privileges to the AWS account.INCORRECT: "Store and retrieve credentials from AWS CodeCommit" is incorrect as this is not a suitable solution for storing this data as CodeCommit is used for storing source code.References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switchroleec2_instanceprofiles.html'},{question:"An application is being deployed on an Amazon EC2 instance running Linux. The EC2 instance will need to manage other AWS services. How can the EC2 instance be configured to make API calls to AWS service securely?",answers:[{text:"Create an AWS IAM Role, attach a policy with the necessary privileges and attach the role to the instance’s instance profile",isCorrect:!0},{text:"Store the access key ID and secret access key as encrypted AWS Lambda environment variables and invoke Lambda for each API call",isCorrect:!1},{text:"Run the “aws configure” AWS CLI command and specify the access key ID and secret access key",isCorrect:!1},{text:"Store a users’ console login credentials in the application code so the application can call AWS STS and gain temporary security credentials",isCorrect:!1}],explanation:'Applications must sign their API requests with AWS credentials. Therefore, if you are an application developer, you need a strategy for managing credentials for your applications that run on EC2 instances. For example, you can securely distribute your AWS credentials to the instances, enabling the applications on those instances to use your credentials to sign requests, while protecting your credentials from other users.However, it\'s challenging to securely distribute credentials to each instance, especially those that AWS creates on your behalf, such as Spot Instances or instances in Auto Scaling groups. You must also be able to update the credentials on each instance when you rotate your AWS credentials.IAM roles are designed so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use.Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles as follows:1. Create an IAM role.2. Define which accounts or AWS services can assume the role.3. Define which API actions and resources the application can use after assuming the role.4. Specify the role when you launch your instance or attach the role to an existing instance.5. Have the application retrieve a set of temporary credentials and use them.For example, you can use IAM roles to grant permissions to applications running on your instances that need to use a bucket in Amazon S3. You can specify permissions for IAM roles by creating a policy in JSON format. These are similar to the policies that you create for IAM users. If you change a role, the change is propagated to all instances.Therefore, the best solution is to create an AWS IAM Role with the necessary privileges (through an IAM policy) and attach the role to the instance’s instance profile.CORRECT: "Create an AWS IAM Role, attach a policy with the necessary privileges and attach the role to the instance’s instanceprofile" is the correct answer.INCORRECT: "Run the “aws configure” AWS CLI command and specify the access key ID and secret access key" is incorrect as this in insecure as the access key ID and secret access key are stored in plaintext on the instance’s local disk.INCORRECT: "Store a users’ console login credentials in the application code so the application can call AWS STS and gain temporary security credentials" is incorrect. This is a nonsense solution that would not work for multiple reasons. Firstly, the user console login credentials and not used for API access; secondly the STS service will not accept user login credentials andreturn temporary access credentials.INCORRECT: "Store the access key ID and secret access key as encrypted AWS Lambda environment variables and invoke Lambda for each API call" is incorrect. You can encrypt Lambda variables with KMS keys; however, this is not an ideal solution as you will still need to decrypt the keys through the Lambda code and then pass them to the EC2 instance. There could be security risks in this process. This is generally a poor use case for Lambda and IAM Roles are a far superior way of providing the necessary access.References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iamrolesforamazonec2.html'},{question:"A company is developing a new online game that will run on top of Amazon ECS. Four distinct Amazon ECS services will be part of the architecture, each requiring specific permissions to various AWS services. The company wants to optimize the use of the underlying Amazon EC2 instances by bin packing the containers based on memory reservation. Which configuration would allow the Development team to meet these requirements MOST securely?",answers:[{text:"Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then, create an IAM group and configure the ECS cluster to reference that group",isCorrect:!1},{text:"Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then configure each ECS service to reference the associated IAM role",isCorrect:!1},{text:"Create a new Identity and Access Management (IAM) instance profile containing the required permissions for the various ECS services, then associate that instance role with the underlying EC2 instances",isCorrect:!1},{text:"Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then configure each ECS task definition to reference the associated IAM role",isCorrect:!0}],explanation:'With IAM roles for Amazon ECS tasks, you can specify an IAM role that can be used by the containers in a task. Applications must sign their AWS API requests with AWS credentials, and this feature provides a strategy for managing credentials for your applications to use, similar to the way that Amazon EC2 instance profiles provide credentials to EC2 instances.Instead of creating and distributing your AWS credentials to the containers or using the EC2 instance’s role, you can associate an IAM role with an ECS task definition or RunTask API operation. The applications in the task’s containers can then use the AWS SDK or CLI to make API requests to authorized AWS services.In this case each service requires access to different AWS services so following the principal of least privilege it is best to assign as a separate role to each task definition.CORRECT: "Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then configure each ECS task definition to reference the associated IAM role" is the correct answer.INCORRECT: "Create a new Identity and Access Management (IAM) instance profile containing the required permissions for the various ECS services, then associate that instance role with the underlying EC2 instances" is incorrect. It is a best practice to use IAM roles for tasks instead of assigning the roles to the container instances.INCORRECT: "Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then configure each ECS service to reference the associated IAM role" is incorrect as the reference should be made within the task definition.INCORRECT: "Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then, create an IAM group and configure the ECS cluster to reference that group" is incorrect as the reference should be made within the task definition.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskiamroles.html'},{question:"A company is creating a gaming application that will be deployed on mobile devices. The application will send data to a Lambda functionbased RESTful API. The application will assign each API request a unique identifier. The volume of API requests from the application can randomly vary at any given time of day. During request throttling, the application might need to retry requests. The API must be able to address duplicate requests without inconsistencies or data loss. Which of the following would you recommend to handle these requirements?",answers:[{text:"Persist the unique identifier for each request in an RDS MySQL table. Change the Lambda function to check the table for the identifier before processing the request",isCorrect:!1},{text:"Persist the unique identifier for each request in a DynamoDB table. Change the Lambda function to send a client error response when the function receives a duplicate request",isCorrect:!1},{text:"Persist the unique identifier for each request in a DynamoDB table. Change the Lambda function to check the table for the identifier before processing the request",isCorrect:!0},{text:"Persist the unique identifier for each request in an ElastiCache for Memcached cache. Change the Lambda function to check the cache for the identifier before processing the request",isCorrect:!1}],explanation:"Correct option:Persist the unique identifier for each request in a DynamoDB table. Change the Lambda function to check the table for the identifier before processing the requestDynamoDB is a fully managed, serverless, keyvalue NoSQL database designed to run highperformance applications at any scale. DynamoDB offers builtin security, continuous backups, automated multiRegion replication, inmemory caching, and data import and export tools. Ondemand backup and restore allows you to create full backups of your DynamoDB. Pointintime recovery (PITR) helps protect your DynamoDB tables from accidental write or delete operations. PITR provides continuous backups of your DynamoDB table data, and you can restore that table to any point in time up to the second during the preceding 35 days.These features ensure that there is no data loss for the application, thereby meeting a key requirement for the given use case. The solution should also be able to address any duplicate requests without inconsistencies, so the Lambda function should be changed to inspect the table for the given identifier and process the request only if the identifier is unique.DynamoDB Overview: via https://aws.amazon.com/dynamodb/ DynamoDBIncorrect options:Persist the unique identifier for each request in an ElastiCache for Memcached cache. Change the Lambda function to check the cache for the identifier before processing the request Memcached is designed for simplicity and it does not offer any snapshot or replication features. This can lead to data loss for applications. Therefore, this option is not the right fit for the given use case.via https://aws.amazon.com/elasticache/redisvsmemcached/ Persist the unique identifier for each request in an RDS MySQL table. Change the Lambda function to check the table for the identifier before processing the request DynamoDB is a better fit than RDS MySQL to handle massive traffic spikes for write requests. DynamoDB is a keyvalue and document database that supports tables of virtually any size with horizontal scaling. DynamoDB scales to more than 10 trillion requests per day and with tables that have more than ten million read and write requests per second and petabytes of data storage. DynamoDB can be used to build applications that need consistent singledigit millisecond performance. MySQL RDS can be scaled vertically, however, it cannot match the performance benefits offered by DynamoDB for the given use case.Persist the unique identifier for each request in a DynamoDB table. Change the Lambda function to send a client error response when the function receives a duplicate request The solution should be able to address any duplicates without any inconsistencies. If Lambda sends a client error response upon receiving a duplicate request, it represents an inconsistent response. So this option is incorrect.References:https://aws.amazon.com/dynamodb/ https://aws.amazon.com/elasticache/redisvsmemcached/"},{question:"An XRay daemon is being used on an Amazon ECS cluster to assist with debugging stability issues. A developer requires more detailed timing information and data related to downstream calls to AWS services. What should the developer use to obtain this extra detail?",answers:[{text:"Annotations",isCorrect:!1},{text:"Subsegments",isCorrect:!0},{text:"Filter expressions",isCorrect:!1},{text:"Metadata",isCorrect:!1}],explanation:'A segment can break down the data about the work done into subsegments. Subsegments provide more granular timing information and details about downstream calls that your application made to fulfill the original request.A subsegment can contain additional details about a call to an AWS service, an external HTTP API, or an SQL database. You can even define arbitrary subsegments to instrument specific functions or lines of code in your application.CORRECT: "Subsegments" is the correct answer.INCORRECT: "Annotations" is incorrect. Annotations are simple keyvalue pairs that are indexed for use with filter expressions.Use annotations to record data that you want to use to group traces in the console, or when calling the the GetTraceSummaries API.INCORRECT: "Metadata" is incorrect. Metadata are keyvalue pairs with values of any type, including objects and lists, but that are not indexed. Use metadata to record data you want to store in the trace but don\'t need to use for searching traces.INCORRECT: "Filter expressions" is incorrect. You can use filter expressions to find traces related to specific paths or users.References:https://docs.aws.amazon.com/xray/latest/devguide/xraysdkdotnetsubsegments.html https://github.com/awsdocs/awsxraydeveloperguide/blob/master/docsource/xrayconcepts.md#xrayconceptsfilterexpressions'},{question:"A Development team need to push an update to an application that is running on AWS Elastic Beanstalk. The business SLA states that the application must maintain full performance capabilities during updates whilst minimizing cost. Which Elastic Beanstalk deployment policy should the development team select?",answers:[{text:"Immutable",isCorrect:!1},{text:"Rolling with additional batch",isCorrect:!0},{text:"Rolling",isCorrect:!1},{text:"All at once",isCorrect:!1}],explanation:'AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, and Immutable) and options that let you configure batch size and health check behavior during deployments.For this scenario we need to ensure we do not reduce the capacity of the application but we also need to minimize cost. In the table below you can see the different deployment policies available and how they impact capacity and cost:The Rolling with additional batch deployment policy does require extra cost but the extra cost is the size of a batch of instances, therefore you can reduce cost by reducing the batch size. The Immutable deployment policy requires a total deployment of new instances – i.e. if you have 4 instances this will double to 8 instances.Therefore, the best deployment policy to use for this scenario is the Rolling with additional batch.CORRECT: "Rolling with additional batch" is the correct answer.INCORRECT: "Immutable" is incorrect as this would require a higher cost as you need a total deployment of new instances.INCORRECT: "Rolling" is incorrect as this will result in a reduction in capacity which will affect performance.INCORRECT: "All at once" is incorrect as this results in a total reduction in capacity, i.e. your entire application is taken down at once while the application update is installed.References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.rollingversiondeploy.html'},{question:"An organization has an account for each environment: Production, Testing, Development. A Developer with an IAM user in the Development account needs to launch resources in the Production and Testing accounts. What is the MOST efficient way to provide access?",answers:[{text:"Create an IAM group in the Production and Testing accounts and add the Developer’s user from the Development account to the groups",isCorrect:!1},{text:"Create a role with the required permissions in the Production and Testing accounts and have the Developer assume that role",isCorrect:!0},{text:"Create an IAM permissions policy in the Production and Testing accounts and reference the IAM user in the Development account",isCorrect:!1},{text:"Create a separate IAM user in each account and have the Developer login separately to each account",isCorrect:!1}],explanation:'You can grant your IAM users’ permission to switch to roles within your AWS account or to roles defined in other AWS accounts that you own. This is known as crossaccount access.In the image below a user in the Development account needs to access an S3 bucket in the Production account:The user is able to assume the role in the Production account and access the S3 bucket. This is more efficient than providing the user with multiple accounts. In this scenario the user requests to switch to the role through either the console or the API/CLI.CORRECT: "Create a role with the required permissions in the Production and Testing accounts and have the Developer assume that role" is the correct answer.INCORRECT: "Create a separate IAM user in each account and have the Developer login separately to each account" is incorrect as this is not the most efficient method of providing access. Crossaccount access is preferred .INCORRECT: "Create an IAM group in the Production and Testing accounts and add the Developer’s user from the Development account to the groups" is incorrect as you cannot add an IAM user from another AWS account to a group.INCORRECT: "Create an IAM permissions policy in the Production and Testing accounts and reference the IAM user in the Development account" is incorrect as you cannot reference an IAM user from another AWS account in a permissions policy.References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_commonscenarios_awsaccounts.html'},{question:"An application serves customers in several different geographical regions. Information about the location users connect from is written to logs stored in Amazon CloudWatch Logs. The company needs to publish an Amazon CloudWatch custom metric that tracks connections for each location. Which approach will meet these requirements?",answers:[{text:".Configure a CloudWatch Events rule that creates a custom metric from the CloudWatch Logs group.",isCorrect:!1},{text:"Create a CloudWatch metric filter to extract metrics from the log files with location as a dimension.",isCorrect:!0},{text:"Stream data to an Amazon Elasticsearch cluster in nearreal time and export a custom metric.",isCorrect:!1},{text:"Create a CloudWatch Logs Insights query to extract the location information from the logs and to create a custom metric with location as a dimension..",isCorrect:!1}],explanation:"Create a CloudWatch Logs Insights query to extract the location information from the logs and to create a custommetric with location as a dimension."},{question:"A Developer is creating a new web application that will be deployed using AWS Elastic Beanstalk from the AWS Management Console. The Developer is about to create a source bundle which will be uploaded using the console. Which of the following are valid requirements for creating the source bundle? (Select TWO.)",answers:[{text:"Must include a parent folder or toplevel directory.",isCorrect:!1},{text:"Must include the cron.yaml file.",isCorrect:!1},{text:"Must consist of one or more ZIP files.",isCorrect:!1},{text:"Must not exceed 512 MB.",isCorrect:!0},{text:"Must not include a parent folder or toplevel directory.",isCorrect:!0}],explanation:'When you use the AWS Elastic Beanstalk console to deploy a new application or an application version, you\'ll need to upload a source bundle. Your source bundle must meet the following requirements:• Consist of a single ZIP file or WAR file (you can include multiple WAR files inside your ZIP file)• Not exceed 512 MB• Not include a parent folder or toplevel directory (subdirectories are fine)If you want to deploy a worker application that processes periodic background tasks, your application source bundle must also include a cron.yaml file, but in other cases it is not required.CORRECT: "Must not include a parent folder or toplevel directory" is a correct answer.CORRECT: "Must not exceed 512 MB" is also a correct answer.INCORRECT: "Must include the cron.yaml file" is incorrect. As mentioned above, this is not required in all cases.INCORRECT: "Must include a parent folder or toplevel directory" is incorrect. A parent folder or toplevel directory must NOT be included.INCORRECT: "Must consist of one or more ZIP files" is incorrect. You bundle into a single ZIP or WAR file.References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applicationssourcebundle.html'},{question:"As an AWS Certified Developer Associate, you have been asked to create an AWS Elastic Beanstalk environment to handle deployment for an application that has high traffic and high availability needs. You need to deploy the new version using Beanstalk while making sure that performance and availability are not affected. Which of the following is the MOST optimal way to do this while keeping the solution costeffective?",answers:[{text:"Deploy using 'Rolling' deployment policy",isCorrect:!1},{text:"Deploy using 'Rolling with additional batch' deployment policy",isCorrect:!0},{text:"Deploy using 'Immutable' deployment policy",isCorrect:!1},{text:"Deploy using 'All at once' deployment policy",isCorrect:!1}],explanation:"Correct option:AWS Elastic Beanstalk offers several deployment policies and settings. Choosing the right deployment policy for your application is a tradeoff based on a few considerations and depends on your business needs.Deploy using 'Rolling with additional batch' deployment policy With this method, Elastic Beanstalk launches an extra batch of instances, then performs a rolling deployment. Launching the extra batch takes time, and ensures that the same bandwidth is retained throughout the deployment. This policy also avoids any reduced availability, although at a cost of an even longer deployment time compared to the Rolling method. Finally, this option is suitable if you must maintain the same bandwidth throughout the deployment.Overview of Elastic Beanstalk Deployment Policies: via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html Incorrect options:Deploy using 'Immutable' deployment policy A slower deployment method, that ensures your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails. With this method, Elastic Beanstalk performs an immutable update to deploy your application. In an immutable update, a second Auto Scaling group is launched in your environment and the new version serves traffic alongside the old version until the new instances pass health checks.Deploy using 'All at once' deployment policy This is the quickest deployment method. Suitable if you can accept a short loss of service, and if quick deployments are important to you. With this method, Elastic Beanstalk deploys the new application version to each instance. Then, the web proxy or application server might need to restart. As a result, your application might be unavailable to users (or have low availability) for a short time.Deploy using 'Rolling' deployment policy With this method, your application is deployed to your environment one batch of instances at a time. Most bandwidth is retained throughout the deployment. Avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. Suitable if you can't accept any period of completely lost service. The use case states that the application has high traffic and high availability requirements, so full capacity must be maintained during deployments, hence rolling with additional batch deployment is a better fit than the rolling deployment.Reference:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html"},{question:"A developer created an operational dashboard for a serverless application using Amazon API Gateway, AWS Lambda, Amazon S3, and Amazon DynamoDB. Users will connect to the dashboard from a variety of mobile applications, desktops and tablets.The developer needs an authentication mechanism that can allow users to signin and will remember the devices users sign in from and suppress the second factor of authentication for remembered devices. Which AWS service should the developer use to support this scenario?",answers:[{text:"AWS KMS",isCorrect:!1},{text:"Amazon Cognito",isCorrect:!0},{text:"Amazon IAM",isCorrect:!1},{text:"AWS Directory Service",isCorrect:!1}],explanation:'Amazon Cognito lets you add user signup, signin, and access control to your web and mobile apps quickly and easily. Cognito supports multiple devices types including mobile applications, desktops and tablets.You can enable device remembering for Amazon Cognito user pools. A remembered device can serve in place of the security code delivered via SMS as a second factor of authentication. This suppresses the second authentication challenge from remembered devices and thus reduces the friction users experience with multifactor authentication (MFA).Therefore, Amazon Cognito is the best answer and will support all of the requirements in the scenario.CORRECT: "Amazon Cognito" is the correct answer.INCORRECT: "AWS Directory Service" is incorrect as this service enables directoryaware workloads and AWS resources to use managed Active Directory in the AWS Cloud.INCORRECT: "AWS KMS" is incorrect as KMS is used to manage encryption keys; it does not enable authentication from mobile devices.INCORRECT: "Amazon IAM" is incorrect as IAM is not the best authentication solution for mobile users. It also does not support device remembering or any ability to suppress MFA when it is enabled.References:https://aws.amazon.com/blogs/mobile/trackingandrememberingdevicesusingamazoncognitoyouruserpools/ https://aws.amazon.com/cognito/details/'},{question:"A Developer is setting up a code update to Amazon ECS using AWS CodeDeploy. The Developer needs to complete the code update quickly. Which of the following deployment types should the Developer use?",answers:[{text:"Linear",isCorrect:!1},{text:"Canary",isCorrect:!1},{text:"Inplace",isCorrect:!1},{text:"Blue/green",isCorrect:!0}],explanation:'CodeDeploy provides two deployment type options – inplace and blue/green. Note that AWS Lambda and Amazon ECS deployments cannot use an inplace deployment type.The Blue/green deployment type on an Amazon ECS compute platform works like this:• Traffic is shifted from the task set with the original version of an application in an Amazon ECS service to a replacement task set in the same service.• You can set the traffic shifting to linear or canary through the deployment configuration.• The protocol and port of a specified load balancer listener is used to reroute production traffic.• During a deployment, a test listener can be used to serve traffic to the replacement task set while validation tests are run.CORRECT: "Blue/green" is the correct answer.INCORRECT: "Canary" is incorrect as this is a traffic shifting option, not a deployment type. Traffic is shifted in two increments.INCORRECT: "Linear" is incorrect as this is a traffic shifting option, not a deployment type. Traffic is shifted in two increments.INCORRECT: "Inplace" is incorrect as AWS Lambda and Amazon ECS deployments cannot use an inplace deployment type.References: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html'},{question:"A Developer needs to manage AWS services from a local development server using the AWS CLI. How can the Developer ensure that the CLI uses their IAM permissions?",answers:[{text:"Run the aws configure command and provide the Developer’s IAM access key ID and secret access key",isCorrect:!0},{text:"Put the Developer’s IAM user account in an IAM group that has the necessary permissions",isCorrect:!1},{text:"Save the Developer’s IAM login credentials as environment variables and reference them when executing AWS CLI commands",isCorrect:!1},{text:"Create an IAM Role with the required permissions and attach it to the local server’s instance profile",isCorrect:!1}],explanation:'You can configure the AWS CLI on Linux, MacOS, and Windows. Computers can be located anywhere as long as they can connect to the AWS API.For this scenario, the best solution is to run aws configure and use the IAM user’s access key ID and secret access key. This will mean that commands run using the AWS CLI will use the user’s IAM permissions as required.CORRECT: "Run the aws configure command and provide the Developer’s IAM access key ID and secret access key" is the correct answer.INCORRECT: "Create an IAM Role with the required permissions and attach it to the local server’s instance profile" is incorrect as this is not an Amazon EC2 instance so you cannot attach an IAM role.INCORRECT: "Put the Developer’s IAM user account in an IAM group that has the necessary permissions" is incorrect as this does not assist with configuring the AWS CLI.INCORRECT: "Save the Developer’s IAM login credentials as environment variables and reference them when executing AWS CLI commands" is incorrect as the IAM login credentials cannot be used with the AWS CLI. You need to use an access key ID and secret access key with the AWS CLI and these are configured for use by running aws configure.References: https://docs.aws.amazon.com/cli/latest/userguide/clichapconfigure.html'},{question:"An application needs to generate SMS text messages and emails for a large number of subscribers. Which AWS service can be used to send these messages to customers?",answers:[{text:"Amazon SNS",isCorrect:!0},{text:"Amazon SQS",isCorrect:!1},{text:"Amazon SES",isCorrect:!1},{text:"Amazon SWF",isCorrect:!1}],explanation:'Amazon Simple Notification Service (Amazon SNS) is a web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. In Amazon SNS, there are two types of clients—publishers and subscribers—also referred to as producers and consumers.Publishers communicate asynchronously with subscribers by producing and sending a message to a topic, which is a logical access point and communication channel.Subscribers (that is, web servers, email addresses, Amazon SQS queues, AWS Lambda functions) consume or receive the message or notification over one of the supported protocols (that is, Amazon SQS, HTTP/S, email, SMS, Lambda) when they are subscribed to the topic.CORRECT: "Amazon SNS" is the correct answer.INCORRECT: "Amazon SES" is incorrect as this service only sends email, not SMS text messages.INCORRECT: "Amazon SQS" is incorrect as this is a hosted message queue for decoupling application components.INCORRECT: "Amazon SWF" is incorrect as the Simple Workflow Service is used for orchestrating multistep workflows.References: https://docs.aws.amazon.com/sns/latest/dg/welcome.html'},{question:"A company maintains a REST API service using Amazon API Gateway with native API key validation. The company recently launched a new registration page, which allows users to sign up for the service. The registration page creates a new API key using CreateApiKey and sends the new key to the user. When the user attempts to call the API using this key, the user receives a 403 Forbidden error. Existing users are unaffected and can still call the API. What code updates will grant these new users' access to the API?",answers:[{text:"The updateAuthorizer method must be called to update the API’s authorizer to include the newly created API key",isCorrect:!1},{text:"The createDeployment method must be called so the API can be redeployed to include the newly created API key",isCorrect:!1},{text:"The importApiKeys method must be called to import all newly created API keys into the current stage of the API",isCorrect:!1},{text:"The createUsagePlanKey method must be called to associate the newly created API key with the correct usage plan",isCorrect:!0}],explanation:'A usage plan specifies who can access one or more deployed API stages and methods—and also how much and how fast they can access them. The plan uses API keys to identify API clients and meters access to the associated API stages for each key. It also lets you configure throttling limits and quota limits that are enforced on individual client API keys. API keys are alphanumeric string values that you distribute to application developer customers to grant access to your API. You can use API keys together with usage plans or Lambda authorizers to control access to your APIs. API Gateway can generate API keys on your behalf, or you can import them from a CSV file. You can generate an API key in API Gateway, or import it into APIGateway from an external source.To associate the newly created key with a usage plan the CreatUsagePlanKey API can be called. This creates a usage plan key for adding an existing API key to a usage plan.CORRECT: "The createUsagePlanKey method must be called to associate the newly created API key with the correct usage plan" is the correct answer.INCORRECT: "The createDeployment method must be called so the API can be redeployed to include the newly created API key" is incorrect as you do not need to redeploy an API to a stage in order to associate an API key.INCORRECT: "The updateAuthorizer method must be called to update the API’s authorizer to include the newly created API key" is incorrect as this updates and authorizer resource, not an API key.INCORRECT: "The importApiKeys method must be called to import all newly created API keys into the current stage of the API" is incorrect as this imports API keys to API Gateway from an external source such as a CSV file which is not relevant to this scenario.References:https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayapiusageplans.html http://docs.amazonaws.cn/en_us/sdkfornet/v3/apidocs/items/APIGateway/MAPIGatewayCreateUsagePlanKeyCreateUsagePlanKeyRequest.html'},{question:"A serverless application uses an IAM role to authenticate and authorize access to an Amazon DynamoDB table. A Developer is troubleshooting access issues affecting the application. The Developer has access to the IAM role that the application is using. Which of the following commands will help the Developer to test the role permissions using the AWS CLI?",answers:[{text:"aws sts getsessiontoken",isCorrect:!1},{text:"aws iam getrolepolicy",isCorrect:!1},{text:"aws sts assumerole",isCorrect:!0},{text:"aws dynamodb describeendpoints",isCorrect:!1}],explanation:'The AWS CLI “aws sts assume role” command will enable the Developer to assume the role and gain temporary security credentials. The Developer can then use those security credentials to troubleshoot access issues that are affecting the application.CORRECT: "aws sts assumerole" is the correct answer.INCORRECT: "aws sts getsessiontoken" is incorrect. This is used to get temporary credentials for an AWS account or IAM user. It can subsequently be used to call the assumerole API.INCORRECT: "aws iam getrolepolicy" is incorrect. This command retrieves the specified inline policy document that is embedded with the specified IAM role.INCORRECT: "aws dynamodb describeendpoints" is incorrect. This command returns the regional endpoint information.References: https://docs.aws.amazon.com/cli/latest/reference/sts/assumerole.html'},{question:"You are storing bids information on your betting application and you would like to automatically expire DynamoDB table data after one week. What should you use?",answers:[{text:"Use TTL",isCorrect:!0},{text:"Use DAX",isCorrect:!1},{text:"Use DynamoDB Streams",isCorrect:!1},{text:"Use a Lambda function",isCorrect:!1}],explanation:"Correct option:Use TTLTime To Live (TTL) for DynamoDB allows you to define when items in a table expire so that they can be automatically deleted from the database. TTL is provided at no extra cost as a way to reduce storage usage and reduce the cost of storing irrelevant data without using provisioned throughput. With TTL enabled on a table, you can set a timestamp for deletion on a peritem basis, allowing you to limit storage usage to only those records that are relevant.Incorrect options:Use DynamoDB Streams These help you get a changelog of your DynamoDB table but won't help you delete expired data. Note that data expired using a TTL will appear as an event in your DynamoDB streams.Use DAX Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, inmemory cache for DynamoDB that delivers up to a 10x performance improvement: from milliseconds to microseconds: even at millions of requests per second. This is a caching technology for your DynamoDB tables.Use a Lambda function This could work but would require setting up indexes, queries, or scans to work, as well as trigger them often enough using a CloudWatch Events. This bandaid solution would never be as good as using the TTL feature in DynamoDB.Reference: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html"},{question:"A Developer is attempting to call the Amazon CloudWatch API and is receiving HTTP 400: ThrottlingException errors intermittently. When a call fails, no data is retrieved. What best practice should the Developer first attempt to resolve this issue?",answers:[{text:"Contact AWS Support for a limit increase",isCorrect:!1},{text:"Use the AWS CLI to get the metrics",isCorrect:!1},{text:"Analyze the applications and remove the API call",isCorrect:!1},{text:"Retry the call with exponential backoff",isCorrect:!0}],explanation:'Occasionally ,you may receive the 400 ThrottlingException error for PutMetricData API calls in Amazon CloudWatch with a detailed response similar the following:CloudWatch requests are throttled for each Amazon Web Services (AWS) account on a perRegion basis to help service performance. For current PutMetricData API request limits, see CloudWatch Limits.All calls to the PutMetricData API in an AWS Region count towards the maximum allowed request rate. This number includes calls from any custom or thirdparty application, such as calls from the CloudWatch Agent, the AWS Command Line Interface (AWS CLI), or the AWS Management Console.Resolutions: It\'s a best practice to use the following methods to reduce your call rate and avoid API throttling:• Distribute your API calls evenly over time rather than making several API calls in a short time span. If you require data to be available with a oneminute resolution, you have an entire minute to emit that metric. Use jitter (randomized delay) to send data points at various times.• Combine as many metrics as possible into a single API call. For example, a single PutMetricData call can include 20 metrics and 150 data points. You can also use preaggregated data sets, such as StatisticSet, to publish aggregated data points, thus reducing the number of PutMetricData calls per second.• Retry your call with exponential backoff and jitter.Following attempting the above resolutions AWS suggest the following: “If you still require a higher limit, you can request a limit increase. Increasing the rate limit can have a high financial impact on your AWS bill.”Therefore, the first thing the Developer should do, from the list of options presented, is to retry the call with exponential backoff.CORRECT: "Retry the call with exponential backoff" is the correct answer.INCORRECT: "Contact AWS Support for a limit increase" is incorrect. As mentioned above, there are other resolutions the Developer should attempt before contacting support to raise the limit.INCORRECT: "Use the AWS CLI to get the metrics" is incorrect as this will still make the same API calls.INCORRECT: "Analyze the applications and remove the API call" is incorrect as this is not a good resolution to the issue as this may mean that important monitoring and logging data is not recorded for the application.References: https://aws.amazon.com/premiumsupport/knowledgecenter/cloudwatch400errorthrottling/'},{question:"An application is hosted in AWS Elastic Beanstalk and is connected to a database running on Amazon RDS MySQL. A Developer needs to instrument the application to trace database queries and calls to downstream services using AWS XRay. How can the Developer enable tracing for the application?",answers:[{text:"Enable active tracing in the Elastic Beanstalk console",isCorrect:!1},{text:"Enable XRay tracing using an AWS Lambda function",isCorrect:!1},{text:"Add a xraydaemon.config file to the root of the source code to enable the XRay deamon",isCorrect:!1},{text:"Add a .ebextensions/xraydaemon.config file to the source code to enable the XRay daemon",isCorrect:!0}],explanation:'To relay trace data from your application to AWS XRay, you can run the XRay daemon on your Elastic Beanstalk environment\'s Amazon EC2 instances.Elastic Beanstalk platforms provide a configuration option that you can set to run the daemon automatically. You can enable the daemon in a configuration file in your source code or by choosing an option in the Elastic Beanstalk console. When you enable the configuration option, the daemon is installed on the instance and runs as a service.The above code will ensure the XRay daemon starts and the Developer can enable tracing for the application as required.CORRECT: "Add a .ebextensions/xraydaemon.config file to the source code to enable the XRay daemon" is the correct answer.INCORRECT: "Add a xraydaemon.config file to the root of the source code to enable the XRay deamon" is incorrect as all.config files must be stored in the .ebextensions folder in the source code.INCORRECT: "Enable active tracing in the Elastic Beanstalk console" is incorrect as you cannot enable active tracing through the console for Elastic Beanstalk. This is available for AWS Lambda and API Gateway.INCORRECT: "Enable XRay tracing using an AWS Lambda function" is incorrect as there is no need to add a Lambda function to the application to add tracing support. The developer can enable tracing by enabling the XRay daemon.References: https://docs.aws.amazon.com/xray/latest/devguide/xraydaemonbeanstalk.html'},{question:"A Developer is deploying an AWS Lambda update using AWS CodeDeploy. In the appspec.yaml file, which of the following is a valid structure for the order of hooks that should be specified?",answers:[{text:"BeforeInstall > AfterInstall > AfterAllowTestTraffic > BeforeAllowTraffic > AfterAllowTraffic",isCorrect:!1},{text:"BeforeAllowTraffic > AfterAllowTraffic",isCorrect:!0},{text:"BeforeBlockTraffic > AfterBlockTraffic > BeforeAllowTraffic > AfterAllowTraffic",isCorrect:!1},{text:"BeforeInstall > AfterInstall > ApplicationStart > ValidateService",isCorrect:!1}],explanation:'The content in the \'hooks\' section of the AppSpec file varies, depending on the compute platform for your deployment. The \'hooks\' section for an EC2/OnPremises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts.The \'hooks\' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment.Therefore, in this scenario a valid structure for the order of hooks that should be specified in the appspec.yml file is:BeforeAllowTraffic > AfterAllowTrafficCORRECT: "BeforeAllowTraffic > AfterAllowTraffic" is the correct answer.INCORRECT: "BeforeInstall > AfterInstall > ApplicationStart > ValidateService" is incorrect as this would be valid for Amazon EC2.INCORRECT: "BeforeInstall > AfterInstall > AfterAllowTestTraffic > BeforeAllowTraffic > AfterAllowTraffic" is incorrect as this would be valid for Amazon ECS.INCORRECT: "BeforeBlockTraffic > AfterBlockTraffic > BeforeAllowTraffic > AfterAllowTraffic" is incorrect as this is a partial listing of hooks for Amazon EC2 but is incomplete.References: https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfilestructurehooks.html'},{question:"As part of his development work, an AWS Certified Developer Associate is creating policies and attaching them to IAM identities. After creating necessary Identitybased policies, he is now creating Resourcebased policies. Which is the only resourcebased policy that the IAM service supports?",answers:[{text:"AWS Organizations Service Control Policies (SCP)",isCorrect:!1},{text:"Trust policy",isCorrect:!0},{text:"Permissions boundary",isCorrect:!1},{text:"Access control list (ACL)",isCorrect:!1}],explanation:"Correct option:You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. Resourcebased policies are JSON policy documents that you attach to a resource such as an Amazon S3 bucket. These policies grant the specified principal permission to perform specific actions on that resource and define under what conditions this applies.Trust policy Trust policies define which principal entities (accounts, users, roles, and federated users) can assume the role. An IAM role is both an identity and a resource that supports resourcebased policies. For this reason, you must attach both a trust policy and an identitybased policy to an IAM role. The IAM service supports only one type of resourcebased policy called a role trust policy, which is attached to an IAM role.Incorrect options:AWS Organizations Service Control Policies (SCP) If you enable all features of AWS organization, then you can apply service control policies (SCPs) to any or all of your accounts. SCPs are JSON policies that specify the maximum permissions for an organization or organizational unit (OU). The SCP limits permissions for entities in member accounts, including each AWS account root user. An explicit deny in any of these policies overrides the allow.Access control list (ACL) Access control lists (ACLs) are service policies that allow you to control which principals in another account can access a resource. ACLs cannot be used to control access for a principal within the same account. Amazon S3, AWS WAF, and Amazon VPC are examples of services that support ACLs.Permissions boundary AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identitybased policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identitybased policies and its permissions boundaries.References:https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_resourcebased https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html"},{question:"Users of an application using Amazon API Gateway, AWS Lambda and Amazon DynamoDB have reported errors when using the application. Which metrics should a Developer monitor in Amazon CloudWatch to determine the number of clientside and serverside errors?",answers:[{text:"CacheHitCount and CacheMissCount",isCorrect:!1},{text:"IntegrationLatency and Latency",isCorrect:!1},{text:"4XXError and 5XXError",isCorrect:!0},{text:"Errors",isCorrect:!1}],explanation:'To determine the number of clientside errors captured in a given period the Developer should look at the 4XXError metric. To determine the number of serverside errors captured in a given period the Developer should look at the 5XXError.CORRECT: "4XXError and 5XXError" is the correct answer.INCORRECT: "CacheHitCount and CacheMissCount" is incorrect as these count the number of requests served from the cache and the number of requests served from the backend.INCORRECT: "IntegrationLatency and Latency" is incorrect as these measure the amount of time between when API Gateway relays a request to the backend and when it receives a response from the backend and the time between when API Gateway receives a request from a client and when it returns a response to the client.INCORRECT: "Errors" is incorrect as this is not a metric related to Amazon API Gateway.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaymetricsanddimensions.html'},{question:"A company is using Amazon RDS MySQL instances for its application database tier and apache Tomcat servers for its web tier. Most of the database queries from web applications are repeated read requests.A Developer plans to add an inmemory store to improve performance for repeated read requests. Which AWS service would BEST fit these requirements?",answers:[{text:"Amazon SQS",isCorrect:!1},{text:"Amazon RDS MultiAZ",isCorrect:!1},{text:"Amazon RDS read replica",isCorrect:!1},{text:"Amazon ElastiCache",isCorrect:!0}],explanation:'There are two options that can assist with improving performance for read requests: Amazon RDS read replicas, and Amazon ElastiCache. Both of these solutions will provide horizontal scaling for read requests to reduce the impact on the main database.However, only Amazon ElastiCache is an inmemory database so the best solution is for the Developer to use Amazon ElastiCache to improve performance for repeated read requests.CORRECT: "Amazon ElastiCache" is the correct answer.INCORRECT: "Amazon RDS MultiAZ" is incorrect as multiAZ is used for faulttolerance and disaster recovery, not improving read performance.INCORRECT: "Amazon SQS" is incorrect as SQS is a hosted message queue use for decoupling.INCORRECT: "Amazon RDS read replica" is incorrect as this is not an inmemory store.References: https://aws.amazon.com/elasticache/'},{question:"A Developer created a new AWS account and must create a scalable AWS Lambda function that meets the following requirements for concurrent execution:• Average execution time of 100 seconds• 50 requests per secondWhich step must be taken prior to deployment to prevent errors?",answers:[{text:"Implement deadletter queues to capture invocation errors",isCorrect:!1},{text:"Contact AWS Support to increase the concurrent execution limits",isCorrect:!0},{text:"Add an event source from Amazon API Gateway to the Lambda function",isCorrect:!1},{text:"Implement error handling within the application code",isCorrect:!1}],explanation:"Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.In this scenario, the average execution time is 100 seconds and 50 requests are received per second. This means the concurrency requirement is 100 x 50 = 5,000. As 5,000 is well above the default allowed concurrency of 1,000 executions a second. Therefore, the Developer will need to contact AWS Support to increase the concurrent execution limits.CORRECT: “Contact AWS Support to increase the concurrent execution limits” is the correct answer.INCORRECT: “Implement deadletter queues to capture invocation errors” is incorrect as this is a method of capturing information for later analysis. The Developer first needs to ensure the Lambda can scale to its expected load.INCORRECT: “Add an event source from Amazon API Gateway to the Lambda function ” is incorrect as this is not necessary and will not ensure Lambda can scale to handle the load.INCORRECT: “Implement error handling within the application code” is incorrect as there’s little point relying on error handling when you know the function will not be able to scale to expected load.References: https://docs.aws.amazon.com/lambda/latest/dg/configurationconcurrency.html"},{question:"A company is deploying a static website hosted from an Amazon S3 bucket. The website must support encryption in transit for website visitors. Which combination of actions must the Developer take to meet this requirement? (Select TWO.)",answers:[{text:"Configure an Amazon CloudFront distribution with an SSL/TLS certificate.",isCorrect:!0},{text:"Configure an Amazon CloudFront distribution with an AWS WAF WebACL.",isCorrect:!1},{text:"Create an Amazon CloudFront distribution. Set the S3 bucket as an origin.",isCorrect:!0},{text:"Create an AWS WAF WebACL with a secure listener.",isCorrect:!1},{text:"Configure the S3 bucket with an SSL/TLS certificate.",isCorrect:!1}],explanation:'Amazon S3 static websites use the HTTP protocol only and you cannot enable HTTPS. To enable HTTPS connections to your S3 static website, use an Amazon CloudFront distribution that is configured with an SSL/TLS certificate. This will ensure that connections between clients and the CloudFront distribution are encrypted intransit as per the requirements.CORRECT: "Create an Amazon CloudFront distribution. Set the S3 bucket as an origin" is a correct answer.CORRECT: "Configure an Amazon CloudFront distribution with an SSL/TLS certificate" is also a correct answer.INCORRECT: "Create an AWS WAF WebACL with a secure listener" is incorrect. You cannot configure a secure listener on a WebACL.INCORRECT: "Configure an Amazon CloudFront distribution with an AWS WAF WebACL" is incorrect. This will not enable encrypted connections.INCORRECT: "Configure the S3 bucket with an SSL/TLS certificate" is incorrect. You cannot manually add SSL/TLS certificates to Amazon S3, and it is not possible to directly configure an S3 bucket that is configured as a static website to accept encrypted connections.References: https://aws.amazon.com/premiumsupport/knowledgecenter/cloudfrontservestaticwebsite/'},{question:"A Developer is creating a web application that will be used by employees working from home. The company uses a SAML directory on-premises for storing user information. The Developer must integrate with the SAML directory and authorize each employee to access only their own data when using the application. Which approach should the Developer take?",answers:[{text:"Create a unique IAM role for each employee and have each employee assume the role to access the application so they can access their personal data only.",isCorrect:!1},{text:"Use an Amazon Cognito identity pool, federate with the SAML provider, and use a trust policy with an IAM condition key to limit employee access.",isCorrect:!0},{text:"Create the application within an Amazon VPC and use a VPC endpoint with a trust policy to grant access to the employees.",isCorrect:!1},{text:"Use Amazon Cognito user pools, federate with the SAML provider, and use user pool groups with an IAM policy.",isCorrect:!1}],explanation:'Amazon Cognito leverages IAM roles to generate temporary credentials for your application\'s users. Access to permissions is controlled by a role\'s trust relationships.In this example the Developer must limit access to specific identities in the SAML directory. The Developer can create a trust policy with an IAM condition key that limits access to a specific set of app users by checking the value of cognitoidentity.amazonaws.com:sub:CORRECT: "Use an Amazon Cognito identity pool, federate with the SAML provider, and use a trust policy with an IAM condition key to limit employee access" is the correct answer.INCORRECT: "Use Amazon Cognito user pools, federate with the SAML provider, and use user pool groups with an IAM policy" is incorrect. A user pool can be used to authenticate but the identity pool is used to provide authorized access to AWS services.INCORRECT: "Create the application within an Amazon VPC and use a VPC endpoint with a trust policy to grant access to the employees" is incorrect. You cannot provide access to an onpremises SAML directory using a VPC endpoint.INCORRECT: "Create a unique IAM role for each employee and have each employee assume the role to access the application so they can access their personal data only" is incorrect. This is not an integration into the SAML directory and would be very difficult to manage.References:https://docs.aws.amazon.com/cognito/latest/Developerguide/roletrustandpermissions.html https://docs.aws.amazon.com/cognito/latest/Developerguide/iamroles.html'},{question:"A new application will be hosted on the domain name dctlabs.com using an Amazon API Gateway REST API front end. The Developer needs to configure the API with a path to dctlabs.com/products that will be accessed using the HTTP GET verb. How MUST the Developer configure the API? (Select TWO.)",answers:[{text:"Create a GET resource",isCorrect:!1},{text:"Create a /GET method",isCorrect:!1},{text:"Create a /products method",isCorrect:!1},{text:"Create a /products resource",isCorrect:!0},{text:"Create a GET method",isCorrect:!0}],explanation:'An API Gateway REST API is a collection of HTTP resources and methods that are integrated with backend HTTP endpoints, Lambda functions, or other AWS services. You can deploy this collection in one or more stages. Typically, API resources are organized in a resource tree according to the application logic. Each API resource can expose one or more API methods thathave unique HTTP verbs supported by API Gateway.As you can see from the image above, the Developer would need to create a resource which in this case would be /products.The Developer would then create a GET method within the resource.CORRECT: "Create a /products resource" is a correct answer.CORRECT: "Create a GET method" is a correct answer.INCORRECT: "Create a /products method" is incorrect as a resource should be created.INCORRECT: "Create a GET resource" is incorrect as a method should be created.INCORRECT: "Create a /GET method" is incorrect as a method is not preceded by a slash.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaybasicconcept.html'},{question:"A developer has been asked to create an application that can be deployed across a fleet of EC2 instances. The configuration must allow for full control over the deployment steps using the bluegreen deployment. Which service will help you achieve that?",answers:[{text:"CodePipeline",isCorrect:!1},{text:"CodeBuild",isCorrect:!1},{text:"CodeDeploy",isCorrect:!0},{text:"Elastic Beanstalk",isCorrect:!1}],explanation:"Correct option:CodeDeployAWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, onpremises instances, or serverless Lambda functions. AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy.The blue/green deployment type uses the blue/green deployment model controlled by CodeDeploy. This deployment type enables you to verify a new deployment of service before sending production traffic to it.CodeDeploy offers lot of control over deployment steps. Please see this note for more details: via https://aws.amazon.com/aboutaws/whatsnew/2017/01/awscodedeployintroducesbluegreendeployments/ Incorrect options:CodeBuild AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. It cannot be used to deploy applications.Elastic Beanstalk AWS Elastic Beanstalk offers hooks but not as much control as CodeDeploy. Because AWS Elastic Beanstalk performs an inplace update when you update your application versions, your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly.CodePipeline CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change. CodePipeline by itself cannot deploy applications.Reference:https://docs.amazonaws.cn/en_us/codedeploy/latest/userguide/deploymentconfigurations.html"},{question:"A team of Developers are working on a shared project and need to be able to collaborate on code. The shared application code must be encrypted at rest, stored on a highly available and durable architecture, and support multiple versions and batch change tracking. Which AWS service should the Developer use?",answers:[{text:"AWS CodeBuild",isCorrect:!1},{text:"AWS CodeCommit",isCorrect:!0},{text:"Amazon S3",isCorrect:!1},{text:"AWS Cloud9",isCorrect:!1}],explanation:'AWS CodeCommit is a fully managed source control service that hosts secure Gitbased repositories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem. AWS CodeCommit automatically encrypts your files in transit and at rest.AWS CodeCommit helps you collaborate on code with teammates via pull requests, branching, and merging. You can implement workflows that include code reviews and feedback by default, and control who can make changes to specific branches.CORRECT: "AWS CodeCommit" is the correct answer.INCORRECT: "AWS CodeBuild" is incorrect. AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packagesINCORRECT: "Amazon S3" is incorrect. Amazon S3 is an objectbased storage system and does not support the features required here.INCORRECT: "AWS Cloud9" is incorrect. AWS Cloud9 is a cloudbased integrated development environment (IDE) that lets you write, run, and debug your code with just a browser.References: https://aws.amazon.com/codecommit/'},{question:"A Developer is creating a serverless application that uses an Amazon DynamoDB table. The application must make idempotent, all-or-nothing operations for multiple groups of write actions. Which solution will meet these requirements?",answers:[{text:"Update the items in the table using the BatchWriteltem operation and configure idempotency at the table level.",isCorrect:!1},{text:"Enable DynamoDB streams and capture new images. Update the items in the table using the BatchWriteltem.",isCorrect:!1},{text:"Update the items in the table using the TransactWriteltems operation to group the changes.",isCorrect:!0},{text:"Create an Amazon SQS FIFO queue and use the SendMessageBatch operation to group the changes.",isCorrect:!1}],explanation:'TransactWriteItems is a synchronous and idempotent write operation that groups up to 25 write actions in a single allornothing operation. These actions can target up to 25 distinct items in one or more DynamoDB tables within the same AWS account and in the same Region. The aggregate size of the items in the transaction cannot exceed 4 MB. The actions arecompleted atomically so that either all of them succeed or none of them succeeds.A TransactWriteItems operation differs from a BatchWriteItem operation in that all the actions it contains must be completed successfully, or no changes are made at all. With a BatchWriteItem operation, it is possible that only some of the actions in the batch succeed while the others do not.CORRECT: "Update the items in the table using the TransactWriteltems operation to group the changes" is the correct answer.INCORRECT: "Update the items in the table using the BatchWriteltem operation and configure idempotency at the table level" is incorrect. As explained above, the TransactWriteItems operation must be used.INCORRECT: "Enable DynamoDB streams and capture new images. Update the items in the table using the BatchWriteltem" is incorrect. DynamoDB streams will not assist with making idempotent write operations.INCORRECT: "Create an Amazon SQS FIFO queue and use the SendMessageBatch operation to group the changes" is incorrect. Amazon SQS should not be used as it does not assist and this solution is supposed to use a DynamoDB tableReferences: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_TransactWriteItems.html'},{question:"A Developer is creating a serverless application that will process sensitive data. The AWS Lambda function must encrypt all data that is written to /tmp storage at rest. How should the Developer encrypt this data?",answers:[{text:"Attach the Lambda function to a VPC and encrypt Amazon EBS volumes at rest using the AWS managed CMK. Mount the EBS volume to /tmp.",isCorrect:!1},{text:"Enable default encryption on an Amazon S3 bucket using an AWS KMS customer managed customer master key (CMK). Mount the S3 bucket to /tmp.",isCorrect:!1},{text:"Configure Lambda to use an AWS KMS customer managed customer master key (CMK). Use the CMK to generate a data key and encrypt all data prior to writing to /tmp storage.",isCorrect:!0},{text:"Enable secure connections over HTTPS for the AWS Lambda API endpoints using Transport Layer Security (TLS).",isCorrect:!1}],explanation:'On a perfunction basis, you can configure Lambda to use an encryption key that you create and manage in AWS Key Management Service. These are referred to as customer managed customer master keys (CMKs) or customer managed keys. If you don\'t configure a customer managed key, Lambda uses an AWS managed CMK named aws/lambda, which Lambda creates in your account.The CMK can be used to generate a data encryption key that can be used for encrypting all data uploaded to Lambda or generated by Lambda.CORRECT: "Configure Lambda to use an AWS KMS customer managed customer master key (CMK). Use the CMK to generate a data key and encrypt all data prior to writing to /tmp storage" is the correct answer.INCORRECT: "Attach the Lambda function to a VPC and encrypt Amazon EBS volumes at rest using the AWS managed CMK. Mount the EBS volume to /tmp" is incorrect. You cannot attach an EBS volume to a Lambda function.INCORRECT: "Enable default encryption on an Amazon S3 bucket using an AWS KMS customer managed customer master key (CMK). Mount the S3 bucket to /tmp" is incorrect. You cannot mount an S3 bucket to a Lambda function.INCORRECT: "Enable secure connections over HTTPS for the AWS Lambda API endpoints using Transport Layer Security (TLS)" is incorrect. The Lambda API endpoints are always encrypted using TLS and this is encryption intransit not encryption atrest.References: https://docs.aws.amazon.com/lambda/latest/dg/securitydataprotection.html'},{question:"A development team wants to build an application using serverless architecture. The team plans to use AWS Lambda functions extensively to achieve this goal. The developers of the team work on different programming languages like Python, .NET and Javascript. The team wants to model the cloud infrastructure using any of these programming languages. Which AWS service/tool should the team use for the given usecase?",answers:[{text:"AWS Serverless Application Model (SAM)",isCorrect:!1},{text:"AWS Cloud Development Kit (CDK)",isCorrect:!0},{text:"AWS CloudFormation",isCorrect:!1},{text:"AWS CodeDeploy",isCorrect:!1}],explanation:"Correct option:AWS Cloud Development Kit (CDK) The AWS Cloud Development Kit (AWS CDK) is an opensource software development framework to define your cloud application resources using familiar programming languages.Provisioning cloud applications can be a challenging process that requires you to perform manual actions, write custom scripts, maintain templates, or learn domainspecific languages. AWS CDK uses the familiarity and expressive power of programming languages such as JavaScript/TypeScript, Python, Java, and .NET for modeling your applications. It provides you with highlevel components called constructs that preconfigure cloud resources with proven defaults, so you can build cloud applications without needing to be an expert. AWS CDK provisions your resources in a safe, repeatable manner through AWS CloudFormation. It also enables you to compose and share your own custom constructs that incorporate your organization's requirements, helping you start new projects faster.Incorrect options:AWS CloudFormation When AWS CDK applications are run, they compile down to fully formed CloudFormation JSON/YAML templates that are then submitted to the CloudFormation service for provisioning. Because the AWS CDK leverages CloudFormation, you still enjoy all the benefits CloudFormation provides such as safe deployment, automatic rollback, and drift detection. But, CloudFormation by itself is not sufficient for the current use case.AWS Serverless Application Model (SAM) The AWS Serverless Application Repository is a managed repository for serverless applications. It enables teams, organizations, and individual developers to store and share reusable applications, and easily assemble and deploy serverless architectures in powerful new ways. Using the Serverless Application Repository, you don't need to clone, build, package, or publish source code to AWS before deploying it.AWS Serverless Application Model and AWS CDK both abstract AWS infrastructure as code making it easier for you to define your cloud infrastructure. If you prefer defining your serverless infrastructure in concise declarative templates, SAM is the better fit. If you want to define your AWS infrastructure in a familiar programming language, as is the requirement in the current use case, AWS CDK is the right fit.AWS CodeDeploy AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your onpremises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. CodeDeploy can be used with AWS CDK for deployments.Reference:https://aws.amazon.com/cdk/faqs/"},{question:"A developer has created an Amazon API Gateway with caching enabled in front of AWS Lambda. For some requests, it is necessary to ensure the latest data is received from the endpoint. How can the developer ensure the data is not stale?",answers:[{text:"Send requests with the CacheDelete: maxage=0 header",isCorrect:!1},{text:"Modify the TTL on the cache to a lower number",isCorrect:!1},{text:"The cache must be disabled",isCorrect:!1},{text:"Send requests with the CacheControl: maxage=0 header",isCorrect:!0}],explanation:'You can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The request must contain the CacheControl: maxage=0 header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint.CORRECT: "Send requests with the CacheControl: maxage=0 header" is the correct answer.INCORRECT: "Modify the TTL on the cache to a lower number" is incorrect as that would expire all entries after the TTL expires.The question states that for some requests (not all requests) that latest data must be received, in this case the best way to ensure this is to use invalidate the cache entries using the header in the correct answer.INCORRECT: "The cache must be disables" is incorrect as you can achieve this requirement using invalidation as detailed in the explanation above.INCORRECT: "Send requests with the CacheDelete: maxage=0 header " is incorrect as that is the wrong header to use. The Developer should use the CacheControl: maxage=0 header instead.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaycaching.html'},{question:"A SaaS company runs a HealthCare web application that is used worldwide by users. There have been requests by mobile developers to expose public APIs for the applicationspecific functionality. You decide to make the APIs available to mobile developers as product offerings. Which of the following options will allow you to do that?",answers:[{text:"Use AWS Billing Usage Plans",isCorrect:!1},{text:"Use CloudFront Usage Plans",isCorrect:!1},{text:"Use API Gateway Usage Plans",isCorrect:!0},{text:"Use AWS Lambda Custom Authorizers",isCorrect:!1}],explanation:"Correct option:Use API Gateway Usage PlansAmazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud.How API Gateway Works: via https://aws.amazon.com/apigateway/ A usage plan specifies who can access one or more deployed API stages and methods—and also how much and how fast they can access them. The plan uses API keys to identify API clients and meters access to the associated API stages for each key.You can configure usage plans and API keys to allow customers to access selected APIs at agreedupon request rates and quotas that meet their business requirements and budget constraints.Overview of API Gateway Usage Plans and API keys: via https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayapiusageplans.html Incorrect options:Use AWS Billing Usage Plans AWS Billing and Cost Management is the service that you use to pay your AWS bill, monitor your usage, and analyze and control your costs. There is no such thing as AWS Billing Usage Plans. You cannot use AWS Billing to set up public APIs for the application.Use CloudFront Usage Plans Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developerfriendly environment. There is no such thing as CloudFront Usage Plans. You cannot use CloudFront to set up public APIs for the application.Use AWS Lambda Custom Authorizers Lambda is a separate service than Gateway API, therefore, it cannot be used to determine the API usage limits.Reference:https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayapiusageplans.html"},{question:"A company will be uploading several terabytes of data to Amazon S3. What is the SIMPLEST solution to ensure that the data is encrypted before it is sent to S3 and whilst in transit?",answers:[{text:"Use serverside encryption with S3 managed keys and SSL",isCorrect:!1},{text:"Use serverside encryption with client provided keys",isCorrect:!1},{text:"Use clientside encryption and a hardware VPN to a VPC and an S3 endpoint",isCorrect:!1},{text:"Use clientside encryption with a KMS managed CMK and SSL",isCorrect:!0}],explanation:'Clientside encryption is the act of encrypting data before sending it to Amazon S3. You have the following options:1. Use a customer master key (CMK) stored in AWS Key Management Service (AWS KMS).2. Use a master key you store within your application.Additionally, using HTTPS/SSL to encrypt the data as it is transmitted over the Internet adds an additional layer of protection.CORRECT: "Use clientside encryption with a KMS managed CMK and SSL" is the correct answer.INCORRECT: "Use serverside encryption with client provided keys" is incorrect as this will encrypt the data as it is written to the S3 bucket. The questions states that you need to encrypt date before it is sent to S3.INCORRECT: "Use clientside encryption and a hardware VPN to a VPC and an S3 endpoint" is incorrect. You can configure a hardware VPN to a VPC and configure an S3 endpoint to access S3 privately (rather than across the Internet). However, this is certainly not the simplest solution. Encrypting the data using clientside encryption and then using HTTPS/SSL to transmit the data is operationally easier to configure and manage and provides ample security.INCORRECT: "Use serverside encryption with S3 managed keys and SSL" is incorrect as this does not encrypt the data before it is sent to S3 which is a requirement.References: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html'},{question:"As an AWS Certified Developer Associate, you have configured the AWS CLI on your workstation. Your default region is useast1 and your IAM user has permissions to operate commands on services such as EC2, S3 and RDS in any region. You would like to execute a command to stop an EC2 instance in the useast2 region. What of the following is the MOST optimal solution to address this usecase?",answers:[{text:"Use the region parameter",isCorrect:!0},{text:"You need to override the default region by using aws configure",isCorrect:!1},{text:"You should create a new IAM user just for that other region",isCorrect:!1},{text:"Use boto3 dependency injection",isCorrect:!1}],explanation:"Correct option:Use the region parameter: If the region parameter is not set, then the CLI command is executed against the default AWS region.You can also review all general options for AWS CLI: via https://docs.aws.amazon.com/cli/latest/topic/configvars.html#generaloptionsIncorrect options:You need to override the default region by using aws configure This is not the most optimal way as you will have to change it again to reset the default region.You should create a new IAM user just for that other region This is not the most optimal way as you would need to manage two IAM user profiles.Use boto3 dependency injection With the CLI you do not use boto3. This option is a distractor.Reference:https://docs.aws.amazon.com/cli/latest/topic/configvars.html#generaloptions"},{question:"You have chosen AWS Elastic Beanstalk to upload your application code and allow it to handle details such as provisioning resources and monitoring. When creating configuration files for AWS Elastic Beanstalk which naming convention should you follow?",answers:[{text:".config/.ebextensions",isCorrect:!1},{text:".config_.ebextensions",isCorrect:!1},{text:".ebextensions/.config",isCorrect:!0},{text:".ebextensions_.config",isCorrect:!1}],explanation:"Correct option:.ebextensions/.config : You can add AWS Elastic Beanstalk configuration files (.ebextensions) to your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML or JSON formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle.via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html Incorrect options:.ebextensions_.config.config/.ebextensions.config_.ebextensionsThese three options contradict the explanation provided earlier. So these are incorrect.Reference: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html"},{question:"CodeCommit is a managed version control service that hosts private Git repositories in the AWS cloud. Which of the following credential types is NOT supported by IAM for CodeCommit?",answers:[{text:"SSH Keys",isCorrect:!1},{text:"AWS Access Keys",isCorrect:!1},{text:"Git credentials",isCorrect:!1},{text:"IAM username and password",isCorrect:!0}],explanation:"Correct option:IAM username and password IAM username and password credentials cannot be used to access CodeCommit.Incorrect options:Git credentials These are IAM generated user name and password pair you can use to communicate with CodeCommit repositories over HTTPS.SSH Keys Are locally generated publicprivate key pair that you can associate with your IAM user to communicate with CodeCommit repositories over SSH.AWS access keys You can use these keys with the credential helper included with the AWS CLI to communicate with CodeCommit repositories over HTTPS.Reference:https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_sshkeys.html"},{question:"A company uses Elastic Beanstalk to manage its IT infrastructure on AWS Cloud and it would like to deploy the new application version to the EC2 instances. When the deployment is executed, some instances should serve requests with the old application version, while other instances should serve requests using the new application version until the deployment is completed. Which deployment meets this requirement without incurring additional costs?",answers:[{text:"All at once",isCorrect:!1},{text:"Immutable",isCorrect:!1},{text:"Rolling",isCorrect:!0},{text:"Rolling with additional batches",isCorrect:!1}],explanation:"Correct option:RollingWith Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.How Elastic BeanStalk Works: via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html The rolling deployment policy deploys the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch. The cost remains the same as the number of EC2 instances does not increase. This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time.Overview of Elastic Beanstalk Deployment Policies: via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html Incorrect options:Immutable The 'Immutable' deployment policy ensures that your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails.All at once This policy deploys the new version to all instances simultaneously. Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time.Rolling with additional batches This policy deploys the new version in batches, but first launches a new batch of instances to ensure full capacity during the deployment process. This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. These increase the costs as you're adding extra instances during the deployment.Reference:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html"},{question:"A Development team would use a GitHub repository and would like to migrate their application code to AWS CodeCommit. What needs to be created before they can migrate a cloned repository to CodeCommit over HTTPS?",answers:[{text:"A public and private SSH key file",isCorrect:!1},{text:"A set of Git credentials generated with IAM",isCorrect:!0},{text:"An Amazon EC2 IAM role with CodeCommit permissions",isCorrect:!1},{text:"A GitHub secure authentication token",isCorrect:!1}],explanation:'AWS CodeCommit is a managed version control service that hosts private Git repositories in the AWS cloud. To use CodeCommit, you configure your Git client to communicate with CodeCommit repositories. As part of this configuration, you provide IAM credentials that CodeCommit can use to authenticate you. IAM supports CodeCommit with three types ofcredentials:• Git credentials, an IAM generated user name and password pair you can use to communicate with CodeCommit repositories over HTTPS.• SSH keys, a locally generated publicprivate key pair that you can associate with your IAM user to communicate with CodeCommit repositories over SSH.• AWS access keys, which you can use with the credential helper included with the AWS CLI to communicate with CodeCommit repositories over HTTPS.In this scenario the Development team need to connect to CodeCommit using HTTPS so they need either AWS access keys to use the AWS CLI or Git credentials generated by IAM. Access keys are not offered as an answer choice so the best answer is that they need to create a set of Git credentials generated with IAMCORRECT: "A set of Git credentials generated with IAM" is the correct answer.INCORRECT: "A GitHub secure authentication token" is incorrect as they need to authenticate to AWS CodeCommit, not GitHub(they have already accessed and cloned the repository).INCORRECT: "A public and private SSH key file" is incorrect as these are used to communicate with CodeCommit repositories using SSH, not HTTPS.INCORRECT: "An Amazon EC2 IAM role with CodeCommit permissions" is incorrect as you need the Git credentials generated through IAM to connect to CodeCommit.References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_sshkeys.html'},{question:"A company has an application that logs all information to Amazon S3. Whenever there is a new log file, an AWS Lambda function is invoked to process the log files. The code works, gathering all of the necessary information. However, when checking the Lambda function logs, duplicate entries with the same request ID are found. What is the BEST explanation for the duplicate entries?",answers:[{text:"The Lambda function failed, and the Lambda service retried the invocation with a delay",isCorrect:!0},{text:"There was an S3 outage, which caused duplicate entries of the same log file",isCorrect:!1},{text:"The application stopped intermittently and then resumed",isCorrect:!1},{text:"The S3 bucket name was specified incorrectly",isCorrect:!1}],explanation:'From the AWS documentation:“When an error occurs, your function may be invoked multiple times. Retry behavior varies by error type, client, event source, and invocation type. For example, if you invoke a function asynchronously and it returns an error, Lambda executes the function up to two more times. For more information, see Retry Behavior.For asynchronous invocation, Lambda adds events to a queue before sending them to your function. If your function does not have enough capacity to keep up with the queue, events may be lost. Occasionally, your function may receive the same event multiple times, even if no error occurs. To retain events that were not processed, configure your function with a deadletterqueue.”Therefore, the most likely explanation is that the function failed, and Lambda retried the invocation.CORRECT: "The Lambda function failed, and the Lambda service retried the invocation with a delay" is the correct answer.INCORRECT: "The S3 bucket name was specified incorrectly" is incorrect. If this was the case all attempts would fail but this is not the case.INCORRECT: "There was an S3 outage, which caused duplicate entries of the same log file" is incorrect. There cannot be duplicate log files in Amazon S3 as every object must be unique within a bucket. Therefore, if the same log file was uploaded twice it would just overwrite the previous version of the file. Also, if a separate request was made to Lambda it would have adifferent request ID.INCORRECT: "The application stopped intermittently and then resumed" is incorrect. The issue is duplicate entries of the same request ID.References: https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html'},{question:"An application requires an inmemory caching engine. The cache should provide high availability as repopulating data is expensive. How can this requirement be met?",answers:[{text:"Amazon Aurora with a Global Database",isCorrect:!1},{text:"Use Amazon ElastiCache Memcached with partitions",isCorrect:!1},{text:"Amazon RDS with a Read Replica",isCorrect:!1},{text:"Use Amazon ElastiCache Redis with replicas",isCorrect:!0}],explanation:'Singlenode Amazon ElastiCache Redis clusters are inmemory entities with limited data protection services (AOF). If your cluster fails for any reason, you lose all the cluster\'s data.However, if you\'re running the Redis engine, you can group 2 to 6 nodes into a cluster with replicas where 1 to 5 readonly nodes contain replicate data of the group\'s single read/write primary node.In this scenario, if one node fails for any reason, you do not lose all your data since it is replicated in one or more other nodes.Due to replication latency, some data may be lost if it is the primary read/write node that fails.Therefore, the best solution is to use ElastiCache Redis with replicas.CORRECT: "Use Amazon ElastiCache Redis with replicas" is the correct answer.INCORRECT: "Use Amazon ElastiCache Memcached with partitions" is incorrect as partitions are not copies of data so if you lose a partition you lose the data contained within it (no high availability).INCORRECT: "Amazon RDS with a Read Replica" is incorrect as this is not an inmemory database and is readonly.INCORRECT: "Amazon Aurora with a Global Database" is incorrect as this is not an inmemory database and this configuration is for scaling a database globally.References: https://docs.aws.amazon.com/AmazonElastiCache/latest/redug/Replication.html'},{question:"You are creating a Cloud Formation template to deploy your CMS application running on an EC2 instance within your AWS account. Since the application will be deployed across multiple regions, you need to create a map of all the possible values for the base AMI. How will you invoke the !FindInMap function to fulfill this use case?",answers:[{text:"!FindInMap [ MapName, TopLevelKey, SecondLevelKey ]",isCorrect:!0},{text:"!FindInMap [ MapName ]",isCorrect:!1},{text:"!FindInMap [ MapName, TopLevelKey ]",isCorrect:!1},{text:"!FindInMap [ MapName, TopLevelKey, SecondLevelKey, ThirdLevelKey ]",isCorrect:!1}],explanation:'Correct option:!FindInMap [ MapName, TopLevelKey, SecondLevelKey ] The intrinsic function Fn::FindInMap returns the value corresponding to keys in a twolevel map that is declared in the Mappings section. YAML Syntax for the full function name: Fn::FindInMap: [ MapName, TopLevelKey, SecondLevelKey ]Short form of the above syntax is : !FindInMap [ MapName, TopLevelKey, SecondLevelKey ]Where,MapName Is the logical name of a mapping declared in the Mappings section that contains the keys and values. TopLevelKey The toplevel key name. Its value is a list of keyvalue pairs. SecondLevelKey The secondlevel key name, which is set to one of the keys from the list assigned to TopLevelKey.Consider the following YAML template:Mappings: RegionMap: useast1: HVM64: "ami0ff8a91507f77f867" HVMG2: "ami0a584ac55a7631c0c" uswest1: HVM64: "ami0bdb828fd58c52235" HVMG2: "ami066ee5fd4a9ef77f1" euwest1: HVM64: "ami047bb4163c506cd98" HVMG2: "ami31c2f645" apsoutheast1: HVM64: "ami08569b978cc4dfa10" HVMG2: "ami0be9df32ae9f92309" apnortheast1: HVM64: "ami06cd52961ce9f0d85" HVMG2: "ami053cdd503598e4a9d"Resources: myEC2Instance: Type: "AWS::EC2::Instance" Properties: ImageId: !FindInMap RegionMap !Ref \'AWS::Region\' HVM64 InstanceType: m1.smallThe example template contains an AWS::EC2::Instance resource whose ImageId property is set by the FindInMap function.MapName is set to the map of interest, "RegionMap" in this example. TopLevelKey is set to the region where the stack is created, which is determined by using the "AWS::Region" pseudo parameter. SecondLevelKey is set to the desired architecture, "HVM64" for this example.FindInMap returns the AMI assigned to FindInMap. For a HVM64 instance in useast1, FindInMap would return "ami0ff8a91507f77f867".Incorrect options:!FindInMap [ MapName, TopLevelKey ]!FindInMap [ MapName ]!FindInMap [ MapName, TopLevelKey, SecondLevelKey, ThirdLevelKey ]These three options contradict the explanation provided above, hence these options are incorrect.Reference:https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsicfunctionreferencefindinmap.html'},{question:"A Developer is deploying an application in a microservices architecture on Amazon ECS. The Developer needs to choose the best task placement strategy to MINIMIZE the number of instances that are used. Which task placement strategy should be used?",answers:[{text:"weighted",isCorrect:!1},{text:"spread",isCorrect:!1},{text:"random",isCorrect:!1},{text:"binpack",isCorrect:!0}],explanation:'A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service.Amazon ECS supports the following task placement strategies:binpack Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.random Place tasks randomly.spread Place tasks evenly based on the specified value. Accepted values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availabilityzone. Service tasks are spread based on the tasks from that service. Standalone tasks are spread based on the tasks from the same task group.The binpack task placement strategy is the most suitable for this scenario as it minimizes the number of instances used which is a requirement for this solution.CORRECT: "binpack" is the correct answer.INCORRECT: "random" is incorrect as this would assign tasks randomly to EC2 instances which would not result in minimizing the number of instances used.INCORRECT: "spread" is incorrect as this would spread the tasks based on a specified value. This is not used for minimizing the number of instances used.INCORRECT: "weighted" is incorrect as this is not an ECS task placement strategy. Weighted is associated with Amazon Route 53 routing policies.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskplacementstrategies.html'},{question:"A Developer has recently created an application that uses an AWS Lambda function, an Amazon DynamoDB table, and also sends notifications using Amazon SNS. The application is not working as expected and the Developer needs to analyze what is happening across all components of the application. What is the BEST way to analyze the issue?",answers:[{text:"Enable XRay tracing for the Lambda function",isCorrect:!0},{text:"Create an Amazon CloudWatch Events rule",isCorrect:!1},{text:"Assess the application with Amazon Inspector",isCorrect:!1},{text:"Monitor the application with AWS Trusted Advisor",isCorrect:!1}],explanation:'AWS XRay makes it easy for developers to analyze the behavior of their production, distributed applications with endtoend tracing capabilities. You can use XRay to identify performance bottlenecks, edge case errors, and other hard to detect issues.AWS XRay provides an endtoend, crossservice view of requests made to your application. It gives you an applicationcentric view of requests flowing through your application by aggregating the data gathered from individual services in your application into a single unit called a trace. You can use this trace to follow the path of an individual request as it passes through each service or tier in your application so that you can pinpoint where issues are occurring.AWS XRay will assist the developer with visually analyzing the endtoend view of connectivity between the application components and how they are performing using a Service Map. XRay also provides aggregated data about the application.CORRECT: "Enable XRay tracing for the Lambda function" is the correct answer.INCORRECT: "Create an Amazon CloudWatch Events rule" is incorrect as this feature of CloudWatch is used to trigger actions based on changes in the state of AWS services.INCORRECT: "Assess the application with Amazon Inspector" is incorrect. Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS.INCORRECT: "Monitor the application with AWS Trusted Advisor" is incorrect. AWS Trusted Advisor is an online tool that provides you real time guidance to help you provision your resources following AWS best practices.References: https://aws.amazon.com/xray/features/'},{question:"A company runs many microservices applications that use Docker containers. The company are planning to migrate the containers to Amazon ECS. The workloads are highly variable and therefore the company prefers to be charged per running task. Which solution is the BEST fit for the company's requirements?",answers:[{text:"An Amazon ECS Service with Auto Scaling",isCorrect:!1},{text:"Amazon ECS with the Fargate launch type",isCorrect:!0},{text:"Amazon ECS with the EC2 launch type",isCorrect:!1},{text:"An Amazon ECS Cluster with Auto Scaling",isCorrect:!1}],explanation:'The key requirement is that the company should be charged per running task. Therefore, the best answer is to use Amazon ECS with the Fargate launch type as with this model AWS charge you for running tasks rather than running container instances.The Fargate launch type allows you to run your containerized applications without the need to provision and manage the backend infrastructure. You just register your task definition and Fargate launches the container for you. The Fargate Launch Type is a serverless infrastructure managed by AWS.CORRECT: "Amazon ECS with the Fargate launch type" is the correct answer.INCORRECT: "Amazon ECS with the EC2 launch type" is incorrect as with this launch type you pay for running container instances (EC2 instances).INCORRECT: "An Amazon ECS Service with Auto Scaling" is incorrect as this does not specify the launch type. You can run an ECS Service on the Fargate or EC2 launch types.INCORRECT: "An Amazon ECS Cluster with Auto Scaling" is incorrect as this does not specify the launch type. You can run an ECS Cluster on the Fargate or EC2 launch types.References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html'},{question:"After a test deployment in ElasticBeanstalk environment, a developer noticed that all accumulated Amazon EC2 burst balances were lost. Which of the following options can lead to this behavior?",answers:[{text:"When a canary deployment fails, it resets the EC2 burst balances to zero",isCorrect:!1},{text:"The deployment was either run with immutable updates or in traffic splitting mode",isCorrect:!0},{text:"The deployment was run as a Rolling deployment, resulting in the resetting of EC2 burst balances",isCorrect:!1},{text:"The deployment was run as a Allatonce deployment, flushing all the accumulated EC2 burst balances",isCorrect:!1}],explanation:"Correct option:The deployment was either run with immutable updates or in traffic splitting mode Immutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments.Trafficsplitting deployments let you perform canary testing as part of your application deployment. In a trafficsplitting deployment, Elastic Beanstalk launches a full set of new instances just like during an immutable deployment. It then forwards a specified percentage of incoming client traffic to the new application version for a specified evaluation period.Some policies replace all instances during the deployment or update. This causes all accumulated Amazon EC2 burst balances to be lost. It happens in the following cases:Managed platform updates with instance replacement enabledImmutable updatesDeployments with immutable updates or traffic splitting enabledIncorrect options:The deployment was run as a Rolling deployment, resulting in the resetting of EC2 burst balances With rolling deployments, Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. Rolling deployments do not result in loss of EC2 burst balances.The deployment was run as a Allatonce deployment, flushing all the accumulated EC2 burst balances The traditional Allatonce deployment, wherein all the instances are updated simultaneously, does not result in loss of EC2 burst balances.When a canary deployment fails, it resets the EC2 burst balances to zero This is incorrect and given only as a distractor.Reference:https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.rollingversiondeploy.html"},{question:"A startup with newly created AWS account is testing different EC2 instances. They have used Burstable performance instance T2.micro for 35 seconds and stopped the instance. At the end of the month, what is the instance usage duration that the company is charged for?",answers:[{text:"0 seconds",isCorrect:!0},{text:"35 seconds",isCorrect:!1},{text:"60 seconds",isCorrect:!1},{text:"30 seconds",isCorrect:!1}],explanation:"Correct option:Burstable performance instances, which are T3, T3a, and T2 instances, are designed to provide a baseline level of CPU performance with the ability to burst to a higher level when required by your workload. Burstable performance instances are the only instance types that use credits for CPU usage.0 seconds AWS states that, if your AWS account is less than 12 months old, you can use a t2.micro instance for free within certain usage limits.Incorrect options:35 seconds60 seconds30 secondsThese three options contradict the explanation provided earlier, so these are incorrect.Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstableperformanceinstances.html"},{question:"A Developer is writing code to run in a cron job on an Amazon EC2 instance that sends status information about the application to Amazon CloudWatch. Which method should the Developer use?",answers:[{text:"Use the AWS CLI putmetricdata command.",isCorrect:!0},{text:"Use the AWS CLI putmetricalarm command.",isCorrect:!1},{text:"Use the CloudWatch console with detailed monitoring.",isCorrect:!1},{text:"Use the unified CloudWatch agent to publish custom metrics.",isCorrect:!1}],explanation:'The putmetricdata command publishes metric data points to Amazon CloudWatch. CloudWatch associates the data points with the specified metric. If the specified metric does not exist, CloudWatch creates the metric.CORRECT: "Use the AWS CLI putmetricdata command" is the correct answer.INCORRECT: "Use the AWS CLI putmetricalarm command" is incorrect. This command creates or updates an alarm and associates it with the specified metric, metric math expression, or anomaly detection model.INCORRECT: "Use the unified CloudWatch agent to publish custom metrics" is incorrect. It is not necessary to use the unified CloudWatch agent. In this case the Developer can use the AWS CLI with the cron job.INCORRECT: "Use the CloudWatch console with detailed monitoring" is incorrect. You cannot collect custom metric data using the CloudWatch console with detailed monitoring. Detailed monitoring sends data at 1minute rather than 5minute frequencies but will not collect custom data.References: https://docs.aws.amazon.com/cli/latest/reference/cloudwatch/putmetricdata.html'},{question:"A developer is troubleshooting problems with a Lambda function that is invoked by Amazon SNS and repeatedly fails. How can the developer save discarded events for further processing?",answers:[{text:"Configure a Dead Letter Queue (DLQ)",isCorrect:!0},{text:"Enable CloudWatch Logs for the Lambda function",isCorrect:!1},{text:"Enable SNS notifications for failed events",isCorrect:!1},{text:"Enable Lambda streams",isCorrect:!1}],explanation:'You can configure a dead letter queue (DLQ) on AWS Lambda to give you more control over message handling for all asynchronous invocations, including those delivered via AWS events (S3, SNS, IoT, etc.).A deadletter queue saves discarded events for further processing. A deadletter queue acts the same as an onfailure destination in that it is used when an event fails all processing attempts or expires without being processed.However, a deadletter queue is part of a function\'s versionspecific configuration, so it is locked in when you publish a version. Onfailure destinations also support additional targets and include details about the function\'s response in the invocation record.You can setup a DLQ by configuring the \'DeadLetterConfig\' property when creating or updating your Lambda function. You can provide an SQS queue or an SNS topic as the \'TargetArn\' for your DLQ, and AWS Lambda will write the event object invoking the Lambda function to this endpoint after the standard retry policy (2 additional retries on failure) is exhausted.CORRECT: "Configure a Dead Letter Queue (DLQ)" is the correct answer.INCORRECT: "Enable CloudWatch Logs for the Lambda function" is incorrect as CloudWatch logs will record metrics about the function but will not record records of the discarded events.INCORRECT: "Enable Lambda streams" is incorrect as this is not something that exists (DynamoDB streams does exist).INCORRECT: "Enable SNS notifications for failed events" is incorrect. Sending notifications from SNS will not include the data required for troubleshooting. A DLQ is the correct solution.References: https://digitalcloud.training/certificationtraining/awsdeveloperassociate/awscompute/awslambda/'},{question:"As a developer, you are working on creating an application using AWS Cloud Development Kit (CDK). Which of the following represents the correct order of steps to be followed for creating an app using AWS CDK?",answers:[{text:"Create the app from a template provided by AWS CloudFormation > Add code to the app to create resources within stacks > Build the app (optional) > Synthesize one or more stacks in the app > Deploy stack(s) to your AWS account",isCorrect:!1},{text:"Create the app from a template provided by AWS CDK > Add code to the app to create resources within stacks > Synthesize one or more stacks in the app > Deploy stack(s) to your AWS account > Build the app",isCorrect:!1},{text:"Create the app from a template provided by AWS CDK > Add code to the app to create resources within stacks > Build the app (optional) > Synthesize one or more stacks in the app > Deploy stack(s) to your AWS account",isCorrect:!0},{text:"Create the app from a template provided by AWS CloudFormation > Add code to the app to create resources within stacks > Synthesize one or more stacks in the app > Deploy stack(s) to your AWS account > Build the app",isCorrect:!1}],explanation:"Correct option:Create the app from a template provided by AWS CDK > Add code to the app to create resources within stacks > Build the app (optional) > Synthesize one or more stacks in the app > Deploy stack(s) to your AWS accountThe standard AWS CDK development workflow is similar to the workflow you're already familiar as a developer. There are a few extra steps:Create the app from a template provided by AWS CDK Each AWS CDK app should be in its own directory, with its own local module dependencies. Create a new directory for your app. Now initialize the app using the cdk init command, specifying the desired template (\"app\") and programming language. The cdk init command creates a number of files and folders inside the created home directory to help you organize the source code for your AWS CDK app.Add code to the app to create resources within stacks Add custom code as is needed for your application.Build the app (optional) In most programming environments, after making changes to your code, you'd build (compile) it. This isn't strictly necessary with the AWS CDK—the Toolkit does it for you so you can't forget. But you can still build manually whenever you want to catch syntax and type errors.Synthesize one or more stacks in the app to create an AWS CloudFormation template Synthesize one or more stacks in the app to create an AWS CloudFormation template. The synthesis step catches logical errors in defining your AWS resources. If your app contains more than one stack, you'd need to specify which stack(s) to synthesize.Deploy one or more stacks to your AWS account It is optional (though good practice) to synthesize before deploying. The AWS CDK synthesizes your stack before each deployment. If your code has security implications, you'll see a summary of these and need to confirm them before deployment proceeds. cdk deploy is used to deploy the stack using CloudFormation templates. This command displays progress information as your stack is deployed. When it's done, the command prompt reappears.Incorrect options:Create the app from a template provided by AWS CloudFormation > Add code to the app to create resources within stacks > Build the app (optional) > Synthesize one or more stacks in the app > Deploy stack(s) to your AWS accountCreate the app from a template provided by AWS CloudFormation > Add code to the app to create resources within stacks > Synthesize one or more stacks in the app > Deploy stack(s) to your AWS account > Build the appFor both these options, you cannot use AWS CloudFormation to create the app. So these options are incorrect.Create the app from a template provided by AWS CDK > Add code to the app to create resources within stacks > Synthesize one or more stacks in the app > Deploy stack(s) to your AWS account > Build the app You cannot have the build step after deployment. So this option is incorrect.Reference:https://docs.aws.amazon.com/cdk/latest/guide/hello_world.html"},{question:"A company runs multiple microservices that each use their own Amazon DynamoDB table. The “customers” microservice needs data that originates in the “orders” microservice. What approach represents the SIMPLEST method for the “customers” table to get near real-time updates from the “orders” table?",answers:[{text:"Use Amazon Kinesis Firehose to deliver all changes in the “orders” table to the “customers” table",isCorrect:!1},{text:"Enable Amazon DynamoDB streams on the “orders” table, configure the “customers” microservice to read records from the stream",isCorrect:!0},{text:"Use Amazon CloudWatch Events to send notifications every time an item is added or modified in the “orders” table",isCorrect:!1},{text:"Enable DynamoDB streams for the “customers” table, trigger an AWS Lambda function to read records from the stream and write them to the “orders” table",isCorrect:!1}],explanation:'DynamoDB Streams captures a timeordered sequence of itemlevel modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in nearreal time.Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the "before" and "after" images of modified items.For this scenario, we can enable a DynamoDB stream on the “orders” table and the configure the “customers” microservice to read records from the stream and then write those records, or relevant attributes of those records, to the “customers’ table.CORRECT: "Enable Amazon DynamoDB streams on the “orders” table, configure the “customers” microservice to read records from the stream" is the correct answer.INCORRECT: "Enable DynamoDB streams for the “customers” table, trigger an AWS Lambda function to read records from the stream and write them to the “orders” table" is incorrect. This could be a good solution if it wasn’t backward. We can trigger a Lambda function to then process the records from the stream. However, we should be enabling the stream on the “orders” table, not the “customers” table, and then writing the records to the “customers” table, not the “orders” table.INCORRECT: "Use Amazon CloudWatch Events to send notifications every time an item is added or modified in the “orders” table" is incorrect. CloudWatch Events is used to respond to changes in the state of specific AWS services. It does not support DynamoDB.INCORRECT: "Use Amazon Kinesis Firehose to deliver all changes in the “orders” table to the “customers” table" is incorrect. Kinesis Firehose cannot be configured to ingest data from a DynamoDB table, nor is DynamoDB a supported destination.References:https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html'},{question:"A Developer is writing an imaging microservice on AWS Lambda. The service is dependent on several libraries that are not available in the Lambda runtime environment. Which strategy should the Developer follow to create the Lambda deployment package?",answers:[{text:"Create a ZIP file with the source code and all dependent libraries",isCorrect:!0},{text:"Create a ZIP file with the source code and a script that installs the dependent libraries at runtime",isCorrect:!1},{text:"Create a ZIP file with the source code and a buildspec.yml file that installs the dependent libraries on AWS Lambda",isCorrect:!1},{text:"Create a ZIP file with the source code and an appspec.yml file. Add the libraries to the appspec.yml file and upload to Amazon S3. Deploy using CloudFormation",isCorrect:!1}],explanation:'A deployment package is a ZIP archive that contains your function code and dependencies. You need to create a deployment package if you use the Lambda API to manage functions, or if you need to include libraries and dependencies other than the AWS SDK.You can upload the package directly to Lambda, or you can use an Amazon S3 bucket, and then upload it to Lambda. If the deployment package is larger than 50 MB, you must use Amazon S3.CORRECT: "Create a ZIP file with the source code and all dependent libraries" is the correct answer.INCORRECT: "Create a ZIP file with the source code and a script that installs the dependent libraries at runtime" is incorrect as the Developer should not run a script at runtime as this will cause latency. Instead, the Developer should include the dependent libraries in the ZIP package.INCORRECT: "Create a ZIP file with the source code and an appspec.yml file. Add the libraries to the appspec.yml file and upload to Amazon S3. Deploy using CloudFormation" is incorrect. The appspec.yml file is used with CodeDeploy, you cannot add libraries into it, and it is not deployed using CloudFormation.INCORRECT: "Create a ZIP file with the source code and a buildspec.yml file that installs the dependent libraries on AWS Lambda" is incorrect as the buildspec.yml file is used with CodeBuild for compiling source code and running tests. It cannot be used to install dependent libraries within Lambda.References: https://docs.aws.amazon.com/lambda/latest/dg/pythonpackage.html'},{question:"The Technical Lead of your team has reviewed a CloudFormation YAML template written by a new recruit and specified that an invalid section has been added to the template. Which of the following represents an invalid section of the CloudFormation template?",answers:[{text:"Conditions' section of the template",isCorrect:!1},{text:"Dependencies' section of the template",isCorrect:!0},{text:"Parameters' section of the template",isCorrect:!1},{text:"Resources' section of the template",isCorrect:!1}],explanation:"Correct option:Templates include several major sections. The Resources section is the only required section. Sample CloudFormation YAML template: via https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/templateanatomy.html 'Dependencies' section of the template As you can see, there is no section called 'Dependencies' in the template. Although dependencies can be mentioned, there is no section itself for dependencies.Incorrect options:'Conditions' section of the template This optional section includes conditions that control whether certain resources are created or whether certain resource properties are assigned a value during stack creation or update. For example, you could conditionally create a resource that depends on whether the stack is for a production or test environment.'Resources' section of the template This is the only required section and specifies the stack resources and their properties, such as an Amazon Elastic Compute Cloud instance or an Amazon Simple Storage Service bucket. You can refer to resources in the Resources and Outputs sections of the template.'Parameters' section of the template This optional section is helpful in passing Values to your template at runtime (when you create or update a stack). You can refer to parameters from the Resources and Outputs sections of the template.Reference: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/templateanatomy.html"},{question:"An application uses AWS Lambda which makes remote calls to several downstream services. A developer wishes to add data to custom subsegments in AWS XRay that can be used with filter expressions. Which type of data should be used?",answers:[{text:"Annotations",isCorrect:!0},{text:"Trace ID",isCorrect:!1},{text:"Daemon",isCorrect:!1},{text:"Metadata",isCorrect:!1}],explanation:'AWS XRay helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With XRay, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. XRay provides an endtoend view of requests as they travel through your application and shows a map of your application’s underlying components.You can record additional information about requests, the environment, or your application with annotations and metadata.You can add annotations and metadata to the segments that the XRay SDK creates, or to custom subsegments that you create.Annotations are keyvalue pairs with string, number, or Boolean values. Annotations are indexed for use with filter expressions.Use annotations to record data that you want to use to group traces in the console, or when calling the GetTraceSummaries API.CORRECT: "Annotations" is the correct answer.INCORRECT: "Metadata" is incorrect. Metadata are keyvalue pairs that can have values of any type, including objects and lists, but are not indexed for use with filter expressions. Use metadata to record additional data that you want stored in the trace butdon\'t need to use with search.INCORRECT: "Trace ID" is incorrect. An XRay trace ID is used to group a set of data points in AWS XRay.INCORRECT: "Daemon" is incorrect as this is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS XRay API.References: https://docs.aws.amazon.com/xray/latest/devguide/xraysdkjavasegment.html'},{question:"A company is using Amazon API Gateway to manage access to a set of microservices implemented as AWS Lambda functions. The company has made some minor changes to one of the APIs. The company wishes to give existing customers using the API up to 6 months to migrate from version 1 to version 2. What approach should a Developer use to implement the change?",answers:[{text:"Use API Gateway to deploy a new stage named v2 to the API and provide users with its URL",isCorrect:!0},{text:"Use API Gateway to automatically propagate the change to clients, specifying 180 days in the phased deployment parameter",isCorrect:!1},{text:"Update the underlying Lambda function and provide clients with the new Lambda invocation URL",isCorrect:!1},{text:"Update the underlying Lambda function, create an Amazon CloudFront distribution with the updated Lambda function as its origin",isCorrect:!1}],explanation:'Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services as well as data stored in the AWS Cloud.A stage is a named reference to a deployment, which is a snapshot of the API. You use a Stage to manage and optimize a particular deployment. For example, you can set up stage settings to enable caching, customize request throttling, configure logging, define stage variables or attach a canary release for testing.You deploy your API to a stage and it is given a unique URL that contains the stage name. This URL can be used to direct customers to your URL based on the stage (or version) you’d like them to use.Therefore, the best approach is to use API Gateway to deploy a new stage named v2 to the API and provide users with its URL.CORRECT: "Use API Gateway to deploy a new stage named v2 to the API and provide users with its URL" is the correct answer.INCORRECT: "Update the underlying Lambda function and provide clients with the new Lambda invocation URL" is incorrect as the API has been updated, not the Lambda function. We deploy API updates to stages, so we need to deploy a new stage.INCORRECT: "Use API Gateway to automatically propagate the change to clients, specifying 180 days in the phased deployment parameter" is incorrect as this is not a valid method of migrating users from one stage in API Gateway to another.INCORRECT: "Update the underlying Lambda function, create an Amazon CloudFront distribution with the updated Lambda function as its origin" is incorrect as the API has been updated, not the Lambda function.References: https://docs.aws.amazon.com/apigateway/latest/developerguide/setupstages.html'}]},{id:"aws-developer-6",title:"AWS Developer Associate (DVA-C02) Practice Test Latest 2025 6",description:"Additional practice questions covering AWS development topics.",questions:[{question:"A company's ecommerce website is expecting hundreds of thousands of visitors on Black Friday. The marketing department is concerned that high volumes of orders might stress SQS leading to message failures. The company has approached you for the steps to be taken as a precautionary measure against the high volumes. What step will you suggest as a Developer Associate?",answers:[{text:"Preconfigure the SQS queue to increase the capacity when messages hit a certain threshold",isCorrect:!1},{text:"Convert the queue into FIFO ordered queue, since messages to the down system will be processed faster once they are ordered",isCorrect:!1},{text:"Enable autoscaling in the SQS queue",isCorrect:!1},{text:"Amazon SQS is highly scalable and does not need any intervention to handle the expected high volumes",isCorrect:!0}],explanation:"Correct option:Amazon SQS is highly scalable and does not need any intervention to handle the expected high volumesAmazon SQS leverages the AWS cloud to dynamically scale, based on demand. SQS scales elastically with your application so you don't have to worry about capacity planning and preprovisioning. For most standard queues (depending on queue traffic and message backlog), there can be a maximum of approximately 120,000 inflight messages (received from a queue by a consumer, but not yet deleted from the queue).Info on Queue Quotas: via https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsquotas.html Incorrect options:Preconfigure the SQS queue to increase the capacity when messages hit a certain threshold This is an incorrect statement. Amazon SQS scales dynamically, automatically provisioning the needed capacity.Enable autoscaling in the SQS queue SQS queues are, by definition, autoscalable and do not need any configuration changes for autoscaling.Convert the queue into FIFO ordered queue, since messages to the down system will be processed faster once they are ordered This is a wrong statement. You cannot convert an existing standard queue to FIFO queue. To make the move, you must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue.Standard to FIFO queue conversion: via https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFOqueues.htmlReferences:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsquotas.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFOqueues.html"},{question:"You have been hired at a company that needs an experienced developer to help with a continuous integration/continuous delivery (CI/CD) workflow on AWS. You configure the company's workflow to run an AWS CodePipeline pipeline whenever the application's source code changes in a repository hosted in AWS Code Commit and compiles source code with AWS Code Build. You are configuring ProjectArtifacts in your build stage. Which of the following should you do?",answers:[{text:"Contact AWS Support to allow AWS CodePipeline to manage build outputs",isCorrect:!1},{text:"Give AWS CodeBuild permissions to upload the build output to your Amazon S3 bucket",isCorrect:!0},{text:"Give AWS CodeCommit permissions to upload the build output to your Amazon S3 bucket",isCorrect:!1},{text:"Configure AWS CodeBuild to store output artifacts on EC2 servers",isCorrect:!1}],explanation:"Correct option:Give AWS CodeBuild permissions to upload the build output to your Amazon S3 bucketIf you choose ProjectArtifacts and your value type is S3 then the build project stores build output in Amazon Simple Storage Service (Amazon S3). For that, you will need to give AWS CodeBuild permissions to upload.Incorrect options:Configure AWS CodeBuild to store output artifacts on EC2 servers EC2 servers are not a valid output location, so this option is ruled out.Give AWS CodeCommit permissions to upload the build output to your Amazon S3 bucket AWS CodeCommit is the repository that holds source code and has no control over compiling the source code, so this option is incorrect.Contact AWS Support to allow AWS CodePipeline to manage build outputs You can set AWS CodePipeline to manage its build output locations instead of AWS CodeBuild. There is no need to contact AWS Support.Reference: https://docs.aws.amazon.com/codebuild/latest/userguide/createproject.html#createprojectcli"},{question:"A media company uses Amazon Simple Queue Service (SQS) queue to manage their transactions. With changing business needs, the payload size of the messages is increasing. The Team Lead of the project is worried about the 256 KB message size limit that SQS has. What can be done to make the queue accept messages of a larger size?",answers:[{text:"Use the MultiPart API",isCorrect:!1},{text:"Use gzip compression",isCorrect:!1},{text:"Get a service limit increase from AWS",isCorrect:!1},{text:"Use the SQS Extended Client",isCorrect:!0}],explanation:"Correct option:Use the SQS Extended Client To manage large Amazon Simple Queue Service (Amazon SQS) messages, you can use Amazon Simple Storage Service (Amazon S3) and the Amazon SQS Extended Client Library for Java. This is especially useful for storing and consuming messages up to 2 GB. Unless your application requires repeatedly creating queues and leaving them inactive or storing large amounts of data in your queues, consider using Amazon S3 for storing your data.Incorrect options:Use the MultiPart API This is an incorrect statement. There is no multipart API for Amazon Simple Queue Service.Get a service limit increase from AWS While it is possible to get service limits extended for certain AWS services, AWS already offers Extended Client to deal with queues that have larger messages.Use gzip compression You can compress the messages before sending them to the queue. The messages also need to be encoded after this to cater to SQS message standards. This adds bulk to the messages and will not be an optimal solution for the current scenario.Reference: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqss3messages.html"},{question:"While troubleshooting, a developer realized that the Amazon EC2 instance is unable to connect to the Internet using the Internet Gateway. Which conditions should be met for Internet connectivity to be established? (Select two)",answers:[{text:"The route table in the instance’s subnet should have a route to an Internet Gateway",isCorrect:!0},{text:"The instance's subnet is associated with multiple route tables with conflicting configurations",isCorrect:!1},{text:"The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic",isCorrect:!0},{text:"The subnet has been configured to be Public and has no access to the internet",isCorrect:!1},{text:"The instance's subnet is not associated with any route table",isCorrect:!1}],explanation:"Correct options:The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic The network access control lists (ACLs) that are associated with the subnet must have rules to allow inbound and outbound traffic on port 80 (for HTTP traffic) and port 443 (for HTTPs traffic). This is a necessary condition for Internet Gateway connectivityThe route table in the instance’s subnet should have a route to an Internet Gateway A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. The route table in the instance’s subnet should have a route defined to the Internet Gateway.Incorrect options:The instance's subnet is not associated with any route table This is an incorrect statement. A subnet is implicitly associated with the main route table if it is not explicitly associated with a particular route table. So, a subnet is always associated with some route table.The instance's subnet is associated with multiple route tables with conflicting configurations This is an incorrect statement. A subnet can only be associated with one route table at a time.The subnet has been configured to be Public and has no access to internet This is an incorrect statement. Public subnets have access to the internet via Internet Gateway.Reference:https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.htmll"},{question:"What is the run order of the hooks for inplace deployments using CodeDeploy?",answers:[{text:"Application Stop > Before Install > Application Start > ValidateService",isCorrect:!0},{text:"Application Stop > Before Install > ValidateService > Application Start",isCorrect:!1},{text:"Before Install > Application Stop > ValidateService > Application Start",isCorrect:!1},{text:"Before Install > Application Stop > Application Start > ValidateService",isCorrect:!1}],explanation:"Correct option:Application Stop > Before Install > Application Start > ValidateServiceIn CodeDeploy, a deployment is a process of installing content on one or more instances. This content can consist of code, web and configuration files, executables, packages, scripts, and so on. CodeDeploy deploys content that is stored in a source repository, according to the configuration rules you specify.via https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html The content in the 'hooks' section of the AppSpec file varies, depending on the compute platform for your deployment. The 'hooks' section for an EC2/OnPremises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts. The 'hooks' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event.via https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfilestructurehooks.html Incorrect options:Before Install > Application Stop > ValidateService > Application StartApplication Stop > Before Install > ValidateService > Application StartBefore Install > Application Stop > Application Start > ValidateServiceAs explained above, these three options contradict the correct order of hooks, so these are incorrect.References:https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfilestructurehooks.html"},{question:"An application running on EC2 instances processes messages from an SQS queue. However, sometimes the messages are not processed and they end up in errors. These messages need to be isolated for further processing and troubleshooting. Which of the following options will help achieve this?",answers:[{text:"Use DeleteMessage",isCorrect:!1},{text:"Reduce the VisibilityTimeout",isCorrect:!1},{text:"Implement a DeadLetter Queue",isCorrect:!0},{text:"Increase the VisibilityTimeout",isCorrect:!1}],explanation:"Correct option:Implement a DeadLetter Queue Amazon SQS supports deadletter queues, which other queues (source queues) can target for messages that can't be processed (consumed) successfully. Deadletter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. Amazon SQS does not create the deadletter queue automatically. You must first create the queue before using it as a deadletter queue.via https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsdeadletterqueues.html Incorrect options:Increase the VisibilityTimeout When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn't automatically delete the message. Immediately after a message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. Increasing visibility timeout will not help in troubleshooting the messages running into error or isolating them from the rest. Hence this is an incorrect option for the current use case.Use DeleteMessage Deletes the specified message from the specified queue. This will not help understand the reason for error or isolate messages ending with the error.Reduce the VisibilityTimeout As explained above, VisibilityTimeout makes sure that the message is not read by any other consumer while it is being processed by one consumer. By reducing the VisibilityTimeout, more consumers will receive the same failed message. Hence, this is an incorrect option for this use case.References:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsdeadletterqueues.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsvisibilitytimeout.htmll"},{question:"You are designing a highperformance application that requires millions of connections. You have several EC2 instances running Apache2 web servers and the application will require capturing the user's source IP address and source port without the use of XForwardedFor. Which of the following options will meet your needs?",answers:[{text:"Elastic Load Balancer",isCorrect:!1},{text:"Application Load Balancer",isCorrect:!1},{text:"Classic Load Balancer",isCorrect:!1},{text:"Network Load Balancer",isCorrect:!0}],explanation:"Network Load BalancerA Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration. Incoming connections remain unmodified, so application software need not support XForwardedFor.via https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html Incorrect options:Application Load Balancer An Application Load Balancer functions at the application layer, the seventh layer of the Open Systems Interconnection (OSI) model. After the load balancer receives a request, it evaluates the listener rules in priority order to determine which rule to apply and then selects a target from the target group for the rule action.One of many benefits of the Application Load Balancer is its support for pathbased routing. You can configure rules for your listener that forward requests based on the URL in the request. This enables you to structure your application as smaller services, and route requests to the correct service based on the content of the URL. For needs relating to network traffic go with Network Load Balancer.Elastic Load Balancer Elastic Load Balancing is the service itself that offers different types of load balancers.Classic Load Balancer It is a basic load balancer that distributes traffic. If your account was created before 20131204, your account supports EC2Classic instances and you will benefit in using this type of load balancer. The classic load balancer can be used regardless of when your account was created and whether you use EC2Classic or whether your instances are in a VPC but just remember its the basic load balancer AWS offers and not advanced as the others.Reference: https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html"},{question:"You have created a continuous delivery service model with automated steps using AWS CodePipeline. Your pipeline uses your code, maintained in a CodeCommit repository, AWS CodeBuild, and AWS Elastic Beanstalk to automatically deploy your code every time there is a code change. However, the deployment to Elastic Beanstalk is taking a very long time due to resolving dependencies on all of your 100 target EC2 instances. Which of the following actions should you take to improve performance with limited code changes?",answers:[{text:"Create a custom platform for Elastic Beanstalk",isCorrect:!1},{text:"Bundle the dependencies in the source code in CodeCommit",isCorrect:!1},{text:"Store the dependencies in S3, to be used while deploying to Beanstalk",isCorrect:!1},{text:"Bundle the dependencies in the source code during the build stage of CodeBuild",isCorrect:!0}],explanation:"Correct option:Bundle the dependencies in the source code during the build stage of CodeBuildAWS CodeBuild is a fully managed build service. There are no servers to provision and scale, or software to install, configure, and operate.A typical application build process includes phases like preparing the environment, updating the configuration, downloading dependencies, running unit tests, and finally, packaging the built artifact.Downloading dependencies is a critical phase in the build process. These dependent files can range in size from a few KBs to multiple MBs. Because most of the dependent files do not change frequently between builds, you can noticeably reduce your build time by caching dependencies.This will allow the code bundle to be deployed to Elastic Beanstalk to have both the dependencies and the code, hence speeding up the deployment time to Elastic BeanstalkIncorrect options:Bundle the dependencies in the source code in CodeCommit This is not the best practice and could make the CodeCommit repository huge.Store the dependencies in S3, to be used while deploying to Beanstalk This option acts as a distractor. S3 can be used as a storage location for your source code, logs, and other artifacts that are created when you use Elastic Beanstalk. Dependencies are used during the process of building code, not while deploying to Beanstalk.Create a custom platform for Elastic Beanstalk This is a more advanced feature that requires code changes, so does not fit the usecase.Reference:https://aws.amazon.com/blogs/devops/howtoenablecachingforawscodebuild//"},{question:"As a developer, you are looking at creating a custom configuration for Amazon EC2 instances running in an Auto Scaling group. The solution should allow the group to autoscale based on the metric of 'average RAM usage' for your Amazon EC2 instances. Which option provides the best solution?",answers:[{text:"Migrate your application to AWS Lambda",isCorrect:!1},{text:"Enable detailed monitoring for EC2 and ASG to get the RAM usage data and create a CloudWatch Alarm on top of it",isCorrect:!1},{text:"Create a custom alarm for your ASG and make your instances trigger the alarm using PutAlarmData API",isCorrect:!1},{text:"Create a custom metric in CloudWatch and make your instances send data to it using PutMetricData. Then, create an alarm based on this metric",isCorrect:!0}],explanation:"Create a custom metric in CloudWatch and make your instances send data to it using PutMetricData. Then, create an alarm based on this metric You can create a custom CloudWatch metric for your EC2 Linux instance statistics by creating a script through the AWS Command Line Interface (AWS CLI). Then, you can monitor that metric by pushing it to CloudWatch.You can publish your own metrics to CloudWatch using the AWS CLI or an API. Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a highresolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.Highresolution metrics can give you more immediate insight into your application's subminute activity. But, every PutMetricData call for a custom metric is charged, so calling PutMetricData more often on a highresolution metric can lead to higher charges.Incorrect options:Create a custom alarm for your ASG and make your instances trigger the alarm using PutAlarmData API This solution will not work, your instances must be aware of each other's RAM utilization status, to know when the average RAM would be too high to trigger the alarm.Enable detailed monitoring for EC2 and ASG to get the RAM usage data and create a CloudWatch Alarm on top of it By enabling detailed monitoring you define the frequency at which the metric data has to be sent to CloudWatch, from 5 minutes to 1minute frequency window. But, you still need to create and collect the custom metric you wish to track.Migrate your application to AWS Lambda This option has been added as a distractor. You cannot use Lambda for the given usecase.References:https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/asinstancemonitoring.html#CloudWatchAlarm"},{question:"A development team is storing sensitive customer data in S3 that will require encryption at rest. The encryption keys must be rotated at least annually. What is the easiest way to implement a solution for this requirement?",answers:[{text:"Use SSEC with automatic key rotation on an annual basis",isCorrect:!1},{text:"Use AWS KMS with automatic key rotation",isCorrect:!0},{text:"Encrypt the data before sending it to Amazon S3",isCorrect:!1},{text:"Import a custom key into AWS KMS and automate the key rotation on an annual basis by using a Lambda function",isCorrect:!1}],explanation:"Use AWS KMS with automatic key rotation Serverside encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. You have three mutually exclusive options, depending on how you choose to manage the encryption keys: ServerSide Encryption with Amazon S3Managed Keys (SSES3), ServerSide Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSEKMS), ServerSide Encryption with CustomerProvided Keys (SSEC).When you use serverside encryption with AWS KMS (SSEKMS), you can use the default AWS managed CMK, or you can specify a customer managed CMK that you have already created. If you don't specify a customer managed CMK, Amazon S3 automatically creates an AWS managed CMK in your AWS account the first time that you add an object encrypted with SSEKMS to a bucket. By default, Amazon S3 uses this CMK for SSEKMS.You can choose to have AWS KMS automatically rotate CMKs every year, provided that those keys were generated within AWS KMS HSMs.Incorrect options:Encrypt the data before sending it to Amazon S3 The act of encrypting data before sending it to Amazon S3 is called ClientSide encryption. You will have to handle the key generation, maintenance and rotation process. Hence, this is not the right choice here.Import a custom key into AWS KMS and automate the key rotation on an annual basis by using a Lambda function When you import a custom key, you are responsible for maintaining a copy of your imported keys in your key management infrastructure so that you can reimport them at any time. Also, automatic key rotation is not supported for imported keys. Using Lambda functions to rotate keys is a possible solution, but not an optimal one for the current use case.Use SSEC with automatic key rotation on an annual basis With ServerSide Encryption with CustomerProvided Keys (SSEC), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects. The keys are not stored anywhere in Amazon S3. There is no automatic key rotation facility for this option.Reference: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html"},{question:"Your web application reads and writes data to your DynamoDB table. The table is provisioned with 400 Write Capacity Units (WCU's) shared across 4 partitions. One of the partitions receives 250 WCU/second while others receive much less. You receive the error 'ProvisionedThroughputExceededException'. What is the likely cause of this error?",answers:[{text:"You have a hot partition",isCorrect:!0},{text:"Configured IAM policy is wrong",isCorrect:!1},{text:"CloudWatch monitoring is lagging",isCorrect:!1},{text:"Write Capacity Units (WCU’s) are applied across to all your DynamoDB tables and this needs reconfiguration",isCorrect:!1}],explanation:"Correct option:You have a hot partitionIt's not always possible to distribute read and write activity evenly. When data access is imbalanced, a \"hot\" partition can receive a higher volume of read and write traffic compared to other partitions. To better accommodate uneven access patterns, DynamoDB adaptive capacity enables your application to continue reading and writing to hot partitions without being throttled, provided that traffic does not exceed your table’s total provisioned capacity or the partition maximum capacity.ProvisionedThroughputExceededException explained: via https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html Hot partition explained: via https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bppartitionkeydesign.html Incorrect options:CloudWatch monitoring is lagging The error is specific to DynamoDB itself and not to any connected service. CloudWatch is a fully managed service from AWS and does not result in throttling.Configured IAM policy is wrong The error is not associated with authorization but to exceeding something preconfigured value. So, it's clearly not a permissions issue.Writecapacity units (WCU’s) are applied across to all your DynamoDB tables and this needs reconfiguration This statement is incorrect. Read Capacity Units and Write Capacity Units are specific to one table.Reference: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html"},{question:"A development team has created a new IAM user that has s3:putObject permission to write to an S3 bucket. This S3 bucket uses serverside encryption with AWS KMS managed keys (SSEKMS) as the default encryption. Using the access key ID and the secret access key of the IAM user, the application received an access denied error when calling the PutObject API. As a Developer Associate, how would you resolve this issue?",answers:[{text:"Correct the ACL of the S3 bucket to allow the IAM user to upload encrypted objects",isCorrect:!1},{text:"Correct the policy of the IAM user to allow the kms:GenerateDataKey action",isCorrect:!0},{text:"Correct the policy of the IAM user to allow the s3:Encrypt action",isCorrect:!1},{text:"Correct the bucket policy of the S3 bucket to allow the IAM user to upload encrypted objects",isCorrect:!1}],explanation:'Correct option:Correct the policy of the IAM user to allow the kms:GenerateDataKey action You can protect data at rest in Amazon S3 by using three different modes of serverside encryption: SSES3, SSEC, or SSEKMS. SSEKMS requires that AWS manage the data key but you manage the customer master key (CMK) in AWS KMS. You can choose a customer managed CMK or the AWS managed CMK for Amazon S3 in your account. If you choose to encrypt your data using the standard features, AWS KMS and Amazon S3 perform the following actions:Amazon S3 requests a plaintext data key and a copy of the key encrypted under the specified CMK.AWS KMS generates a data key, encrypts it under the CMK, and sends both the plaintext data key and the encrypted data key to Amazon S3.Amazon S3 encrypts the data using the data key and removes the plaintext key from memory as soon as possible after use.Amazon S3 stores the encrypted data key as metadata with the encrypted data.The error message indicates that your IAM user or role needs permission for the kms:GenerateDataKey action. This permission is required for buckets that use default encryption with a custom AWS KMS key.In the JSON policy documents, look for policies related to AWS KMS access. Review statements with "Effect": "Allow" to check if the user or role has permissions for the kms:GenerateDataKey action on the bucket\'s AWS KMS key. If this permission is missing, then add the permission to the appropriate policy.In the JSON policy documents, look for statements with "Effect": "Deny". Then, confirm that those statements don\'t deny the s3:PutObject action on the bucket. The statements must also not deny the IAM user or role access to the kms:GenerateDataKey action on the key used to encrypt the bucket. Additionally, make sure the necessary KMS and S3 permissions are not restricted using a VPC endpoint policy, service control policy, permissions boundary, or session policy.Incorrect options:Correct the policy of the IAM user to allow the s3:Encrypt action This is an invalid action given only as a distractor.Correct the bucket policy of the S3 bucket to allow the IAM user to upload encrypted objects The user already has access to the bucket. What the user lacks is access to generate a KMS key, which is mandatory when a bucket is enabled for default encryption.Correct the ACL of the S3 bucket to allow the IAM user to upload encrypted objects Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. It defines which AWS accounts or groups are granted access and the type of access. ACL is another way of giving access to S3 bucket objects. Permissions to use KMS keys will still be needed.References:https://aws.amazon.com/premiumsupport/knowledgecenter/s3accessdeniederrorkms/ https://docs.aws.amazon.com/kms/latest/developerguide/servicess3.html'},{question:"A data analytics company with its IT infrastructure on the AWS Cloud wants to build and deploy its flagship application as soon as there are any changes to the source code. As a Developer Associate, which of the following options would you suggest to trigger the deployment? (Select two)",answers:[{text:"Keep the source code in Amazon EFS and start AWS CodePipeline whenever a file is updated",isCorrect:!1},{text:"Keep the source code in an AWS CodeCommit repository and start AWS CodePipeline whenever a change is pushed to the CodeCommit repository",isCorrect:!0},{text:"Keep the source code in an Amazon EBS volume and start AWS CodePipeline whenever there are updates to the source code",isCorrect:!1},{text:"Keep the source code in an Amazon S3 bucket and start AWS CodePipeline whenever a file in the S3 bucket is updated",isCorrect:!0},{text:"Keep the source code in an Amazon S3 bucket and set up AWS CodePipeline to recur at an interval of every 15 minutes",isCorrect:!1}],explanation:"Keep the source code in an AWS CodeCommit repository and start AWS CodePipeline whenever a change is pushed to the CodeCommit repositoryKeep the source code in an Amazon S3 bucket and start AWS CodePipeline whenever a file in the S3 bucket is updatedAWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define.How CodePipeline Works: via https://aws.amazon.com/codepipeline/ Using change detection methods that you specify, you can make your pipeline start when a change is made to a repository. You can also make your pipeline start on a schedule.When you use the console to create a pipeline that has a CodeCommit source repository or S3 source bucket, CodePipeline creates an Amazon CloudWatch Events rule that starts your pipeline when the source changes. This is the recommended change detection method.If you use the AWS CLI to create the pipeline, the change detection method defaults to starting the pipeline by periodically checking the source (CodeCommit, Amazon S3, and GitHub source providers only). AWS recommends that you disable periodic checks and create the rule manually.via https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelinesaboutstarting.html Incorrect options:Keep the source code in Amazon EFS and start AWS CodePipeline whenever a file is updatedKeep the source code in an Amazon EBS volume and start AWS CodePipeline whenever there are updates to the source codeBoth EFS and EBS are not supported as valid source providers for CodePipeline to check for any changes to the source code, hence these two options are incorrect.Keep the source code in an Amazon S3 bucket and set up AWS CodePipeline to recur at an interval of every 15 minutes As mentioned in the explanation above, although you could have the change detection method start the pipeline by periodically checking the S3 bucket, but this method is inefficient.Reference: https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelinesaboutstarting.html"},{question:"AWS CloudFormation helps model and provision all the cloud infrastructure resources needed for your business. Which of the following services rely on CloudFormation to provision resources (Select two)?",answers:[{text:"AWS Serverless Application Model (AWS SAM)",isCorrect:!0},{text:"AWS Elastic Beanstalk",isCorrect:!0},{text:"AWS CodeBuild",isCorrect:!1},{text:"AWS Lambda",isCorrect:!1},{text:"AWS Autoscaling",isCorrect:!1}],explanation:"Correct option:AWS Elastic Beanstalk AWS Elastic Beanstalk is an easytouse service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. Elastic Beanstalk uses AWS CloudFormation to launch the resources in your environment and propagate configuration changes.AWS Serverless Application Model (AWS SAM) You use the AWS SAM specification to define your serverless application. AWS SAM templates are an extension of AWS CloudFormation templates, with some additional components that make them easier to work with. AWS SAM needs CloudFormation templates as a basis for its configuration.Incorrect options:AWS Lambda AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Hence, Lamda does not need CloudFormation to run its services.AWS Autoscaling AWS Auto Scaling monitors your applications and automatically adjusts the capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto Scaling, it’s easy to setup application scaling for multiple resources across multiple services in minutes. Auto Scaling used CloudFormation but is not a mandatory requirement.AWS CodeBuild AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you don’t need to provision, manage, and scale your own build servers. AWS CodePipeline uses AWS CloudFormation as a deployment action but is not a mandatory service.References:https://docs.aws.amazon.com/serverlessapplicationmodel/latest/developerguide/samspecification.html https://aws.amazon.com/elasticbeanstalk/"},{question:"You work as a developer doing contract work for the government on AWS gov cloud. Your applications use Amazon Simple Queue Service (SQS) for its message queue service. Due to recent hacking attempts, security measures have become stricter and require you to store data in encrypted queues. Which of the following steps can you take to meet your requirements without making changes to the existing code?",answers:[{text:"Use Client side encryption",isCorrect:!1},{text:"Enable SQS KMS encryption",isCorrect:!0},{text:"Use the SSL endpoint",isCorrect:!1},{text:"Use Secrets Manager",isCorrect:!1}],explanation:"Enable SQS KMS encryptionServerside encryption (SSE) lets you transmit sensitive data in encrypted queues. SSE protects the contents of messages in queues using keys managed in AWS Key Management Service (AWS KMS).AWS KMS combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use Amazon SQS with AWS KMS, the data keys that encrypt your message data are also encrypted and stored with the data they protect.You can choose to have SQS encrypt messages stored in both Standard and FIFO queues using an encryption key provided by AWS Key Management Service (KMS).Incorrect options:Use the SSL endpoint The given usecase needs encryption at rest. When using SSL, the data is encrypted during transit, but the data needs to be encrypted at rest as well, so this option is incorrect.Use Clientside encryption For additional security, you can build your application to encrypt messages before they are placed in a message queue but will require a code change, so this option is incorrect.*Use Secrets Manager * AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with builtin integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB. Secrets Manager cannot be used for encrypting data at rest.Reference: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsserversideencryption.html"},{question:"A leading financial services company offers data aggregation services for Wall Street trading firms. The company bills its clients based on per unit of clickstream data provided to the clients. As the company operates in a regulated industry, it needs to have the same ordered clickstream data available for auditing within a window of 7 days. As a Developer Associate, which of the following AWS services do you think provides the ability to run the billing process and auditing process on the given clickstream data in the same order?",answers:[{text:"AWS Kinesis Data Firehose",isCorrect:!1},{text:"AWS Kinesis Data Streams",isCorrect:!0},{text:"Amazon SQS",isCorrect:!1},{text:"AWS Kinesis Data Analytics",isCorrect:!1}],explanation:"Correct option:AWS Kinesis Data StreamsAmazon Kinesis Data Streams (KDS) is a massively scalable and durable realtime data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and locationtracking events. The data collected is available in milliseconds to enable realtime analytics use cases such as realtime dashboards, realtime anomaly detection, dynamic pricing, and more.Amazon Kinesis Data Streams enables realtime processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering). Amazon Kinesis Data Streams is recommended when you need the ability to consume records in the same order a few hours later.For example, you have a billing application and an audit application that runs a few hours behind the billing application. By default, records of a stream are accessible for up to 24 hours from the time they are added to the stream. You can raise this limit to a maximum of 365 days. For the given usecase, Amazon Kinesis Data Streams can be configured to store data for up to 7 days and you can run the audit application up to 7 days behind the billing application.KDS provides the ability to consume records in the same order a few hours later via https://aws.amazon.com/kinesis/datastreams/faqs/ Incorrect options:AWS Kinesis Data Firehose Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near realtime analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect.AWS Kinesis Data Analytics Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in realtime. You can quickly build SQL queries and sophisticated Java applications using builtin templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect.Amazon SQS Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, besteffort ordering, and atleastonce delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers in the same order a few hours later, therefore this option is incorrect.References:https://aws.amazon.com/kinesis/datastreams/faqs/ https://aws.amazon.com/kinesis/datafirehose/faqs/ https://aws.amazon.com/kinesis/dataanalytics/faqs/"},{question:"Your company uses an Application Load Balancer to route incoming enduser traffic to applications hosted on Amazon EC2 instances. The applications capture incoming request information and store it in the Amazon Relational Database Service (RDS) running on Microsoft SQL Server DB engines. As part of new compliance rules, you need to capture the client's IP address. How will you achieve this?",answers:[{text:"You can get the Client IP addresses from Elastic Load Balancing logs",isCorrect:!1},{text:"Use the header XForwardedFrom",isCorrect:!1},{text:"Use the header XForwardedFor",isCorrect:!0},{text:"You can get the Client IP addresses from server access logs",isCorrect:!1}],explanation:"Correct option:Use the header XForwardedFor The XForwardedFor request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the XForwardedFor request header. Elastic Load Balancing stores the IP address of the client in the XForwardedFor request header and passes the header to your server.Incorrect options:You can get the Client IP addresses from server access logs As discussed above, Load Balancers intercept traffic between clients and servers, so server access logs will contain only the IP address of the load balancer.Use the header XForwardedFrom This is a madeup option and given as a distractor.You can get the Client IP addresses from Elastic Load Balancing logs Elastic Load Balancing logs requests sent to the load balancer, including requests that never made it to the targets. For example, if a client sends a malformed request, or there are no healthy targets to respond to the request, the request is still logged. So, this is not the right option if we wish to collect the IP addresses of the clients that have access to the instances.References:https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/xforwardedheaders.html https://docs.aws.amazon.com/elasticloadbalancing/latest/application/loadbalanceraccesslogs.html"},{question:"An ecommerce company uses AWS CloudFormation to implement Infrastructure as Code for the entire organization. Maintaining resources as stacks with CloudFormation has greatly reduced the management effort needed to manage and maintain the resources. However, a few teams have been complaining of failing stack updates owing to outofband fixes running on the stack resources. Which of the following is the best solution that can help in keeping the CloudFormation stack and its resources in sync with each other?",answers:[{text:"Use Tag feature of CloudFormation to monitor the changes happening on specific resources",isCorrect:!1},{text:"Use CloudFormation in Elastic Beanstalk environment to reduce direct changes to CloudFormation resources",isCorrect:!1},{text:"Use Drift Detection feature of CloudFormation",isCorrect:!0},{text:"Use Change Sets feature of CloudFormation",isCorrect:!1}],explanation:"Correct option:Use Drift Detection feature of CloudFormationDrift detection enables you to detect whether a stack's actual configuration differs, or has drifted, from its expected configuration. Use CloudFormation to detect drift on an entire stack, or individual resources within the stack. A resource is considered to have drifted if any of its actual property values differ from the expected property values. This includes if the property or resource has been deleted. A stack is considered to have drifted if one or more of its resources have drifted.To determine whether a resource has drifted, CloudFormation determines the expected resource property values, as defined in the stack template and any values specified as template parameters. CloudFormation then compares those expected values with the actual values of those resource properties as they currently exist in the stack. A resource is considered to have drifted if one or more of its properties have been deleted, or had their value changed.You can then take corrective action so that your stack resources are again in sync with their definitions in the stack template, such as updating the drifted resources directly so that they agree with their template definition. Resolving drift helps to ensure configuration consistency and successful stack operations.Incorrect options:Use CloudFormation in Elastic Beanstalk environment to reduce direct changes to CloudFormation resources Elastic Beanstalk environment provides full access to the resources created. So, it is possible to edit the resources and hence does not solve the issue mentioned for the given use case.Use Tag feature of CloudFormation to monitor the changes happening on specific resources Tags help you identify and categorize the resources created as part of CloudFormation template. This feature is not helpful for the given use case.Use Change Sets feature of CloudFormation When you need to update a stack, understanding how your changes will affect running resources before you implement them can help you update stacks with confidence. Change sets allow you to preview how proposed changes to a stack might impact your running resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set. Change sets are not useful for the given usecase.References:https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/detectdriftstack.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/usingcfnupdatingstackschangesets.htmll"},{question:"A new member of your team is working on creating Dead Letter Queue (DLQ) for AWS Lambda functions. As a Developer Associate, can you help him identify the use cases, wherein AWS Lambda will add a message into a DLQ after being processed? (Select two)",answers:[{text:"The event has been processed successfully",isCorrect:!1},{text:"The Lambda function invocation is synchronous",isCorrect:!1},{text:"The event fails all processing attempts",isCorrect:!0},{text:"The Lambda function invocation is asynchronous",isCorrect:!0},{text:"The Lambda function invocation failed only once but succeeded thereafter",isCorrect:!1}],explanation:"Correct option:The Lambda function invocation is asynchronous When an asynchronous invocation event exceeds the maximum age or fails all retry attempts, Lambda discards it. Or sends it to deadletter queue if you have configured one.The event fails all processing attempt A deadletter queue acts the same as an onfailure destination in that it is used when an event fails all processing attempts or expires without being processed.Incorrect options:The Lambda function invocation is synchronous When you invoke a function synchronously, Lambda runs the function and waits for a response. Queues are generally used with asynchronous invocations since queues implement the decoupling feature of various connected systems. It does not make sense to use queues when the calling code will wait on it for a response.The event has been processed successfully A successfully processed event is not sent to the deadletter queue.The event processing failed only once but succeeded thereafter A successfully processed event is not sent to the deadletter queue.Reference: https://docs.aws.amazon.com/lambda/latest/dg/invocationasync.html"},{question:"A company follows collaborative development practices. The engineering manager wants to isolate the development effort by setting up simulations of API components owned by various development teams. Which API integration type is best suited for this requirement?",answers:[{text:"HTTP",isCorrect:!1},{text:"AWS_PROXY",isCorrect:!1},{text:"HTTP_PROXY",isCorrect:!1},{text:"MOCK",isCorrect:!0}],explanation:"Correct option:MOCKThis type of integration lets API Gateway return a response without sending the request further to the backend. This is useful for API testing because it can be used to test the integration setup without incurring charges for using the backend and to enable collaborative development of an API.In collaborative development, a team can isolate their development effort by setting up simulations of API components owned by other teams by using the MOCK integrations. It is also used to return CORSrelated headers to ensure that the API method permits CORS access. In fact, the API Gateway console integrates the OPTIONS method to support CORS with a mock integration.Incorrect options:AWS_PROXY This type of integration lets an API method be integrated with the Lambda function invocation action with a flexible, versatile, and streamlined integration setup. This integration relies on direct interactions between the client and the integrated Lambda function.HTTP_PROXY The HTTP proxy integration allows a client to access the backend HTTP endpoints with a streamlined integration setup on single API method. You do not set the integration request or the integration response. API Gateway passes the incoming request from the client to the HTTP endpoint and passes the outgoing response from the HTTP endpoint to the client.HTTP This type of integration lets an API expose HTTP endpoints in the backend. With the HTTP integration, you must configure both the integration request and integration response. You must set up necessary data mappings from the method request to the integration request, and from the integration response to the method response.Reference: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayapiintegrationtypes.html"},{question:"You are assigned as the new project lead for a web application that processes orders for customers. You want to integrate eventdriven processing anytime data is modified or deleted and use a serverless approach using AWS Lambda for processing stream events. Which of the following databases should you choose from?",answers:[{text:"RDS",isCorrect:!1},{text:"Kinesis",isCorrect:!1},{text:"ElastiCache",isCorrect:!1},{text:"DynamoDB",isCorrect:!0}],explanation:"DynamoDBA DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB Streams captures a timeordered sequence of itemlevel modifications in any DynamoDB table, and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near realtime.Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified.DynamoDB Streams Overview: via https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html Incorrect options:RDS By itself, RDS cannot be used to stream events like DynamoDB, so this option is ruled out. However, you can use Amazon Kinesis for streaming data from RDS.Please refer to this excellent blog for more details on using Kinesis for streaming data from RDS: https://aws.amazon.com/blogs/database/streamingchangesinadatabasewithamazonkinesis/ElastiCache ElastiCache works as an inmemory data store and cache, it cannot be used to stream data like DynamoDB.Kinesis Kinesis is not a database, so this option is ruled out.Amazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs. You can continuously add various types of data such as clickstreams, application logs, and social media to an Amazon Kinesis data stream from hundreds of thousands of sources.How Kinesis Data Streams Work via https://aws.amazon.com/kinesis/datastreams/Reference: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html"},{question:"Consider an application that enables users to store their mobile phone images in the cloud and supports tens of thousands of users. The application should utilize an Amazon API Gateway REST API that leverages AWS Lambda functions for photo processing while storing photo details in Amazon DynamoDB. The application should allow users to create an account, upload images, and retrieve previously uploaded images, with images ranging in size from 500 KB to 5 MB. How will you design the application with the least operational overhead?",answers:[{text:"Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images as well as the image metadata in a DynamoDB table. Have the Lambda function retrieve previously uploaded images from DynamoDB",isCorrect:!1},{text:"Use Cognito identity pools to create an IAM user for each user of the application during the signup process. Leverage IAM authentication in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key",isCorrect:!1},{text:"Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key",isCorrect:!0},{text:"Use Cognito identity pools to manage user accounts and set up an Amazon Cognito identity pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 key",isCorrect:!1}],explanation:"Correct option:Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 keyA user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Google, Facebook, Amazon, or Apple, and SAML identity providers. Whether your users sign in directly or through a third party, all members of the user pool have a directory profile that you can access through a Software Development Kit (SDK).User pools provide:Signup and signin services.A builtin, customizable web UI to sign in users.Social signin with Facebook, Google, Login with Amazon, and Sign in with Apple, as well as signin with SAML identity providers from your user pool.User directory management and user profiles.Security features such as multifactor authentication (MFA), checks for compromised credentials, account takeover protection, and phone and email verification.Customized workflows and user migration through AWS Lambda triggers.To use an Amazon Cognito user pool with your Amazon API Gateway API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header.For the given use case, you can use a Cognito user pool to manage user accounts and configure an Amazon Cognito user pool authorizer in API Gateway to control access to the API. You should use a Lambda function to store the actual images on S3 and the image metadata on DynamoDB. Finally, you can get the images using the Lambda function that leverages the metadata stored in DynamoDB.Incorrect options:Use Cognito identity pools to manage user accounts and set up an Amazon Cognito identity pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 keyUse Cognito identity pools to create an IAM user for each user of the application during the signup process. Leverage IAM authentication in API Gateway to control access to the API. Set up a Lambda function to store the images in Amazon S3 and save the image object's S3 key as part of the photo details in a DynamoDB table. Have the Lambda function retrieve previously uploaded images by querying DynamoDB for the S3 keyAmazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limitedprivilege AWS credentials to access other AWS services. You cannot use identity pools to manage users or to create IAM users. So both of these options are incorrect.via https://aws.amazon.com/premiumsupport/knowledgecenter/cognitouserpoolsidentitypools/Leverage Cognito user pools to manage user accounts and set up an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Set up a Lambda function to store the images as well as the image metadata in a DynamoDB table. Have the Lambda function retrieve previously uploaded images from DynamoDB You cannot use DynamoDB to store images as the maximum allowed item size is 400KB and the images range in size from 500KB to 5MB. You should also note that storing images on DynamoDB is an antipattern. So this option is incorrect.References:https://docs.aws.amazon.com/cognito/latest/developerguide/cognitouseridentitypools.html https://docs.aws.amazon.com/cognito/latest/developerguide/cognitoidentity.html https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayintegratewithcognito.html https://aws.amazon.com/premiumsupport/knowledgecenter/cognitouserpoolsidentitypools/"},{question:"A developer is migrating an onpremises application to AWS Cloud. The application currently processes user uploads and uploads them to a local directory on the server. All such file uploads must be saved and then made available to all instances in an Auto Scaling group. As a Developer Associate, which of the following options would you recommend for this usecase?",answers:[{text:"Use Amazon EBS as the storage volume and share the files via file synchronization software",isCorrect:!1},{text:"Use Amazon S3 and make code changes in the application so all uploads are put on S3",isCorrect:!0},{text:"Use Amazon EBS and configure the application AMI to use a snapshot of the same EBS instance while launching new instances",isCorrect:!1},{text:"Use Instance Store type of EC2 instances and share the files via file synchronization software",isCorrect:!1}],explanation:"Use Amazon S3 and make code changes in the application so all uploads are put on S3Amazon S3 is an object storage built to store and retrieve any amount of data from anywhere on the Internet. It’s a simple storage service that offers an extremely durable, highly available, and infinitely scalable data storage infrastructure at very low costs.Amazon S3 provides a simple web service interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. Using this web service, you can easily build applications that make use of Internet storage.You can use S3 PutObject API from the application to upload the objects in a single bucket, which is then accessible from all instances.Incorrect options:Use Amazon EBS and configure the application AMI to use a snapshot of the same EBS instance while launching new instances Using EBS to share data between instances is not possible because EBS volume is tied to an instance by definition. Creating a snapshot would only manage to move the stale data into the new instances.Use Instance Store type of EC2 instances and share the files via file synchronization softwareUse Amazon EBS as the storage volume and share the files via file synchronization softwareTechnically you could use file synchronization software on EC2 instances with EBS or Instance Store type, but that involves a lot of development effort and still would not be as productionready as just using S3. So both these options are incorrect.Reference: https://aws.amazon.com/s3/faqs/"},{question:"For an application that stores personal health information (PHI) in an encrypted Amazon RDS for MySQL DB instance, a developer wants to improve its performance by caching frequently accessed data and adding the ability to sort or rank the cached datasets. What is the best approach to meet these requirements subject to the constraint that the PHI stays encrypted at all times?",answers:[{text:"Migrate the frequently accessed data to an EC2 Instance Store that has encryption enabled for data in transit and at rest",isCorrect:!1},{text:"Migrate the frequently accessed data to DynamoDB Accelerator (DAX) that has encryption enabled for data in transit and at rest",isCorrect:!1},{text:"Store the frequently accessed data in an Amazon ElastiCache for Memcached instance with encryption enabled for data in transit and at rest",isCorrect:!1},{text:"Store the frequently accessed data in an Amazon ElastiCache for Redis instance with encryption enabled for data in transit and at rest",isCorrect:!0}],explanation:"Store the frequently accessed data in an Amazon ElastiCache for Redis instance with encryption enabled for data in transit and at restAmazon ElastiCache for Redis is a Rediscompatible inmemory data structure service that can be used as a data store or cache. It delivers the ease of use and power of Redis along with the availability, reliability, scalability, security, and performance suitable for the most demanding applications.In addition to strings, Redis supports lists, sets, sorted sets, hashes, bit arrays, and hyperlog logs. Applications can use these more advanced data structures to support a variety of use cases. For example, you can use Redis Sorted Sets to easily implement a game leaderboard that keeps a list of players sorted by their rank.Incorrect options:Store the frequently accessed data in an Amazon ElastiCache for Memcached instance with encryption enabled for data in transit and at rest Memcached is designed for simplicity and it does not offer support for advanced data structures and operations such as sort or rank.via https://aws.amazon.com/elasticache/redisvsmemcached/ Migrate the frequently accessed data to DynamoDB Accelerator (DAX) that has encryption enabled for data in transit and at rest DAX is a DynamoDBcompatible caching service that enables you to benefit from fast inmemory performance for demanding applications. DAX cannot be used with RDS MySQL as a caching service, so this option is incorrect.Migrate the frequently accessed data to an EC2 Instance Store that has encryption enabled for data in transit and at rest This option is incorrect. EC2 instance store provides temporary blocklevel storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content. It can also be used to store temporary data that you replicate across a fleet of instances, such as a loadbalanced pool of web servers.References:https://aws.amazon.com/elasticache/redis/ https://aws.amazon.com/elasticache/redisvsmemcached/ https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html"},{question:"A team is checking the viability of using AWS Step Functions for creating a banking workflow for loan approvals. The web application will also have human approval as one of the steps in the workflow. As a developer associate, which of the following would you identify as the key characteristics for AWS Step Function? (Select two)",answers:[{text:"Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of 180 days or 6 months",isCorrect:!1},{text:"Standard Workflows on AWS Step Functions are suitable for longrunning, durable, and auditable workflows that can also support any human approval steps",isCorrect:!0},{text:"Both Standard and Express Workflows support all service integrations, activities, and design patterns",isCorrect:!1},{text:"Standard Workflows on AWS Step Functions are suitable for longrunning, durable, and auditable workflows that do not support any human approval steps",isCorrect:!1},{text:"You should use Express Workflows for workloads with high event rates and short duration",isCorrect:!0}],explanation:"Correct options:Standard Workflows on AWS Step Functions are suitable for longrunning, durable, and auditable workflows that can also support any human approval steps Standard Workflows on AWS Step Functions are more suitable for longrunning, durable, and auditable workflows where repeating workflow steps is expensive (e.g., restarting a longrunning media transcode) or harmful (e.g., charging a credit card twice). Example workloads include training and deploying machine learning models, report generation, billing, credit card processing, and ordering and fulfillment processes. Step functions also support any human approval steps.You should use Express Workflows for workloads with high event rates and short duration* You should use Express Workflows for workloads with high event rates and short durations. Express Workflows support event rates of more than 100,000 per second.Incorrect options:Standard Workflows on AWS Step Functions are suitable for longrunning, durable, and auditable workflows that do not support any human approval steps As Step functions support any human approval steps, so this option is incorrect.Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of 180 days or 6 months Express Workflows have a maximum duration of five minutes and Standard workflows have a maximum duration of one year.Both Standard and Express Workflows support all service integrations, activities, and design patterns Standard Workflows support all service integrations, activities, and design patterns. Express Workflows do not support activities, jobrun (.sync), and Callback patterns.Reference:https://aws.amazon.com/stepfunctions/features/ https://aws.amazon.com/blogs/compute/implementingserverlessmanualapprovalstepsinawsstepfunctionsandamazonapigateway/"},{question:"The development team at a social media company is considering using Amazon ElastiCache to boost the performance of their existing databases. As a Developer Associate, which of the following usecases would you recommend as the BEST fit for ElastiCache? (Select two)",answers:[{text:"Use ElastiCache to run highly complex JOIN queries",isCorrect:!1},{text:"Use ElastiCache to improve latency and throughput for writeheavy application workloads",isCorrect:!1},{text:"Use ElastiCache to improve performance of ExtractTransformLoad (ETL) workloads",isCorrect:!1},{text:"Use ElastiCache to improve performance of computeintensive workloads",isCorrect:!0},{text:"Use ElastiCache to improve latency and throughput for readheavy application workloads",isCorrect:!0}],explanation:"Correct option:Use ElastiCache to improve latency and throughput for readheavy application workloadsUse ElastiCache to improve performance of computeintensive workloadsAmazon ElastiCache allows you to run inmemory data stores in the AWS cloud. Amazon ElastiCache is a popular choice for realtime use cases like Caching, Session Stores, Gaming, Geospatial Services, RealTime Analytics, and Queuing.via https://docs.aws.amazon.com/AmazonElastiCache/latest/memug/elasticacheusecases.html Amazon ElastiCache can be used to significantly improve latency and throughput for many readheavy application workloads (such as social networking, gaming, media sharing, and Q&A portals) or computeintensive workloads (such as a recommendation engine) by allowing you to store the objects that are often read in the cache.Overview of Amazon ElastiCache features: via https://aws.amazon.com/elasticache/features/Incorrect options:Use ElastiCache to improve latency and throughput for writeheavy application workloads As mentioned earlier in the explanation, Amazon ElastiCache can be used to significantly improve latency and throughput for many readheavy application workloads. Caching is not a good fit for writeheavy applications as the cache goes stale at a very fast rate.Use ElastiCache to improve performance of ExtractTransformLoad (ETL) workloads ETL workloads involve reading and transforming high volume data which is not a good fit for caching. You should use AWS Glue or Amazon EMR to facilitate ETL workloads.Use ElastiCache to run highly complex JOIN queries Complex JSON queries can be run on relational databases such as RDS or Aurora. ElastiCache is not a good fit for this usecase.References:https://docs.aws.amazon.com/AmazonElastiCache/latest/memug/elasticacheusecases.html https://aws.amazon.com/elasticache/features/"},{question:"A developer wants to enable XRay tracing on an onpremises Linux server running a custom application that is accessed through Amazon API Gateway. What is the most efficient solution that requires minimal configuration?",answers:[{text:"Install and run the CloudWatch Unified Agent on the onpremises servers to capture and relay the XRay data to the XRay service using the PutTraceSegments API call",isCorrect:!1},{text:"Install and run the XRay daemon on the onpremises servers to capture and relay the data to the XRay service",isCorrect:!0},{text:"Configure a Lambda function to analyze the incoming traffic data on the onpremises servers and then relay the XRay data to the XRay service using the PutTelemetryRecords API call",isCorrect:!1},{text:"Install and run the XRay SDK on the onpremises servers to capture and relay the data to the XRay service",isCorrect:!1}],explanation:"Correct option:Install and run the XRay daemon on the onpremises servers to capture and relay the data to the XRay serviceThe AWS XRay daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS XRay API. The daemon works in conjunction with the AWS XRay SDKs and must be running so that data sent by the SDKs can reach the XRay service.To run the XRay daemon locally, onpremises, or on other AWS services, download it, run it, and then give it permission to upload segment documents to XRay.Incorrect options:Install and run the XRay SDK on the onpremises servers to capture and relay the data to the XRay service As mentioned above, you need to run the XRay daemon on the onpremises servers and give it the required permission to upload XRay data to the XRay service. So this option is incorrect.Install and run the CloudWatch Unified Agent on the onpremises servers to capture and relay the XRay data to the XRay service using the PutTraceSegments API call This option has been added as a distractor. CloudWatch Agent cannot relay XRay data to the XRay service using the PutTraceSegments API call.Configure a Lambda function to analyze the incoming traffic data on the onpremises servers and then relay the XRay data to the XRay service using the PutTelemetryRecords API call This option is incorrect as the Lambda function cannot process the XRay data for an onpremises instance and then relay it to the XRay service.Reference: https://docs.aws.amazon.com/xray/latest/devguide/xraydaemon.html"},{question:"A development team has deployed a REST API in Amazon API Gateway to two different stages a test stage and a prod stage. The test stage is used as a test build and the prod stage as a stable build. After the updates have passed the test, the team wishes to promote the test stage to the prod stage. Which of the following represents the optimal solution for this usecase?",answers:[{text:"Update stage variable value from the stage name of test to that of prod",isCorrect:!0},{text:"Delete the existing prod stage. Create a new stage with the same name (prod) and deploy the tested version on this stage",isCorrect:!1},{text:"API performance is optimized in a different way for prod environments. Hence, promoting test to prod is not correct. The promotion should be done by redeploying the API to the prod stage",isCorrect:!1},{text:"Deploy the API without choosing a stage. This way, the working deployment will be updated in all stages",isCorrect:!1}],explanation:"Correct option:Update stage variable value from the stage name of test to that of prodAfter creating your API, you must deploy it to make it callable by your users. To deploy an API, you create an API deployment and associate it with a stage. A stage is a logical reference to a lifecycle state of your API (for example, dev, prod, beta, v2). API stages are identified by the API ID and stage name. They're included in the URL that you use to invoke the API. Each stage is a named reference to a deployment of the API and is made available for client applications to call.Stages enable robust version control of your API. In our current usecase, after the updates pass the test, you can promote the test stage to the prod stage. The promotion can be done by redeploying the API to the prod stage or updating a stage variable value from the stage name of test to that of prod.Incorrect options:Deploy the API without choosing a stage. This way, the working deployment will be updated in all stages An API can only be deployed to a stage. Hence, it is not possible to deploy an API without choosing a stage.Delete the existing prod stage. Create a new stage with the same name (prod) and deploy the tested version on this stage* This is possible, but not an optimal way of deploying a change. Also, as prod refers to real production system, this option will result in downtime.API performance is optimized in a different way for prod environments. Hence, promoting test to prod is not correct. The promotion should be done by redeploying the API to the prod stage For each stage, you can optimize API performance by adjusting the default accountlevel request throttling limits and enabling API caching. And these settings can be changed/updated at any time.Reference:https://docs.aws.amazon.com/apigateway/latest/developerguide/howtodeployapi.htmll"},{question:"You are creating a mobile application that needs access to the AWS API Gateway. Users will need to register first before they can access your API and you would like the user management to be fully managed. Which authentication option should you use for your API Gateway layer?",answers:[{text:"Use IAM permissions with sigv4",isCorrect:!1},{text:"Use API Gateway User Pools",isCorrect:!1},{text:"Use Lambda Authorizer",isCorrect:!1},{text:"Use Cognito User Pools",isCorrect:!0}],explanation:"Correct option:Use Cognito User Pools As an alternative to using IAM roles and policies or Lambda authorizers, you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway. To use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn't authorized to make the call because the client did not have credentials that could be authorized.Incorrect options:Use Lambda Authorizer A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API. A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity. This won't be a fully managed user management solution but it would allow you to check for access at the AWS API Gateway level.Use IAM permissions with sigv4 Signature Version 4 is the process to add authentication information to AWS requests sent by HTTP. For security, most requests to AWS must be signed with an access key, which consists of an access key ID and secret access key. These two keys are commonly referred to as your security credentials. But, we cannot possibly create an IAM user for every visitor of the site, so this is where social identity providers come in to help.Use API Gateway User Pools This is a madeup option.References:https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayintegratewithcognito.html https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayuselambdaauthorizer.html https://docs.aws.amazon.com/general/latest/gr/signatureversion4.html"},{question:"An IT company is using AWS CloudFormation to manage its IT infrastructure. It has created a template to provision a stack with a VPC and a subnet. The output value of this subnet has to be used in another stack. As a Developer Associate, which of the following options would you suggest to provide this information to another stack?",answers:[{text:"Use Fn::ImportValue",isCorrect:!1},{text:"Use 'Expose' field in the Output section of the stack's template",isCorrect:!1},{text:"Use Fn::Transform",isCorrect:!1},{text:"Use 'Export' field in the Output section of the stack's template",isCorrect:!0}],explanation:"Use 'Export' field in the Output section of the stack's templateTo share information between stacks, export a stack's output values. Other stacks that are in the same AWS account and region can import the exported values.To export a stack's output value, use the Export field in the Output section of the stack's template. To import those values, use the Fn::ImportValue function in the template for the other stacks.Incorrect options:Use 'Expose' field in the Output section of the stack's template 'Expose' is a madeup option, and only given as a distractor.Use Fn::ImportValue To import the values exported by another stack, we use the Fn::ImportValue function in the template for the other stacks. This function is not useful for the current scenario.Use Fn::Transform The intrinsic function Fn::Transform specifies a macro to perform custom processing on part of a stack template. Macros enable you to perform custom processing on templates, from simple actions like findandreplace operations to extensive transformations of entire templates. This function is not useful for the current scenario.Reference: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/usingcfnstackexports.html"},{question:"The development team at a retail company is gearing up for the upcoming Thanksgiving sale and wants to make sure that the application's serverless backend running via Lambda functions does not hit latency bottlenecks as a result of the traffic spike. As a Developer Associate, which of the following solutions would you recommend to address this usecase?",answers:[{text:"Configure Application Auto Scaling to manage Lambda provisioned concurrency on a schedule",isCorrect:!0},{text:"Add an Application Load Balancer in front of the Lambda functions",isCorrect:!1},{text:"Configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule",isCorrect:!1},{text:"No need to make any special provisions as Lambda is automatically scalable because of its serverless nature",isCorrect:!1}],explanation:"Correct option:Configure Application Auto Scaling to manage Lambda provisioned concurrency on a scheduleConcurrency is the number of requests that a Lambda function is serving at any given time. If a Lambda function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.Due to a spike in traffic, when Lambda functions scale, this causes the portion of requests that are served by new instances to have higher latency than the rest. To enable your function to scale without fluctuations in latency, use provisioned concurrency. By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with very low latency.You can configure Application Auto Scaling to manage provisioned concurrency on a schedule or based on utilization. Use scheduled scaling to increase provisioned concurrency in anticipation of peak traffic. To increase provisioned concurrency automatically as needed, use the Application Auto Scaling API to register a target and create a scaling policy.Please see this note for more details on provisioned concurrency: via https://docs.aws.amazon.com/lambda/latest/dg/configurationconcurrency.html Incorrect options:Configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule To ensure that a function can always reach a certain level of concurrency, you can configure the function with reserved concurrency. When a function has reserved concurrency, no other function can use that concurrency. More importantly, reserved concurrency also limits the maximum concurrency for the function, and applies to the function as a whole, including versions and aliases.You cannot configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule.Add an Application Load Balancer in front of the Lambda functions This is a distractor as just adding the Application Load Balancer will not help in scaling the Lambda functions to address the surge in traffic.No need to make any special provisions as Lambda is automatically scalable because of its serverless nature It's true that Lambda is serverless, however, due to the surge in traffic the Lambda functions can still hit the concurrency limits. So this option is incorrect.Reference: https://docs.aws.amazon.com/lambda/latest/dg/configurationconcurrency.html"},{question:"A cybersecurity company is running a serverless backend with several computeheavy workflows running on Lambda functions. The development team has noticed a performance lag after analyzing the performance metrics for the Lambda functions. As a Developer Associate, which of the following options would you suggest as the BEST solution to address the computeheavy workloads?",answers:[{text:"Use provisioned concurrency to account for the computeheavy workflows",isCorrect:!1},{text:"Increase the amount of memory available to the Lambda functions",isCorrect:!0},{text:"Use reserved concurrency to account for the computeheavy workflows",isCorrect:!1},{text:"Invoke the Lambda functions asynchronously to process the computeheavy workflows",isCorrect:!1}],explanation:"Correct option:Increase the amount of memory available to the Lambda functionsAWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.In the AWS Lambda resource model, you choose the amount of memory you want for your function which allocates proportional CPU power and other resources. This means you will have access to more compute power when you choose one of the new larger settings. You can set your memory in 64MB increments from 128MB to 3008MB. You access these settings when you create a function or update its configuration. The settings are available using the AWS Management Console, AWS CLI, or SDKs.via https://docs.aws.amazon.com/lambda/latest/dg/configurationconsole.htmlTherefore, by increasing the amount of memory available to the Lambda functions, you can run the computeheavy workflows.Incorrect options:Invoke the Lambda functions asynchronously to process the computeheavy workflows When you invoke a function asynchronously, you don't wait for a response from the function code. You hand off the event to Lambda and Lambda handles the rest. You can configure how Lambda handles errors and can send invocation records to a downstream resource to chain together components of your application. The method of invocation has no bearing on the Lambda function's ability to process the computeheavy workflows.Use reserved concurrency to account for the computeheavy workflowsUse provisioned concurrency to account for the computeheavy workflowsConcurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency. The type of concurrency has no bearing on the Lambda function's ability to process the computeheavy workflows. So both these options are incorrect.Reference: https://docs.aws.amazon.com/lambda/latest/dg/configurationconsole.html"},{question:"Your AWS CodeDeploy deployment to T2 instances succeed. The new application revision makes API calls to Amazon S3 however the application is not working as expected due to authorization exceptions and you were assigned to troubleshoot the issue. Which of the following should you do?",answers:[{text:"Fix the IAM permissions for the EC2 instance role",isCorrect:!0},{text:"Fix the IAM permissions for the CodeDeploy service role",isCorrect:!1},{text:"Make the S3 bucket public",isCorrect:!1},{text:"Enable CodeDeploy Proxy",isCorrect:!1}],explanation:"Fix the IAM permissions for the EC2 instance roleYou should use an IAM role to manage temporary credentials for applications that run on an EC2 instance. When you use a role, you don't have to distribute longterm credentials (such as a user name and password or access keys) to an EC2 instance. Instead, the role supplies temporary permissions that applications can use when they make calls to other AWS resources. In this case, make sure your role has access to the S3 bucket.Incorrect options:Fix the IAM permissions for the CodeDeploy service role The fact that CodeDeploy deployed the application to EC2 instances tells us that there was no issue between those two. The actual issue is between the EC2 instances and S3.Make the S3 bucket public This is not a good practice, you should strive to provide least privilege access. You may have files in here that should not be allowed public access and you are opening the door to security breaches.Enable CodeDeploy Proxy This is not correct as we don't need to look into CodeDeploy settings but rather between EC2 and S3 permissions.Reference: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switchroleec2.html"},{question:"An ecommerce company has an order processing workflow with several tasks to be done in parallel as well as decision steps to be evaluated for successful processing of the order. All the tasks are implemented via Lambda functions. Which of the following is the BEST solution to meet these business requirements?",answers:[{text:"Use AWS Batch to orchestrate the workflow",isCorrect:!1},{text:"Use AWS Step Functions activities to orchestrate the workflow",isCorrect:!1},{text:"Use AWS Glue to orchestrate the workflow",isCorrect:!1},{text:"Use AWS Step Functions state machines to orchestrate the workflow",isCorrect:!0}],explanation:"Correct option:Use AWS Step Functions state machines to orchestrate the workflowAWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or task, allowing you to scale and change applications quickly.How Step Functions Work: via https://aws.amazon.com/stepfunctions/ The following are key features of AWS Step Functions:Step Functions are based on the concepts of tasks and state machines. You define state machines using the JSONbased Amazon States Language. A state machine is defined by the states it contains and the relationships between them. States are elements in your state machine. Individual states can make decisions based on their input, perform actions, and pass output to other states. In this way, a state machine can orchestrate workflows.Please see this note for a simple example of a State Machine: via https://docs.aws.amazon.com/stepfunctions/latest/dg/amazonstateslanguagestatemachinestructure.html Incorrect options:Use AWS Step Functions activities to orchestrate the workflow In AWS Step Functions, activities are a way to associate code running somewhere (known as an activity worker) with a specific task in a state machine. When a Step Function reaches an activity task state, the workflow waits for an activity worker to poll for a task. For example, an activity worker can be an application running on an Amazon EC2 instance or an AWS Lambda function. AWS Step Functions activities cannot orchestrate a workflow.Use AWS Glue to orchestrate the workflow AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue cannot orchestrate a workflow.Use AWS Batch to orchestrate the workflow AWS Batch runs batch computing jobs on the AWS Cloud. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. AWS Batch cannot orchestrate a workflow.Reference:https://aws.amazon.com/stepfunctions/ https://docs.aws.amazon.com/stepfunctions/latest/dg/amazonstateslanguagestatemachinestructure.html"},{question:"You have been asked by your Team Lead to enable detailed monitoring of the Amazon EC2 instances your team uses. As a Developer working on AWS CLI, which of the below command will you run?",answers:[{text:"aws ec2 runinstances imageid ami09092360 monitoring Enabled=true",isCorrect:!1},{text:"aws ec2 monitorinstances instanceid i1234567890abcdef0",isCorrect:!1},{text:"aws ec2 monitorinstances instanceids i1234567890abcdef0",isCorrect:!0},{text:"aws ec2 runinstances imageid ami09092360 monitoring State=enabled",isCorrect:!1}],explanation:"Correct option:aws ec2 monitorinstances instanceids i1234567890abcdef0 This enables detailed monitoring for a running instance.EC2 detailed monitoring: via https://docs.aws.amazon.com/cli/latest/reference/ec2/monitorinstances.html Incorrect options:aws ec2 runinstances imageid ami09092360 monitoring Enabled=true This syntax is used to enable detailed monitoring when launching an instance from AWS CLI.aws ec2 runinstances imageid ami09092360 monitoring State=enabled This is an invalid syntaxaws ec2 monitorinstances instanceid i1234567890abcdef0 This is an invalid syntaxReferences:https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/usingcloudwatchnew.html https://docs.aws.amazon.com/cli/latest/reference/ec2/runinstances.html"},{question:"You have configured a Network ACL and a Security Group for the load balancer and Amazon EC2 instances to allow inbound traffic on port 80. However, users are still unable to connect to your website after launch. Which additional configuration is required to make the website accessible to all users over the internet?",answers:[{text:"Add a rule to the Network ACLs to allow outbound traffic on ports 32768 61000",isCorrect:!1},{text:"Add a rule to the Network ACLs to allow outbound traffic on ports 1024 65535",isCorrect:!0},{text:"Add a rule to the Network ACLs to allow outbound traffic on ports 1025 5000",isCorrect:!1},{text:"Add a rule to the Security Group allowing outbound traffic on port 80",isCorrect:!1}],explanation:"Add a rule to the Network ACLs to allow outbound traffic on ports 1024 65535A Network Access Control List (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.When you create a custom Network ACL and associate it with a subnet, by default, this custom Network ACL denies all inbound and outbound traffic until you add rules. A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic. Network ACLs are stateless, which means that responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa).The client that initiates the request chooses the ephemeral port range. The range varies depending on the client's operating system. Requests originating from Elastic Load Balancing use ports 102465535. List of ephemeral port ranges:Many Linux kernels (including the Amazon Linux kernel) use ports 3276861000.Requests originating from Elastic Load Balancing use ports 102465535.Windows operating systems through Windows Server 2003 use ports 10255000.Windows Server 2008 and later versions use ports 4915265535.A NAT gateway uses ports 102465535.AWS Lambda functions use ports 102465535.Incorrect options:Add a rule to the Network ACLs to allow outbound traffic on ports 1025 5000 As discussed above, Windows operating systems through Windows Server 2003 use ports 10255000. ELB uses the port range 102465535.Add a rule to the Network ACLs to allow outbound traffic on ports 32768 61000 As discussed above, Linux kernels (including the Amazon Linux kernel) use ports 10255000. ELB uses the port range 102465535.Add a rule to the Security Group allowing outbound traffic on port 80 A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. Security groups act at the instance level, not the subnet level. Security groups are stateful — if you send a request from your instance, the response traffic for that request is allowed to flow in regardless of inbound security group rules. Responses to allowed inbound traffic are allowed to flow out, regardless of outbound rules.References:https://docs.aws.amazon.com/vpc/latest/userguide/vpcnetworkacls.html https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html"},{question:"A highfrequency stock trading firm is migrating their messaging queues from selfmanaged messageoriented middleware systems to Amazon SQS. The development team at the company wants to minimize the costs of using SQS. As a Developer Associate, which of the following options would you recommend to address the given usecase?",answers:[{text:"Use SQS visibility timeout to retrieve messages from your Amazon SQS queues",isCorrect:!1},{text:"Use SQS message timer to retrieve messages from your Amazon SQS queues",isCorrect:!1},{text:"Use SQS long polling to retrieve messages from your Amazon SQS queues",isCorrect:!0},{text:"Use SQS short polling to retrieve messages from your Amazon SQS queues",isCorrect:!1}],explanation:"Use SQS long polling to retrieve messages from your Amazon SQS queuesAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds.Exam Alert:Please review the differences between Short Polling vs Long Polling: via https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsshortandlongpolling.html via https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsshortandlongpolling.htmlIncorrect options:Use SQS short polling to retrieve messages from your Amazon SQS queues With short polling, Amazon SQS sends the response right away, even if the query found no messages. You end up paying more because of the increased number of empty receives.Use SQS visibility timeout to retrieve messages from your Amazon SQS queues Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to retrieve messages from your Amazon SQS queues. This option has been added as a distractor.Use SQS message timer to retrieve messages from your Amazon SQS queues You can use message timers to set an initial invisibility period for a message added to a queue. So, if you send a message with a 60second timer, the message isn't visible to consumers for its first 60 seconds in the queue. The default (minimum) delay for a message is 0 seconds. The maximum is 15 minutes. You cannot use message timer to retrieve messages from your Amazon SQS queues. This option has been added as a distractor.Reference: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsshortandlongpolling.html"},{question:"Your company leverages Amazon CloudFront to provide content via the internet to customers with low latency. Aside from latency, security is another concern and you are looking for help in enforcing endtoend connections using HTTPS so that content is protected. Which of the following options is available for HTTPS in AWS CloudFront?",answers:[{text:"Between clients and CloudFront only",isCorrect:!1},{text:"Between CloudFront and backend only",isCorrect:!1},{text:"Between clients and CloudFront as well as between CloudFront and backend",isCorrect:!0},{text:"Neither between clients and CloudFront nor between CloudFront and backend",isCorrect:!1}],explanation:"Between clients and CloudFront as well as between CloudFront and backendFor web distributions, you can configure CloudFront to require that viewers use HTTPS to request your objects, so connections are encrypted when CloudFront communicates with viewers.Requiring HTTPS for Communication Between Viewers and CloudFront: via https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/usinghttpsviewerstocloudfront.htmlYou also can configure CloudFront to use HTTPS to get objects from your origin, so connections are encrypted when CloudFront communicates with your origin.Requiring HTTPS for Communication Between CloudFront and Your Custom Origin: via https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/usinghttpscloudfronttocustomorigin.htmlIncorrect options:Between clients and CloudFront only This is incorrect as you can choose to require HTTPS between CloudFront and your origin.Between CloudFront and backend only This is incorrect as you can choose to require HTTPS between viewers and CloudFront.Neither between clients and CloudFront nor between CloudFront and backend This is incorrect as you can choose HTTPS settings both for communication between viewers and CloudFront as well as between CloudFront and your origin.References:https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/secureconnectionssupportedviewerprotocolsciphers.html#secureconnectionssupportedcipherscloudfronttoorigin https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/usinghttpsviewerstocloudfront.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/usinghttpscloudfronttocustomorigin.html"},{question:"Your mobile application needs to perform API calls to DynamoDB. You do not want to store AWS secret and access keys onto the mobile devices and need all the calls to DynamoDB made with a different identity per mobile device. Which of the following services allows you to achieve this?",answers:[{text:"Cognito Sync",isCorrect:!1},{text:"IAM",isCorrect:!1},{text:"Cognito User Pools",isCorrect:!1},{text:"Cognito Identity Pools",isCorrect:!0}],explanation:'"Cognito Identity Pools"Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token. Identity pools provide AWS credentials to grant your users access to other AWS services.Cognito Overview: via https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.html Incorrect options:"Cognito User Pools" AWS Cognito User Pools is there to authenticate users for your applications which looks similar to Cognito Identity Pools. The difference is that Identity Pools allows a way to authorize your users to use the various AWS services and User Pools is not about authorizing to AWS services but to provide add signup and signin functionality to web and mobile applications."Cognito Sync" You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status."IAM" This is not a good solution because it would require you to have an IAM user for each mobile device which is not a good practice or manageable way of handling deployment.Exam Alert:Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools: via https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.html Reference: https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.html'},{question:"Your application is deployed automatically using AWS Elastic Beanstalk. Your YAML configuration files are stored in the folder .ebextensions and new files are added or updated often. The DevOps team does not want to redeploy the application every time there are configuration changes, instead, they would rather manage configuration externally, securely, and have it load dynamically into the application at runtime. What option allows you to do this?",answers:[{text:"Use SSM Parameter Store",isCorrect:!0},{text:"Use Environment variables",isCorrect:!1},{text:"Use Stage Variables",isCorrect:!1},{text:"Use S3",isCorrect:!1}],explanation:"Correct option:Use SSM Parameter StoreAWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. For the given usecase, as the DevOps team does not want to redeploy the application every time there are configuration changes, so they can use the SSM Parameter Store to store the configuration externally.Incorrect options:Use Environment variables Environment variables provide another way to specify configuration options and credentials, and can be useful for scripting or temporarily setting a named profile as the default. Your application is not running AWS CLI. Since the usecase requires the configuration to be stored securely, so using Environment variables is ruled out, as these are not encrypted at rest and these are visible in clear text in the AWS Console as well as in the response of some actions of the Elastic BeanStalk API.Use Stage Variables You can use stage variables for managing multiple release stages for API Gateway, this is not what you are looking for here.Use S3 S3 offers the same benefit as the SSM Parameter Store where there are no servers to manage. With S3 you have to set encryption and choose other security options and there are more chances of misconfiguring security if you share your S3 bucket with other objects. You would have to create a custom setup to come close to the parameter store. Use Parameter Store and let AWS handle the rest.Reference: https://docs.aws.amazon.com/systemsmanager/latest/userguide/systemsmanagerparamstore.html"},{question:"A developer is designing an AWS CloudFormation template for deploying Amazon EC2 instances in numerous AWS accounts. The developer needs to select EC2 instances from a list of preapproved instance types. What measures could the developer take to integrate the list of authorized instance types into the CloudFormation template?",answers:[{text:"Configure a pseudo parameter with the list of EC2 instance types as AllowedValues in the CloudFormation template",isCorrect:!1},{text:"Configure a mapping having a list of EC2 instance types as parameters in the CloudFormation template",isCorrect:!1},{text:"Configure a parameter with the list of EC2 instance types as AllowedValues in the CloudFormation template",isCorrect:!0},{text:"Configure separate parameters for each EC2 instance type in the CloudFormation template",isCorrect:!1}],explanation:"Correct option:Configure a parameter with the list of EC2 instance types as AllowedValues in the CloudFormation templateYou can use the Parameters section to customize your templates. Parameters enable you to input custom values to your template each time you create or update a stack.via https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameterssectionstructure.html AllowedValues refers to an array containing the list of values allowed for the parameter. When applied to a parameter of type String, the parameter value must be one of the allowed values. When applied to a parameter of type CommaDelimitedList, each value in the list must be one of the specified allowed values.Incorrect options:Configure separate parameters for each EC2 instance type in the CloudFormation template Creating separate parameters for each instance type is semantically incorrect as the underlying value will point to the same resource but have multiple inputs.Configure a mapping having a list of EC2 instance types as parameters in the CloudFormation template The Mappings section matches a key to a corresponding set of named values. For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. You use the Fn::FindInMap intrinsic function to retrieve values in a map. A mapping is not a list, rather, it consists of key value pairs. You can't include parameters, pseudo parameters, or intrinsic functions in the Mappings section. So, this option is incorrect.Configure a pseudo parameter with the list of EC2 instance types as AllowedValues in the CloudFormation template Pseudo parameters are parameters that are predefined by AWS CloudFormation. You don't declare them in your template. So, this option is incorrect.References:https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameterssectionstructure.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappingssectionstructure.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudoparameterreference.html"},{question:"A data analytics company is processing realtime InternetofThings (IoT) data via Kinesis Producer Library (KPL) and sending the data to a Kinesis Data Streams driven application. The application has halted data processing because of a ProvisionedThroughputExceeded exception. Which of the following actions would help in addressing this issue? (Select two)",answers:[{text:"Use Kinesis enhanced fanout for Kinesis Data Streams",isCorrect:!1},{text:"Increase the number of shards within your data streams to provide enough capacity",isCorrect:!0},{text:"Use Amazon SQS instead of Kinesis Data Streams",isCorrect:!1},{text:"Configure the data producer to retry with an exponential backoff",isCorrect:!0},{text:"Use Amazon Kinesis Agent instead of Kinesis Producer Library (KPL) for sending data to Kinesis Data Streams",isCorrect:!1}],explanation:"Correct option:Configure the data producer to retry with an exponential backoffIncrease the number of shards within your data streams to provide enough capacityAmazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs. You can continuously add various types of data such as clickstreams, application logs, and social media to an Amazon Kinesis data stream from hundreds of thousands of sources.How Kinesis Data Streams Work via https://aws.amazon.com/kinesis/datastreams/ The capacity limits of an Amazon Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a ProvisionedThroughputExceeded exception.If this is due to a temporary rise of the data stream’s input data rate, retry (with exponential backoff) by the data producer will eventually lead to the completion of the requests.If this is due to a sustained rise of the data stream’s input data rate, you should increase the number of shards within your data stream to provide enough capacity for the put data calls to consistently succeed.Incorrect options:Use Amazon Kinesis Agent instead of Kinesis Producer Library (KPL) for sending data to Kinesis Data Streams Kinesis Agent works with data producers. Using Kinesis Agent instead of KPL will not help as the constraint is the capacity limit of the Kinesis Data Stream.Use Amazon SQS instead of Kinesis Data Streams This is a distractor as using SQS will not help address the ProvisionedThroughputExceeded exception for the Kinesis Data Stream. This option does not address the issues in the usecase.Use Kinesis enhanced fanout for Kinesis Data Streams You should use enhanced fanout if you have, or expect to have, multiple consumers retrieving data from a stream in parallel. Therefore, using enhanced fanout will not help address the ProvisionedThroughputExceeded exception as the constraint is the capacity limit of the Kinesis Data Stream.Please review this note for more details on enhanced fanout for Kinesis Data Streams: via https://aws.amazon.com/kinesis/datastreams/faqs/References:https://aws.amazon.com/kinesis/datastreams/ https://aws.amazon.com/kinesis/datastreams/faqs/"},{question:"A junior developer has been asked to configure access to an Amazon EC2 instance hosting a web application. The developer has configured a new security group to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules. A custom Network Access Control List (NACL) connected with the instance's subnet is configured to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules. Which of the following solutions would you suggest if the EC2 instance needs to accept and respond to requests from the internet?",answers:[{text:"An outbound rule on the security group has to be configured, to allow the response to be sent to the client on the HTTP port",isCorrect:!1},{text:"An outbound rule must be added to the Network ACL (NACL) to allow the response to be sent to the client on the ephemeral port range",isCorrect:!0},{text:"The configuration is complete on the EC2 instance for accepting and responding to requests",isCorrect:!1},{text:"Outbound rules need to be configured both on the security group and on the NACL for sending responses to the Internet Gateway",isCorrect:!1}],explanation:"Correct option:An outbound rule must be added to the Network ACL (NACL) to allow the response to be sent to the client on the ephemeral port rangeSecurity groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic. By default, each custom Network ACL denies all inbound and outbound traffic until you add rules.To enable the connection to a service running on an instance, the associated network ACL must allow both: 1. Inbound traffic on the port that the service is listening on 2. Outbound traffic to ephemeral portsWhen a client connects to a server, a random port from the ephemeral port range (102465535) becomes the client's source port.The designated ephemeral port becomes the destination port for return traffic from the service. Outbound traffic to the ephemeral port must be allowed in the network ACL.Incorrect options:The configuration is complete on the EC2 instance for accepting and responding to requests As explained above, this is an incorrect statement.An outbound rule on the security group has to be configured, to allow the response to be sent to the client on the HTTP port Security groups are stateful. Therefore you don't need a rule that allows responses to inbound traffic.Outbound rules need to be configured both on the security group and on the NACL for sending responses to the Internet Gateway* Security Groups are stateful. Hence, return traffic is automatically allowed, so there is no need to configure an outbound rule on the security group.References:https://aws.amazon.com/premiumsupport/knowledgecenter/resolveconnectionsgaclinbound/ https://docs.aws.amazon.com/vpc/latest/userguide/vpcnetworkacls.html#naclephemeralports"},{question:"You have a Javabased application running on EC2 instances loaded with AWS CodeDeploy agents. You are considering different options for deployment, one is the flexibility that allows for incremental deployment of your new application versions and replaces existing versions in the EC2 instances. The other option is a strategy in which an Auto Scaling group is used to perform a deployment. Which of the following options will allow you to deploy in this manner? (Select two)",answers:[{text:"Pilot Light Deployment",isCorrect:!1},{text:"Cattle Deployment",isCorrect:!1},{text:"Warm Standby Deployment",isCorrect:!1},{text:"Inplace Deployment",isCorrect:!0},{text:"Blue/green Deployment",isCorrect:!0}],explanation:'Inplace DeploymentThe application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete.Blue/green DeploymentWith a blue/green deployment, you provision a new set of instances on which CodeDeploy installs the latest version of your application. CodeDeploy then reroutes load balancer traffic from an existing set of instances running the previous version of your application to the new set of instances running the latest version. After traffic is rerouted to the new instances, the existing instances can be terminated.CodeDeploy Deployment Types: via https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.htmlIncorrect options:Cattle Deployment This is a good option if you have cattle in a farmWarm Standby Deployment This is not a valid CodeDeploy deployment option. The term "Warm Standby" is used to describe a Disaster Recovery scenario in which a scaleddown version of a fully functional environment is always running in the cloud.Pilot Light Deployment This is not a valid CodeDeploy deployment option. "Pilot Light" is a Disaster Recovery approach where you simply replicate part of your IT structure for a limited set of core services so that the AWS cloud environment seamlessly takes over in the event of a disaster.References:https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html https://aws.amazon.com/blogs/publicsector/rapidlyrecovermissioncriticalsystemsinadisaster/'},{question:"You have a workflow process that pulls code from AWS CodeCommit and deploys to EC2 instances associated with tag group ProdBuilders. You would like to configure the instances to archive no more than two application revisions to conserve disk space. Which of the following will allow you to implement this?",answers:[{text:"AWS CloudWatch Log Agent",isCorrect:!1},{text:"CodeDeploy Agent",isCorrect:!0},{text:"Integrate with AWS CodePipeline",isCorrect:!1},{text:"Have a load balancer in front of your instances",isCorrect:!1}],explanation:'Correct option:"CodeDeploy Agent"The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The CodeDeploy agent archives revisions and log files on instances. The CodeDeploy agent cleans up these artifacts to conserve disk space. You can use the :max_revisions: option in the agent configuration file to specify the number of application revisions to the archive by entering any positive integer. CodeDeploy also archives the log files for those revisions. All others are deleted, except for the log file of the last successful deployment.More info here: via https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeployagent.html Incorrect options:AWS CloudWatch Log Agent The CloudWatch Logs agent provides an automated way to send log data to CloudWatch Logs from Amazon EC2 instances. This is an incorrect choice for the current use case.Integrate with AWS CodePipeline AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodeCommit and CodePipeline are already integrated services. CodePipeline cannot help in version control and management of archives on an EC2 instance.Have a load balancer in front of your instances Load Balancer helps balance incoming traffic across different EC2 instances. It is an incorrect choice for the current use case.References:https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeployagent.html https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html'},{question:"A communication platform serves millions of customers and deploys features in a production environment on AWS via CodeDeploy. You are reviewing scripts for the deployment process located in the AppSpec file. Which of the following options lists the correct order of lifecycle events?",answers:[{text:"ValidateService => BeforeInstall =>DownloadBundle => ApplicationStart",isCorrect:!1},{text:"BeforeInstall => ApplicationStart => DownloadBundle => ValidateService",isCorrect:!1},{text:"BeforeInstall => ValidateService =>DownloadBundle => ApplicationStart",isCorrect:!1},{text:"DownloadBundle => BeforeInstall => ApplicationStart => ValidateService",isCorrect:!0}],explanation:"DownloadBundle => BeforeInstall => ApplicationStart => ValidateServiceAWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your onpremises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications.You can specify one or more scripts to run in a hook. Each hook for a lifecycle event is specified with a string on a separate line.Please review the correct order of lifecycle events: via https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfilestructurehooks.html#referenceappspecfilestructurehooksrunorderIncorrect options:BeforeInstall => ApplicationStart => DownloadBundle => ValidateServiceValidateService => BeforeInstall =>DownloadBundle => ApplicationStartBeforeInstall => ValidateService =>DownloadBundle => ApplicationStartThese three options contradict the details provided in the explanation above, so these options are not correct.Reference: https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfilestructurehooks.html#referenceappspecfilestructurehooksrunorder"},{question:"A developer wants to securely store an access token that allows a transactionprocessing application running on Amazon EC2 instances to authenticate and send a chat message (via the chat API) to the company's support team when an invalid transaction is detected. While minimizing management overhead, the chat API access token must be encrypted both at rest and in transit, and also be accessible from other AWS accounts. What is the most efficient solution to address this scenario?",answers:[{text:"Store AWS KMS encrypted access token in a DynamoDB table and configure a resourcebased policy for the DynamoDB table to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the DynamoDB table. Fetch the token from the Dynamodb table and then use the decrypted access token to send the message to the chat",isCorrect:!1},{text:"Leverage SSEKMS to store the access token as an encrypted object on S3 and configure a resourcebased policy for the S3 bucket to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the S3 object. Fetch the token from S3 and then use the decrypted access token to send the message to the chat",isCorrect:!1},{text:"Leverage AWS Secrets Manager with an AWS KMS customermanaged key to store the access token as a secret and configure a resourcebased policy for the secret to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Secrets Manager. Fetch the token from Secrets Manager and then use the decrypted access token to send the message to the chat",isCorrect:!0},{text:"Leverage AWS Systems Manager Parameter Store with an AWS KMS customermanaged key to store the access token as a SecureString parameter and configure a resourcebased policy for the parameter to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Parameter Store. Fetch the token from Parameter Store using the with decryption flag and then use the decrypted access token to send the message to the chat",isCorrect:!1}],explanation:"Correct option:Leverage AWS Secrets Manager with an AWS KMS customermanaged key to store the access token as a secret and configure a resourcebased policy for the secret to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Secrets Manager. Fetch the token from Secrets Manager and then use the decrypted access token to send the message to the chatAWS Secrets Manager is an AWS service that encrypts and stores your secrets, and transparently decrypts and returns them to you in plaintext. It's designed especially to store application secrets, such as login credentials, that change periodically and should not be hardcoded or stored in plaintext in the application. In place of hardcoded credentials or table lookups, your application calls Secrets Manager.Secrets Manager also supports features that periodically rotate the secrets associated with commonly used databases. It always encrypts newly rotated secrets before they are stored.Secrets Manager integrates with AWS Key Management Service (AWS KMS) to encrypt every version of every secret value with a unique data key that is protected by an AWS KMS key. This integration protects your secrets under encryption keys that never leave AWS KMS unencrypted. It also enables you to set custom permissions on the KMS key and audit the operations that generate, encrypt, and decrypt the data keys that protect your secrets.To grant permission to retrieve secret values, you can attach policies to secrets or identities.via https://docs.aws.amazon.com/secretsmanager/latest/userguide/authandaccess_examples.html For the given use case, you can use the resourcebased policy to the secret to allow access from other accounts. Then you need to update the IAM role of the EC2 instances with permissions to access Secrets Manager which will retrieve the token from Secrets Manager and use the decrypted access token to send the message to the support team via the chat API.Incorrect options:Leverage AWS Systems Manager Parameter Store with an AWS KMS customermanaged key to store the access token as a SecureString parameter and configure a resourcebased policy for the parameter to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access Parameter Store. Fetch the token from Parameter Store using the with decryption flag and then use the decrypted access token to send the message to the chat You cannot use a resourcebased policy with a parameter in the Parameter Store. Parameter Store supports parameter policies that are available for parameters that use the advanced parameters tier. Parameter policies help you manage a growing set of parameters by allowing you to assign specific criteria to a parameter such as an expiration date or time to live. Parameter policies are especially helpful in forcing you to update or delete passwords and configuration data stored in Parameter Store, a capability of AWS Systems Manager. So this option is incorrect.Store AWS KMS encrypted access token in a DynamoDB table and configure a resourcebased policy for the DynamoDB table to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the DynamoDB table. Fetch the token from the Dynamodb table and then use the decrypted access token to send the message to the chat You should note that DynamoDB does not support resourcebased policies. Moreover, it's a security bad practice to keep sensitive access credentials in code, database or a flat file on a file system or object storage. Therefore, this option is incorrect.Leverage SSEKMS to store the access token as an encrypted object on S3 and configure a resourcebased policy for the S3 bucket to allow access from other accounts. Modify the IAM role of the EC2 instances with permissions to access the S3 object. Fetch the token from S3 and then use the decrypted access token to send the message to the chat It is considered a security bad practice to keep sensitive access credentials in code, database, or a flat file on a file system or object storage. Therefore, this option is incorrect.References:https://docs.aws.amazon.com/secretsmanager/latest/userguide/authandaccess_examples.html https://docs.aws.amazon.com/systemsmanager/latest/userguide/parameterstorepolicies.html"},{question:"A developer is working on an AWS Lambda function that reads data from Amazon S3 objects and writes the data to an Amazon DynamoDB table. Although the function triggers successfully from an S3 event notification upon object creation, it encounters a failure while attempting to write data to the DynamoDB table. What is the probable reason for the failure?",answers:[{text:"The Lambda function's provisioned concurrency limit has been exceeded",isCorrect:!1},{text:"DynamoDB table does not have a Gateway VPC Endpoint, which is required by the Lambda function for a successful write",isCorrect:!1},{text:"The Lambda function's reserved concurrency limit has been exceeded",isCorrect:!1},{text:"The Lambda function does not have IAM permissions to write to DynamoDB",isCorrect:!0}],explanation:"The Lambda function does not have IAM permissions to write to DynamoDBYou need to use an identitybased policy that allows read and write access to a specific Amazon DynamoDB table. To use this policy, attach the policy to a Lambda service role. A service role is a role that you create in your account to allow a service to perform actions on your behalf. That service role must include AWS Lambda as the principal in the trust policy. The role is then used to grant a Lambda function access to a DynamoDB table. By using an IAM policy and role to control access, you don’t need to embed credentials in code and can tightly control which services the Lambda function can access.Incorrect options:The Lambda function's provisioned concurrency limit has been exceededThe Lambda function's reserved concurrency limit has been exceededReserved concurrency – Reserved concurrency guarantees the maximum number of concurrent instances for the function. When a function has reserved concurrency, no other function can use that concurrency. There is no charge for configuring reserved concurrency for a function.Provisioned concurrency – Provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond immediately to your function's invocations. Note that configuring provisioned concurrency incurs charges to your AWS account.Neither reserved concurrency nor provisioned concurrency has any relevance to the given use case. Both options have been added as distractors.DynamoDB table does not have a Gateway VPC Endpoint, which is required by the Lambda function for a successful write Gateway endpoints provide reliable connectivity to Amazon S3 and DynamoDB without requiring an internet gateway or a NAT device for your VPC. Gateway endpoints do not enable AWS PrivateLink. This option acts as a distractor since the Lambda function is not provisioned within a VPC by default, so there is no need of a Gateway VPC Endpoint to access DynamoDB.References:https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_lambdaaccessdynamodb.html https://aws.amazon.com/blogs/security/howtocreateanawsiampolicytograntawslambdaaccesstoanamazondynamodbtable/"},{question:"Your company manages MySQL databases on EC2 instances to have full control. Applications on other EC2 instances managed by an ASG make requests to these databases to get information that displays data on dashboards viewed on mobile phones, tablets, and web browsers. Your manager would like to scale your Auto Scaling group based on the number of requests per minute. How can you achieve this?",answers:[{text:"You enable detailed monitoring and use that to scale your ASG",isCorrect:!1},{text:"Attach an Elastic Load Balancer",isCorrect:!1},{text:"Attach additional Elastic File Storage",isCorrect:!1},{text:"You create a CloudWatch custom metric and build an alarm to scale your ASG",isCorrect:!0}],explanation:"You create a CloudWatch custom metric and build an alarm to scale your ASGHere we need to scale on the metric \"number of requests per minute\", which is a custom metric we need to create, as it's not readily available in CloudWatch.Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a highresolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.Incorrect options:Attach an Elastic Load Balancer This is not what you need for autoscaling. An Elastic Load Balancer distributes workloads across multiple compute resources and checks your instances' health status to name a few, but it does not automatically increase and decrease the number of instances based on the application requirement.Attach additional Elastic File Storage This is a file storage service designed for performance. Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and onpremises resources. It is built to scale ondemand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth. This cannot be used to facilitate autoscaling.How EFS Works: via https://aws.amazon.com/efs/You enable detailed monitoring and use that to scale your ASG The detailed monitoring metrics won't provide information about database /applicationlevel requests per minute, so this option is not correct.Reference: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html"},{question:"An IT company has its serverless stack integrated with AWS XRay. The developer at the company has noticed a high volume of data going into XRay and the AWS monthly usage charges have skyrocketed as a result. The developer has requested changes to mitigate the issue. As a Developer Associate, which of the following solutions would you recommend to obtain tracing trends while reducing costs with minimal disruption?",answers:[{text:"Custom configuration for the XRay agents",isCorrect:!1},{text:"Implement a network sampling rule",isCorrect:!1},{text:"Enable XRay sampling",isCorrect:!0},{text:"Use Filter Expressions in the XRay console",isCorrect:!1}],explanation:"AWS XRay helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With XRay, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. XRay provides an endtoend view of requests as they travel through your application, and shows a map of your application’s underlying components.How XRay Works: via https://aws.amazon.com/xray/ Enable XRay samplingTo ensure efficient tracing and provide a representative sample of the requests that your application serves, the XRay SDK applies a sampling algorithm to determine which requests get traced. By default, the XRay SDK records the first request each second, and five percent of any additional requests. XRay sampling is enabled directly from the AWS console, hence your application code does not need to change.You can also customize the XRay sampling rules: via https://docs.aws.amazon.com/xray/latest/devguide/xrayconsolesampling.htmlIncorrect options:Use Filter Expressions in the XRay console When you choose a time period of traces to view in the XRay console, you might get more results than the console can display. You can narrow the results to just the traces that you want to find by using a filter expression. This option is not correct because it does not reduce the volume of data sent into the XRay console.via https://docs.aws.amazon.com/xray/latest/devguide/xrayconsolefilters.htmlCustom configuration for the XRay agents You cannot do a custom configuration, instead you can do custom sampling rules. So this option is incorrect.Implement a network sampling rule This option has been added as a distractor.References:https://aws.amazon.com/xray/ https://docs.aws.amazon.com/xray/latest/devguide/xrayconsolesampling.html"},{question:"A financial services company with over 10,000 employees has hired you as the new Senior Developer. Initially caching was enabled to reduce the number of calls made to all API endpoints and improve the latency of requests to the company's API Gateway. For testing purposes, you would like to invalidate caching for the API clients to get the most recent responses. Which of the following should you do?",answers:[{text:"Using the request parameter ?cachecontrolmaxage=0",isCorrect:!1},{text:"Use the Request parameter: ?bypass_cache=1",isCorrect:!1},{text:"Using the Header CacheControl: maxage=0",isCorrect:!0},{text:"Using the Header BypassCache=1",isCorrect:!1}],explanation:"Using the Header CacheControl: maxage=0A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the CacheControl: maxage=0 header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint.via https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaycaching.html#invalidatemethodcaching Incorrect options:Use the Request parameter: ?bypass_cache=1 Method parameters take query string but this is not one of them.Using the Header BypassCache=1 This is a madeup option.Using the request parameter ?cachecontrolmaxage=0 To invalidate cache it requires a header and not a request parameter.Reference: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaycaching.html#invalidatemethodcaching"},{question:"A large firm stores its static data assets on Amazon S3 buckets. Each service line of the firm has its own AWS account. For a business use case, the Finance department needs to give access to their S3 bucket's data to the Human Resources department. Which of the below options is NOT feasible for crossaccount access of S3 bucket objects?",answers:[{text:"Use Resourcebased policies and AWS Identity and Access Management (IAM) policies for programmaticonly access to S3 bucket objects",isCorrect:!1},{text:"Use Crossaccount IAM roles for programmatic and console access to S3 bucket objects",isCorrect:!1},{text:"Use Access Control List (ACL) and IAM policies for programmaticonly access to S3 bucket objects",isCorrect:!1},{text:"Use IAM roles and resourcebased policies delegate access across accounts within different partitions via programmatic access only",isCorrect:!0}],explanation:"Correct option:Use IAM roles and resourcebased policies delegate access across accounts within different partitions via programmatic access only This statement is incorrect and hence the right choice for this question. IAM roles and resourcebased policies delegate access across accounts only within a single partition. For example, assume that you have an account in US West (N. California) in the standard aws partition. You also have an account in China (Beijing) in the awscn partition. You can't use an Amazon S3 resourcebased policy in your account in China (Beijing) to allow access for users in your standard AWS account.Incorrect options:Use Resourcebased policies and AWS Identity and Access Management (IAM) policies for programmaticonly access to S3 bucket objects Use bucket policies to manage crossaccount control and audit the S3 object's permissions. If you apply a bucket policy at the bucket level, you can define who can access (Principal element), which objects they can access (Resource element), and how they can access (Action element). Applying a bucket policy at the bucket level allows you to define granular access to different objects inside the bucket by using multiple policies to control access. You can also review the bucket policy to see who can access objects in an S3 bucket.Use Access Control List (ACL) and IAM policies for programmaticonly access to S3 bucket objects Use object ACLs to manage permissions only for specific scenarios and only if ACLs meet your needs better than IAM and S3 bucket policies. Amazon S3 ACLs allow users to define only the following permissions sets: READ, WRITE, READ_ACP, WRITE_ACP, and FULL_CONTROL. You can use only an AWS account or one of the predefined Amazon S3 groups as a grantee for the Amazon S3 ACL.Use Crossaccount IAM roles for programmatic and console access to S3 bucket objects Not all AWS services support resourcebased policies. This means that you can use crossaccount IAM roles to centralize permission management when providing crossaccount access to multiple services. Using crossaccount IAM roles simplifies provisioning crossaccount access to S3 objects that are stored in multiple S3 buckets, removing the need to manage multiple policies for S3 buckets. This method allows crossaccount access to objects that are owned or uploaded by another AWS account or AWS services. If you don't use crossaccount IAM roles, the object ACL must be modified.References:https://docs.aws.amazon.com/AmazonS3/latest/dev/examplewalkthroughsmanagingaccessexample3.html https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_crossaccountwithroles.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compareresourcepolicies.html https://aws.amazon.com/premiumsupport/knowledgecenter/crossaccountaccesss3/"},{question:"You team maintains a public API Gateway that is accessed by clients from another domain. Usage has been consistent for the last few months but recently it has more than doubled. As a result, your costs have gone up and would like to prevent other unauthorized domains from accessing your API. Which of the following actions should you take?",answers:[{text:"Restrict access by using CORS",isCorrect:!0},{text:"Use Accountlevel throttling",isCorrect:!1},{text:"Assign a Security Group to your API Gateway",isCorrect:!1},{text:"Use Mapping Templates",isCorrect:!1}],explanation:"Correct option:Restrict access by using CORS Crossorigin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. When your API's resources receive requests from a domain other than the API's own domain and you want to restrict servicing these requests, you must disable crossorigin resource sharing (CORS) for selected methods on the resource.Incorrect options:Use Accountlevel throttling To prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API. By default, API Gateway limits the steadystate request rate to 10,000 requests per second (rps). It limits the burst (that is, the maximum bucket size) to 5,000 requests across all APIs within an AWS account. This is Accountlevel throttling. As you see, this is about limit on the number of requests and is not a suitable answer for the current scenario.Use Mapping Templates A mapping template is a script expressed in Velocity Template Language (VTL) and applied to the payload using JSONPath expressions. Mapping templates help format/structure the data in a way that it is easily readable, unlike a server response that might always be easy to ready. Mapping Templates have nothing to do with access and are not useful for the current scenario.Assign a Security Group to your API Gateway API Gateway does not use security groups but uses resource policies, which are JSON policy documents that you attach to an API to control whether a specified principal (typically an IAM user or role) can invoke the API. You can restrict IP address using this, the downside being, an IP address can be changed by the accessing user. So, this is not an optimal solution for the current use case.References:https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html https://docs.aws.amazon.com/apigateway/latest/developerguide/httpapiprotect.html https://docs.aws.amazon.com/apigateway/latest/developerguide/restapidatatransformations.html"},{question:"A company uses Amazon RDS as its database. For improved user experience, it has been decided that a highly reliable fullymanaged caching layer has to be configured in front of RDS. Which of the following is the right choice, keeping in mind that cache content regeneration is a costly activity?",answers:[{text:"Implement Amazon ElastiCache Memcached",isCorrect:!1},{text:"Implement Amazon ElastiCache Redis in Cluster Mode",isCorrect:!0},{text:"Migrate the database to Amazon Redshift",isCorrect:!1},{text:"Install Redis on an Amazon EC2 instance",isCorrect:!1}],explanation:'Correct option:Implement Amazon ElastiCache Redis in ClusterMode One can leverage ElastiCache for Redis with cluster mode enabled to enhance reliability and availability with little change to your existing workload. Cluster mode comes with the primary benefit of horizontal scaling of your Redis cluster, with almost zero impact on the performance of the cluster.When building production workloads, you should consider using a configuration with replication, unless you can easily recreate your data. Enabling ClusterMode provides a number of additional benefits in scaling your cluster. In short, it allows you to scale in or out the number of shards (horizontal scaling) versus scaling up or down the node type (vertical scaling). This means that ClusterMode can scale to very large amounts of storage (potentially 100s of terabytes) across up to 90 shards, whereas a single node can only store as much data in memory as the instance type has capacity for.Redis Cluster config: via https://aws.amazon.com/blogs/database/workwithclustermodeonamazonelasticacheforredis/ Incorrect options:Install Redis on an Amazon EC2 instance It is possible to install Redis directly onto Amazon EC2 instance. But, unlike ElastiCache for Redis, which is a managed service, you will need to maintain and manage your Redis installation.Implement Amazon ElastiCache Memcached Redis and Memcached are popular, opensource, inmemory data stores. Although they are both easy to use and offer high performance, there are important differences to consider when choosing an engine. Memcached is designed for simplicity while Redis offers a rich set of features that make it effective for a wide range of use cases. Redis offers snapshots facility, replication, and supports transactions, which Memcached cannot and hence ElastiCache Redis is the right choice for our use case.Migrate the database to Amazon Redshift Amazon Redshift belongs to "Big Data as a Service" cloud facility, while Redis can be primarily classified under "InMemory Databases". "Data Warehousing" is the primary reason why developers consider Amazon Redshift over the competitors, whereas "Performance" is the key factor in picking Redis.References:https://aws.amazon.com/blogs/database/workwithclustermodeonamazonelasticacheforredis/ https://aws.amazon.com/elasticache/redisvsmemcached/'},{question:"A senior cloud engineer designs and deploys online fraud detection solutions for credit card companies processing millions of transactions daily. The Elastic Beanstalk application sends files to Amazon S3 and then sends a message to an Amazon SQS queue containing the path of the uploaded file in S3. The engineer wants to postpone the delivery of any new messages to the queue for at least 10 seconds. Which SQS feature should the engineer leverage?",answers:[{text:"Use visibility timeout parameter",isCorrect:!1},{text:"Implement applicationside delay",isCorrect:!1},{text:"Use DelaySeconds parameter",isCorrect:!0},{text:"Enable LongPolling",isCorrect:!1}],explanation:"Use DelaySeconds parameterAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, besteffort ordering, and atleastonce delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.Delay queues let you postpone the delivery of new messages to a queue for several seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.via https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsdelayqueues.html Incorrect options:Implement applicationside delay You can customize your application to delay sending messages but it is not a robust solution. You can run into a scenario where your application crashes before sending a message, then that message would be lost.Use visibility timeout parameter Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to postpone the delivery of new messages to the queue for a few seconds.Enable LongPolling Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds. You cannot use LongPolling to postpone the delivery of new messages to the queue for a few seconds.Reference: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsdelayqueues.html"},{question:"You have moved your onpremise infrastructure to AWS and are in the process of configuring an AWS Elastic Beanstalk deployment environment for production, development, and testing. You have configured your production environment to use a rolling deployment to prevent your application from becoming unavailable to users. For the development and testing environment, you would like to deploy quickly and are not concerned about downtime. Which of the following deployment policies meet your needs?",answers:[{text:"All at once",isCorrect:!0},{text:"Rolling with additional batches",isCorrect:!1},{text:"Immutable",isCorrect:!1},{text:"Rolling",isCorrect:!1}],explanation:"All at onceThis is the quickest deployment method. Suitable if you can accept a short loss of service, and if quick deployments are important to you. With this method, Elastic Beanstalk deploys the new application version to each instance. Then, the web proxy or application server might need to restart. As a result, your application might be unavailable to users (or have low availability) for a short time.Overview of Elastic Beanstalk Deployment Policies: via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html Incorrect options:Rolling With this method, your application is deployed to your environment one batch of instances at a time. Most bandwidth is retained throughout the deployment. Avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. Suitable if you can't accept any period of completely lost service.Rolling with additional batches With this method, Elastic Beanstalk launches an extra batch of instances, then performs a rolling deployment. Launching the extra batch takes time, and ensures that the same bandwidth is retained throughout the deployment. This policy also avoids any reduced availability, although at a cost of an even longer deployment time compared to the Rolling method. Finally, this option is suitable if you must maintain the same bandwidth throughout the deployment.Immutable A slower deployment method, that ensures your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails. With this method, Elastic Beanstalk performs an immutable update to deploy your application. In an immutable update, a second Auto Scaling group is launched in your environment and the new version serves traffic alongside the old version until the new instances pass health checks. Reference: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.deployexistingversion.html"},{question:"A digital marketing company has its website hosted on an Amazon S3 bucket A. The development team notices that the web fonts that are hosted on another S3 bucket B are not loading on the website. Which of the following solutions can be used to address this issue?",answers:[{text:"Configure CORS on the bucket B that is hosting the web fonts to allow Bucket A origin to make the requests",isCorrect:!0},{text:"Enable versioning on both the buckets to facilitate correct functioning of the website",isCorrect:!1},{text:"Update bucket policies on both bucket A and bucket B to allow successful loading of the web fonts on the website",isCorrect:!1},{text:"Configure CORS on the bucket A that is hosting the website to allow any origin to respond to requests",isCorrect:!1}],explanation:"Correct option:Configure CORS on the bucket B that is hosting the web fonts to allow Bucket A origin to make the requestsCrossorigin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain.To configure your bucket to allow crossorigin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support for each origin, and other operationspecific information.For the given usecase, you would create a in for bucket B to allow access from the S3 website origin hosted on bucket A.Please see this note for more details: via https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html Incorrect options:Enable versioning on both the buckets to facilitate the correct functioning of the website This option is a distractor and versioning will not help to address the web fonts loading issue on the website.Update bucket policies on both bucket A and bucket B to allow successful loading of the web fonts on the website It's not the bucket policies but the CORS configuration on bucket B that needs to be updated to allow web fonts to be loaded on the website. Updating bucket policies will not help to address the web fonts loading issue on the website.Configure CORS on the bucket A that is hosting the website to allow any origin to respond to requests The CORS configuration needs to be updated on bucket B to allow web fonts to be loaded on the website hosted on bucket A. So this option in incorrect.Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html"},{question:"A company is using a Border Gateway Protocol (BGP) based AWS VPN connection to connect from its onpremises data center to Amazon EC2 instances in the company's account. The development team can access an EC2 instance in subnet A but is unable to access an EC2 instance in subnet B in the same VPC. Which logs can be used to verify whether the traffic is reaching subnet B?",answers:[{text:"Subnet logs",isCorrect:!1},{text:"BGP logs",isCorrect:!1},{text:"VPN logs",isCorrect:!1},{text:"VPC Flow Logs",isCorrect:!0}],explanation:"Correct option:VPC Flow Logs VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. After you've created a flow log, you can retrieve and view its data in the chosen destination.You can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored.Flow log data for a monitored network interface is recorded as flow log records, which are log events consisting of fields that describe the traffic flow.To create a flow log, you specify:The resource for which to create the flow logThe type of traffic to capture (accepted traffic, rejected traffic, or all traffic)The destinations to which you want to publish the flow log dataIncorrect options:VPN logsSubnet logsBGP logsThese three options are incorrect and have been added as distractors.Reference: https://docs.aws.amazon.com/vpc/latest/userguide/flowlogs.html"},{question:"A developer wants a seamless ability to return to older versions of a Lambda function that is being deployed. Which of the following solutions offers the LEAST operational overhead?",answers:[{text:"Use CodeDeploy to configure blue/green deployments for the different Lambda function versions",isCorrect:!1},{text:"Use a Lambda function alias that can point to the different versions",isCorrect:!0},{text:"Use Lambda function layers that can point to the different versions",isCorrect:!1},{text:"Use a Route 53 weighted policy that can point to the different Lambda function versions",isCorrect:!1}],explanation:"Use a Lambda function alias that can point to the different versionsYou can use versions to manage the deployment of your functions. For example, you can publish a new version of a function for beta testing without affecting users of the stable production version. Lambda creates a new version of your function each time that you publish the function. The new version is a copy of the unpublished version of the function.By publishing a version of your function, you can store your code and configuration as a separate resource that cannot be changed.A Lambda alias is like a pointer to a specific function version. Users can access the function version using the alias Amazon Resource Name (ARN). Each alias has a unique ARN. An alias can point only to a function version, not to another alias. You can update an alias to point to the different versions of the Lambda function.Incorrect options:Use a Route 53 weighted policy that can point to the different Lambda function versions This option is a distractor, as Route 53 cannot be used for the given use case. Route 53 weighted policy lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software.Use CodeDeploy to configure blue/green deployments for the different Lambda function versions A deployment to the AWS Lambda compute platform is always a blue/green deployment. You do not specify a deployment type option. When you deploy to an AWS Lambda compute platform, the deployment configuration specifies the way traffic is shifted to the new Lambda function versions in your application. You can shift traffic using a canary, linear, or allatonce deployment configuration. Once deployed, you cannot go back to the previous versions of your Lambda function. So this option is incorrect.Use Lambda function layers that can point to the different versions Lambda layers provide a convenient way to package libraries and other dependencies that you can use with your Lambda functions. Using layers reduces the size of uploaded deployment archives and makes it faster to deploy your code. You cannot use the Lambda function layers to point to the different versions of the Lambda function.References:https://docs.aws.amazon.com/lambda/latest/dg/configurationaliases.html https://docs.aws.amazon.com/lambda/latest/dg/configurationversions.html https://docs.aws.amazon.com/codedeploy/latest/userguide/deploymentconfigurations.html#deploymentconfigurationlambda https://docs.aws.amazon.com/lambda/latest/dg/configurationlayers.html"},{question:"A company has a cloud system in AWS with components that send and receive messages using SQS queues. While reviewing the system you see that it processes a lot of information and would like to be aware of any limits of the system. Which of the following represents the maximum number of messages that can be stored in an SQS queue?",answers:[{text:"10000000",isCorrect:!1},{text:"100000",isCorrect:!1},{text:"no limit",isCorrect:!0},{text:"10000",isCorrect:!1}],explanation:'Correct option:"no limit": There are no message limits for storing in SQS, but \'inflight messages\' do have limits. Make sure to delete messages after you have processed them. There can be a maximum of approximately 120,000 inflight messages (received from a queue by a consumer, but not yet deleted from the queue).Incorrect options:"10000""100000""10000000"These three options contradict the details provided in the explanation above, so these are incorrect.Reference:https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqslimits.htmll'},{question:"A .NET developer team works with many ASP.NET web applications that use EC2 instances to host them on IIS. The deployment process needs to be configured so that multiple versions of the application can run in AWS Elastic Beanstalk. One version would be used for development, testing, and another version for load testing. Which of the following methods do you recommend?",answers:[{text:"Create an Application Load Balancer to route based on hostname so you can pass on parameters to the development Elastic Beanstalk environment. Create a file in .ebextensions/ to know how to handle the traffic coming from the ALB",isCorrect:!1},{text:"Define a dev environment with a single instance and a 'load test' environment that has settings close to production environment",isCorrect:!0},{text:"Use only one Beanstalk environment and perform configuration changes using an Ansible script",isCorrect:!1},{text:"You cannot have multiple development environments in Elastic Beanstalk, just one development and one production environment",isCorrect:!1}],explanation:"Define a dev environment with a single instance and a 'load test' environment that has settings close to production environmentAWS Elastic Beanstalk makes it easy to create new environments for your application. You can create and manage separate environments for development, testing, and production use, and you can deploy any version of your application to any environment. Environments can be longrunning or temporary. When you terminate an environment, you can save its configuration to recreate it later.It is common practice to have many environments for the same application. You can deploy multiple environments when you need to run multiple versions of an application. So for the given usecase, you can set up 'dev' and 'load test' environment.via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.managing.html You cannot have multiple development environments in Elastic Beanstalk, just one development, and one production environment Incorrect, use the Create New Environment wizard in the AWS Management Console for BeanStalk to guide you on this.Use only one Beanstalk environment and perform configuration changes using an Ansible script Ansible is an opensource deployment tool that integrates with AWS. It allows us to deploy the infrastructure. Elastic Beanstalk provisions the servers that you need for hosting the application and it also handles multiple environments, so Beanstalk is a better option.Create an Application Load Balancer to route based on hostname so you can pass on parameters to the development Elastic Beanstalk environment. Create a file in .ebextensions/ to know how to handle the traffic coming from the ALB This is not a good design if you need to load test because you will have two versions on the same instances and may not be able to access resources in the system due to the load testing.Reference: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeatures.environments.html"},{question:"An ecommerce company has implemented AWS CodeDeploy as part of its AWS cloud CI/CD strategy. The company has configured automatic rollbacks while deploying a new version of its flagship application to Amazon EC2. What occurs if the deployment of the new version fails?",answers:[{text:"The last known working deployment is automatically restored using the snapshot stored in Amazon S3",isCorrect:!1},{text:"AWS CodePipeline promotes the most recent working deployment with a SUCCEEDED status to production",isCorrect:!1},{text:"CodeDeploy switches the Route 53 alias records back to the known good green deployment and terminates the failed blue deployment",isCorrect:!1},{text:"A new deployment of the last known working version of the application is deployed with a new deployment ID",isCorrect:!0}],explanation:"A new deployment of the last known working version of the application is deployed with a new deployment IDAWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running onpremises. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during deployment, and handles the complexity of updating your applications.CodeDeploy rolls back deployments by redeploying a previously deployed revision of an application as a new deployment. These rolledback deployments are technically new deployments, with new deployment IDs, rather than restored versions of a previous deployment.To roll back an application to a previous revision, you just need to deploy that revision. AWS CodeDeploy keeps track of the files that were copied for the current revision and removes them before starting a new deployment, so there is no difference between redeploy and rollback. However, you need to make sure that the previous revisions are available for rollback.Incorrect options:The last known working deployment is automatically restored using the snapshot stored in Amazon S3 CodeDeploy deployment does not have a snapshot stored on S3, so this option is incorrect.AWS CodePipeline promotes the most recent working deployment with a SUCCEEDED status to production The usecase does not talk about using CodePipeline, so this option just acts as a distractor.CodeDeploy switches the Route 53 alias records back to the known good green deployment and terminates the failed blue deployment The usecase does not talk about the blue/green deployment, so this option has just been added as a distractor.Reference: https://docs.aws.amazon.com/codedeploy/latest/userguide/deploymentsrollbackandredeploy.html"},{question:"As a Fullstack Web Developer, you are involved with every aspect of a company's platform from development with PHP and JavaScript to the configuration of NoSQL databases with Amazon DynamoDB. You are not concerned about your response receiving stale data from your database and need to perform 16 eventually consistent reads per second of 12 KB in size each. How many read capacity units (RCUs) do you need?",answers:[{text:"12",isCorrect:!1},{text:"48",isCorrect:!1},{text:"192",isCorrect:!1},{text:"24",isCorrect:!0}],explanation:"via https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html 24One read capacity unit represents two eventually consistent reads per second, for an item up to 4 KB in size. So that means that for an item of 12KB in size, we need 3 RCU (12 KB / 4 KB) for two eventually consistent reads per second. As we need 16 eventually consistent reads per second, we need 3 * (16 / 2) = 24 RCU.Incorrect options:1219248These three options contradict the details provided in the explanation above, so these are incorrect.Reference: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html"},{question:"Your team lead has asked you to learn AWS CloudFormation to create a collection of related AWS resources and provision them in an orderly fashion. You decide to provide AWSspecific parameter types to catch invalid values. When specifying parameters which of the following is not a valid Parameter type?",answers:[{text:"CommaDelimitedList",isCorrect:!1},{text:"AWS::EC2::KeyPair::KeyName",isCorrect:!1},{text:"String",isCorrect:!1},{text:"DependentParameter",isCorrect:!0}],explanation:"Correct option:AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and thirdparty resources and provision them in an orderly and predictable fashion.How CloudFormation Works: via https://aws.amazon.com/cloudformation/Parameter types enable CloudFormation to validate inputs earlier in the stack creation process.CloudFormation currently supports the following parameter types:String – A literal stringNumber – An integer or floatList – An array of integers or floatsCommaDelimitedList – An array of literal strings that are separated by commasAWS::EC2::KeyPair::KeyName – An Amazon EC2 key pair nameAWS::EC2::SecurityGroup::Id – A security group IDAWS::EC2::Subnet::Id – A subnet IDAWS::EC2::VPC::Id – A VPC IDList – An array of VPC IDsList – An array of security group IDsList – An array of subnet IDsDependentParameterIn CloudFormation, parameters are all independent and cannot depend on each other. Therefore, this is an invalid parameter type.Incorrect options:StringCommaDelimitedListAWS::EC2::KeyPair::KeyNameAs mentioned in the explanation above, these are valid parameter types.Reference:https://aws.amazon.com/blogs/devops/usingthenewcloudformationparametertypes//"},{question:"A HealthCare mobile app uses proprietary Machine Learning algorithms to provide early diagnosis using patient health metrics. To protect this sensitive data, the development team wants to transition to a scalable user management system with login/signup functionality that also supports MultiFactor Authentication (MFA). Which of the following options can be used to implement a solution with the LEAST amount of development effort? (Select two)",answers:[{text:"Use Lambda functions and DynamoDB to create a custom solution for user management",isCorrect:!1},{text:"Use Amazon Cognito for usermanagement and facilitating the login/signup process",isCorrect:!0},{text:"Use Amazon Cognito to enable MultiFactor Authentication (MFA) when users login",isCorrect:!0},{text:"Use Lambda functions and RDS to create a custom solution for user management",isCorrect:!1},{text:"Use Amazon SNS to send MultiFactor Authentication (MFA) code via SMS to mobile app users",isCorrect:!1}],explanation:"Correct options:Use Amazon Cognito for usermanagement and facilitating the login/signup processUse Amazon Cognito to enable MultiFactor Authentication (MFA) when users loginAmazon Cognito lets you add user signup, signin, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports signin with social identity providers, such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0.A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a thirdparty identity provider (IdP). Whether your users signin directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK.Cognito user pools provide support for signup and signin services as well as security features such as multifactor authentication (MFA).via https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.htmlExam Alert:Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools: via https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.html Incorrect options:Use Lambda functions and DynamoDB to create a custom solution for user managementUse Lambda functions and RDS to create a custom solution for user managementAs the problem statement mentions that the solution should require the least amount of development effort, so you cannot use Lambda functions with DynamoDB or RDS to create a custom solution. So both these options are incorrect.Use Amazon SNS to send MultiFactor Authentication (MFA) code via SMS to mobile app users Amazon SNS cannot be used to send MFA codes via SMS to the user's mobile devices as this functionality is only meant to be used for IAM users. An SMS (short message service) MFA device can be any mobile device with a phone number that can receive standard SMS text messages. AWS will soon end support for SMS multifactor authentication (MFA).Please see this for more details: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_sms.html Reference: https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.html"},{question:"You are working for a shipping company that is automating the creation of ECS clusters with an Auto Scaling Group using an AWS CloudFormation template that accepts cluster name as its parameters. Initially, you launch the template with input value 'MainCluster', which deployed five instances across two availability zones. The second time, you launch the template with an input value 'SecondCluster'. However, the instances created in the second run were also launched in 'MainCluster' even after specifying a different cluster name. What is the root cause of this issue?",answers:[{text:"The cluster name Parameter has not been updated in the file /etc/ecs/ecs.config during bootstrap",isCorrect:!0},{text:"The EC2 instance is missing IAM permissions to join the other clusters",isCorrect:!1},{text:"The security groups on the EC2 instance are pointing to the wrong ECS cluster",isCorrect:!1},{text:"The ECS agent Docker image must be rebuilt to connect to the other clusters",isCorrect:!1}],explanation:"The cluster name Parameter has not been updated in the file /etc/ecs/ecs.config during bootstrap In the ecs.config file you have to configure the parameter ECS_CLUSTER='your_cluster_name' to register the container instance with a cluster named 'your_cluster_name'.Sample config for ECS Container Agent: via https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.htmlIncorrect options:The EC2 instance is missing IAM permissions to join the other clusters EC2 instances are getting registered to the first cluster, so permissions are not an issue here and hence this statement is an incorrect choice for the current use case.The ECS agent Docker image must be rebuilt to connect to the other clusters Since the first set of instances got created from the template without any issues, there is no issue with the ECS agent here.The security groups on the EC2 instance are pointing to the wrong ECS cluster Security groups govern the rules about the incoming network traffic to your ECS containers. The issue here is not about user access and hence is a wrong choice for the current use case.References:https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_container_instance.html"},{question:"A junior developer working on ECS instances terminated a container instance in Amazon Elastic Container Service (Amazon ECS) as per instructions from the team lead. But the container instance continues to appear as a resource in the ECS cluster. As a Developer Associate, which of the following solutions would you recommend to fix this behavior?",answers:[{text:"A custom software on the container instance could have failed and resulted in the container hanging in an unhealthy state till restarted again",isCorrect:!1},{text:"You terminated the container instance while it was in RUNNING state, that lead to this synchronization issues",isCorrect:!1},{text:"The container instance has been terminated with AWS CLI, whereas, for ECS instances, Amazon ECS CLI should be used to avoid any synchronization issues",isCorrect:!1},{text:"You terminated the container instance while it was in STOPPED state, that lead to this synchronization issues",isCorrect:!0}],explanation:"You terminated the container instance while it was in STOPPED state, that lead to this synchronization issues If you terminate a container instance while it is in the STOPPED state, that container instance isn't automatically removed from the cluster. You will need to deregister your container instance in the STOPPED state by using the Amazon ECS console or AWS Command Line Interface. Once deregistered, the container instance will no longer appear as a resource in your Amazon ECS cluster.Incorrect options:You terminated the container instance while it was in RUNNING state, that lead to this synchronization issues This is an incorrect statement. If you terminate a container instance in the RUNNING state, that container instance is automatically removed, or deregistered, from the cluster.The container instance has been terminated with AWS CLI, whereas, for ECS instances, Amazon ECS CLI should be used to avoid any synchronization issues This is incorrect and has been added as a distractor.A custom software on the container instance could have failed and resulted in the container hanging in an unhealthy state till restarted again This is an incorrect statement. It is already mentioned in the question that the developer has terminated the instance.References:https://aws.amazon.com/premiumsupport/knowledgecenter/deregisterecsinstance/ https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html"},{question:"You are a development team lead setting permissions for other IAM users with limited permissions. On the AWS Management Console, you created a dev group where new developers will be added, and on your workstation, you configured a developer profile. You would like to test that this user cannot terminate instances. Which of the following options would you execute?",answers:[{text:"Using the CLI, create a dummy EC2 and delete it using another CLI call",isCorrect:!1},{text:"Retrieve the policy using the EC2 metadata service and use the IAM policy simulator",isCorrect:!1},{text:"Use the AWS CLI dryrun option",isCorrect:!0},{text:"Use the AWS CLI test option",isCorrect:!1}],explanation:"Correct option:Use the AWS CLI dryrun option: The dryrun option checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is DryRunOperation, otherwise, it is UnauthorizedOperation.Incorrect options:Use the AWS CLI test option This is a madeup option and has been added as a distractor.Retrieve the policy using the EC2 metadata service and use the IAM policy simulator EC2 metadata service is used to retrieve dynamic information such as instanceid, localhostname, publichostname. This cannot be used to check whether you have the required permissions for the action.Using the CLI, create a dummy EC2 and delete it using another CLI call That would not work as the current EC2 may have permissions that the dummy instance does not have. If permissions were the same it can work but it's not as elegant as using the dryrun option.References:https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html https://docs.aws.amazon.com/cli/latest/reference/ec2/terminateinstances.htmll"},{question:"Your web application architecture consists of multiple Amazon EC2 instances running behind an Elastic Load Balancer with an Auto Scaling group having the desired capacity of 5 EC2 instances. You would like to integrate AWS CodeDeploy for automating application deployment. The deployment should reroute traffic from your application's original environment to the new environment. Which of the following options will meet your deployment criteria?",answers:[{text:"Opt for Rolling deployment",isCorrect:!1},{text:"Opt for Immutable deployment",isCorrect:!1},{text:"Opt for Inplace deployment",isCorrect:!1},{text:"Opt for Blue/Green deployment",isCorrect:!0}],explanation:"Correct option:Opt for Blue/Green deployment A Blue/Green deployment is used to update your applications while minimizing interruptions caused by the changes of a new application version. CodeDeploy provisions your new application version alongside the old version before rerouting your production traffic. The behavior of your deployment depends on which compute platform you use:AWS Lambda: Traffic is shifted from one version of a Lambda function to a new version of the same Lambda function.Amazon ECS: Traffic is shifted from a task set in your Amazon ECS service to an updated, replacement task set in the same Amazon ECS service.EC2/OnPremises: Traffic is shifted from one set of instances in the original environment to a replacement set of instances.Incorrect options:Opt for Rolling deployment This deployment type is present for AWS Elastic Beanstalk and not for EC2 instances directly.Opt for Immutable deployment This deployment type is present for AWS Elastic Beanstalk and not for EC2 instances directly.Opt for Inplace deployment Under this deployment type, the application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete.References:https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcomedeploymentoverviewbluegreen https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html"},{question:"The Development team at a media company is working on securing their databases. Which of the following AWS database engines can be configured with IAM Database Authentication? (Select two)",answers:[{text:"RDS Oracle",isCorrect:!1},{text:"RDS MySQL",isCorrect:!0},{text:"RDS PostGreSQL",isCorrect:!0},{text:"RDS SQL Server",isCorrect:!1},{text:"RDS Db2",isCorrect:!1}],explanation:"Correct options:You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token. An authentication token is a unique string of characters that Amazon RDS generates on request. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM.RDS MySQL IAM database authentication works with MySQL and PostgreSQL engines for Aurora as well as MySQL, MariaDB and RDS PostgreSQL engines for RDS.RDS PostGreSQL IAM database authentication works with MySQL and PostgreSQL engines for Aurora as well as MySQL, MariaDB and RDS PostgreSQL engines for RDS.Incorrect options:RDS OracleRDS SQL ServerThese two options contradict the details in the explanation above, so these are incorrect.RDS Db2 This option has been added as a distractor. Db2 is a family of data management products, including database servers, developed by IBM. RDS does not support Db2 database engine.Reference: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html"},{question:"A company uses AWS CodeDeploy to deploy applications from GitHub to EC2 instances running Amazon Linux. The deployment process uses a file called appspec.yml for specifying deployment hooks. A final lifecycle event should be specified to verify the deployment success. Which of the following hook events should be used to verify the success of the deployment?",answers:[{text:"ApplicationStart",isCorrect:!1},{text:"ValidateService",isCorrect:!0},{text:"AfterInstall",isCorrect:!1},{text:"AllowTraffic",isCorrect:!1}],explanation:"Correct option:AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your onpremises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications.An EC2/OnPremises deployment hook is executed once per deployment to an instance. You can specify one or more scripts to run in a hook.via https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfilestructurehooks.html#referenceappspecfilestructurehooksrunorder ValidateService: ValidateService is the last deployment lifecycle event. It is used to verify the deployment was completed successfully.Incorrect options:AfterInstall You can use this deployment lifecycle event for tasks such as configuring your application or changing file permissionsApplicationStart You typically use this deployment lifecycle event to restart services that were stopped during ApplicationStopAllowTraffic During this deployment lifecycle event, internet traffic is allowed to access instances after a deployment. This event is reserved for the AWS CodeDeploy agent and cannot be used to run scriptsReference:https://docs.aws.amazon.com/codedeploy/latest/userguide/referenceappspecfilestructurehooks.html#referenceappspecfilestructurehooksrunorderr"},{question:"A company has more than 100 million members worldwide enjoying 125 million hours of TV shows and movies each day. The company uses AWS for nearly all its computing and storage needs, which use more than 10,000 server instances on AWS. This results in an extremely complex and dynamic networking environment where applications are constantly communicating inside AWS and across the Internet. Monitoring and optimizing its network is critical for the company. The company needs a solution for ingesting and analyzing the multiple terabytes of realtime data its network generates daily in the form of flow logs. Which technology/service should the company use to ingest this data economically and has the flexibility to direct this data to other downstream systems?",answers:[{text:"AWS Glue",isCorrect:!1},{text:"Amazon Kinesis Data Streams",isCorrect:!0},{text:"Amazon Kinesis Firehose",isCorrect:!1},{text:"Amazon Simple Queue Service (SQS)",isCorrect:!1}],explanation:"Correct option:Amazon Kinesis Data StreamsAmazon Kinesis Data Streams (KDS) is a massively scalable and durable realtime data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and locationtracking events. The data collected is available in milliseconds to enable realtime analytics use cases such as realtime dashboards, realtime anomaly detection, dynamic pricing, and more.Kinesis Data Streams enables realtime processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).Incorrect options:Amazon Simple Queue Service (SQS) Amazon Simple Queue Service (Amazon SQS) offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with messagelevel ack/fail semantics), such as automated workflows. AWS recommends using Amazon SQS for cases where individual message fail/success are important, message delays are needed and there is only one consumer for the messages received (if more than one consumers need to consume the message, then AWS suggests configuring more queues).Amazon Kinesis Firehose Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near realtime analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration.Kinesis data streams is highly customizable and best suited for developers building custom applications or streaming data for specialized needs. Data Streams also provide greater flexibility in integrating downstream applications than Firehose. Data Streams is also a costeffective option compared to Firehose. Therefore, KDS is the right solution.AWS Glue AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. AWS Glue provides all of the capabilities needed for data integration so that you can start analyzing your data and putting it to use in minutes instead of months. Glue is not best suited to handle realtime data.References:https://aws.amazon.com/kinesis/datastreams/ https://aws.amazon.com/sqs/ https://aws.amazon.com/kinesis/datafirehose/"},{question:"To meet compliance guidelines, a company needs to ensure replication of any data stored in its S3 buckets. Which of the following characteristics are correct while configuring an S3 bucket for replication? (Select two)",answers:[{text:"Object tags cannot be replicated across AWS Regions using CrossRegion Replication",isCorrect:!1},{text:"SameRegion Replication (SRR) and CrossRegion Replication (CRR) can be configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags",isCorrect:!0},{text:"S3 lifecycle actions are not replicated with S3 replication",isCorrect:!0},{text:"Once replication is enabled on a bucket, all old and new objects will be replicated",isCorrect:!1},{text:"Replicated objects do not retain metadata",isCorrect:!1}],explanation:"Correct options:SameRegion Replication (SRR) and CrossRegion Replication (CRR) can be configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags Amazon S3 Replication (CRR and SRR) is configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags. You add a replication configuration on your source bucket by specifying a destination bucket in the same or different AWS region for replication.S3 lifecycle actions are not replicated with S3 replication With S3 Replication (CRR and SRR), you can establish replication rules to make copies of your objects into another storage class, in the same or a different region. Lifecycle actions are not replicated, and if you want the same lifecycle configuration applied to both source and destination buckets, enable the same lifecycle configuration on both.Incorrect options:Object tags cannot be replicated across AWS Regions using CrossRegion Replication Object tags can be replicated across AWS Regions using CrossRegion Replication. For customers with CrossRegion Replication already enabled, new permissions are required for tags to replicate.Once replication is enabled on a bucket, all old and new objects will be replicated Replication only replicates the objects added to the bucket after replication is enabled on the bucket. Any objects present in the bucket before enabling replication are not replicated.Replicated objects do not retain metadata You can use replication to make copies of your objects that retain all metadata, such as the original object creation time and version IDs. This capability is important if you need to ensure that your replica is identical to the source object.Reference: https://docs.amazonaws.cn/en_us/AmazonS3/latest/userguide/replication.html"},{question:"A financial services company wants to ensure that the customer data is always kept encrypted on Amazon S3 but wants an AWS managed solution that allows full control to create, rotate and remove the encryption keys. As a Developer Associate, which of the following would you recommend to address the given usecase?",answers:[{text:"ServerSide Encryption with Amazon S3Managed Keys (SSES3)",isCorrect:!1},{text:"ServerSide Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSEKMS)",isCorrect:!0},{text:"ServerSide Encryption with Secrets Manager",isCorrect:!1},{text:"ServerSide Encryption with CustomerProvided Keys (SSEC)",isCorrect:!1}],explanation:"Correct option:ServerSide Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSEKMS)You have the following options for protecting data at rest in Amazon S3:ServerSide Encryption – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.ClientSide Encryption – Encrypt data clientside and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.When you use serverside encryption with AWS KMS (SSEKMS), you can use the default AWS managed CMK, or you can specify a customermanaged CMK that you have already created.Creating your own customermanaged CMK gives you more flexibility and control over the CMK. For example, you can create, rotate, and disable customermanaged CMKs. You can also define access controls and audit the customermanaged CMKs that you use to protect your data.Please see this note for more details: via https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html Incorrect options:ServerSide Encryption with Amazon S3Managed Keys (SSES3) When you use ServerSide Encryption with Amazon S3Managed Keys (SSES3), each object is encrypted with a unique key. As an additional safeguard, AWS encrypts the key itself with a master key that it regularly rotates. So this option is incorrect for the given usecase.ServerSide Encryption with CustomerProvided Keys (SSEC) With ServerSide Encryption with CustomerProvided Keys (SSEC), you will need to create the encryption keys as well as manage the corresponding process to rotate and remove the encryption keys. Amazon S3 manages the data encryption, as it writes to disks, as well as the data decryption when you access your objects. So this option is incorrect for the given usecase.ServerSide Encryption with Secrets Manager AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. You cannot combine ServerSide Encryption with Secrets Manager to create, rotate, or disable the encryption keys.Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html"},{question:"An Amazon Simple Queue Service (SQS) has to be configured between two AWS accounts for shared access to the queue. AWS account A has the SQS queue in its account and AWS account B has to be given access to this queue. Which of the following options need to be combined to allow this crossaccount access? (Select three)",answers:[{text:"The account A administrator attaches a trust policy to the role that identifies account B as the principal who can assume the role.",isCorrect:!1},{text:"The account B administrator creates an IAM role and attaches a trust policy to the role with account B as the principal.",isCorrect:!0},{text:"The account A administrator delegates the permission to assume the role to any users in account A.",isCorrect:!1},{text:"The account A administrator creates an IAM role and attaches a permissions policy.",isCorrect:!1},{text:"The account B administrator delegates the permission to assume the role to any users in account B.",isCorrect:!0},{text:"The account A administrator attaches a trust policy to the role that identifies account B as the AWS service principal who can assume the role.",isCorrect:!1}],explanation:"The account A administrator creates an IAM role and attaches a permissions policyThe account A administrator attaches a trust policy to the role that identifies account B as the principal who can assume the roleThe account B administrator delegates the permission to assume the role to any users in account BTo grant crossaccount permissions, you need to attach an identitybased permissions policy to an IAM role. For example, the AWS account A administrator can create a role to grant crossaccount permissions to AWS account B as follows:The account A administrator creates an IAM role and attaches a permissions policy—that grants permissions on resources in account A—to the role.The account A administrator attaches a trust policy to the role that identifies account B as the principal who can assume the role.The account B administrator delegates the permission to assume the role to any users in account B. This allows users in account B to create or access queues in account A.Incorrect options:The account B administrator creates an IAM role and attaches a trust policy to the role with account B as the principal As mentioned above, the account A administrator needs to create an IAM role and then attach a permissions policy. So, this option is incorrect.The account A administrator delegates the permission to assume the role to any users in account A This is irrelevant, as users in account B need to be given access.The account A administrator attaches a trust policy to the role that identifies account B as the AWS service principal who can assume the role AWS service principal is given as principal in the trust policy when you need to grant the permission to assume the role to an AWS service. The given use case talks about giving permission to another account. So, service principal is not an option here.Reference: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqsoverviewofmanagingaccess.html"},{question:"You are a developer working with the AWS CLI to create Lambda functions that contain environment variables. Your functions will require over 50 environment variables consisting of sensitive information of database table names. What is the total set size/number of environment variables you can create for AWS Lambda?",answers:[{text:"The total size of all environment variables shouldn't exceed 8 KB. The maximum number of variables that can be created is 50",isCorrect:!1},{text:"The total size of all environment variables shouldn't exceed 4 KB. There is no limit on the number of variables",isCorrect:!0},{text:"The total size of all environment variables shouldn't exceed 4 KB. The maximum number of variables that can be created is 35",isCorrect:!1},{text:"The total size of all environment variables shouldn't exceed 8 KB. There is no limit on the number of variables",isCorrect:!1}],explanation:"Correct option:The total size of all environment variables shouldn't exceed 4 KB. There is no limit on the number of variablesAn environment variable is a pair of strings that are stored in a function's versionspecific configuration. The Lambda runtime makes environment variables available to your code and sets additional environment variables that contain information about the function and invocation request. The total size of all environment variables doesn't exceed 4 KB. There is no limit defined on the number of variables that can be used.Incorrect options:The total size of all environment variables shouldn't exceed 8 KB. The maximum number of variables that can be created is 50 Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables.The total size of all environment variables shouldn't exceed 8 KB. There is no limit on the number of variables Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables.The total size of all environment variables shouldn't exceed 4 KB. The maximum number of variables that can be created is 35 Incorrect option. The total size of environment variables cannot exceed 4 KB with no restriction on the number of variables.Reference: https://docs.aws.amazon.com/lambda/latest/dg/configurationenvvars.html"},{question:"Your team lead has requested code review of your code for Lambda functions. Your code is written in Python and makes use of the Amazon Simple Storage Service (S3) to upload logs to an S3 bucket. After the review, your team lead has recommended reuse of execution context to improve the Lambda performance. Which of the following actions will help you implement the recommendation?",answers:[{text:"Use environment variables to pass operational parameters",isCorrect:!1},{text:"Enable XRay integration",isCorrect:!1},{text:"Assign more RAM to the function",isCorrect:!1},{text:"Move the Amazon S3 client initialization, out of your function handler",isCorrect:!0}],explanation:"Move the Amazon S3 client initialization, out of your function handler AWS best practices for Lambda suggest taking advantage of execution context reuse to improve the performance of your functions. Initialize SDK clients and database connections outside of the function handler, and cache static assets locally in the /tmp directory. Subsequent invocations processed by the same instance of your function can reuse these resources. This saves execution time and cost. To avoid potential data leaks across invocations, don’t use the execution context to store user data, events, or other information with security implications.Incorrect options:Use environment variables to pass operational parameters This is one of the suggested best practices for Lambda. By using environment variables to pass operational parameters you can avoid hardcoding useful information. But, this is not the right answer for the current usecase, since it talks about reusing context.Assign more RAM to the function Increasing RAM will help speed up the process. But, in the current question, the reviewer has specifically mentioned about reusing context. Hence, this is not the right answer.Enable XRay integration You can use AWS XRay to visualize the components of your application, identify performance bottlenecks, and troubleshoot requests that resulted in an error. Your Lambda functions send trace data to XRay, and XRay processes the data to generate a service map and searchable trace summaries. This is a useful tool for troubleshooting. But, for the current usecase, we already know the bottleneck that needs to be fixed and that is the context reuse.References:https://docs.aws.amazon.com/lambda/latest/dg/bestpractices.html https://docs.aws.amazon.com/lambda/latest/dg/servicesxray.html"},{question:"The development team at a company is looking at building an AWS CloudFormation template that selfpopulates the AWS Region variable while deploying the CloudFormation template. What is the MOST operationally efficient way to determine the Region in which the template is being deployed?",answers:[{text:"Use the AWS::Region pseudo parameter",isCorrect:!0},{text:"Set up a mapping containing the key and the named values for all AWS Regions and then have the CloudFormation template autoselect the desired value",isCorrect:!1},{text:"Create an AWS Lambdabacked custom resource for Region and let the desired value be populated at the time of deployment by the Lambda",isCorrect:!1},{text:"Create a CloudFormation parameter for Region and let the desired value be populated at the time of deployment",isCorrect:!1}],explanation:'Use the AWS::Region pseudo parameterPseudo parameters are parameters that are predefined by AWS CloudFormation. You don\'t declare them in your template. Use them the same way as you would a parameter, as the argument for the Ref function.You can access pseudo parameters in a CloudFormation template like so:Outputs: MyStacksRegion: Value: !Ref "AWS::Region"The AWS::Region pseudo parameter returns a string representing the Region in which the encompassing resource is being created, such as uswest2.Incorrect options:Create a CloudFormation parameter for Region and let the desired value be populated at the time of deployment Although it is certainly possible to use a CloudFormation parameter to populate the desired value of the Region at the time of deployment, however, this is not operationally efficient, as you can directly use the AWS::Region pseudo parameter for this.Set up a mapping containing the key and the named values for all AWS Regions and then have the CloudFormation template autoselect the desired value The Mappings section matches a key to a corresponding set of named values. For example, if you want to set values based on a Region, you can create a mapping that uses the Region name as a key and contains the values you want to specify for each specific Region. You use the Fn::FindInMap intrinsic function to retrieve values in a map. This option is incorrect as the CloudFormation template cannot autoselect the desired value of the Region from a mapping.Create an AWS Lambdabacked custom resource for Region and let the desired value be populated at the time of deployment by the Lambda Custom resources enable you to write custom provisioning logic in templates that AWS CloudFormation runs anytime you create, update (if you changed the custom resource), or delete stacks. When you associate a Lambda function with a custom resource, the function is invoked whenever the custom resource is created, updated, or deleted. This option is a distractor, as Region is not a custom resource that needs to be provisioned.References:https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudoparameterreference.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/templatecustomresources.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappingssectionstructure.html'},{question:"The development team at a company wants to insert vendor records into an Amazon DynamoDB table as soon as the vendor uploads a new file into an Amazon S3 bucket. As a Developer Associate, which set of steps would you recommend to achieve this?",answers:[{text:"Set up an event with Amazon CloudWatch Events that will monitor the S3 bucket and then insert the records into DynamoDB",isCorrect:!1},{text:"Develop a Lambda function that will poll the S3 bucket and then insert the records into DynamoDB",isCorrect:!1},{text:"Create an S3 event to invoke a Lambda function that inserts records into DynamoDB",isCorrect:!0},{text:"Write a cron job that will execute a Lambda function at a scheduled time and insert the records into DynamoDB",isCorrect:!1}],explanation:"Create an S3 event to invoke a Lambda function that inserts records into DynamoDBThe Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. You store this configuration in the notification subresource that is associated with a bucket.Amazon S3 APIs such as PUT, POST, and COPY can create an object. Using these event types, you can enable notification when an object is created using a specific API, or you can use the s3:ObjectCreated:* event type to request notification regardless of the API that was used to create an object.For the given usecase, you would create an S3 event notification that triggers a Lambda function whenever we have a PUT object operation in the S3 bucket. The Lambda function in turn would execute custom code to inserts records into DynamoDB.via https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html Incorrect options:Write a cron job that will execute a Lambda function at a scheduled time and insert the records into DynamoDB This is not efficient because there may not be any unprocessed file in the S3 bucket when the cron triggers the Lambda on schedule. So this is not the correct option.Set up an event with Amazon CloudWatch Events that will monitor the S3 bucket and then insert the records into DynamoDB The CloudWatch event cannot directly insert records into DynamoDB as it's not a supported target type. The CloudWatch event needs to use something like a Lambda function to insert the records into DynamoDB.Develop a Lambda function that will poll the S3 bucket and then insert the records into DynamoDB This is not efficient because there may not be any unprocessed file in the S3 bucket when the Lambda function polls the S3 bucket at a given time interval. So this is not the correct option.Reference:https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.htmll"},{question:"Recently, you started an online learning platform using AWS Lambda and AWS Gateway API. Your first version was successful, and you began developing new features for the second version. You would like to gradually introduce the second version by routing only 10% of the incoming traffic to the new Lambda version. Which solution should you opt for?",answers:[{text:"Deploy your Lambda in a VPC",isCorrect:!1},{text:"Use environment variables",isCorrect:!1},{text:"Use AWS Lambda aliases",isCorrect:!0},{text:"Use Tags to distinguish the different versions",isCorrect:!1}],explanation:"Correct option:Use AWS Lambda aliases A Lambda alias is like a pointer to a specific Lambda function version. You can create one or more aliases for your AWS Lambda function. Users can access the function version using the alias ARN. An alias can only point to a function version, not to another alias. You can update an alias to point to a new version of the function. Event sources such as Amazon S3 invoke your Lambda function. These event sources maintain a mapping that identifies the function to invoke when events occur. If you specify a Lambda function alias in the mapping configuration, you don't need to update the mapping when the function version changes. This is the right choice for the current requirement.Incorrect options:Use Tags to distinguish the different versions You can tag Lambda functions to organize them by owner, project or department. Tags are freeform keyvalue pairs that are supported across AWS services for use in filtering resources and adding detail to billing reports. This does not address the given usecase.Use environment variables You can use environment variables to store secrets securely and adjust your function's behavior without updating code. An environment variable is a pair of strings that are stored in a function's versionspecific configuration. The Lambda runtime makes environment variables available to your code and sets additional environment variables that contain information about the function and invocation request. For example, you can use environment variables to point to test, development or production databases by passing it as an environment variable during runtime. This option does not address the given usecase.Deploy your Lambda in a VPC Amazon Virtual Private Cloud (Amazon VPC) enables you to launch AWS resources into a virtual network that you've defined. This adds another layer of security for your entire architecture. Not the right choice for the given scenario.References:https://docs.aws.amazon.com/lambda/latest/dg/configurationaliases.html https://docs.aws.amazon.com/lambda/latest/dg/configurationtags.html https://docs.aws.amazon.com/lambda/latest/dg/configurationenvvars.html"},{question:"A banking application needs to send realtime alerts and notifications based on any updates from the backend services. The company wants to avoid implementing complex polling mechanisms for these notifications. Which of the following types of APIs supported by the Amazon API Gateway is the right fit?",answers:[{text:"REST APIs",isCorrect:!1},{text:"HTTP APIs",isCorrect:!1},{text:"WebSocket APIs",isCorrect:!0},{text:"REST or HTTP APIs",isCorrect:!1}],explanation:"WebSocket APIsIn a WebSocket API, the client and the server can both send messages to each other at any time. Backend servers can easily push data to connected users and devices, avoiding the need to implement complex polling mechanisms.For example, you could build a serverless application using an API Gateway WebSocket API and AWS Lambda to send and receive messages to and from individual users or groups of users in a chat room. Or you could invoke backend services such as AWS Lambda, Amazon Kinesis, or an HTTP endpoint based on message content.You can use API Gateway WebSocket APIs to build secure, realtime communication applications without having to provision or manage any servers to manage connections or largescale data exchanges. Targeted use cases include realtime applications such as the following:Chat applicationsRealtime dashboards such as stock tickersRealtime alerts and notificationsAPI Gateway provides WebSocket API management functionality such as the following:Monitoring and throttling of connections and messagesUsing AWS XRay to trace messages as they travel through the APIs to backend servicesEasy integration with HTTP/HTTPS endpointsIncorrect options:REST or HTTP APIsREST APIs An API Gateway REST API is made up of resources and methods. A resource is a logical entity that an app can access through a resource path. A method corresponds to a REST API request that is submitted by the user of your API and the response returned to the user.For example, /incomes could be the path of a resource representing the income of the app user. A resource can have one or more operations that are defined by appropriate HTTP verbs such as GET, POST, PUT, PATCH, and DELETE. A combination of a resource path and an operation identifies a method of the API. For example, a POST /incomes method could add an income earned by the caller, and a GET /expenses method could query the reported expenses incurred by the caller.HTTP APIs HTTP APIs enable you to create RESTful APIs with lower latency and lower cost than REST APIs. You can use HTTP APIs to send requests to AWS Lambda functions or to any publicly routable HTTP endpoint.For example, you can create an HTTP API that integrates with a Lambda function on the backend. When a client calls your API, API Gateway sends the request to the Lambda function and returns the function's response to the client.Server push mechanism is not possible in REST and HTTP APIs.Reference: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewayoverviewdeveloperexperience.html"},{question:"You have a threetier web application consisting of a web layer using AngularJS, an application layer using an AWS API Gateway and a data layer in an Amazon Relational Database Service (RDS) database. Your web application allows visitors to look up popular movies from the past. The company is looking at reducing the number of calls made to endpoint and improve latency to the API. What can you do to improve performance?",answers:[{text:"Enable API Gateway Caching",isCorrect:!0},{text:"Use Mapping Templates",isCorrect:!1},{text:"Use Stage Variables",isCorrect:!1},{text:"Use Amazon Kinesis Data Streams to stream incoming data and reduce the burden on Gateway APIs",isCorrect:!1}],explanation:"Correct option:Enable API Gateway Caching You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified timetolive (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled.Incorrect options:Use Mapping Templates A mapping template is a script expressed in Velocity Template Language (VTL) and applied to the payload using JSONPath expressions. Mapping templates help format/structure the data in a way that it is easily readable, unlike a server response that might always be easy to ready. Mapping Templates do not help in latency issues of the APIs.Use Stage Variables Stage variables act like environment variables and can be used to change the behavior of your API Gateway methods for each deployment stage; for example, making it possible to reach a different back end depending on which stage the API is running on. Stage variables do not help in latency issues.Use Amazon Kinesis Data Streams to stream incoming data and reduce the burden on Gateway APIs Amazon Kinesis Data Streams (KDS) is a massively scalable and durable realtime data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and locationtracking events.Reference: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaycaching.html"},{question:"A developer from your team has configured the load balancer to route traffic equally between instances or across Availability Zones. However, Elastic Load Balancing (ELB) routes more traffic to one instance or Availability Zone than the others. Why is this happening and how can it be fixed? (Select two)",answers:[{text:"For Application Load Balancers, crosszone load balancing is disabled by default",isCorrect:!1},{text:"There could be shortlived TCP connections between clients and instances",isCorrect:!1},{text:"After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer, thereby receiving random bursts of traffic",isCorrect:!1},{text:"Instances of a specific capacity type aren’t equally distributed across Availability Zones",isCorrect:!0},{text:"Sticky sessions are enabled for the load balancer",isCorrect:!1}],explanation:"Correct option:Sticky sessions are enabled for the load balancer This can be the reason for potential unequal traffic routing by the load balancer. Sticky sessions are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information in order to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies.When a load balancer first receives a request from a client, it routes the request to a target, generates a cookie named AWSALB that encodes information about the selected target, encrypts the cookie, and includes the cookie in the response to the client. The client should include the cookie that it receives in subsequent requests to the load balancer. When the load balancer receives a request from a client that contains the cookie, if sticky sessions are enabled for the target group and the request goes to the same target group, the load balancer detects the cookie and routes the request to the same target.If you use durationbased session stickiness, configure an appropriate cookie expiration time for your specific use case. If you set session stickiness from individual applications, use session cookies instead of persistent cookies where possible.Instances of a specific capacity type aren’t equally distributed across Availability Zones A Classic Load Balancer with HTTP or HTTPS listeners might route more traffic to highercapacity instance types. This distribution aims to prevent lowercapacity instance types from having too many outstanding requests. It’s a best practice to use similar instance types and configurations to reduce the likelihood of capacity gaps and traffic imbalances.A traffic imbalance might also occur if you have instances of similar capacities running on different Amazon Machine Images (AMIs). In this scenario, the imbalance of the traffic in favor of highercapacity instance types is desirable.Incorrect options:There could be shortlived TCP connections between clients and instances This is an incorrect statement. Longlived TCP connections between clients and instances can potentially lead to unequal distribution of traffic by the load balancer. Longlived TCP connections between clients and instances cause uneven traffic load distribution by design. As a result, new instances take longer to reach connection equilibrium. Be sure to check your metrics for longlived TCP connections that might be causing routing issues in the load balancer.For Application Load Balancers, crosszone load balancing is disabled by default This is an incorrect statement. With Application Load Balancers, crosszone load balancing is always enabled.After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer, thereby receiving random bursts of traffic This is an incorrect statement. After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer. However, even though they remain registered, the load balancer does not route traffic to them.References:https://docs.aws.amazon.com/elasticloadbalancing/latest/application/loadbalancertargetgroups.html#stickysessions https://aws.amazon.com/premiumsupport/knowledgecenter/elbfixunequaltrafficrouting/ https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/howelasticloadbalancingworks.html#availabilityzones"},{question:"As part of their onboarding, the employees at an IT company need to upload their profile photos in a private S3 bucket. The company wants to build an inhouse web application hosted on an EC2 instance that should display the profile photos in a secure way when the employees mark their attendance. As a Developer Associate, which of the following solutions would you suggest to address this usecase?",answers:[{text:"Make the S3 bucket public so that the application can reference the image URL for display",isCorrect:!1},{text:"Save the S3 key for each user's profile photo in a DynamoDB table and use a lambda function to dynamically generate a presigned URL. Reference this URL for display via the web application",isCorrect:!0},{text:"Keep each user's profile image encoded in base64 format in a DynamoDB table and reference it from the application for display",isCorrect:!1},{text:"Keep each user's profile image encoded in base64 format in an RDS table and reference it from the application for display",isCorrect:!1}],explanation:'Correct option:"Save the S3 key for each user\'s profile photo in a DynamoDB table and use a lambda function to dynamically generate a presigned URL. Reference this URL for display via the web application"On Amazon S3, all objects by default are private. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a presigned URL, using their own security credentials, to grant timelimited permission to download the objects.You can also use an IAM instance profile to create a presigned URL. When you create a presigned URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object), and expiration date and time. The presigned URLs are valid only for the specified duration. So for the given usecase, the object key can be retrieved from the DynamoDB table, and then the application can generate the presigned URL using the IAM instance profile.Please see this note for more details: via https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html Incorrect options:"Make the S3 bucket public so that the application can reference the image URL for display" Making the S3 bucket public would violate the security and privacy requirements for the usecase, so this option is incorrect."Keep each user\'s profile image encoded in base64 format in a DynamoDB table and reference it from the application for display""Keep each user\'s profile image encoded in base64 format in an RDS table and reference it from the application for display"It\'s a bad practice to keep the raw image data in the database itself. Also, it would not be possible to create a secure access URL for the image without a significant development effort. Hence both these options are incorrect.Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html'},{question:"Two policies are attached to an IAM user. The first policy states that the user has explicitly been denied all access to EC2 instances. The second policy states that the user has been allowed permission for EC2:Describe action. When the user tries to use 'Describe' action on an EC2 instance using the CLI, what will be the output?",answers:[{text:"The user will get access because it has an explicit allow",isCorrect:!1},{text:"The user will be denied access because one of the policies has an explicit deny on it",isCorrect:!0},{text:"The order of the policy matters. If policy 1 is before 2, then the user is denied access. If policy 2 is before 1, then the user is allowed access",isCorrect:!1},{text:"The IAM user stands in an invalid state, because of conflicting policies",isCorrect:!1}],explanation:"The user will be denied access because the policy has an explicit deny on it User will be denied access because any explicit deny overrides the allow.Policy Evaluation explained: via https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluationlogic.html Incorrect options:The IAM user stands in an invalid state, because of conflicting policies This is an incorrect statement. Access policies can have allow and deny permissions on them and based on policy rules they are evaluated. A user account does not get invalid because of policies.The user will get access because it has an explicit allow As discussed above, explicit deny overrides all other permissions and hence the user will be denied access.The order of the policy matters. If policy 1 is before 2, then the user is denied access. If policy 2 is before 1, then the user is allowed access If policies that apply to a request include an Allow statement and a Deny statement, the Deny statement trumps the Allow statement. The request is explicitly denied.Reference: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluationlogic.html"},{question:"The development team at a company wants to encrypt a 111 GB object using AWS KMS. Which of the following represents the best solution?",answers:[{text:"Make a GenerateDataKey API call that returns a plaintext key and an encrypted copy of a data key. Use a plaintext key to encrypt the data",isCorrect:!0},{text:"Make a GenerateDataKeyWithoutPlaintext API call that returns an encrypted copy of a data key. Use an encrypted key to encrypt the data",isCorrect:!1},{text:"Make a GenerateDataKeyWithPlaintext API call that returns an encrypted copy of a data key. Use a plaintext key to encrypt the data",isCorrect:!1},{text:"Make an Encrypt API call to encrypt the plaintext data as ciphertext using a customer master key (CMK) with imported key material",isCorrect:!1}],explanation:"Correct option:Make a GenerateDataKey API call that returns a plaintext key and an encrypted copy of a data key. Use a plaintext key to encrypt the data GenerateDataKey API, generates a unique symmetric data key for clientside encryption. This operation returns a plaintext copy of the data key and a copy that is encrypted under a customer master key (CMK) that you specify. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data.GenerateDataKey returns a unique data key for each request. The bytes in the plaintext key are not related to the caller or the CMK.To encrypt data outside of AWS KMS:Use the GenerateDataKey operation to get a data key.Use the plaintext data key (in the Plaintext field of the response) to encrypt your data outside of AWS KMS. Then erase the plaintext data key from memory.Store the encrypted data key (in the CiphertextBlob field of the response) with the encrypted data.To decrypt data outside of AWS KMS:Use the Decrypt operation to decrypt the encrypted data key. The operation returns a plaintext copy of the data key.Use the plaintext data key to decrypt data outside of AWS KMS, then erase the plaintext data key from memory.Incorrect options:Make a GenerateDataKeyWithPlaintext API call that returns an encrypted copy of a data key. Use a plaintext key to encrypt the data This is a madeup option, given only as a distractor.Make an Encrypt API call to encrypt the plaintext data as ciphertext using a customer master key (CMK) with imported key material Encrypt API is used to encrypt plaintext into ciphertext by using a customer master key (CMK). The Encrypt operation has two primary use cases:To encrypt small amounts of arbitrary data, such as a personal identifier or database password, or other sensitive information.To move encrypted data from one AWS Region to another.Neither of the two is useful for the given scenario.Make a GenerateDataKeyWithoutPlaintext API call that returns an encrypted copy of a data key. Use an encrypted key to encrypt the data GenerateDataKeyWithoutPlaintext API, generates a unique symmetric data key. This operation returns a data key that is encrypted under a customer master key (CMK) that you specify.GenerateDataKeyWithoutPlaintext is identical to the GenerateDataKey operation except that returns only the encrypted copy of the data key. This operation is useful for systems that need to encrypt data at some point, but not immediately. When you need to encrypt the data, you call the Decrypt operation on the encrypted copy of the key.References:https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html https://docs.aws.amazon.com/kms/latest/APIReference/API_Encrypt.html https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKeyWithoutPlaintext.html"},{question:"A firm maintains a highly available application that receives HTTPS traffic from mobile devices and web browsers. The main Developer would like to set up the Load Balancer routing to route traffic from web servers to smart.com/api and from mobile devices to smart.com/mobile. A developer advises that the previous recommendation is not needed and that requests should be sent to api.smart.com and mobile.smart.com instead. Which of the following routing options were discussed in the given usecase? (select two)",answers:[{text:"Path based",isCorrect:!0},{text:"Host based",isCorrect:!0},{text:"Web browser version",isCorrect:!1},{text:"Cookie value",isCorrect:!1},{text:"Client IP",isCorrect:!1}],explanation:"Path basedYou can create a listener with rules to forward requests based on the URL path. This is known as pathbased routing. If you are running microservices, you can route traffic to multiple backend services using pathbased routing. For example, you can route general requests to one target group and request to render images to another target group.This pathbased routing allows you to route requests to, for example, /api to one set of servers (also known as target groups) and /mobile to another set. Segmenting your traffic in this way gives you the ability to control the processing environment for each category of requests. Perhaps /api requests are best processed on Compute Optimized instances, while /mobile requests are best handled by Memory Optimized instances.Host basedYou can create Application Load Balancer rules that route incoming traffic based on the domain name specified in the Host header. Requests to api.example.com can be sent to one target group, requests to mobile.example.com to another, and all others (by way of a default rule) can be sent to a third. You can also create rules that combine hostbased routing and pathbased routing. This would allow you to route requests to api.example.com/production and api.example.com/sandbox to distinct target groups.via https://docs.aws.amazon.com/elasticloadbalancing/latest/application/loadbalancerlisteners.html#ruleconditiontypes Incorrect options:Client IP This option has been added as a distractor. Routing is not based on the client's IP address.Web browser version Routing has nothing to do with the client's web browser, if it was then there is something sneaky going on.Cookie value Application Load Balancers support load balancergenerated cookies only and you cannot modify them. When routing sticky sessions to route requests to the same target then cookies are needed to be supported by the client's browser.Reference: https://aws.amazon.com/blogs/aws/newhostbasedroutingsupportforawsapplicationloadbalancers/"},{question:"A development team uses shared Amazon S3 buckets to upload files. Due to this shared access, objects in S3 buckets have different owners making it difficult to manage the objects. As a developer associate, which of the following would you suggest to automatically make the S3 bucket owner, also the owner of all objects in the bucket, irrespective of the AWS account used for uploading the objects?",answers:[{text:"Use S3 CORS to make the S3 bucket owner, the owner of all objects in the bucket",isCorrect:!1},{text:"Use Bucket Access Control Lists (ACLs) to control access on S3 bucket and then define its owner",isCorrect:!1},{text:"Use S3 Access Analyzer to identify the owners of all objects and change the ownership to the bucket owner",isCorrect:!1},{text:"Use S3 Object Ownership to default bucket owner to be the owner of all objects in the bucket",isCorrect:!0}],explanation:"Correct option:Use S3 Object Ownership to default bucket owner to be the owner of all objects in the bucketS3 Object Ownership is an Amazon S3 bucket setting that you can use to control ownership of new objects that are uploaded to your buckets. By default, when other AWS accounts upload objects to your bucket, the objects remain owned by the uploading account. With S3 Object Ownership, any new objects that are written by other accounts with the bucketownerfullcontrol canned access control list (ACL) automatically become owned by the bucket owner, who then has full control of the objects.S3 Object Ownership has two settings: 1. Object writer – The uploading account will own the object. 2. Bucket owner preferred – The bucket owner will own the object if the object is uploaded with the bucketownerfullcontrol canned ACL. Without this setting and canned ACL, the object is uploaded and remains owned by the uploading account.Incorrect options:Use S3 CORS to make the S3 bucket owner, the owner of all objects in the bucket Crossorigin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain.Use S3 Access Analyzer to identify the owners of all objects and change the ownership to the bucket owner Access Analyzer for S3 helps review all buckets that have bucket access control lists (ACLs), bucket policies, or access point policies that grant public or shared access. Access Analyzer for S3 alerts you to buckets that are configured to allow access to anyone on the internet or other AWS accounts, including AWS accounts outside of your organization.Use Bucket Access Control Lists (ACLs) to control access on S3 bucket and then define its owner Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. A bucket ACLs allow you to control access at bucket level.None of the above features are useful for the current scenario and hence are incorrect options.References:https://docs.aws.amazon.com/AmazonS3/latest/userguide/aboutobjectownership.html https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html"},{question:"A company uses Amazon Simple Email Service (SES) to costeffectively send susbscription emails to the customers. Intermittently, the SES service throws the error: Throttling – Maximum sending rate exceeded. As a developer associate, which of the following would you recommend to fix this issue?",answers:[{text:"Configure Timeout mechanism for each request made to the SES service",isCorrect:!1},{text:"Raise a service request with Amazon to increase the throttling limit for the SES API",isCorrect:!1},{text:"Implement retry mechanism for all 4xx errors to avoid throttling error",isCorrect:!1},{text:"Use Exponential Backoff technique to introduce delay in time before attempting to execute the operation again",isCorrect:!0}],explanation:"Correct option:Use Exponential Backoff technique to introduce delay in time before attempting to execute the operation again A “Throttling – Maximum sending rate exceeded” error is retriable. This error is different than other errors returned by Amazon SES. A request rejected with a “Throttling” error can be retried at a later time and is likely to succeed.Retries are “selfish.” In other words, when a client retries, it spends more of the server's time to get a higher chance of success. Where failures are rare or transient, that's not a problem. This is because the overall number of retried requests is small, and the tradeoff of increasing apparent availability works well. When failures are caused by overload, retries that increase load can make matters significantly worse. They can even delay recovery by keeping the load high long after the original issue is resolved.The preferred solution is to use a backoff. Instead of retrying immediately and aggressively, the client waits some amount of time between tries. The most common pattern is an exponential backoff, where the wait time is increased exponentially after every attempt.A variety of factors can affect your send rate, e.g. message size, network performance or Amazon SES availability. The advantage of the exponential backoff approach is that your application will selftune and it will call Amazon SES at close to the maximum allowed rate.Incorrect options:Configure Timeout mechanism for each request made to the SES service Requests are configured to timeout if they do not complete successfully in a given time. This helps free up the database, application and any other resource that could potentially keep on waiting to eventually succeed. But, if errors are caused by load, retries can be ineffective if all clients retry at the same time. Throttling error signifies that load is high on SES and it does not make sense to keep retrying.Raise a service request with Amazon to increase the throttling limit for the SES API If throttling error is persistent, then it indicates a high load on the system consistently and increasing the throttling limit will be the right solution for the problem. But, the error is only intermittent here, signifying that decreasing the rate of requests will handle the error.Implement retry mechanism for all 4xx errors to avoid throttling error 4xx status codes indicate that there was a problem with the client request. Common client request errors include providing invalid credentials and omitting required parameters. When you get a 4xx error, you need to correct the problem and resubmit a properly formed client request. Throttling is a server error and not a client error, hence retry on 4xx errors does not make sense here.References:https://aws.amazon.com/builderslibrary/timeoutsretriesandbackoffwithjitter/ https://aws.amazon.com/blogs/messagingandtargeting/howtohandleathrottlingmaximumsendingrateexceedederror//"},{question:"A serverless application built on AWS processes customer orders 24/7 using an AWS Lambda function and communicates with an external vendor's HTTP API for payment processing. The development team wants to notify the support team in near realtime using an existing Amazon Simple Notification Service (Amazon SNS) topic, but only when the external API error rate exceeds 5% of the total transactions processed in an hour. As an AWS Certified Developer Associate, which option will you suggest as the most efficient solution?",answers:[{text:"Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Metric Filter to look at the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Metric Filter on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate",isCorrect:!1},{text:"Configure and push highresolution custom metrics to CloudWatch that record the failures of the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rate",isCorrect:!0},{text:"Configure CloudWatch metrics with detailed monitoring for the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rate",isCorrect:!1},{text:"Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Logs Insights to query the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Logs Insights on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate",isCorrect:!1}],explanation:"Correct option:Configure and push highresolution custom metrics to CloudWatch that record the failures of the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rateYou can publish your own metrics, known as custom metrics, to CloudWatch using the AWS CLI or an API.Each metric is one of the following:Standard resolution, with data having a oneminute granularityHigh resolution, with data at a granularity of one secondMetrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a highresolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.Highresolution metrics can give you more immediate insight into your application's subminute activity. Keep in mind that every PutMetricData call for a custom metric is charged, so calling PutMetricData more often on a highresolution metric can lead to higher charges.You can create metric and composite alarms in Amazon CloudWatch. For the given use case, you can set up a CloudWatch metric alarm that watches the custom metric that captures the API errors and then triggers the alarm when the API error rate exceeds the 5% threshold. The alarm then sends a notification via the existing SNS topic.Incorrect options:Configure CloudWatch metrics with detailed monitoring for the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS topic when the error rate exceeds the specified rate CloudWatch provides two categories of monitoring: basic monitoring and detailed monitoring. Detailed monitoring options differ based on the services that offer it. For example, Amazon EC2 detailed monitoring provides more frequent metrics, published at oneminute intervals, instead of the fiveminute intervals used in Amazon EC2 basic monitoring. Detailed monitoring is offered by only some services. As explained above, you need to use custom metrics to capture data for the external payment processing API calls since detailed monitoring for the standard CloudWatch metrics cannot be used for this scenario.Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Logs Insights to query the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Logs Insights on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate CloudWatch Logs Insights enables you to interactively search and analyze your log data in Amazon CloudWatch Logs. You can perform queries to help you more efficiently and effectively respond to operational issues. This option is not the right fit for the given use case since Lambda cannot monitor the output of the CloudWatch Logs Insights on a realtime basis since it is being invoked on a schedule. Also, it is not an efficient solution since Lambda will need significant custom code to parse and compute the external API error rate from the CloudWatch Logs Insights data.Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Metric Filter to look at the CloudWatch logs. Set up the Lambda function to check the output from CloudWatch Metric Filter on a schedule and send notification via the existing SNS topic when the error rate exceeds the specified rate You can search and filter the log data coming into CloudWatch Logs by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on. This option is not the best fit for the given use case since Lambda cannot monitor the output of the CloudWatch Metric Filter on a realtime basis since it is being invoked on a schedule. Also, it is not an efficient solution since Lambda will need significant custom code to parse and compute the external API error rate from the CloudWatch Metric Filter data.References:https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html https://aws.amazon.com/premiumsupport/knowledgecenter/cloudwatchpushcustommetrics/ https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html"},{question:"A company has a workload that requires 14,000 consistent IOPS for data that must be durable and secure. The compliance standards of the company state that the data should be secure at every stage of its lifecycle on all of the EBS volumes they use. Which of the following statements are true regarding data security on EBS?",answers:[{text:"EBS volumes support inflight encryption but does not support encryption at rest",isCorrect:!1},{text:"EBS volumes don't support any encryption",isCorrect:!1},{text:"EBS volumes support both inflight encryption and encryption at rest using KMS",isCorrect:!0},{text:"EBS volumes do not support inflight encryption but do support encryption at rest using KMS",isCorrect:!1}],explanation:"Correct option:Amazon EBS works with AWS KMS to encrypt and decrypt your EBS volume. You can encrypt both the boot and data volumes of an EC2 instance. When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted:Data at rest inside the volumeAll data moving between the volume and the instanceAll snapshots created from the volumeAll volumes created from those snapshotsEBS volumes support both inflight encryption and encryption at rest using KMS This is a correct statement. Encryption operations occur on the servers that host EC2 instances, ensuring the security of both dataatrest and dataintransit between an instance and its attached EBS storage.Incorrect options:EBS volumes support inflight encryption but do not support encryption at rest This is an incorrect statement. As discussed above, all data moving between the volume and the instance is encrypted.EBS volumes do not support inflight encryption but do support encryption at rest using KMS This is an incorrect statement. As discussed above, data at rest is also encrypted.EBS volumes don't support any encryption This is an incorrect statement. Amazon EBS encryption offers a straightforward encryption solution for your EBS resources associated with your EC2 instances. With Amazon EBS encryption, you aren't required to build, maintain, and secure your own key management infrastructure.Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html"},{question:"Your company has a threeyear contract with a healthcare provider. The contract states that monthly database backups must be retained for the duration of the contract for compliance purposes. Currently, the limit on backup retention for automated backups, on Amazon Relational Database Service (RDS), does not meet your requirements. Which of the following solutions can help you meet your requirements?",answers:[{text:"Enable RDS automatic backups",isCorrect:!1},{text:"Enable RDS MultiAZ",isCorrect:!1},{text:"Enable RDS Read replicas",isCorrect:!1},{text:"Create a cron event in CloudWatch, which triggers an AWS Lambda function that triggers the database snapshot",isCorrect:!0}],explanation:"Correct option:Create a cron event in CloudWatch, which triggers an AWS Lambda function that triggers the database snapshot There are multiple ways to run periodic jobs in AWS. CloudWatch Events with Lambda is the simplest of all solutions. To do this, create a CloudWatch Rule and select “Schedule” as the Event Source. You can either use a cron expression or provide a fixed rate (such as every 5 minutes). Next, select “Lambda Function” as the Target. Your Lambda will have the necessary code for snapshot functionality.Incorrect options:Enable RDS automatic backups You can enable automatic backups but as of 2020, the retention period is 0 to 35 days.Enable RDS Read replicas Amazon RDS server's builtin replication functionality to create a special type of DB instance called a read replica from a source DB instance. Updates made to the source DB instance are asynchronously copied to the read replica. Read replicas are useful for heavy readonly data workloads. These are not suitable for the given usecase.Enable RDS MultiAZ MultiAZ allows you to create a highly available application with RDS. It does not directly help in database backups or retention periods.Reference:https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html https://docs.aws.amazon.com/lambda/latest/dg/withscheduledevents.html"},{question:"A developer wants to integrate userspecific file upload and download features in an application that uses both Amazon Cognito user pools and Cognito identity pools for secure access with Amazon S3. The developer also wants to ensure that only authorized users can access their own files and that the files are securely saved and retrieved. The files are 5 KB to 500 MB in size. What do you recommend as the most efficient solution?",answers:[{text:"Integrate Amazon API Gateway with a Lambda function that validates that the given file is uploaded to S3 and downloaded from S3 only by the authorized user",isCorrect:!1},{text:"Use S3 Event Notifications to trigger a Lambda function that validates that the given file is uploaded and downloaded only by the authorized user",isCorrect:!1},{text:"Leverage an IAM policy with the Amazon Cognito identity prefix to restrict users to use their own folders in Amazon S3",isCorrect:!0},{text:"Use CloudFront Lambda@Edge to validate that the given file is uploaded to S3 and downloaded from S3 only by the authorized user",isCorrect:!1}],explanation:"Correct option:Leverage an IAM policy with the Amazon Cognito identity prefix to restrict users to use their own folders in Amazon S3Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limitedprivilege AWS credentials to access other AWS services. Amazon Cognito identity pools support the following identity providers:Public providers: Login with Amazon (identity pools), Facebook (identity pools), Google (identity pools), Sign in with Apple (identity pools).Amazon Cognito user poolsOpenID Connect providers (identity pools)SAML identity providers (identity pools)Developer authenticated identities (identity pools)You can create an identitybased policy that allows Amazon Cognito users to access objects in a specific S3 bucket. This policy allows access only to objects with a name that includes Cognito, the name of the application, and the federated user's ID, represented by the ${cognitoidentity.amazonaws.com:sub} variable.via https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_cognitobucket.html Incorrect options:Use S3 Event Notifications to trigger a Lambda function that validates that the given file is uploaded and downloaded only by the authorized user While it is certainly possible to build this solution, however, it is not the most optimal solution as it does not prevent an invalid upload of a file into another user's designated folder. So this option is incorrect.Integrate Amazon API Gateway with a Lambda function that validates that the given file is uploaded to S3 and downloaded from S3 only by the authorized user Again, it is certainly possible to build this solution, however, it is not the most optimal solution as it does not prevent an invalid upload of a file into another user's designated folder. So this option is incorrect.Use CloudFront Lambda@Edge to validate that the given file is uploaded to S3 and downloaded from S3 only by the authorized user This option assumes that the solution comprises a CloudFront distribution. This introduces inefficiency in the solution, as one needs to pay for CloudFront/Lambda@Edge and adds unnecessary hops in the data flow for both uploads and downloads.References:https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_cognitobucket.html https://docs.aws.amazon.com/cognito/latest/developerguide/amazoncognitointegratinguserpoolswithidentitypools.html"},{question:"You create an Auto Scaling group to work with an Application Load Balancer. The scaling group is configured with a minimum size value of 5, a maximum value of 20, and the desired capacity value of 10. One of the 10 EC2 instances has been reported as unhealthy. Which of the following actions will take place?",answers:[{text:"The ASG will format the root EBS drive on the EC2 instance and run the User Data again",isCorrect:!1},{text:"The ASG will keep the instance running and restart the application",isCorrect:!1},{text:"The ASG will terminate the EC2 Instance",isCorrect:!0},{text:"The ASG will detach the EC2 instance from the group, and leave it running",isCorrect:!1}],explanation:"Correct option:The ASG will terminate the EC2 InstanceTo maintain the same number of instances, Amazon EC2 Auto Scaling performs a periodic health check on running instances within an Auto Scaling group. When it finds that an instance is unhealthy, it terminates that instance and launches a new one. Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance.Incorrect options:The ASG will detach the EC2 instance from the group, and leave it running The goal of the autoscaling group is to get rid of the bad instance and replace itThe ASG will keep the instance running and restart the application The ASG does not have control of your applicationThe ASG will format the root EBS drive on the EC2 instance and run the User Data again This will not happen, the ASG cannot assume the format of your EBS drive, and User Data only runs once at instance first boot.References:https://aws.amazon.com/premiumsupport/knowledgecenter/autoscalingterminateinstance https://docs.aws.amazon.com/autoscaling/ec2/userguide/asmaintaininstancelevels.html#replaceunhealthyinstancee"},{question:"You have uploaded a zip file to AWS Lambda that contains code files written in Node.Js. When your function is executed you receive the following output, 'Error: Memory Size: 10,240 MB Max Memory Used'. Which of the following explains the problem?",answers:[{text:"Your zip file is corrupt",isCorrect:!1},{text:"The uncompressed zip file exceeds AWS Lambda limits",isCorrect:!1},{text:"You have uploaded a zip file larger than 50 MB to AWS Lambda",isCorrect:!1},{text:"Your Lambda function ran out of RAM",isCorrect:!0}],explanation:"Your Lambda function ran out of RAMAWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.How Lambda function works: via https://aws.amazon.com/lambda/ The maximum amount of memory available to the Lambda function at runtime is 10,240 MB. Your Lambda function was deployed with 10,240 MB of RAM, but it seems your code requested or used more than that, so the Lambda function failed.via https://docs.aws.amazon.com/lambda/latest/dg/configurationconsole.html Incorrect options:Your zip file is corrupt A memory size error states that Lambda was able to extract so the file is not corruptThe uncompressed zip file exceeds AWS Lambda limits This is not correct as your function was able to execute.You have uploaded a zip file larger than 50 MB to AWS Lambda This is not correct as your lambda function was able to executeReference:https://docs.aws.amazon.com/lambda/latest/dg/configurationconsole.htmll"},{question:"A company wants to automate and orchestrate a multisource highvolume flow of data in a scalable data management solution built using AWS services. The solution must ensure that the business rules and transformations run in sequence, handle reprocessing of data in case of errors, and require minimal maintenance. Which AWS service should the company use to manage and automate the orchestration of the data flows?",answers:[{text:"AWS Step Functions",isCorrect:!0},{text:"AWS Batch",isCorrect:!1},{text:"AWS Glue",isCorrect:!1},{text:"Amazon Kinesis Data Streams",isCorrect:!1}],explanation:"Correct option:AWS Step FunctionsAWS Step Functions is a visual workflow service that helps developers use AWS services to build distributed applications, automate processes, orchestrate microservices, and create data and machine learning (ML) pipelines.via https://aws.amazon.com/stepfunctions/ Incorrect options:Amazon Kinesis Data Streams Amazon Kinesis Data Streams is a serverless streaming data service that makes it easy to capture, process, and store data streams at any scale.AWS Glue AWS Glue is a serverless data integration service that makes it easier to discover, prepare, move, and integrate data from multiple sources for analytics, machine learning (ML), and application development.AWS Batch AWS Batch is a set of batch management capabilities that enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized compute resources) based on the volume and specific resource requirements of the batch jobs submitted.References:https://aws.amazon.com/stepfunctions/ https://aws.amazon.com/kinesis/datastreams/ https://aws.amazon.com/glue/ https://aws.amazon.com/batch/faqs/"},{question:"As a Senior Developer, you manage 10 Amazon EC2 instances that make readheavy database requests to the Amazon RDS for PostgreSQL. You need to make this architecture resilient for disaster recovery. Which of the following features will help you prepare for database disaster recovery? (Select two)",answers:[{text:"Use database cloning feature of the RDS DB cluster",isCorrect:!1},{text:"Enable the automated backup feature of Amazon RDS in a multiAZ deployment that creates backups in a single AWS Region",isCorrect:!0},{text:"Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage",isCorrect:!1},{text:"Enable the automated backup feature of Amazon RDS in a multiAZ deployment that creates backups across multiple Regions",isCorrect:!1},{text:"Use crossRegion Read Replicas",isCorrect:!0}],explanation:"Use crossRegion Read ReplicasIn addition to using Read Replicas to reduce the load on your source DB instance, you can also use Read Replicas to implement a DR solution for your production DB environment. If the source DB instance fails, you can promote your Read Replica to a standalone source server. Read Replicas can also be created in a different Region than the source database. Using a crossRegion Read Replica can help ensure that you get back up and running if you experience a regional availability issue.Enable the automated backup feature of Amazon RDS in a multiAZ deployment that creates backups in a single AWS RegionAmazon RDS provides high availability and failover support for DB instances using MultiAZ deployments. Amazon RDS uses several different technologies to provide failover support. MultiAZ deployments for MariaDB, MySQL, Oracle, and PostgreSQL DB instances use Amazon's failover technology.The automated backup feature of Amazon RDS enables pointintime recovery for your database instance. Amazon RDS will backup your database and transaction logs and store both for a userspecified retention period. If it’s a MultiAZ configuration, backups occur on the standby to reduce I/O impact on the primary. Automated backups are limited to a single AWS Region while manual snapshots and Read Replicas are supported across multiple Regions.Incorrect options:Enable the automated backup feature of Amazon RDS in a multiAZ deployment that creates backups across multiple Regions This is an incorrect statement. Automated backups are limited to a single AWS Region while manual snapshots and Read Replicas are supported across multiple Regions.Use RDS Provisioned IOPS (SSD) Storage in place of General Purpose (SSD) Storage Amazon RDS Provisioned IOPS Storage is an SSDbacked storage option designed to deliver fast, predictable, and consistent I/O performance. This storage type enhances the performance of the RDS database, but this isn't a disaster recovery option.Use database cloning feature of the RDS DB cluster This option has been added as a distractor. Database cloning is only available for Aurora and not for RDS.References:https://aws.amazon.com/rds/features/ https://aws.amazon.com/blogs/database/implementingadisasterrecoverystrategywithamazonrds/"},{question:"A telecom service provider stores its critical customer data on Amazon Simple Storage Service (Amazon S3). Which of the following options can be used to control access to data stored on Amazon S3? (Select two)",answers:[{text:"Bucket policies, Identity and Access Management (IAM) policies",isCorrect:!0},{text:"Permissions boundaries, Identity and Access Management (IAM) policies",isCorrect:!1},{text:"Query String Authentication, Permissions boundaries",isCorrect:!1},{text:"IAM database authentication, Bucket policies",isCorrect:!1},{text:"Query String Authentication, Access Control Lists (ACLs)",isCorrect:!0}],explanation:"Correct options:Bucket policies, Identity and Access Management (IAM) policiesQuery String Authentication, Access Control Lists (ACLs)Customers may use four mechanisms for controlling access to Amazon S3 resources: Identity and Access Management (IAM) policies, bucket policies, Access Control Lists (ACLs), and Query String Authentication.IAM enables organizations with multiple employees to create and manage multiple users under a single AWS account. With IAM policies, customers can grant IAM users finegrained control to their Amazon S3 bucket or objects while also retaining full control over everything the users do.With bucket policies, customers can define rules which apply broadly across all requests to their Amazon S3 resources, such as granting write privileges to a subset of Amazon S3 resources. Customers can also restrict access based on an aspect of the request, such as HTTP referrer and IP address.With ACLs, customers can grant specific permissions (i.e. READ, WRITE, FULL_CONTROL) to specific users for an individual bucket or object.With Query String Authentication, customers can create a URL to an Amazon S3 object which is only valid for a limited time. Using query parameters to authenticate requests is useful when you want to express a request entirely in a URL. This method is also referred as presigning a URL.Incorrect options:Permissions boundaries, Identity and Access Management (IAM) policiesQuery String Authentication, Permissions boundariesIAM database authentication, Bucket policiesPermissions boundary A Permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identitybased policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identitybased policies and its permissions boundaries. When you use a policy to set the permissions boundary for a user, it limits the user's permissions but does not provide permissions on its own.IAM database authentication IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token. It is a database authentication technique and cannot be used to authenticate for S3.Therefore, all three options are incorrect.References:https://docs.aws.amazon.com/AmazonS3/latest/userguide/accesscontroloverview.html https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html"},{question:"A Developer is configuring Amazon EC2 Auto Scaling group to scale dynamically. Which metric below is NOT part of Target Tracking Scaling Policy?",answers:[{text:"ALBRequestCountPerTarget",isCorrect:!1},{text:"ASGAverageCPUUtilization",isCorrect:!1},{text:"ApproximateNumberOfMessagesVisible",isCorrect:!0},{text:"ASGAverageNetworkOut",isCorrect:!1}],explanation:"Correct option:ApproximateNumberOfMessagesVisible This is a CloudWatch Amazon SQS queue metric. The number of messages in a queue might not change proportionally to the size of the Auto Scaling group that processes messages from the queue. Hence, this metric does not work for target tracking.Incorrect options:With target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value.It is important to note that a target tracking scaling policy assumes that it should scale out your Auto Scaling group when the specified metric is above the target value. You cannot use a target tracking scaling policy to scale out your Auto Scaling group when the specified metric is below the target value.ASGAverageCPUUtilization This is a predefined metric for target tracking scaling policy. This represents the Average CPU utilization of the Auto Scaling group.ASGAverageNetworkOut This is a predefined metric for target tracking scaling policy. This represents the Average number of bytes sent out on all network interfaces by the Auto Scaling group.ALBRequestCountPerTarget This is a predefined metric for target tracking scaling policy. This represents the Number of requests completed per target in an Application Load Balancer target group.Reference:https://docs.aws.amazon.com/autoscaling/ec2/userguide/asscalingtargettracking.htmll"},{question:"You have launched several AWS Lambda functions written in Java. A new requirement was given that over 1MB of data should be passed to the functions and should be encrypted and decrypted at runtime. Which of the following methods is suitable to address the given usecase?",answers:[{text:"Use Envelope Encryption and store as environment variable",isCorrect:!1},{text:"Use KMS direct encryption and store as file",isCorrect:!1},{text:"Use KMS Encryption and store as environment variable",isCorrect:!1},{text:"Use Envelope Encryption and reference the data as file within the code",isCorrect:!0}],explanation:"Correct option:Use Envelope Encryption and reference the data as file within the codeWhile AWS KMS does support sending data up to 4 KB to be encrypted directly, envelope encryption can offer significant performance benefits. When you encrypt data directly with AWS KMS it must be transferred over the network. Envelope encryption reduces the network load since only the request and delivery of the much smaller data key go over the network. The data key is used locally in your application or encrypting AWS service, avoiding the need to send the entire block of data to AWS KMS and suffer network latency.AWS Lambda environment variables can have a maximum size of 4 KB. Additionally, the direct 'Encrypt' API of KMS also has an upper limit of 4 KB for the data payload. To encrypt 1 MB, you need to use the Encryption SDK and pack the encrypted file with the lambda function.Incorrect options:Use KMS direct encryption and store as file You can only encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information, so this option is not correct for the given usecase.Use Envelope Encryption and store as an environment variable Environment variables must not exceed 4 KB, so this option is not correct for the given usecase.Use KMS Encryption and store as an environment variable You can encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information. Lambda Environment variables must not exceed 4 KB. So this option is not correct for the given usecase.References:https://docs.aws.amazon.com/lambda/latest/dg/gettingstartedlimits.html https://aws.amazon.com/kms/faqs//"},{question:"DevOps engineers are developing an order processing system where notifications are sent to a department whenever an order is placed for a product. The system also pushes identical notifications of the new order to a processing module that would allow EC2 instances to handle the fulfillment of the order. In the case of processing errors, the messages should be allowed to be reprocessed at a later stage. The order processing system should be able to scale transparently without the need for any manual or programmatic provisioning of resources. Which of the following solutions can be used to address this usecase in the most costefficient way?",answers:[{text:"SNS + Lambda",isCorrect:!1},{text:"SNS + Kinesis",isCorrect:!1},{text:"SQS + SES",isCorrect:!1},{text:"SNS + SQS",isCorrect:!0}],explanation:'SNS + SQSAmazon SNS enables message filtering and fanout to a large number of subscribers, including serverless functions, queues, and distributed systems. Additionally, Amazon SNS fans out notifications to end users via mobile push messages, SMS, and email.How SNS Works: via https://aws.amazon.com/sns/ Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, besteffort ordering, and atleastonce delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.Because each buffered request can be processed independently, Amazon SQS can scale transparently to handle the load without any provisioning instructions from you.SNS and SQS can be used to create a fanout messaging scenario in which messages are "pushed" to multiple subscribers, which eliminates the need to periodically check or poll for updates and enables parallel asynchronous processing of the message by the subscribers. SQS can allow for later reprocessing and dead letter queues. This is called the fanout pattern.Incorrect options:SNS + Kinesis You can use Amazon Kinesis Data Streams to collect and process large streams of data records in realtime. Kinesis Data Streams stores records from 24 hours (by default) to 8760 hours (365 days). However, you need to manually provision shards in case the load increases or you need to use CloudWatch alarms to set up auto scaling for the shards. Since Kinesis only supports transparent scaling in the ondemand mode, however, it is not cost efficient for the given use case, so this option is not the right fit for the given use case.SNS + Lambda Amazon SNS and AWS Lambda are integrated so you can invoke Lambda functions with Amazon SNS notifications. The Lambda function receives the message payload as an input parameter and can manipulate the information in the message, publish the message to other SNS topics, or send the message to other AWS services. However, your EC2 instances cannot "poll" from Lambda functions and as such, this would not work.SQS + SES This will not work as the messages need to be processed twice (once for sending the notification and later for order fulfillment) and SQS only allows for one consuming application.References:https://aws.amazon.com/sns/ https://aws.amazon.com/gettingstarted/tutorials/sendfanouteventnotifications/'},{question:"A pharmaceutical company uses Amazon EC2 instances for application hosting and Amazon CloudFront for content delivery. A new research paper with critical findings has to be shared with a research team that is spread across the world. Which of the following represents the most optimal solution to address this requirement without compromising the security of the content?",answers:[{text:"Using CloudFront's FieldLevel Encryption to help protect sensitive data",isCorrect:!1},{text:"Use CloudFront signed URL feature to control access to the file",isCorrect:!0},{text:"Use CloudFront signed cookies feature to control access to the file",isCorrect:!1},{text:"Configure AWS Web Application Firewall (WAF) to monitor and control the HTTP and HTTPS requests that are forwarded to CloudFront",isCorrect:!1}],explanation:"Use CloudFront signed URL feature to control access to the fileA signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content.Here's an overview of how you configure CloudFront for signed URLs and how CloudFront responds when a user uses a signed URL to request a file:In your CloudFront distribution, specify one or more trusted key groups, which contain the public keys that CloudFront can use to verify the URL signature. You use the corresponding private keys to sign the URLs.Develop your application to determine whether a user should have access to your content and to create signed URLs for the files or parts of your application that you want to restrict access to.A user requests a file for which you want to require signed URLs. Your application verifies that the user is entitled to access the file: they've signed in, they've paid for access to the content, or they've met some other requirement for access.Your application creates and returns a signed URL to the user. The signed URL allows the user to download or stream the content.This step is automatic; the user usually doesn't have to do anything additional to access the content. For example, if a user is accessing your content in a web browser, your application returns the signed URL to the browser. The browser immediately uses the signed URL to access the file in the CloudFront edge cache without any intervention from the user.CloudFront uses the public key to validate the signature and confirm that the URL hasn't been tampered with. If the signature is invalid, the request is rejected. If the request meets the requirements in the policy statement, CloudFront does the standard operations: determines whether the file is already in the edge cache, forwards the request to the origin if necessary, and returns the file to the user.Incorrect options:Use CloudFront signed cookies feature to control access to the file CloudFront signed cookies allow you to control who can access your content when you don't want to change your current URLs or when you want to provide access to multiple restricted files, for example, all of the files in the subscribers' area of a website. Our requirement has only one file that needs to be shared and hence signed URL is the optimal solution.Signed URLs take precedence over signed cookies. If you use both signed URLs and signed cookies to control access to the same files and a viewer uses a signed URL to request a file, CloudFront determines whether to return the file to the viewer based only on the signed URL.Configure AWS Web Application Firewall (WAF) to monitor and control the HTTP and HTTPS requests that are forwarded to CloudFront AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to CloudFront, and lets you control access to your content. Based on conditions that you specify, such as the values of query strings or the IP addresses that requests originate from, CloudFront responds to requests either with the requested content or with an HTTP status code 403 (Forbidden). A firewall is optimal for broader use cases than restricted access to a single file.Using CloudFront's FieldLevel Encryption to help protect sensitive data CloudFront's fieldlevel encryption further encrypts sensitive data in an HTTPS form using fieldspecific encryption keys (which you supply) before a POST request is forwarded to your origin. This ensures that sensitive data can only be decrypted and viewed by certain components or services in your application stack. This feature is not useful for the given use case.References:https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/privatecontentsignedurls.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distributionwebawswaf.html https://aws.amazon.com/aboutaws/whatsnew/2017/12/introducingfieldlevelencryptiononamazoncloudfront/"},{question:"A developer is configuring an Application Load Balancer (ALB) to direct traffic to the application's EC2 instances and Lambda functions. Which of the following characteristics of the ALB can be identified as correct? (Select two)",answers:[{text:"An ALB has three possible target types: Instance, IP and Lambda",isCorrect:!0},{text:"An ALB has three possible target types: Hostname, IP and Lambda",isCorrect:!1},{text:"If you specify targets using an instance ID, traffic is routed to instances using any private IP address from one or more network interfaces",isCorrect:!1},{text:"You can not specify publicly routable IP addresses to an ALB",isCorrect:!0},{text:"If you specify targets using IP addresses, traffic is routed to instances using the primary private IP address",isCorrect:!1}],explanation:"An ALB has three possible target types: Instance, IP and LambdaWhen you create a target group, you specify its target type, which determines the type of target you specify when registering targets with this target group. After you create a target group, you cannot change its target type. The following are the possible target types:Instance The targets are specified by instance IDIP The targets are IP addressesLambda The target is a Lambda functionYou can not specify publicly routable IP addresses to an ALBWhen the target type is IP, you can specify IP addresses from specific CIDR blocks only. You can't specify publicly routable IP addresses.Incorrect options:If you specify targets using an instance ID, traffic is routed to instances using any private IP address from one or more network interfaces If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance.If you specify targets using IP addresses, traffic is routed to instances using the primary private IP address If you specify targets using IP addresses, you can route traffic to an instance using any private IP address from one or more network interfaces. This enables multiple applications on an instance to use the same port.An ALB has three possible target types: Hostname, IP and Lambda This is incorrect, as described in the correct explanation above.Reference: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/loadbalancertargetgroups.html"},{question:"You have an Auto Scaling group configured to a minimum capacity of 1 and a maximum capacity of 5, designed to launch EC2 instances across 3 Availability Zones. During a low utilization period, an entire Availability Zone went down and your application experienced downtime. What can you do to ensure that your application remains highly available?",answers:[{text:"Configure ASG fast failover",isCorrect:!1},{text:"Change the scaling metric of autoscaling policy to network bytes",isCorrect:!1},{text:"Increase the minimum instance capacity of the Auto Scaling Group to 2",isCorrect:!0},{text:"Enable RDS MultiAZ",isCorrect:!1}],explanation:"Increase the minimum instance capacity of the Auto Scaling Group to 2 You configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. The minimum and maximum capacity are required to create an Auto Scaling group, while the desired capacity is optional. If you do not define your desired capacity upfront, it defaults to your minimum capacity.Since a minimum capacity of 1 was defined, an instance was launched in only one AZ. This AZ went down, taking the application with it. If the minimum capacity is set to 2. As per Auto Scale AZ configuration, it would have launched 2 instances one in each AZ, making the architecture disasterproof and hence highly available.via https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscalingbenefits.html Incorrect options:Change the scaling metric of autoscaling policy to network bytes With target tracking scaling policies, you select a scaling metric and set a target value. You can use predefined customized metrics. Setting the metric to network bytes will not help in this context since the instances have to be spread across different AZs for high availability. The optimized way of doing it, is by defining minimum and maximum instance capacities, as discussed above.Configure ASG fast failover This is a madeup option, given as a distractor.Enable RDS MultiAZ This configuration will make your database highly available. But for the current scenario, you will need to have more than 1 instance in separate availability zones to keep the application highly available.References:https://docs.aws.amazon.com/autoscaling/ec2/userguide/asgcapacitylimits.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/asmaintaininstancelevels.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/asscalingtargettracking.htmll"},{question:"A telecommunications company that provides internet service for mobile device users maintains over 100 c4.large instances in the useast1 region. The EC2 instances run complex algorithms. The manager would like to track CPU utilization of the EC2 instances as frequently as every 10 seconds. Which of the following represents the BEST solution for the given usecase?",answers:[{text:"Enable EC2 detailed monitoring",isCorrect:!1},{text:"Simply get it from the CloudWatch Metrics",isCorrect:!1},{text:"Create a highresolution custom metric and push the data using a script triggered every 10 seconds",isCorrect:!0},{text:"Open a support ticket with AWS",isCorrect:!1}],explanation:"Create a highresolution custom metric and push the data using a script triggered every 10 secondsUsing highresolution custom metric, your applications can publish metrics to CloudWatch with 1second resolution. You can watch the metrics scroll across your screen seconds after they are published and you can set up highresolution CloudWatch Alarms that evaluate as frequently as every 10 seconds. You can alert with HighResolution Alarms, as frequently as 10second periods. HighResolution Alarms allow you to react and take actions faster and support the same actions available today with standard 1minute alarms.via https://aws.amazon.com/blogs/aws/newhighresolutioncustommetricsandalarmsforamazoncloudwatch/ Incorrect options:Enable EC2 detailed monitoring As part of basic monitoring, Amazon EC2 sends metric data to CloudWatch in 5minute periods. To send metric data for your instance to CloudWatch in 1minute periods, you can enable detailed monitoring on the instance, however, this comes at an additional cost.Simply get it from the CloudWatch Metrics You can get data from metrics. The basic monitoring data is available automatically in a 5minute interval and detailed monitoring data is available in a 1minute interval.Open a support ticket with AWS This option has been added as a distractor.Reference: https://aws.amazon.com/blogs/aws/newhighresolutioncustommetricsandalarmsforamazoncloudwatch/"},{question:"An organization is moving its onpremises resources to the cloud. Source code will be moved to AWS CodeCommit and AWS CodeBuild will be used for compiling the source code using Apache Maven as a build tool. The organization wants the build environment should allow for scaling and running builds in parallel. Which of the following options should the organization choose for their requirement?",answers:[{text:"Choose a highperformance instance type for your CodeBuild instances",isCorrect:!1},{text:"Enable CodeBuild Auto Scaling",isCorrect:!1},{text:"CodeBuild scales automatically, the organization does not have to do anything for scaling or for parallel builds",isCorrect:!0},{text:"Run CodeBuild in an Auto Scaling group",isCorrect:!1}],explanation:"Correct option:CodeBuild scales automatically, the organization does not have to do anything for scaling or for parallel builds AWS CodeBuild is a fully managed build service in the cloud. CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. It provides prepackaged build environments for popular programming languages and build tools such as Apache Maven, Gradle, and more. You can also customize build environments in CodeBuild to use your own build tools. CodeBuild scales automatically to meet peak build requests.Incorrect options:Choose a highperformance instance type for your CodeBuild instances For the current requirement, this is will not make any difference.Run CodeBuild in an Auto Scaling Group AWS CodeBuild is a managed service and scales automatically, does not need Auto Scaling Group to scale it up.Enable CodeBuild Auto Scaling This has been added as a distractor. CodeBuild scales automatically to meet peak build requests.References:https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html https://docs.aws.amazon.com/codebuild/latest/userguide/buildenvrefcomputetypes.html"},{question:"An application runs on an EC2 instance and processes orders on a nightly basis. This EC2 instance needs to access the orders that are stored in S3. How would you recommend the EC2 instance access the orders securely?",answers:[{text:"Use an IAM role",isCorrect:!0},{text:"Use EC2 User Data",isCorrect:!1},{text:"Create an IAM programmatic user and store the access key and secret access key on the EC2 ~/.aws/credentials file.",isCorrect:!1},{text:"Create an S3 bucket policy that authorises public access",isCorrect:!1}],explanation:"Correct option:Use an IAM roleIAM roles have been incorporated so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles.Amazon EC2 uses an instance profile as a container for an IAM role. When you create an IAM role using the IAM console, the console creates an instance profile automatically and gives it the same name as the role to which it corresponds.This is the most secure option as the role assigned to EC2 can be used to access S3 without storing any credentials onto the EC2 instance.Incorrect options:Create an IAM programmatic user and store the access key and secret access key on the EC2 ~/.aws/credentials file. While this would work, this is highly insecure as anyone gaining access to the EC2 instance would be able to steal the credentials stored in that file.Use EC2 User Data EC2 User Data is used to run bootstrap scripts at an instance's first launch. This option is not the right fit for the given usecase.Create an S3 bucket policy that authorizes public access While this would work, it would allow anyone to access your S3 bucket files. So this option is ruled out.Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iamrolesforamazonec2.html"},{question:"You have migrated an onpremise SQL Server database to an Amazon Relational Database Service (RDS) database attached to a VPC inside a private subnet. Also, the related Java application, hosted onpremise, has been moved to an Amazon Lambda function. Which of the following should you implement to connect AWS Lambda function to its RDS instance?",answers:[{text:"Configure lambda to connect to the public subnet that will give internet access and use Security Group to access RDS inside the private subnet",isCorrect:!1},{text:"Use Environment variables to pass in the RDS connection string",isCorrect:!1},{text:"Use Lambda layers to connect to the internet and RDS separately",isCorrect:!1},{text:"Configure Lambda to connect to VPC with private subnet and Security Group needed to access RDS",isCorrect:!0}],explanation:"Correct option:Configure Lambda to connect to VPC with private subnet and Security Group needed to access RDS You can configure a Lambda function to connect to private subnets in a virtual private cloud (VPC) in your account. Use Amazon Virtual Private Cloud (Amazon VPC) to create a private network for resources such as databases, cache instances, or internal services. Connect your lambda function to the VPC to access private resources during execution. When you connect a function to a VPC, Lambda creates an elastic network interface for each combination of the security group and subnet in your function's VPC configuration. This is the right way of giving RDS access to Lambda.Lambda VPC Config: via https://docs.aws.amazon.com/lambda/latest/dg/configurationvpc.html Incorrect options:Use Lambda layers to connect to the internet and RDS separately You can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. Layers will not help in configuring access to RDS instance and hence is an incorrect choice.Configure lambda to connect to the public subnet that will give internet access and use the Security Group to access RDS inside the private subnet This is an incorrect statement. Connecting a Lambda function to a public subnet does not give it internet access or a public IP address. To grant internet access to your function, its associated VPC must have a NAT gateway (or NAT instance) in a public subnet.Use Environment variables to pass in the RDS connection string You can use environment variables to store secrets securely and adjust your function's behavior without updating code. You can use environment variables to exchange data with RDS, but you will still need access to RDS, which is not possible with just environment variables.References:https://docs.aws.amazon.com/lambda/latest/dg/configurationvpc.html https://docs.aws.amazon.com/lambda/latest/dg/configurationlayers.html https://docs.aws.amazon.com/lambda/latest/dg/configurationenvvars.html https://aws.amazon.com/premiumsupport/knowledgecenter/internetaccesslambdafunction/"},{question:"As an AWS Certified Developer Associate, you have been hired to work with the development team at a company to create a REST API using the serverless architecture. Which of the following solutions will you choose to move the company to the serverless architecture paradigm?",answers:[{text:"API Gateway exposing Lambda Functionality",isCorrect:!0},{text:"Fargate with Lambda at the front",isCorrect:!1},{text:"Route 53 with EC2 as backend",isCorrect:!1},{text:"Publicfacing Application Load Balancer with ECS on Amazon EC2",isCorrect:!1}],explanation:'Correct option:API Gateway exposing Lambda FunctionalityAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door" for applications to access data, business logic, or functionality from your backend services.How API Gateway Works: via https://aws.amazon.com/apigateway/ AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.How Lambda function works: via https://aws.amazon.com/lambda/ API Gateway can expose Lambda functionality through RESTful APIs. Both are serverless options offered by AWS and hence the right choice for this scenario, considering all the functionality they offer.Incorrect options:Fargate with Lambda at the front Lambda cannot directly handle RESTful API requests. You can invoke a Lambda function over HTTPS by defining a custom RESTful API using Amazon API Gateway. So, Fargate with Lambda as the frontfacing service is a wrong combination, though both Fargate and Lambda are serverless.Publicfacing Application Load Balancer with ECS on Amazon EC2 ECS on Amazon EC2 does not come under serverless and hence cannot be considered for this use case.Route 53 with EC2 as backend Amazon EC2 is not a serverless service and hence cannot be considered for this use case.References:https://aws.amazon.com/serverless/ https://aws.amazon.com/apigateway/'},{question:"After a code review, a developer has been asked to make his publicly accessible S3 buckets private, and enable access to objects with a timebound constraint. Which of the following options will address the given usecase?",answers:[{text:"It is not possible to implement time constraints on Amazon S3 Bucket access",isCorrect:!1},{text:"Use Bucket policy to block the unintended access",isCorrect:!1},{text:"Use Routing policies to reroute unintended access",isCorrect:!1},{text:"Share presigned URLs with resources that need access",isCorrect:!0}],explanation:"Correct option:Share presigned URLs with resources that need access All objects by default are private, with the object owner having permission to access the objects. However, the object owner can optionally share objects with others by creating a presigned URL, using their own security credentials, to grant timelimited permission to download the objects. When you create a presigned URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object), and expiration date and time. The presigned URLs are valid only for the specified duration.Incorrect options:Use Bucket policy to block the unintended access A bucket policy is a resourcebased AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. Bucket policy can be used to block off unintended access, but it's not possible to provide timebased access, as is the case in the current use case.Use Routing policies to reroute unintended access There is no such facility directly available with Amazon S3.It is not possible to implement time constraints on Amazon S3 Bucket access This is an incorrect statement. As explained above, it is possible to give timebound access permissions on S3 buckets and objects.References:https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html https://docs.aws.amazon.com/AmazonS3/latest/userguide/addbucketpolicy.html"},{question:"An IT company has migrated to a serverless application stack on the AWS Cloud with the compute layer being implemented via Lambda functions. The engineering managers would like to actively troubleshoot any failures in the Lambda functions. As a Developer Associate, which of the following solutions would you suggest for this usecase?",answers:[{text:"Use CodeCommit to identify and notify any failures in the Lambda code",isCorrect:!1},{text:"The developers should insert logging statements in the Lambda function code which are then available via CloudWatch logs",isCorrect:!0},{text:"Use CodeDeploy to identify and notify any failures in the Lambda code",isCorrect:!1},{text:"Use CloudWatch Events to identify and notify any failures in the Lambda code",isCorrect:!1}],explanation:'"The developers should insert logging statements in the Lambda function code which are then available via CloudWatch logs"When you invoke a Lambda function, two types of error can occur. Invocation errors occur when the invocation request is rejected before your function receives it. Function errors occur when your function\'s code or runtime returns an error. Depending on the type of error, the type of invocation, and the client or service that invokes the function, the retry behavior, and the strategy for managing errors varies.Lambda function failures are commonly caused by:Permissions issues Code issues Network issues Throttling Invoke API 500 and 502 errorsYou can insert logging statements into your code to help you validate that your code is working as expected. Lambda automatically integrates with CloudWatch Logs and pushes all logs from your code to a CloudWatch Logs group associated with a Lambda function, which is named /aws/lambda/.Please see this note for more details: via https://docs.aws.amazon.com/lambda/latest/dg/monitoringcloudwatchlogs.html Incorrect options:"Use CloudWatch Events to identify and notify any failures in the Lambda code" Typically Lambda functions are triggered as a response to a CloudWatch Event. CloudWatch Events cannot identify and notify failures in the Lambda code."Use CodeCommit to identify and notify any failures in the Lambda code""Use CodeDeploy to identify and notify any failures in the Lambda code"AWS CodeCommit is a fullymanaged source control service that hosts secure Gitbased repositories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem.AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your onpremises servers.Neither CodeCommit nor CodeDeploy can identify and notify failures in the Lambda code.Reference: https://docs.aws.amazon.com/lambda/latest/dg/monitoringcloudwatchlogs.html'},{question:"A developer is looking at establishing access control for an API that connects to a Lambda function downstream. Which of the following represents a mechanism that CANNOT be used for authenticating with the API Gateway?",answers:[{text:"Standard AWS IAM roles and policies",isCorrect:!1},{text:"Cognito User Pools",isCorrect:!1},{text:"AWS Security Token Service (STS)",isCorrect:!0},{text:"Lambda Authorizer",isCorrect:!1}],explanation:"Correct option:Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud.How API Gateway Works: via https://aws.amazon.com/apigateway/AWS Security Token Service (STS) AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limitedprivilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). However, it is not supported by API Gateway.API Gateway supports the following mechanisms for authentication and authorization: via https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaycontrolaccesstoapi.htmlIncorrect options:Standard AWS IAM roles and policies Standard AWS IAM roles and policies offer flexible and robust access controls that can be applied to an entire API or individual methods. IAM roles and policies can be used for controlling who can create and manage your APIs, as well as who can invoke them.Lambda Authorizer Lambda authorizers are Lambda functions that control access to REST API methods using bearer token authentication—as well as information described by headers, paths, query strings, stage variables, or context variables request parameters. Lambda authorizers are used to control who can invoke REST API methods.Cognito User Pools Amazon Cognito user pools let you create customizable authentication and authorization solutions for your REST APIs. Amazon Cognito user pools are used to control who can invoke REST API methods.References:https://docs.aws.amazon.com/apigateway/latest/developerguide/apigatewaycontrolaccesstoapi.htmlhttps://docs.aws.amazon.com/STS/latest/APIReference/welcome.htmll"},{question:"A development team is considering Amazon ElastiCache for Redis as its inmemory caching solution for its relational database. Which of the following options are correct while configuring ElastiCache? (Select two)",answers:[{text:"You can scale write capacity for Redis by adding replica nodes",isCorrect:!1},{text:"All the nodes in a Redis cluster must reside in the same region",isCorrect:!0},{text:"If you have no replicas and a node fails, you experience no loss of data when using Redis with cluster mode enabled",isCorrect:!1},{text:"While using Redis with cluster mode enabled, you cannot manually promote any of the replica nodes to primary",isCorrect:!0},{text:"While using Redis with cluster mode enabled, asynchronous replication mechanisms are used to keep the read replicas synchronized with the primary. If cluster mode is disabled, the replication mechanism is done synchronously",isCorrect:!1}],explanation:"All the nodes in a Redis cluster must reside in the same regionAll the nodes in a Redis cluster (cluster mode enabled or cluster mode disabled) must reside in the same region.While using Redis with cluster mode enabled, you cannot manually promote any of the replica nodes to primaryWhile using Redis with cluster mode enabled, there are some limitations:You cannot manually promote any of the replica nodes to primary.MultiAZ is required.You can only change the structure of a cluster, the node type, and the number of nodes by restoring from a backup.Incorrect options:While using Redis with cluster mode enabled, asynchronous replication mechanisms are used to keep the read replicas synchronized with the primary. If cluster mode is disabled, the replication mechanism is done synchronously When you add a read replica to a cluster, all of the data from the primary is copied to the new node. From that point on, whenever data is written to the primary, the changes are asynchronously propagated to all the read replicas, for both the Redis offerings (cluster mode enabled or cluster mode disabled).If you have no replicas and a node fails, you experience no loss of data when using Redis with cluster mode enabled If you have no replicas and a node fails, you experience loss of all data in that node's shard, when using Redis with cluster mode enabled. If you have no replicas and the node fails, you experience total data loss in Redis with cluster mode disabled.You can scale write capacity for Redis by adding replica nodes This increases only the read capacity of the Redis cluster, write capacity is not enhanced by read replicas.Reference: https://docs.aws.amazon.com/AmazonElastiCache/latest/redug/Replication.Redis.Groups.html"},{question:"You are a system administrator whose company recently moved its production application to AWS and migrated data from MySQL to AWS DynamoDB. You are adding new tables to AWS DynamoDB and need to allow your application to query your data by the primary key and an alternate key. This option must be added when first creating tables otherwise changes cannot be made afterward. Which of the following actions should you take?",answers:[{text:"Create a GSI",isCorrect:!1},{text:"Migrate away from DynamoDB",isCorrect:!1},{text:"Create a LSI",isCorrect:!0},{text:"Call Scan",isCorrect:!1}],explanation:"Create an LSILSI stands for Local Secondary Index. Some applications only need to query data using the base table's primary key; however, there may be situations where an alternate sort key would be helpful. To give your application a choice of sort keys, you can create one or more local secondary indexes on a table and issue Query or Scan requests against these indexes.Differences between GSI and LSI: via https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html Incorrect options:Call Scan Scan is an operation on the data. Once you create your local secondary indexes on a table you can then issue Scan requests again.Create a GSI GSI (Global Secondary Index) is an index with a partition key and a sort key that can be different from those on the base table.Migrate away from DynamoDB Migrating to another database that is not NoSQL may cause you to make changes that require substantial code changes.Reference: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html"},{question:"You are running a cloud file storage website with an Internetfacing Application Load Balancer, which routes requests from users over the internet to 10 registered Amazon EC2 instances. Users are complaining that your website always asks them to reauthenticate when they switch pages. You are puzzled because this behavior is not seen in your local machine or dev environment. What could be the reason?",answers:[{text:"The Load Balancer does not have stickiness enabled",isCorrect:!0},{text:"The Load Balancer does not have TLS enabled",isCorrect:!1},{text:"The EC2 instances are logging out the users because the instances never have access to the client IPs because of the Load Balancer",isCorrect:!1},{text:"Application Load Balancer is in slowstart mode, which gives ALB a little more time to read and write session data",isCorrect:!1}],explanation:"Correct option:The Load Balancer does not have stickiness enabled Sticky sessions are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies.When a load balancer first receives a request from a client, it routes the request to a target, generates a cookie named AWSALB that encodes information about the selected target, encrypts the cookie, and includes the cookie in the response to the client. The client should include the cookie that it receives in subsequent requests to the load balancer. When the load balancer receives a request from a client that contains the cookie, if sticky sessions are enabled for the target group and the request goes to the same target group, the load balancer detects the cookie and routes the request to the same target.More info here: via https://docs.aws.amazon.com/elasticloadbalancing/latest/application/loadbalancertargetgroups.html#stickysessions Incorrect options:Application Load Balancer is in slowstart mode, which gives ALB a little more time to read and write session data This is an invalid statement. The load balancer serves as a single point of contact for clients and distributes incoming traffic across its healthy registered targets. By default, a target starts to receive its full share of requests as soon as it is registered with a target group and passes an initial health check. Using slow start mode gives targets time to warm up before the load balancer sends them a full share of requests. This does not help in session management.The EC2 instances are logging out the users because the instances never have access to the client IPs because of the Load Balancer This is an incorrect statement. Elastic Load Balancing stores the IP address of the client in the XForwardedFor request header and passes the header to the server. If needed, the server can read IP addresses from this data.The Load Balancer does not have TLS enabled To use an HTTPS listener, you must deploy at least one SSL/TLS server certificate on your load balancer. The load balancer uses a server certificate to terminate the frontend connection and then decrypt requests from clients before sending them to the targets. This does not help in session management.References:https://docs.aws.amazon.com/elasticloadbalancing/latest/application/loadbalancertargetgroups.html#stickysessions https://docs.aws.amazon.com/elasticloadbalancing/latest/application/loadbalancertargetgroups.html#slowstartmode"},{question:"A company wants to share information with a third party via an HTTP API endpoint managed by the third party. The company has the necessary API key to access the endpoint and the integration of the API key with the company's application code must not impact the application's performance. What is the most secure approach?",answers:[{text:"Keep the API credentials in an encrypted file in S3 and use the credentials to make the API call by fetching the API credentials from S3 at runtime by using the AWS SDK",isCorrect:!1},{text:"Keep the API credentials in a local code variable and use the local code variable at runtime to make the API call",isCorrect:!1},{text:"Keep the API credentials in an encrypted table in MySQL RDS and use the credentials to make the API call by fetching the API credentials from RDS at runtime by using the AWS SDK",isCorrect:!1},{text:"Keep the API credentials in AWS Secrets Manager and use the credentials to make the API call by fetching the API credentials at runtime by using the AWS SDK",isCorrect:!0}],explanation:"Correct option:Keep the API credentials in AWS Secrets Manager and use the credentials to make the API call by fetching the API credentials at runtime by using the AWS SDKSecrets Manager enables you to replace hardcoded credentials in your code, including passwords, with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure the secret can't be compromised by someone examining your code, because the secret no longer exists in the code. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a specified schedule. This enables you to replace longterm secrets with shortterm ones, significantly reducing the risk of compromise.via https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.htmlIn the past, when you created a custom application to retrieve information from a database, you typically embedded the credentials, the secret, for accessing the database directly in the application. When the time came to rotate the credentials, you had to do more than just create new credentials. You had to invest time to update the application to use the new credentials. Then you distributed the updated application. If you had multiple applications with shared credentials and you missed updating one of them, the application failed. Because of this risk, many customers choose not to regularly rotate credentials, which effectively substitutes one risk for another. You can also use caching with Secrets Manager to significantly improve the availability and latency of applications.Incorrect options:Keep the API credentials in an encrypted table in MySQL RDS and use the credentials to make the API call by fetching the API credentials from RDS at runtime by using the AWS SDKKeep the API credentials in an encrypted file in S3 and use the credentials to make the API call by fetching the API credentials from S3 at runtime by using the AWS SDKKeep the API credentials in a local code variable and use the local code variable at runtime to make the API callIt is considered a security bad practice to keep sensitive access credentials in code, database, or a flat file on a file system or object storage. Therefore, all three options are incorrect.References:https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html https://aws.amazon.com/blogs/security/improveavailabilityandlatencyofapplicationsbyusingawssecretmanagerspythonclientsidecachinglibrary/"},{question:"Your company hosts a static website on Amazon Simple Storage Service (S3) written in HTML5. The website targets aviation enthusiasts and it has grown a worldwide audience with hundreds of thousands of visitors accessing the website now on a monthly basis. While users in the United States have a great user experience, users from other parts of the world are experiencing slow responses and lag. Which service can mitigate this issue?",answers:[{text:"Use Amazon S3 Caching",isCorrect:!1},{text:"Use Amazon S3 Transfer Acceleration",isCorrect:!1},{text:"Use Amazon ElastiCache for Redis",isCorrect:!1},{text:"Use Amazon CloudFront",isCorrect:!0}],explanation:"Correct option:Use Amazon CloudFrontStoring your static content with S3 provides a lot of advantages. But to help optimize your application’s performance and security while effectively managing cost, AWS recommends that you also set up Amazon CloudFront to work with your S3 bucket to serve and protect the content. CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more costeffective than delivering it from S3 directly to your users.By caching your content in Edge Locations, CloudFront reduces the load on your S3 bucket and helps ensure a faster response for your users when they request content. In addition, data transfer out for content by using CloudFront is often more costeffective than serving files directly from S3, and there is no data transfer fee from S3 to CloudFront.A security feature of CloudFront is Origin Access Identity (OAI), which restricts access to an S3 bucket and its content to only CloudFront and operations it performs.CloudFront Overview: via https://aws.amazon.com/blogs/networkingandcontentdelivery/amazons3amazoncloudfrontamatchmadeinthecloud/ Incorrect options:Use Amazon ElastiCache for Redis Amazon ElastiCache can be used to significantly improve latency and throughput for many readheavy application workloads (such as social networking, gaming, media sharing, and Q&A portals). ElastiCache is often used with databases that need millisecond latency. For the current scenario, we do not need a caching layer since the data load is not that heavy.Use Amazon S3 Caching This is a madeup option, given as a distractor.Use Amazon S3 Transfer Acceleration Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3 bucket. However, S3 Transfer Acceleration leverages Amazon CloudFront’s globally distributed AWS Edge Locations. Each time S3 Transfer Acceleration is used to upload an object, AWS checks whether S3 Transfer Acceleration is likely to be faster than a regular Amazon S3 transfer. If it finds that S3 Transfer Acceleration might not be significantly faster, AWS shifts back to normal Amazon S3 transfer mode. So, this is not the right option for our use case.References:https://aws.amazon.com/blogs/networkingandcontentdelivery/amazons3amazoncloudfrontamatchmadeinthecloud/ https://aws.amazon.com/elasticache/"},{question:"You have an Amazon Kinesis Data Stream with 10 shards, and from the metrics, you are well below the throughput utilization of 10 MB per second to send data. You send 3 MB per second of data and yet you are receiving ProvisionedThroughputExceededException errors frequently. What is the likely cause of this?",answers:[{text:"The partition key that you have selected isn't distributed enough",isCorrect:!0},{text:"The data retention period is too long",isCorrect:!1},{text:"Metrics are slow to update",isCorrect:!1},{text:"You have too many shards",isCorrect:!1}],explanation:'The partition key that you have selected isn\'t distributed enoughAmazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs.A Kinesis data stream is a set of shards. A shard is a uniquely identified sequence of data records in a stream. A stream is composed of one or more shards, each of which provides a fixed unit of capacity.The partition key is used by Kinesis Data Streams to distribute data across shards. Kinesis Data Streams segregates the data records that belong to a stream into multiple shards, using the partition key associated with each data record to determine the shard to which a given data record belongs.Kinesis Data Streams Overview: via https://docs.aws.amazon.com/streams/latest/dev/keyconcepts.htmlFor the given usecase, as the partition key is not distributed enough, all the data is getting skewed at a few specific shards and not leveraging the entire cluster of shards.You can also use metrics to determine which are your "hot" or "cold" shards, that is, shards that are receiving much more data, or much less data, than expected. You could then selectively split the hot shards to increase capacity for the hash keys that target those shards. Similarly, you could merge cold shards to make better use of their unused capacity.Incorrect options:Metrics are slow to update Metrics are a CloudWatch concept. This option has been added as a distractor.You have too many shards Too many shards is not the issue as you would see a LimitExceededException in that case.The data retention period is too long Your streaming data is retained for up to 365 days. The data retention period is not an issue causing this error.References:https://docs.aws.amazon.com/streams/latest/dev/keyconcepts.html https://docs.aws.amazon.com/streams/latest/dev/kinesisusingsdkjavareshardingstrategies.html'},{question:"Your company is planning to move away from reserving EC2 instances and would like to adopt a more agile form of serverless architecture. Which of the following is the simplest and the least effort way of deploying the Docker containers on this serverless architecture?",answers:[{text:"Amazon Elastic Kubernetes Service (Amazon EKS) on Fargate",isCorrect:!1},{text:"Amazon Elastic Container Service (Amazon ECS) on Fargate",isCorrect:!0},{text:"Amazon Elastic Container Service (Amazon ECS) on EC2",isCorrect:!1},{text:"AWS Elastic Beanstalk",isCorrect:!1}],explanation:"Correct option:Amazon Elastic Container Service (Amazon ECS) on Fargate Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type.AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.ECS Fargate Overview: via https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.htmlIncorrect options:Amazon Elastic Container Service (Amazon ECS) on EC2 Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. ECS uses EC2 instances and hence cannot be called a serverless solution.Amazon Elastic Kubernetes Service (Amazon EKS) on Fargate Amazon Elastic Kubernetes Service (Amazon EKS) is a fully managed Kubernetes service. You can choose to run your EKS clusters using AWS Fargate, which is a serverless compute for containers. Since the usecase talks about the simplest and the least effort way to deploy Docker containers, EKS is not the best fit as you can use ECS Fargate to build a much easier solution. EKS is better suited to run Kubernetes on AWS without needing to install and operate your own Kubernetes control plane or worker nodes.AWS Elastic Beanstalk AWS Elastic Beanstalk is an easytouse service for deploying and scaling web applications and services. You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, autoscaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. Beanstalk uses EC2 instances for its deployment, hence cannot be called a serverless architecture.References:https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html https://aws.amazon.com/eks/ https://aws.amazon.com/elasticbeanstalk/"},{question:"You are getting ready for an event to show off your Alexa skill written in JavaScript. As you are testing your voice activation commands you find that some intents are not invoking as they should and you are struggling to figure out what is happening. You included the following code console.log(JSON.stringify(this.event)) in hopes of getting more details about the request to your Alexa skill. You would like the logs stored in an Amazon Simple Storage Service (S3) bucket named MyAlexaLog. How do you achieve this?",answers:[{text:"Use CloudWatch integration feature with Kinesis",isCorrect:!1},{text:"Use CloudWatch integration feature with S3",isCorrect:!0},{text:"Use CloudWatch integration feature with Glue",isCorrect:!1},{text:"Use CloudWatch integration feature with Lambda",isCorrect:!1}],explanation:"Use CloudWatch integration feature with S3You can export log data from your CloudWatch log groups to an Amazon S3 bucket and use this data in custom processing and analysis, or to load onto other systems.Exporting CloudWatch Log Data to Amazon S3: via https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html Incorrect options:Use CloudWatch integration feature with Kinesis You can use both to do custom processing or analysis but with S3 you don't have to process anything. Instead, you configure the CloudWatch settings to send logs to S3.Use CloudWatch integration feature with Lambda You can use both to do custom processing or analysis but with S3 you don't have to process anything. Instead, you configure the CloudWatch settings to send logs to S3.Use CloudWatch integration feature with Glue AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. Glue is not the right fit for the given usecase.Reference: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html"},{question:"Your company is in the process of building a DevOps culture and is moving all of its onpremise resources to the cloud using serverless architectures and automated deployments. You have created a CloudFormation template in YAML that uses an AWS Lambda function to pull HTML files from GitHub and place them into an Amazon Simple Storage Service (S3) bucket that you specify. Which of the following AWS CLI commands can you use to upload AWS Lambda functions and AWS CloudFormation templates to AWS?",answers:[{text:"cloudformation zip and cloudformation upload",isCorrect:!1},{text:"cloudformation zip and cloudformation deploy",isCorrect:!1},{text:"cloudformation package and cloudformation upload",isCorrect:!1},{text:"cloudformation package and cloudformation deploy",isCorrect:!0}],explanation:"Correct option:cloudformation package and cloudformation deployAWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and thirdparty resources and provision them in an orderly and predictable fashion.How CloudFormation Works: via https://aws.amazon.com/cloudformation/ The cloudformation package command packages the local artifacts (local paths) that your AWS CloudFormation template references. The command will upload local artifacts, such as your source code for your AWS Lambda function.The cloudformation deploy command deploys the specified AWS CloudFormation template by creating and then executing a changeset.Incorrect options:cloudformation package and cloudformation upload The cloudformation upload command does not exist.cloudformation zip and cloudformation upload Both commands do not exist, this is a madeup option.cloudformation zip and cloudformation deploy The cloudformation zip command does not exist, this is a madeup option.Reference: https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html"},{question:"Your development team uses the AWS SDK for Java on a web application that uploads files to several Amazon Simple Storage Service (S3) buckets using the SSEKMS encryption mechanism. Developers are reporting that they are receiving permission errors when trying to push their objects over HTTP. Which of the following headers should they include in their request?",answers:[{text:"xamzserversideencryption': 'aws:kms'",isCorrect:!0},{text:"xamzserversideencryption': 'SSES3'",isCorrect:!1},{text:"xamzserversideencryption': 'AES256'",isCorrect:!1},{text:"xamzserversideencryption': 'SSEKMS'",isCorrect:!1}],explanation:"xamzserversideencryption': 'aws:kms'Serverside encryption is the encryption of data at its destination by the application or service that receives it. AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. Amazon S3 uses AWS KMS customer master keys (CMKs) to encrypt your Amazon S3 objects. AWS KMS encrypts only the object data. Any object metadata is not encrypted.If the request does not include the xamzserversideencryption header, then the request is denied.via https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html Incorrect options:'xamzserversideencryption': 'SSES3' This is an invalid header value. The correct value is 'xamzserversideencryption': 'AES256'. This refers to ServerSide Encryption with Amazon S3Managed Encryption Keys (SSES3).'xamzserversideencryption': 'SSEKMS' Invalid header value. SSEKMS is an encryption option.'xamzserversideencryption': 'AES256' This is the correct header value if you are using SSES3 serverside encryption.Reference: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html"},{question:"You company runs business logic on smaller software components that perform various functions. Some functions process information in a few seconds while others seem to take a long time to complete. Your manager asked you to decouple components that take a long time to ensure software applications stay responsive under load. You decide to configure Amazon Simple Queue Service (SQS) to work with your Elastic Beanstalk configuration. Which of the following Elastic Beanstalk environment should you choose to meet this requirement?",answers:[{text:"Single Instance with Elastic IP",isCorrect:!1},{text:"Single Instance Worker node",isCorrect:!1},{text:"Loadbalancing, Autoscaling environment",isCorrect:!1},{text:"Dedicated worker environment",isCorrect:!0}],explanation:"With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.Elastic BeanStalk Key Concepts: via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.htmlDedicated worker environment If your AWS Elastic Beanstalk application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker environment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your application stays responsive under load.A longrunning task is anything that substantially increases the time it takes to complete a request, such as processing images or videos, sending emails, or generating a ZIP archive. These operations can take only a second or two to complete, but a delay of a few seconds is a lot for a web request that would otherwise complete in less than 500 ms.via https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeaturesmanagingenvtiers.html Incorrect options:Single Instance Worker node Worker machines in Kubernetes are called nodes. Amazon EKS worker nodes are standard Amazon EC2 instances. EKS worker nodes are not to be confused with the Elastic Beanstalk worker environment. Since we are talking about the Elastic Beanstalk environment, this is not the correct choice.Loadbalancing, Autoscaling environment A loadbalancing and autoscaling environment uses the Elastic Load Balancing and Amazon EC2 Auto Scaling services to provision the Amazon EC2 instances that are required for your deployed application. Amazon EC2 Auto Scaling automatically starts additional instances to accommodate increasing load on your application. If your application requires scalability with the option of running in multiple Availability Zones, use a loadbalancing, autoscaling environment. This is not the right environment for the given usecase since it will add costs to the overall solution.Single Instance with Elastic IP A singleinstance environment contains one Amazon EC2 instance with an Elastic IP address. A singleinstance environment doesn't have a load balancer, which can help you reduce costs compared to a loadbalancing, autoscaling environment. This is not a highly available architecture, because if that one instance goes down then your application is down. This is not recommended for production environments.Reference: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/usingfeaturesmanagingenvtypes.html"},{question:"As part of employee skills upgrade, the developers of your team have been delegated few responsibilities of DevOps engineers. Developers now have full control over modeling the entire software delivery process, from coding to deployment. As the team lead, you are now responsible for any manual approvals needed in the process. Which of the following approaches supports the given workflow?",answers:[{text:"Create one CodePipeline for your entire flow and add a manual approval step",isCorrect:!0},{text:"Create multiple CodePipelines for each environment and link them using AWS Lambda",isCorrect:!1},{text:"Create deeply integrated AWS CodePipelines for each environment",isCorrect:!1},{text:"Use CodePipeline with Amazon Virtual Private Cloud",isCorrect:!1}],explanation:"Correct option:Create one CodePipeline for your entire flow and add a manual approval step You can add an approval action to a stage in a CodePipeline pipeline at the point where you want the pipeline to stop so someone can manually approve or reject the action. Approval actions can't be added to Source stages. Source stages can contain only source actions.Incorrect options:Create multiple CodePipelines for each environment and link them using AWS Lambda You can create Lambda functions and add them as actions in your pipelines but the approval step is confined to a workflow process and cannot be outsourced to any other AWS service.Create deeply integrated AWS CodePipelines for each environment You can use an AWS CloudFormation template in conjunction with AWS CodePipeline and AWS CodeCommit to create a test environment that deploys to your production environment when the changes to your application are approved, helping you automate a continuous delivery workflow. This is a possible answer but not an optimized way of achieving what the client needs.Use CodePipeline with Amazon Virtual Private Cloud AWS CodePipeline supports Amazon Virtual Private Cloud (Amazon VPC) endpoints powered by AWS PrivateLink. This means you can connect directly to CodePipeline through a private endpoint in your VPC, keeping all traffic inside your VPC and the AWS network. This is a robust security feature but is of no value for our current usecase.References:https://docs.aws.amazon.com/codepipeline/latest/userguide/approvalsactionadd.html https://docs.aws.amazon.com/codepipeline/latest/userguide/vpcsupport.html"},{question:"The app development team at a social gaming mobile app wants to simplify the user sign up process for the app. The team is looking for a fully managed scalable solution for user management in anticipation of the rapid growth that the app foresees. As a Developer Associate, which of the following solutions would you suggest so that it requires the LEAST amount of development effort?",answers:[{text:"Create a custom solution using Lambda and DynamoDB to facilitate sign up and user management for the mobile app",isCorrect:!1},{text:"Create a custom solution using EC2 and DynamoDB to facilitate sign up and user management for the mobile app",isCorrect:!1},{text:"Use Cognito User pools to facilitate sign up and user management for the mobile app",isCorrect:!0},{text:"Use Cognito Identity pools to facilitate sign up and user management for the mobile app",isCorrect:!1}],explanation:"Correct option:Use Cognito User pools to facilitate sign up and user management for the mobile appAmazon Cognito provides authentication, authorization, and user management for your web and mobile apps. Your users can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, Google or Apple.A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a thirdparty identity provider (IdP). Whether your users signin directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK.Cognito is fully managed by AWS and works out of the box so it meets the requirements for the given usecase.via https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.html Incorrect options:Use Cognito Identity pools to facilitate sign up and user management for the mobile app You can use Identity pools to grant your users access to other AWS services. With an identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users, as well as the specific identity providers that you can use to authenticate users for identity pools.Exam Alert:Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools: via https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.htmlCreate a custom solution with EC2 and DynamoDB to facilitate sign up and user management for the mobile appCreate a custom solution with Lambda and DynamoDB to facilitate sign up and user management for the mobile appAs the problem statement mentions that the solution needs to be fully managed and should require the least amount of development effort, so you cannot use EC2 or Lambda functions with DynamoDB to create a custom solution.Reference:https://docs.aws.amazon.com/cognito/latest/developerguide/whatisamazoncognito.htmll"},{question:"A CRM application is hosted on Amazon EC2 instances with the database tier using DynamoDB. The customers have raised privacy and security concerns regarding sending and receiving data across the public internet. As a developer associate, which of the following would you suggest as an optimal solution for providing communication between EC2 instances and DynamoDB without using the public internet?",answers:[{text:"Create an Internet Gateway to provide the necessary communication channel between EC2 instances and DynamoDB",isCorrect:!1},{text:"Configure VPC endpoints for DynamoDB that will provide required internal access without using public internet",isCorrect:!0},{text:"The firm can use a virtual private network (VPN) to route all DynamoDB network traffic through their own corporate network infrastructure",isCorrect:!1},{text:"Create a NAT Gateway to provide the necessary communication channel between EC2 instances and DynamoDB",isCorrect:!1}],explanation:"Correct option:Configure VPC endpoints for DynamoDB that will provide required internal access without using public internetWhen you create a VPC endpoint for DynamoDB, any requests to a DynamoDB endpoint within the Region (for example, dynamodb.uswest2.amazonaws.com) are routed to a private DynamoDB endpoint within the Amazon network. You don't need to modify your applications running on EC2 instances in your VPC. The endpoint name remains the same, but the route to DynamoDB stays entirely within the Amazon network, and does not access the public internet. You use endpoint policies to control access to DynamoDB. Traffic between your VPC and the AWS service does not leave the Amazon network.Using Amazon VPC Endpoints to Access DynamoDB: via https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpcendpointsdynamodb.html Incorrect options:The firm can use a virtual private network (VPN) to route all DynamoDB network traffic through their own corporate network infrastructure You can address the requested security concerns by using a virtual private network (VPN) to route all DynamoDB network traffic through your own corporate network infrastructure. However, this approach can introduce bandwidth and availability challenges and hence is not an optimal solution here.Create a NAT Gateway to provide the necessary communication channel between EC2 instances and DynamoDB You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. NAT Gateway is not useful here since the instance and DynamoDB are present in AWS network and do not need NAT Gateway for communicating with each other.Create an Internet Gateway to provide the necessary communication channel between EC2 instances and DynamoDB An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An internet gateway serves two purposes: to provide a target in your VPC route tables for internetroutable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. Using an Internet Gateway would imply that the EC2 instances are connecting to DynamoDB using the public internet. Therefore, this option is incorrect.References:https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpcendpointsdynamodb.html https://docs.aws.amazon.com/vpc/latest/userguide/vpcnatgateway.html https://docs.aws.amazon.com/vpc/latest/userguide/Carrier_Gateway.html"},{question:"A developer wants to securely store and retrieve various types of variables, such as remote API authentication information, API URL, and related credentials across different environments of an application deployed on Amazon Elastic Container Service (Amazon ECS). What would be the best approach that needs minimal modifications in the application code?",answers:[{text:"Configure the application to fetch the variables from each of the deployed environments by defining the authentication information and API URL in the ECS task definition as unique names during the deployment process",isCorrect:!1},{text:"Configure the application to fetch the variables from AWS KMS by storing the API URL and credentials as unique keys in KMS for each environment",isCorrect:!1},{text:"Configure the application to fetch the variables from an encrypted file that is stored with the application by storing the API URL and credentials in unique files for each environment",isCorrect:!1},{text:"Configure the application to fetch the variables and credentials from AWS Systems Manager Parameter Store by leveraging hierarchical unique paths in Parameter Store for each variable in each environment",isCorrect:!0}],explanation:"Configure the application to fetch the variables and credentials from AWS Systems Manager Parameter Store by leveraging hierarchical unique paths in Parameter Store for each variable in each environmentParameter Stores is a capability of AWS Systems Manager that provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. You can reference Systems Manager parameters in your scripts, commands, SSM documents, and configuration and automation workflows by using the unique name that you specified when you created the parameter.Managing dozens or hundreds of parameters as a flat list is timeconsuming and prone to errors. It can also be difficult to identify the correct parameter for a task. This means you might accidentally use the wrong parameter, or you might create multiple parameters that use the same configuration data.You can use parameter hierarchies to help you organize and manage parameters. A hierarchy is a parameter name that includes a path that you define by using forward slashes (/).via https://docs.aws.amazon.com/systemsmanager/latest/userguide/sysmanparamstorehierarchies.html Incorrect options:Configure the application to fetch the variables from AWS KMS by storing the API URL and credentials as unique keys in KMS for each environment AWS KMS lets you create, manage, and control cryptographic keys across your applications and AWS services. KMS is not a keyvalue service that can be used for the given use case.Configure the application to fetch the variables from an encrypted file that is stored with the application by storing the API URL and credentials in unique files for each environment It is not considered a security best practice to store sensitive data and credentials in an encrypted file with the application. So this option is incorrect.Configure the application to fetch the variables from each of the deployed environments by defining the authentication information and API URL in the ECS task definition as unique names during the deployment process ECS task definition can be thought of as a blueprint for your application. Task definitions specify various parameters for your application. Examples of task definition parameters are which containers to use, which launch type to use, which ports should be opened for your application, and what data volumes should be used with the containers in the task. The specific parameters available for the task definition depend on which launch type you are using. The task definition is a text file, in JSON format, that describes one or more containers, up to a maximum of ten, that form your application. A task is the instantiation of a task definition within a cluster. After you create a task definition for your application within Amazon ECS, you can specify the number of tasks to run on your cluster.AWS recommends storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters. Environment variables specified in the task definition are readable by all users and roles that are allowed the DescribeTaskDefinition action for the task definition. So this option is incorrect.via https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdefenvfiles.html References:https://docs.aws.amazon.com/systemsmanager/latest/userguide/sysmanparamstorehierarchies.html https://aws.amazon.com/kms/ https://ecsworkshop.com/introduction/ecs_basics/task_definition/ https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdefenvfiles.htmll"},{question:"A company has created an Amazon S3 bucket that holds customer data. The team lead has just enabled access logging to this bucket. The bucket size has grown substantially after starting access logging. Since no new files have been added to the bucket, the perplexed team lead is looking for an answer. Which of the following reasons explains this behavior?",answers:[{text:"A DDoS attack on your S3 bucket can potentially blow up the size of data in the bucket if the bucket security is compromised during the attack",isCorrect:!1},{text:"Erroneous Bucket policies for batch uploads can sometimes be responsible for the exponential growth of S3 Bucket size",isCorrect:!1},{text:"S3 access logging is pointing to the same bucket and is responsible for the substantial growth of bucket size",isCorrect:!0},{text:"Object Encryption has been enabled and each object is stored twice as part of this configuration",isCorrect:!1}],explanation:"Correct option:S3 access logging is pointing to the same bucket and is responsible for the substantial growth of bucket size When your source bucket and target bucket are the same bucket, additional logs are created for the logs that are written to the bucket. The extra logs about logs might make it harder to find the log that you are looking for. This configuration would drastically increase the size of the S3 bucket.via https://aws.amazon.com/premiumsupport/knowledgecenter/s3serveraccesslogssamebucket/ Incorrect options: Erroneous Bucket policies for batch uploads can sometimes be responsible for the exponential growth of S3 Bucket size This is an incorrect statement. A bucket policy is a resourcebased AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. A bucket policy, for batch processes or normal processes, will not increase the size of the bucket or the objects in it.A DDOS attack on your S3 bucket can potentially blow up the size of data in the bucket if the bucket security is compromised during the attack This is an incorrect statement. AWS handles DDoS attacks on all of its managed services. However, a DDoS attack will not increase the size of the bucket.Object Encryption has been enabled and each object is stored twice as part of this configuration Encryption does not increase a bucket's size, that too, on daily basis, as if the case in the current scenarioReferences:https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html https://docs.aws.amazon.com/AmazonS3/latest/userguide/setpermissions.html"},{question:"Your company has been hired to build a resilient mobile voting app for an upcoming music award show that expects to have 5 to 20 million viewers. The mobile voting app will be marketed heavily months in advance so you are expected to handle millions of messages in the system. You are configuring Amazon Simple Queue Service (SQS) queues for your architecture that should receive messages from 20 KB to 200 KB. Is it possible to send these messages to SQS?",answers:[{text:"No, the max message size is 128KB",isCorrect:!1},{text:"Yes, the max message size is 256KB",isCorrect:!0},{text:"Yes, the max message size is 512KB",isCorrect:!1},{text:"No, the max message size is 64KB",isCorrect:!1}],explanation:"Yes, the max message size is 256KBThe minimum message size is 1 byte (1 character). The maximum is 262,144 bytes (256 KB).via https://aws.amazon.com/sqs/faqs/ Incorrect options:Yes, the max message size is 512KB The max size is 256KBNo, the max message size is 128KB The max size is 256KBNo, the max message size is 64KB The max size is 256KBReference: https://aws.amazon.com/sqs/faqs/"},{question:"Recently in your organization, the AWS XRay SDK was bundled into each Lambda function to record outgoing calls for tracing purposes. When your team leader goes to the XRay service in the AWS Management Console to get an overview of the information collected, they discover that no data is available. What is the most likely reason for this issue?",answers:[{text:"Enable XRay sampling",isCorrect:!1},{text:"Change the security group rules",isCorrect:!1},{text:"XRay only works with AWS Lambda aliases",isCorrect:!1},{text:"Fix the IAM Role",isCorrect:!0}],explanation:'Correct option:AWS XRay helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With XRay, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. XRay provides an endtoend view of requests as they travel through your application, and shows a map of your application’s underlying components.How XRay Works: via https://aws.amazon.com/xray/Fix the IAM RoleCreate an IAM role with write permissions and assign it to the resources running your application. You can use AWS Identity and Access Management (IAM) to grant XRay permissions to users and compute resources in your account. This should be one of the first places you start by checking that your permissions are properly configured before exploring other troubleshooting options.Here is an example of XRay ReadOnly permissions via an IAM policy:{ "Version": "20121017", "Statement": [ { "Effect": "Allow", "Action": [ "xray:GetSamplingRules", "xray:GetSamplingTargets", "xray:GetSamplingStatisticSummaries", "xray:BatchGetTraces", "xray:GetServiceGraph", "xray:GetTraceGraph", "xray:GetTraceSummaries", "xray:GetGroups", "xray:GetGroup" ], "Resource": [ "*" ] } ]}Another example of write permissions for using XRay via an IAM policy:{ "Version": "20121017", "Statement": [ { "Effect": "Allow", "Action": [ "xray:PutTraceSegments", "xray:PutTelemetryRecords", "xray:GetSamplingRules", "xray:GetSamplingTargets", "xray:GetSamplingStatisticSummaries" ], "Resource": [ "*" ] } ]}Incorrect options:Enable XRay sampling If permissions are not configured correctly sampling will not work, so this option is not correct.XRay only works with AWS Lambda aliases This is not true, aliases are pointers to specific Lambda function versions. To use the XRay SDK on Lambda, bundle it with your function code each time you create a new version.Change the security group rules You grant permissions to your Lambda function to access other resources using an IAM role and not via security groups.Reference: https://docs.aws.amazon.com/xray/latest/devguide/security_iam_troubleshoot.html'}]},{id:"aws-developer-7",title:"AWS Certified Developer Associate Practice Exams 1",description:"Additional practice questions covering AWS development topics.",questions:[{question:"A company is storing highly classified documents on its file server. These documents contain blueprints for electronic devices and are never to be made public due to a legal agreement. To comply with the strict policy, you must explore the capabilities of AWS KMS to improve data security. Which of the following is the MOST suitable procedure for encrypting data?",answers:[{text:"Use a combination of symmetric and asymmetric encryption. Encrypt the data with a symmetric key and use the asymmetric private key to decrypt the data.",isCorrect:!1},{text:"Use a symmetric key for encryption and decryption.",isCorrect:!1},{text:"Generate a data key using a KMS key. Then, encrypt data with the ciphertext version of the data key.",isCorrect:!1},{text:"Generate a data key using a symmetric key. Then, encrypt data with the data key.",isCorrect:!0}],explanation:"Your data is protected when you encrypt it, but you have to protect your encryption key. One strategy is to encrypt it. Envelope encryption is the practice of encrypting plaintext data with a data key and then encrypting the data key under another key. You can even encrypt the data encryption key under another encryption key and encrypt that encryption key under another encryption key. But, eventually, one key must remain in plaintext so you can decrypt the keys and your data. This top-level plaintext key-encryption key is known as the master key. Envelope encryption offers several benefits: Protecting data keysWhen you encrypt a data key, you don't have to worry about storing the encrypted data key, because the data key is inherently protected by encryption. You can safely store the encrypted data key alongside the encrypted data. Encrypting the same data under multiple master keysEncryption operations can be time-consuming, particularly when the data being encrypted are large objects. Instead of re-encrypting raw data multiple times with different keys, you can re-encrypt only the data keys that protect the raw data. Combining the strengths of multiple algorithmsIn general, symmetric key algorithms are faster and produce smaller ciphertexts than public-key algorithms. But public-key algorithms provide inherent separation of roles and easier key management. Envelope encryption lets you combine the strengths of each strategy. To perform envelope encryption using KMS keys, you must first generate a data key using your KMS key and use its plaintext version to encrypt data. Hence, the correct answer is the option that says: Generate a data key using a KMS key. Then, encrypt data with the plaintext data key. The option that says: Generate a data key using a KMS key. Then, encrypt data with the ciphertext version data key is incorrect. A ciphertext data key cannot be used for encryption because it is an encrypted version of the data key. You must use the plaintext version of the data key. The option that says: Use a KMS key for encryption and decryption is incorrect. KMS keys can only be used to encrypt data up to 4KB in size, making it not suitable for encrypting documents. The option that says: Use a combination of symmetric and asymmetric encryption. Encrypt the data with a symmetric key and use the asymmetric private key to decrypt the data is incorrect because the keys on each encryption algorithm are separate entities and are in no way related. You cannot encrypt data with a symmetric key and expect it to be decrypted by an asymmetric private key. References: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping https://docs.aws.amazon.com/kms/latest/developerguide/services-ebs.html Check out this AWS KMS Cheat Sheet: https://tutorialsdojo.com/aws-key-management-service-aws-kms/"},{question:"An IAM user with programmatic access wants to get information about specific EC2 instances on the us-east-1 region. Due to strict policy, the user was compelled to use the describe-instances operation using AWS Command Line Interface (CLI). He wants to check whether he has the required permission to initiate the command without actually making the request. Which of the following actions should be done to solve the problem?",answers:[{text:"Add the --dry-run parameter to the describe-instances command.",isCorrect:!0},{text:"Add the --generate-cli-skeleton parameter to the describe-instances command.",isCorrect:!1},{text:"Add the --filters parameter to the describe-instances command.",isCorrect:!1},{text:"Add the >--max-items parameter to the describe-instances command.",isCorrect:!1}],explanation:"The describe-instances command describes the specified instances or all instances. Optionally, you can add parameters to the describe-instances to modify its function. Here is the list of the available parameters for describe-instances: [--dry-run | --no-dry-run] [--instance-ids ] [--filters ] [--cli-input-json ] [--starting-token ] [--page-size ] [--max-items ] [--generate-cli-skeleton] The --dry-run parameter checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is DryRun-Operation. Otherwise, it is UnauthorizedOperation. Hence, the correct answer is: Add the --dry-run parameter to the describe-instances command The option that says: Add the --generate-cli-skeleton parameter to the describe-instances command is incorrect because this is a parameter that will generate and display a parameter template that you can customize and use as input on a later command. The generated template includes all of the parameters that the command supports. The option that says: Add the --filters parameter to the describe-instances command is incorrect. Use this parameter if you want to get only the details that you like from the output of your command. It will not help you check for the permission required to initiate the command. The option that says: Add the max-items parameter to the describe-instances command is incorrect because it just defines the total number of items to be returned in the command’s output. It has nothing to do with checking the permission required to initiate the command. References: https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-instances.html https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-help.html"},{question:"A development team has a serverless architecture composed of multiple Lambda functions that invoke one another. As the number of Lambda functions increases, the team finds it increasingly difficult to manage the coordination and dependencies between them, leading to errors, duplication of code, and difficulty debugging and troubleshooting issues. Which refactorization should the team implement?",answers:[{text:"Create an AWS AppSync GraphQL API endpoint and configure each Lambda function as a resolver.",isCorrect:!1},{text:"Create an AWS Step Functions state machine and convert each Lambda function into individual Task states.",isCorrect:!0},{text:"Use AWS AppConfig’s feature flag to gradually release new code changes to each Lambda function.",isCorrect:!1},{text:"Use AWS CodePipeline to define the source, build, and deployment stages for each Lambda function.",isCorrect:!1}],explanation:"State Machine is a technique in modeling systems whose output depends on the entire history of their inputs, not just on the most recent input. In this case, the Lambda functions invoke one another, creating a large state machine. AWS Step Functions lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. Using Step Functions, you can design and run workflows that stitch together services, such as AWS Lambda, AWS Fargate, and Amazon SageMaker, into feature-rich applications. Step Functions automatically triggers and tracks each step, and retries when there are errors so your application executes in order and as expected. With Step Functions, you can craft long-running workflows such as machine learning model training, report generation, and IT automation. You can manage the coordination of a state machine in Step Functions using the Amazon States Language. The Amazon States Language is a JSON-based, structured language used to define your state machine, a collection of states, that can do work (Task states), determine which states to transition to next (Choice states), stop execution with an error (Fail states), and so on. Hence, the correct answer is Create an AWS Step Functions state machine and convert each Lambda function into individual Task states. The option that says: Use AWS CodePipeline to define the source, build, and deployment stages for each Lambda function is incorrect. While this approach can help with the deployment process, it does not directly address the coordination and dependency issues between the Lambda functions. AWS CodePipeline is primarily used for automating the build, test, and deploy phases of your release process every time there is a code change. The option that says: Create an AWS AppSync GraphQL API endpoint and configure each Lambda function as a resolver is incorrect. AWS AppSync is a service that enables you to query multiple sources from a single GraphQL API endpoint. While it provides features such as schema generation, resolvers, and real-time subscriptions, it is not specifically designed as a management tool for coordinating and managing the dependencies between multiple Lambda functions. The option that says: Use AWS AppConfig’s feature flag to gradually release new code changes to each Lambda function is incorrect. While this approach can help with rolling out new code changes, it does not directly address the coordination and dependency issues between the Lambda functions. References: https://aws.amazon.com/step-functions/ https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html https://docs.aws.amazon.com/step-functions/latest/dg/concepts-amazon-states-language.html Check out this AWS Step Functions Cheat Sheet: https://tutorialsdojo.com/aws-step-functions/"},{question:"A serverless application consists of multiple Lambda Functions and a DynamoDB table. The application must be deployed by calling the CloudFormation APIs using AWS CLI. The CloudFormation template and the files containing the code for all the Lambda functions are located on a local computer. What should the Developer do to deploy the application?",answers:[{text:"Use the aws cloudformation validate-template command and deploy using aws cloudformation deploy.",isCorrect:!1},{text:"Use the aws cloudformation package command and deploy using aws cloudformation deploy.",isCorrect:!0},{text:"Use the aws cloudformation update-stack command and deploy using aws cloudformation deploy.",isCorrect:!1},{text:"Use the aws cloudformation deploy command.",isCorrect:!1}],explanation:"The aws cloudformation package command packages the local artifacts (local paths) that your AWS CloudFormation template references. The command uploads local artifacts, such as source code for an AWS Lambda function or a Swagger file for an AWS API Gateway REST API, to an S3 bucket. The command returns a copy of your template, replacing references to local artifacts with the S3 location where the command uploaded the artifacts. Use this command to quickly upload local artifacts that might be required by your template. After you package your template's artifacts, run the aws cloudformation deploy command to deploy the returned template. Since we have local artifacts (source code for the AWS Lambda functions), we should use the package command. After running the package command, we must deploy the packaged output file by running the deploy command. Hence, the correct answer is: Use the aws cloudformation package command and deploy using aws cloudformation deploy. The option that says: Use the aws cloudformation validate-template command and deploy using aws cloudformation deploy is incorrect because the validate-template command will just check the template if it is a valid JSON or YAML file. The option that says: Use the aws cloudformation deploy command is incorrect because the artifacts are located in the local computer and since the deploy command uses S3 as the location for artifacts that were defined in the Cloudformation template, using the deploy command alone will not work. You must package the template first. The option that says: Use the aws cloudformation update-stack command and deploy using aws cloudformation deploy is incorrect because this just updates an existing stack. References: https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html https://docs.aws.amazon.com/cli/latest/reference/cloudformation/deploy/index.html Check out this AWS CloudFormation Cheat Sheet: https://tutorialsdojo.com/aws-cloudformation/"},{question:"A transcoding media service is being developed in AWS. Photos uploaded to Amazon S3 will trigger Step Functions to coordinate a series of processes that will perform image analysis tasks. The final output should contain the input plus the result of the final state to conform to the application's logic flow. What should the developer do?",answers:[{text:"Declare an OutputPath field filter on the Amazon States Language specification.",isCorrect:!1},{text:"Declare an InputPath field filter on the Amazon States Language specification.",isCorrect:!1},{text:"Declare a Parameters field filter on the Amazon States Language specification.",isCorrect:!1},{text:"Declare a ResultPath field filter on the Amazon States Language specification.",isCorrect:!0}],explanation:"A Step Functions execution receives a JSON text as input and passes that input to the first state in the workflow. Individual states receive JSON as input and usually pass JSON as output to the next state. Understanding how this information flows from state to state and learning how to filter and manipulate this data is key to effectively designing and implementing workflows in AWS Step Functions. In the Amazon States Language, these fields filter and control the flow of JSON from state to state: - InputPath - OutputPath - ResultPath - Parameters Both the InputPath and Parameters fields provide a way to manipulate JSON as it moves through your workflow. InputPath can limit the input that is passed by filtering the JSON notation by using a path. The Parameters field enables you to pass a collection of key-value pairs, where the values are either static values that you define in your state machine definition, or that are selected from the input using a path. AWS Step Functions applies the InputPath field first, and then the Parameters field. You can first filter your raw input to a selection you want using InputPath, and then apply Parameters to manipulate that input further, or add new values. The output of a state can be a copy of its input, the result it produces (for example, the output from a Task state’s Lambda function), or a combination of its input and result. Use ResultPath to control which combination of these is passed to the state output. OutputPath enables you to select a portion of the state output to pass to the next state. This enables you to filter out unwanted information, and pass only the portion of JSON that you care about. Out of these field filters, the ResultPath field filter is the only one that can control input values and its previous results to be passed to the state output. Hence, the correct answer is: Declare a ResultPath field filter on the Amazon States Language specification. The option that says: Declare an InputPath field filter on the Amazon State Language specification is incorrect because it just operates on the input level by filtering the JSON notation by using a path. It cannot control both ends of a state (input and output). The option that says: Declare an OutputPath field filter on the Amazon State Language specification is incorrect because it just operates on the output level. It is used to filter out unwanted information and pass only the portion of JSON that you care about that will be passed onto the next state. The option that says: Declare a Parameters field filter on the Amazon State Language specification is incorrect because this is used in conjunction with the InputPath field filter, which means it can only be used on the input level of a state. References: https://docs.aws.amazon.com/step-functions/latest/dg/concepts-input-output-filtering.html https://docs.aws.amazon.com/step-functions/latest/dg/how-step-functions-works.html Check out this AWS Step Functions Cheat Sheet: https://tutorialsdojo.com/aws-step-functions/"},{question:"A developer is writing a custom script that will run in an Amazon EC2 instance. The script needs to access the local IP address from the instance to manage a connection to an application outside the AWS Cloud. The developer found out that the details about an instance can be viewed by visiting a certain Uniform Resource Identifier (URI). Which of the following is the correct URI?",answers:[{text:"http://254.169.254.169/latest/meta-data/",isCorrect:!1},{text:"http://169.254.169.254/latest/meta-data/",isCorrect:!0},{text:"http://169.254.169.254/latest/user-data/",isCorrect:!1},{text:"http://254.169.254.169/latest/user-data/",isCorrect:!1}],explanation:'Instance metadata is the data about your instance that you can use to configure or manage the running instance. Instance metadata is divided into categories, for example, hostname, events, and security groups. To view all categories of instance metadata from within a running instance, use the http://169.254.169.254/latest/meta-data/ URI. Note that the IP address 169.254.169.254 is a link-local address and is valid only from the instance. Hence, the correct answer is http://169.254.169.254/latest/meta-data/. The option that says: http://169.254.169.254/latest/user-data/ is incorrect because this URI is used to retrieve user data from within a running instance. The correct path should be "/latest/meta-data/". The option that says: http://254.169.254.169/latest/user-data/ is incorrect because that is not the right IP address. The IP address should be: 169.254.169.254. The option that says: http://254.169.254.169/latest/meta-data/ is incorrect. Although the path is right, the IP address used is invalid. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html#instancedata-meta-data-retrieval-examples https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html Check out this Amazon EC2 Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/'},{question:"A developer is looking for a way to decrease the latency in retrieving data from an Amazon RDS MySQL database. He wants to implement a caching solution that supports Multi-AZ replication with sub-millisecond response times. What must the developer do that requires the LEAST amount of effort?",answers:[{text:"Convert the database schema using the AWS Schema Conversion Tool and move the data to DynamoDB. Enable Amazon DynamoDB Accelerator (DAX).",isCorrect:!1},{text:"Set up AWS Global Accelerator and integrate it with your application to improve overall performance.",isCorrect:!1},{text:"Set up an Elasticache for Memcached cluster between the application and database. Configure it to run with replication to achieve high availability.",isCorrect:!1},{text:"Set up an Elasticache for Redis cluster between the application and database. Configure it to run with replication to achieve high availability.",isCorrect:!0}],explanation:"Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. Amazon ElastiCache offers fully managed Redis and Memcached for most demanding applications that require sub-millisecond response times. However, Redis is the only service in Elasticache that supports replication. Hence, the correct answer is: Set up an Elasticache for Redis between the application and database. Configure it to run with replication to achieve high availability. The option that says: Convert the database schema using the AWS Schema Conversion Tool and move the data to DynamoDB. Enable Amazon DynamoDB Accelerator (DAX) is incorrect. DAX is a fully managed in-memory cache for DynamoDB. You don't have to change the schema of the MySQL database just to achieve a high-performing, high-availability caching solution. You can readily do that with Elasticache. Additionally, changing a schema takes a lot of work, which fails to meet the required solution for the problem. The option that says: Set up an Elasticache for Memcached cluster between the application and database. Configure it to run with replication to achieve high availability is incorrect. While Memcached provides sub-millisecond latency, it does not support replication. The option that says: Set up AWS Global Accelerator and integrate it with your application to improve overall performance is incorrect because Global Accelerator is not a caching solution. It is a service used to improve the performance of your network traffic by utilizing the AWS global infrastructure instead of the public Internet. References: https://aws.amazon.com/elasticache/redis-vs-memcached/ https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/SelectEngine.html Check out this Amazon Elasticache Cheat Sheet: https://tutorialsdojo.com/amazon-elasticache/"},{question:"A full-stack developer has developed an application written in Node.js to host an upcoming mobile game tournament. The developer has decided to deploy the application using AWS Elastic Beanstalk because of its ease-of-use. Upon experimenting, he learned that he could configure the webserver environment with several resources. Which of the following services can the developer configure with Elastic Beanstalk? (Select THREE.)",answers:[{text:"Amazon EC2 Instance",isCorrect:!0},{text:"Application Load Balancer",isCorrect:!0},{text:"Amazon CloudFront",isCorrect:!1},{text:"Amazon Athena",isCorrect:!1},{text:"AWS Lambda",isCorrect:!1},{text:"Amazon CloudWatch",isCorrect:!0}],explanation:"AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. You can upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources. With ElasticBeanstalk, you can: - Select the operating system that matches your application requirements (e.g., Amazon Linux or Windows Server 2016) - Choose from several Amazon EC2 instances, including On-Demand, Reserved Instances, and Spot Instances. - Choose from several available database and storage options. - Enable login access to Amazon EC2 instances for immediate and direct troubleshooting - Quickly improve application reliability by running in more than one Availability Zone. - Enhance application security by enabling HTTPS protocol on the load balancer - Access built-in Amazon CloudWatch monitoring and getting notifications on application health and other important events - Adjust application server settings (e.g., JVM settings) and pass environment variables - Run other application components, such as a memory caching service, side-by-side in Amazon EC2. - Access log files without logging in to the application servers Hence, the correct answers are: Amazon EC2 Instance, Amazon CloudWatch, and Application Load Balancer. You cannot configure Amazon Athena, AWS Lambda, and Amazon CloudFront on ElasticBeanstalk. References: https://aws.amazon.com/elasticbeanstalk/faqs/ https://aws.amazon.com/elasticbeanstalk/ Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/"},{question:"A code that runs on a Lambda function performs a GetItem call from a DynamoDB table. The function runs three times every week. You noticed that the application kept receiving a ProvisionedThroughputExceededException error for 10 seconds most of the time. How should you handle this error?",answers:[{text:"Reduce the frequency of requests using error retries and exponential backoff.",isCorrect:!0},{text:"Enable DynamoDB Accelerator (DAX) to reduce response times from milliseconds to microseconds.",isCorrect:!1},{text:"Create a Local Secondary Index (LSI) to the existing DynamoDB table to increase the provisioned throughput.",isCorrect:!1},{text:"Refactor the code in the Lambda function to optimize its performance.",isCorrect:!1}],explanation:"When your program sends a request, DynamoDB attempts to process it. If the request is successful, DynamoDB returns an HTTP success status code (200 OK), along with the results from the requested operation. If the request is unsuccessful, DynamoDB returns an error. An HTTP 400 status code indicates a problem with your request, such as authentication failure, missing required parameters, or exceeding a table's provisioned throughput. You have to fix the issue in your application before submitting the request again. ProvisionedThroughputExceededException means that your request rate is too high. The AWS SDKs for DynamoDB automatically retries requests that receive this exception. Your request is eventually successful unless your retry queue is too large to finish. To handle this error, you can reduce the frequency of requests using error retries and exponential backoff. Hence, the correct answer is: Reduce the frequency of requests using error retries and exponential backoff. The option that says: Enable DynamoDB Accelerator (DAX) to reduce response times from milliseconds to microseconds is incorrect because DAX is used to provide a fully managed, in-memory caching solution. This option is not the right way to handle errors due to high request rates. The option that says: Refactor the code in the Lambda function to optimize its performance is incorrect because this will just improve the code's readability and maintainability. This won't have any impact on reducing the frequency of requests. The option that says: Create a Local Secondary Index ( LSI ) to the existing DynamoDb table to increase the provisioned throughput is incorrect. LSI is used to give flexibility to your queries against the DynamoDB table. LSI uses an alternative sort key aside from the original sort key defined at the creation of the table. Additionally, you cannot create an LSI on an existing table. It can only be added during the creation of a DynamoDB table. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.RetryAndBackoff https://docs.aws.amazon.com/general/latest/gr/api-retries.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ Amazon DynamoDB Overview: https://youtu.be/3ZOyUNIeorU"},{question:"A company has launched a new serverless application using AWS Lambda. The app ran smoothly for a few weeks until it was featured on a popular website. As its popularity grew, so did the number of users receiving an error. Upon viewing the Lambda function’s monitoring graph, the developer discovered a lot of throttled invocation requests. What can the developer do to troubleshoot this issue? (Select THREE.)",answers:[{text:"Increase Lambda function timeout",isCorrect:!1},{text:"Request a service quota increase",isCorrect:!0},{text:"Use exponential backoff in the application.",isCorrect:!0},{text:"Configure reserved concurrency",isCorrect:!0},{text:"Deploy the Lambda function in VPC",isCorrect:!1},{text:"Use a compiled language like GoLang to improve the function’s performance",isCorrect:!1}],explanation:"AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. With Lambda, you can run code for virtually any type of application or backend service - all with zero administration. Just upload your code, and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile app. Lambda Throttling refers to the rejection of the Lambda function to invocation requests. At this event, the Lambda will return a throttling error exception which you need to handle. This happens because your current concurrency execution count is greater than your concurrency limit. Throttling is intended to protect your resources and downstream applications. Though Lambda automatically scales to accommodate your incoming traffic, your function can still be throttled for various reasons. The following are the recommended solutions to handle throttling issues: Configure reserved concurrency - by default, there are 900 unreserved concurrencies shared across all functions in a region. To prevent other functions from consuming the available concurrent executions, reserve a portion of it to your Lambda function based on the demand of your current workload. Use exponential backoff in your app - a technique that uses progressively longer waits between retries for consecutive error responses. This can be used to handle throttling issues by preventing collision between simultaneous requests. Use a dead-letter queue - If you're using Amazon S3 and Amazon EventBridge (Amazon CloudWatch Events), configure your function with a dead letter queue to catch any events that are discarded due to constant throttles. This can protect your data if you're seeing significant throttling. Request a service quota increase - you can reach AWS support to request for a higher service quota for concurrent executions. Hence, the correct answers are: - Use exponential backoff in your application - Configure reserved concurrency - Request a service quota increase The option that says: Deploy the Lambda function in VPC is incorrect because this has nothing to do with fixing throttling errors. The only time you should do this is if you need to have a connection between a Lambda function and a resource running in your VPC. The option that says: Use a compiled language like GoLang to improve the function's performance is incorrect because no matter what language you use, the Lambda function will still throw a throttling error if the current concurrency execution count is greater than your concurrency limit. The option that says: Increase Lambda function timeout is incorrect. If the time a function runs exceeds its current timeout value, it will throw a timeout error. The scenario is a throttling issue and not a timeout. References: https://aws.amazon.com/premiumsupport/knowledge-center/lambda-troubleshoot-throttling/ https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"}]},{id:"aws-developer-8",title:"AWS Certified Developer Associate Practice Exams 2",description:"Additional practice questions covering AWS development topics.",questions:[{question:"A recently deployed Lambda function has an intermittent issue in processing customer data. You enabled the active tracing option in order to detect, analyze, and optimize performance issues of your function using the X-Ray service. Which of the following environment variables are used by AWS Lambda to facilitate communication with X-Ray? (Select TWO.)",answers:[{text:"AUTO_INSTRUMENT",isCorrect:!1},{text:"_X_AMZN_TRACE_ID",isCorrect:!0},{text:"AWS_XRAY_DEBUG_MODE",isCorrect:!1},{text:"AWS_XRAY_TRACING_NAME",isCorrect:!1},{text:"AWS_XRAY_CONTEXT_MISSING",isCorrect:!0}],explanation:"AWS X-Ray is an AWS service that allows you to detect, analyze, and optimize performance issues with your AWS Lambda applications. X-Ray collects metadata from the Lambda service and any upstream or downstream services that make up your application. X-Ray uses this metadata to generate a detailed service graph that illustrates performance bottlenecks, latency spikes, and other issues that impact the performance of your Lambda application. AWS Lambda uses environment variables to facilitate communication with the X-Ray daemon and configure the X-Ray SDK. _X_AMZN_TRACE_ID: Contains the tracing header, which includes the sampling decision, trace ID, and parent segment ID. If Lambda receives a tracing header when your function is invoked, that header will be used to populate the _X_AMZN_TRACE_ID environment variable. If a tracing header was not received, Lambda will generate one for you. AWS_XRAY_CONTEXT_MISSING: The X-Ray SDK uses this variable to determine its behavior in the event that your function tries to record X-Ray data, but a tracing header is not available. Lambda sets this value to LOG_ERROR by default. AWS_XRAY_DAEMON_ADDRESS: This environment variable exposes the X-Ray daemon's address in the following format: IP_ADDRESS:PORT. You can use the X-Ray daemon's address to send trace data to the X-Ray daemon directly without using the X-Ray SDK. Therefore, the correct answers for this scenario are the _X_AMZN_TRACE_ID and AWS_XRAY_CONTEXT_MISSING environment variables. AWS_XRAY_TRACING_NAME is incorrect because this is primarily used in X-Ray SDK where you can set a service name that the SDK uses for segments. AWS_XRAY_DEBUG_MODE is incorrect because this is used to configure the SDK to output logs to the console without using a logging library. AUTO_INSTRUMENT is incorrect because this is primarily used in X-Ray SDK for Django Framework only. This allows the recording of subsegments for built-in database and template rendering operations. References: https://docs.aws.amazon.com/lambda/latest/dg/lambda-x-ray.html#viewing-lambda-xray-results https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-nodejs-configuration.html https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-python-configuration.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/ Instrumenting your Application with AWS X-Ray: https://tutorialsdojo.com/instrumenting-your-application-with-aws-x-ray/"},{question:"A developer needs to encrypt all objects being uploaded by their application to the S3 bucket to comply with the company's security policy. The bucket will use server-side encryption with Amazon S3-Managed encryption keys (SSE-S3) to encrypt the data using 256-bit Advanced Encryption Standard (AES-256) block cipher. Which of the following request headers should the developer use?",answers:[{text:"x-amz-server-side-encryption-customer-algorithm",isCorrect:!1},{text:"x-amz-server-side-encryption",isCorrect:!0},{text:"x-amz-server-side-encryption-customer-key-MD5",isCorrect:!1},{text:"x-amz-server-side-encryption-customer-key",isCorrect:!1}],explanation:"Server-side encryption protects data at rest. If you use Server-Side Encryption with Amazon S3-Managed Encryption Keys (SSE-S3), Amazon S3 will encrypt each object with a unique key and as an additional safeguard, it encrypts the key itself with a master key that it rotates regularly. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data. If you need server-side encryption for all of the objects that are stored in a bucket, use a bucket policy. For example, the following bucket policy denies permissions to upload an object unless the request includes the x-amz-server-side-encryption header to request server-side encryption: However, if you chose to use server-side encryption with customer-provided encryption keys (SSE-C), you must provide encryption key information using the following request headers: x-amz-server-side​-encryption​-customer-algorithm x-amz-server-side​-encryption​-customer-key x-amz-server-side​-encryption​-customer-key-MD5 Hence, using the x-amz-server-side-encryption header is correct as this is the one being used for Amazon S3-Managed Encryption Keys (SSE-S3). All other options are incorrect since they are used for SSE-C. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html Check out this Amazon S3 Cheat Sheet: https://tutorialsdojo.com/amazon-s3/"},{question:"A serverless application is composed of several Lambda functions which reads data from RDS. These functions must share the same connection string that should be encrypted to improve data security. Which of the following is the MOST secure way to meet the above requirement?",answers:[{text:"Create a Secure String Parameter using the AWS Systems Manager Parameter Store.",isCorrect:!0},{text:"Use AWS Lambda environment variables encrypted with CloudHSM.",isCorrect:!1},{text:"Use AWS Lambda environment variables encrypted with KMS which will be shared by the Lambda functions.",isCorrect:!1},{text:"Create an IAM Execution Role that has access to RDS and attach it to the Lambda functions.",isCorrect:!1}],explanation:"AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. You can store values as plain text or encrypted data. You can then reference values by using the unique name that you specified when you created the parameter. Parameter Store offers the following benefits and features: - Use a secure, scalable, hosted secrets management service (No servers to manage). - Improve your security posture by separating your data from your code. - Store configuration data and secure strings in hierarchies and track versions. - Control and audit access at granular levels. - Configure change notifications and trigger automated actions. - Tag parameters individually, and then secure access from different levels, including operational, parameter, Amazon EC2 tag, or path levels. - Reference AWS Secrets Manager secrets by using Parameter Store parameters. Hence, creating a Secure String Parameter using the AWS Systems Manager Parameter Store is the correct solution for this scenario. The option that says: Use AWS Lambda environment variables encrypted with KMS which will be shared by the Lambda functions is incorrect. Even though the credentials will be encrypted, these environment variables will only be used by an individual Lambda function, and cannot be shared. The option that says: Create an IAM Execution Role that has access to RDS and attach it to the Lambda functions is incorrect because this solution will not encrypt the database credentials for RDS. The option that says: Use AWS Lambda environment variables encrypted with CloudHSM is incorrect because Lambda primarily uses KMS for encryption and not CloudHSM. References: https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html https://aws.amazon.com/blogs/compute/sharing-secrets-with-aws-lambda-using-aws-systems-manager-parameter-store/ Check out this AWS Systems Manager Cheat Sheet: https://tutorialsdojo.com/aws-systems-manager/"},{question:"A web application is running in an ECS Cluster and updates data in DynamoDB several times a day. The clients retrieve data directly from the DynamoDB through APIs exposed by Amazon API Gateway. Although API caching is enabled, there are specific clients that want to retrieve the latest data from DynamoDB for every API request sent. What should be done to only allow authorized clients to invalidate an API Gateway cache entry when submitting API requests? (Select TWO.)",answers:[{text:"Modify the cache settings to retrieve the latest data from DynamoDB if the request header's authorization signature matches your API's trusted clients list.",isCorrect:!1},{text:"Tick the Require Authorization checkbox in the Cache Settings of your API via the console.",isCorrect:!0},{text:"The client must send a request which contains the Cache-Control: max-age=1 header.",isCorrect:!1},{text:"The client must send a request which contains the Cache-Control: max-age=0 header.",isCorrect:!0},{text:"Provide your clients an authorization token from STS to query data directly from DynamoDB.",isCorrect:!1}],explanation:"A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the Cache-Control: max-age=0 header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint. Ticking the Require authorization checkbox ensures that not every client can invalidate the API cache. If most or all of the clients invalidate the API cache, this could significantly increase the latency of your API. Hence, to only allow authorized clients to invalidate an API Gateway cache entry when submitting API requests, you can just tick the Require Authorization checkbox in the Cache Settings of your API via the console and instruct the client to send a request which contains the Cache-Control: max-age=0 header. Instructing the client to send a request which contains the Cache-Control: max-age=1 header is incorrect because the value of the max-age should be 0 and not 1. Providing your clients an authorization token from STS to query data directly from DynamoDB is incorrect because this will not enable your clients to invalidate the cache in API Gateway. Considering that your clients are using APIs to interact with DynamoDB, you should not provide them access to directly submit queries to your table but only through API Gateway. Modifying the cache settings to retrieve the latest data from DynamoDB if the request header's authorization signature matches your API's trusted clients list is incorrect because this configuration can't be done. There is no feature in API Gateway Cache Settings that would allow you to make a list of authorized signatures that are allowed to invalidate cache entries. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching https://aws.amazon.com/api-gateway/faqs/#Throttling_and_Caching Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A developer is creating a script using AWS CLI to retrieve a list of objects in an S3 bucket. However, the script is timing out if the bucket has tens of thousands of objects. Which solution would most likely rectify the issue?",answers:[{text:"Enable CORS",isCorrect:!1},{text:"Apply the pagination parameters in the AWS CLI command",isCorrect:!0},{text:"Increase the AWS CLI timeout value",isCorrect:!1},{text:"Enable Amazon S3 Transfer Acceleration",isCorrect:!1}],explanation:"For commands that possibly return a long list of items, the AWS CLI provides parameters allowing you to limit the number of items included in the output when the AWS CLI queries a service's API. By default, the AWS CLI retrieves all accessible items with a page size of 1,000. If you need help running list commands on a large number of resources, the default page size of 1000 may be too large. This can cause calls to AWS services to exceed the maximum allowed time, resulting in a \"timed out\" error. One of the pagination options you can use is the --page-size option. This option tells the AWS CLI to request a smaller number of items from each call to the AWS service. aws s3api list-objects --bucket tdbucket --page-size 100 The CLI still retrieves the entire list, but it makes a greater number of service API calls in the background and retrieves fewer items with each request. This increases the probability that individual calls will succeed in without the use of a timeout. Hence, the correct answer is: Apply the pagination parameters in the AWS CLI command. The option that says: Increase the AWS CLI timeout value is incorrect. Increasing CLI parameters like --cli-connect-timeout or --cli-read-timeout would only prolong the process and increase susceptibility to timeouts due to network latency. On the other hand, pagination would handle large data sets by retrieving objects in manageable chunks, aligning with S3's response limits and preventing timeouts. The option that says: Enabling Amazon S3 Transfer Acceleration is incorrect because this is only a bucket-level feature that enables faster data transfers to and from Amazon S3. Although this will improve the retrieval times of your objects, this feature will still not paginate the result, which may still cause time-out errors. The option that says: Enabling CORS is incorrect because the Cross-origin resource sharing (CORS) is simply thats allow client web applications that are loaded in one domain to communicate with resources in a different domain. This is not useful in paginating the results from an AWS CLI call. References: https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html https://docs.aws.amazon.com/cli/latest/reference/s3api/list-objects.html Check out this AWS CloudShell Cheat Sheet: https://tutorialsdojo.com/aws-cloudshell/"},{question:"A programmer is developing a Node.js application that will be run on a Linux server in their on-premises data center. The application will access various AWS services such as S3, DynamoDB, and ElastiCache using the AWS SDK. Which of the following is the MOST suitable way to provide access for the developer to accomplish the specified task?",answers:[{text:"Go to the AWS Console and create a new IAM user with programmatic access. In the application server, create the credentials file at ~/.aws/credentials with the access keys of the IAM user.",isCorrect:!0},{text:"Go to the AWS Console and create a new IAM User with the appropriate permissions. In the application server, create the credentials file at ~/.aws/credentials with the username and the hashed password of the IAM User.",isCorrect:!1},{text:"Create an IAM role with the appropriate permissions to access the required AWS services. Assign the role to the on-premises Linux server.",isCorrect:!1},{text:"Create an IAM role with the appropriate permissions to access the required AWS services and assign the role to the on-premises Linux server. Whenever the application needs to access any AWS services, request for temporary security credentials from STS using the AssumeRole API.",isCorrect:!1}],explanation:"If you have resources that are running inside AWS that need programmatic access to various AWS services, then the best practice is always to use IAM roles. However, applications running outside of an AWS environment will need access keys for programmatic access to AWS resources. For example, monitoring tools running on-premises and third-party automation tools will need access keys. Access keys are long-term credentials for an IAM user or the AWS account root user. You can use access keys to sign programmatic requests to the AWS CLI or AWS API (directly or using the AWS SDK). In order to use the AWS SDK for your application, you have to create your credentials file first at ~/.aws/credentials for Linux servers or at C:\\Users\\USER_NAME\\.aws\\credentials for Windows users and then save your access keys. Hence, the correct answer is: Go to the AWS Console and create a new IAM user with programmatic access. In the application server, create the credentials file at ~/.aws/credentials with the access keys of the IAM user. The option that says: Create an IAM role with the appropriate permissions to access the required AWS services and assign the role to the on-premises Linux server. Whenever the application needs to access any AWS services, request for temporary security credentials from STS using the AssumeRole API is incorrect because the scenario says that the application is running in a Linux server on-premises and not on an EC2 instance. You cannot directly assign an IAM Role to a server on your on-premises data center. Although it may be possible to use a combination of STS and IAM Role, the use of access keys for AWS SDK is still preferred, especially if the application server is on-premises. The option that says: Create an IAM role with the appropriate permissions to access the required AWS services. Assign the role to the on-premises Linux server is also incorrect because, just as mentioned above, the use of an IAM Role is not a suitable solution for this scenario. The option that says: Go to the AWS Console and create a new IAM User with the appropriate permissions. In the application server, create the credentials file at ~/.aws/credentials with the username and the hashed password of the IAM User is incorrect. An IAM user's username and password can only be used to interact with AWS via its Management Console. These credentials are intended for human use and are not suitable for use in automated systems, such as applications and scripts that make programmatic calls to AWS services. References: https://aws.amazon.com/developers/getting-started/nodejs/ https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys https://aws.amazon.com/blogs/security/guidelines-for-protecting-your-aws-account-while-using-programmatic-access/ Check out this AWS IAM Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"},{question:"A developer is working on a Lambda function which has an event source mapping to process requests from API Gateway. The function will consistently have 10 requests per second and it will take a maximum of 50 seconds to complete each request. What should the developer do to prevent the function from throttling?",answers:[{text:"Use Dead Letter Queues (DLQ) to reprocess failed requests.",isCorrect:!1},{text:"Do nothing since Lambda will automatically scale to handle the load.",isCorrect:!0},{text:"Implement traffic shifting in Lambda using Aliases.",isCorrect:!1},{text:"Submit a Service Limit Increase request to AWS to raise your concurrent executions limit.",isCorrect:!1}],explanation:"If you create a Lambda function to process events from event sources that aren't poll-based (for example, Lambda can process every event from other sources, like Amazon S3 or API Gateway), each published event is a unit of work, in parallel, up to your account limits. Therefore, the number of invocations these event sources make influences the concurrency. You can use this formula to estimate the capacity used by your function: concurrent executions = (invocations per second) x (average execution duration in seconds) For example, consider a Lambda function that processes Amazon S3 events. Suppose that the Lambda function takes on average three seconds and Amazon S3 publishes 10 events per second. Then, you will have 30 concurrent executions of your Lambda function. See the calculation shown below to visualize the process: = (10 events per second) x (3 seconds average execution duration) = 30 concurrent executions In this scenario, it is expected that the Lambda function takes a maximum of 50 seconds for every execution with 10 requests per second. Using the formula above, the function will have 500 concurrent executions. = (10 events per second) x (50 seconds average execution duration) = 500 concurrent executions AWS Lambda dynamically scales function execution in response to increased traffic, up to your concurrency limit. Under sustained load, your function's concurrency bursts to an initial level between 500 and 3000 concurrent executions that varies per region. After the initial burst, the function's capacity increases by an additional 500 concurrent executions each minute until either the load is accommodated, or the total concurrency of all functions in the region hits the limit. By default, AWS Lambda limits the total concurrent executions across all functions within a given region to 1000. This limit can be raised by requesting for AWS to increase the limit of the concurrent executions of your account. Since the expected concurrent executions of the Lambda function is well within the default concurrency limit, the best thing to do here is to do nothing since Lambda will automatically scale to handle the load. Submitting a Service Limit Increase request to AWS to raise your concurrent executions limit is incorrect because this is totally unnecessary given that the concurrent executions that will be used by your function are within the limits. Using Dead Letter Queues (DLQ) to reprocess failed requests is incorrect because DLQs are primarily used to direct unprocessed events to an Amazon SQS queue or an Amazon SNS topic to analyze the failure. Implementing traffic shifting in Lambda using Aliases is incorrect because this just allows you to point to two different versions of the Lambda function and dictate what percentage of incoming traffic is sent to each version. References: https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html#function-configuration Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A new IT policy requires you to trace all calls that your Node.js application sends to external HTTP web APIs as well as SQL database queries. You have to instrument your application, which is hosted in Elastic Beanstalk, in order to properly trace the calls via the X-Ray console. What should you do to comply with the given requirement?",answers:[{text:"Enable active tracing in the Elastic Beanstalk by including the healthcheckurl.config configuration file in the .ebextensions directory of your source code.",isCorrect:!1},{text:"Use a user data script to run the daemon automatically.",isCorrect:!1},{text:"Create a Docker image that runs the X-Ray daemon.",isCorrect:!1},{text:"Enable the X-Ray daemon by including the xray-daemon.config configuration file in the .ebextensions directory of your source code.",isCorrect:!0}],explanation:"You can use the AWS Elastic Beanstalk console or a configuration file to run the AWS X-Ray daemon on the instances in your environment. X-Ray is an AWS service that gathers data about the requests that your application serves, and uses it to construct a service map that you can use to identify issues with your application and opportunities for optimization. To relay trace data from your application to AWS X-Ray, you can run the X-Ray daemon on your Elastic Beanstalk environment's Amazon EC2 instances. Elastic Beanstalk platforms provide a configuration option that you can set to run the daemon automatically. You can enable the daemon in a configuration file in your source code or by choosing an option in the Elastic Beanstalk console. When you enable the configuration option, the daemon is installed on the instance and runs as a service. Hence, the correct answer is to: enable the X-Ray daemon by including the xray-daemon.config configuration file in the .ebextensions directory of your source code, just as shown above. Using a user data script to run the daemon automatically is incorrect because this is only applicable if you want to enable X-Ray to your EC2 instances. Creating a Docker image that runs the X-Ray daemon is incorrect because this is what you need to do if you want to enable X-Ray on ECS Cluster and not on Elastic Beanstalk. Enabling active tracing in the Elastic Beanstalk by including the healthcheckurl.config configuration file in the .ebextensions directory of your source code is incorrect because this configuration file only sets the application health check URL and not X-Ray Tracing. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-beanstalk.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-debugging.html Check out these AWS X-Ray and Elastic Beanstalk Cheat Sheets: https://tutorialsdojo.com/aws-x-ray/ https://tutorialsdojo.com/aws-elastic-beanstalk/"},{question:"An application hosted in an Amazon ECS Cluster processes a large data stream and stores the result in a DynamoDB table. There is an urgent requirement to detect new entries in the table and automatically trigger a Lambda function to run some verification tests on the processed data. Which of the following options can satisfy the requirement with minimal configuration?",answers:[{text:"Run the Lambda function using SNS each time the ECS Cluster successfully processes financial data.",isCorrect:!1},{text:"Detect the new entries in the DynamoDB table using AWS Copilot, then automatically invoke the Lambda function for processing.",isCorrect:!1},{text:"Enable DynamoDB Streams to detect the new entries and automatically trigger the Lambda function.",isCorrect:!0},{text:"Set up an Amazon EventBridge (Amazon CloudWatch Events) rule to automatically trigger the Lambda function whenever a new entry is created in the DynamoDB table.",isCorrect:!1}],explanation:"Amazon DynamoDB is integrated with AWS Lambda so that you can create triggers—pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables. If you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records. You can create a Lambda function which can perform a specific action that you specify, such as sending a notification or initiating a workflow. For instance, you can set up a Lambda function to simply copy each stream record to persistent storage, such as EFS or S3, to create a permanent audit trail of write activity in your table. Suppose you have a mobile gaming app that writes to a TutorialsDojoCourses table. Whenever the TopCourse attribute of the TutorialsDojoScores table is updated, a corresponding stream record is written to the table's stream. This event could then trigger a Lambda function that posts a congratulatory message on a social media network. (The function would simply ignore any stream records that are not updates to TutorialsDojoCourses or that do not modify the TopCourse attribute.) Hence, the correct answer is to enable DynamoDB Streams to detect the new entries and automatically trigger the Lambda function. In this way, the requirement can be met with minimal configuration change as DynamoDB streams can be used as an event source to automatically trigger Lambda functions whenever there is a new entry. The option that says: Set up an Amazon EventBridge (Amazon CloudWatch Events) rule to automatically trigger the Lambda function whenever a new entry is created in the DynamoDB table is incorrect. An Amazon EventBridge (Amazon CloudWatch Events) rule is not capable of detecting table-level events from Amazon DynamoDB. The option that says: Run the Lambda function using SNS each time the ECS Cluster successfully processes financial data is incorrect because you don't need to create an SNS topic just to invoke Lambda functions. You can simply enable DynamoDB streams to meet the requirement with less configuration. The option that says: Detect new entries in the DynamoDB table using AWS Copilot then automatically invoke the Lambda function for processing is incorrect because AWS Copilot is just a command-line tool that allows developers to quickly and easily build, release, and operate containerized applications on AWS. It does not have the capability to detect new entries in a DynamoDB table. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html Check out this Amazon DynamoDB cheat sheet: https://tutorialsdojo.com/amazon-dynamodb/ AWS Lambda Integration with Amazon DynamoDB Streams: https://tutorialsdojo.com/aws-lambda-integration-with-amazon-dynamodb-streams/"},{question:"Your application is processing one Kinesis data stream which has four shards, and each instance has one KCL worker. To scale up processing in your application, you reshard your stream to increase the number of open shards to six. What is the MAXIMUM number of EC2 instances that you should launch to achieve optimum performance?",answers:[{text:"12",isCorrect:!1},{text:"5",isCorrect:!1},{text:"6",isCorrect:!0},{text:"3",isCorrect:!1}],explanation:"Resharding enables you to increase or decrease the number of shards in a stream in order to adapt to changes in the rate of data flowing through the stream. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that shard. It also tracks the shards in the stream using an Amazon DynamoDB table. Typically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances. To scale up processing in your application, you should test a combination of these approaches: - Increasing the instance size (because all record processors run in parallel within a process) - Increasing the number of instances up to the maximum number of open shards (because shards can be processed independently) - Increasing the number of shards (which increases the level of parallelism) Thus, the maximum number of instances you can launch is 6, to match the number of open shards in a ratio of 1:1. Although you can launch 3 instances in which each instance handles 2 shards, this is not the maximum number of instances you can deploy for your application. Hence, this option is incorrect. Take note that the maximum number of your instances is not half the number of open shards. Just like the above option, you can also launch 5 instances in which each instance handles 3 shards. However, this is not the maximum number of instances you can launch. Keep in mind that the maximum number of your instances can be equal to the number of open shards of the Kinesis stream. Therefore, this option is also incorrect. Launching 12 instances is incorrect because you should ensure that the number of instances does not exceed the number of open shards. The maximum number of instances that you should deploy is 6. References: https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/ Kinesis Scaling, Resharding and Parallel Processing: https://tutorialsdojo.com/kinesis-scaling-resharding-and-parallel-processing/"},{question:"You are developing a Lambda function which processes event notifications from Amazon S3. It is expected that the function will have: - 50 requests per second - 100 seconds to complete each request What should you do to prevent any issues when the function has been deployed and becomes operational?",answers:[{text:"No additional action needed since Lambda will automatically scale based on the incoming requests.",isCorrect:!1},{text:"Implement exponential backoff in your application.",isCorrect:!1},{text:"Request for AWS to increase the limit of your concurrent executions.",isCorrect:!0},{text:"Increase the concurrency limit of the function.",isCorrect:!1}],explanation:"Concurrent executions refers to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the concurrent execution count will differ depending on whether or not your Lambda function is processing events from a poll-based event source. If you create a Lambda function to process events from event sources that aren't poll-based (for example, Lambda can process every event from other sources, like Amazon S3 or API Gateway), each published event is a unit of work, in parallel, up to your account limits. Therefore, the number of invocations these event sources make influences the concurrency. You can use this formula to estimate the capacity used by your function: concurrent executions = (invocations per second) x (average execution duration in seconds) For example, consider a Lambda function that processes Amazon S3 events. Suppose that the Lambda function takes on average three seconds and Amazon S3 publishes 10 events per second. Then, you will have 30 concurrent executions of your Lambda function. See the calculation shown below to visualize the process: = (10 events per second) x (3 seconds average execution duration)= 30 concurrent executions In this scenario, it is expected that the Lambda function takes an average of 100 seconds for every execution with 50 requests per second. Using the formula above, the function will have 5,000 concurrent executions. = (50 events per second) x (100 seconds average execution duration)= 5,000 concurrent executions AWS Lambda dynamically scales function execution in response to increased traffic, up to your concurrency limit. Under sustained load, your function's concurrency bursts to an initial level between 500 and 3000 concurrent executions that varies per region. After the initial burst, the function's capacity increases by an additional 500 concurrent executions each minute until either the load is accommodated, or the total concurrency of all functions in the region hits the limit. By default, AWS Lambda limits the total concurrent executions across all functions within a given region to 1000. This limit can be raised by requesting for AWS to increase the limit of the concurrent executions of your account. Since the expected concurrent executions of the Lambda function will exceed the default concurrency limit, the best thing to do here is to request for AWS to increase the limit of your concurrent executions. Choosing to do no additional action since Lambda will automatically scale based on the incoming requests is incorrect because the dynamic scaling of AWS Lambda has its limits. Because the value of the expected concurrency executions has exceeded the default limit, it is best to contact AWS to increase the concurrent executions of your account to prevent any throttling issues when the function has been deployed and becomes operational. Implementing an exponential backoff in your application is incorrect because this doesn't address the concurrency issue of your Lambda function. This will just configure your application to have progressively longer waits between API call retries for consecutive error responses. Increasing the concurrency limit of the function is incorrect because, by default, you can only set the limit as high as 900 per function, which is quite insufficient to handle the expected 5,000 concurrency executions. To properly provide the required capacity needed by the function, you have to request for AWS to increase the concurrency limit of your account. References: https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html#function-configuration Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A developer is instructed to configure a worker daemon to queue messages based on a specific schedule using a worker environment hosted in Elastic Beanstalk. Periodic tasks should be defined to automatically add jobs to your worker environment's queue at regular intervals. Which configuration file should the developer add to the source bundle to meet the above requirement?",answers:[{text:"appspec.yml",isCorrect:!1},{text:"cron.yaml",isCorrect:!0},{text:"env.yaml",isCorrect:!1},{text:"Dockerrun.aws.json",isCorrect:!1}],explanation:"AWS resources created for a worker environment tier include an Auto Scaling group, one or more Amazon EC2 instances, and an IAM role. For the worker environment tier, Elastic Beanstalk also creates and provisions an Amazon SQS queue if you don’t already have one. When you launch a worker environment tier, Elastic Beanstalk installs the necessary support files for your programming language of choice and a daemon on each EC2 instance in the Auto Scaling group. The daemon is responsible for pulling requests from an Amazon SQS queue and then sending the data to the web application running in the worker environment tier that will process those messages. If you have multiple instances in your worker environment tier, each instance has its own daemon, but they all read from the same Amazon SQS queue. You can define periodic tasks in a file named cron.yaml in your source bundle to add jobs to your worker environment's queue automatically at a regular intervals. For example, you can configure and upload a cron.yaml file, which creates two periodic tasks: one that runs every 12 hours and a second that runs at 11 pm UTC every day. Hence, using the cron.yaml is the correct configuration file to be used in this scenario. Dockerrun.aws.json is incorrect because this configuration file is primarily used in multi-container Docker environments that are hosted in Elastic Beanstalk. This can be used alone or combined with source code and content in a source bundle to create an environment on a Docker platform. env.yaml is incorrect because this is primarily used to configure the environment name, solution stack, and environment links to use when creating your environment in Elastic Beanstalk. appspec.yml is incorrect because this is used to manage each application deployment as a series of lifecycle event hooks in CodeDeploy and not in Elastic Beanstalk. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html#worker-periodictasks https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/"},{question:"A serverless application composed of Lambda, API Gateway, and DynamoDB has been running without issues for quite some time. As part of the IT compliance of the company, a developer was instructed to ensure that all of the new changes made to the items in DynamoDB are recorded and stored in another DynamoDB table in another region. In this scenario, which of the following is the MOST ideal way to comply with the requirements?",answers:[{text:"Create an Amazon EventBridge (Amazon CloudWatch Events) rule that tracks table-level events in DynamoDB. Set a Lambda function as a rule target to process and save new changes to the other table.",isCorrect:!1},{text:"Enable DynamoDB Streams and configure a Lambda function to process and save new changes to the other table.",isCorrect:!0},{text:"Set up DynamoDB Accelerator",isCorrect:!1},{text:"Enable DynamoDB Point-in-Time Recovery to automatically sync the two tables.",isCorrect:!1}],explanation:'DynamoDB Streams enables solutions such as these, and many others. DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table, and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real-time. A DynamoDB stream is an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attribute(s) of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the "before" and "after" images of modified items. Hence, the correct answer is: Enable DynamoDB Streams and configure a Lambda function to process and save new changes to the other table. The option that says: Set up DynamoDB Accelerator is incorrect because the DynamoDB Accelerator (DAX) feature simply takes the performance of the DynamoDB table to the next level with response times in microseconds for millions of requests per second for read-heavy workloads. You have to use DynamoDB Streams instead. The option that says: Enable DynamoDB Point-in-Time Recovery to automatically sync the two tables is incorrect because this feature just helps protect your DynamoDB tables from accidental write or delete operations. The option that says: Create an Amazon EventBridge (Amazon CloudWatch Events) rule that tracks table-level events in DynamoDB. Set a Lambda function as a rule target to process and save new changes to the other table is incorrect. An Amazon EventBridge (Amazon CloudWatch Events) rule is not capable of detecting table-level events from Amazon DynamoDB. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.CrossRegionRepl.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/ AWS Lambda Integration with Amazon DynamoDB Streams: https://tutorialsdojo.com/aws-lambda-integration-with-amazon-dynamodb-streams/'},{question:"You have several API Gateway APIs with Lambda Integration for each release life cycle of your application. There is a requirement to consolidate multiple releases into a single API Gateway for the ALPHA, BETA, RC (Release Candidate), and PROD releases. For example, their clients can connect to their ALPHA release by using the alpha.tutorialsdojo.com endpoint and beta release through the beta.tutorialsdojo.com endpoint. As the AWS developer, how can you satisfy this requirement?",answers:[{text:"Modify the Integration Request of the API Gateway to manage different endpoints for each release.",isCorrect:!1},{text:"Modify the Integration Response of the API Gateway to add different endpoints for each release.",isCorrect:!1},{text:"Set up Stage Variables for each release.",isCorrect:!0},{text:"Use Layers to the underlying Lambda functions of the API Gateway.",isCorrect:!1}],explanation:"Amazon API Gateway is a service that simplifies the creation, publishing, maintenance, monitoring, and securing of APIs at any scale. API Gateway enables you to define and manage the interface between front-end clients and backend services. It handles all tasks associated with processing hundreds of thousands of concurrent API calls, including traffic management, authorization, access control, monitoring, and API version management. One of its key features is the ability to create different stages for an API, allowing developers to maintain multiple versions of the same API, such as development, testing, and production. Stage Variables in API Gateway are name-value pairs that can be used to configure different settings for each stage of an API. These variables allow developers to reference different backend resources and settings dynamically without changing the API deployment. For instance, stage variables can be used to specify different Lambda function ARNs, database endpoints, or other environment-specific configurations. This capability is handy for managing multiple environments (like ALPHA, BETA, RC, and PROD) within the same API Gateway, ensuring that each environment can be independently configured and managed using a single consolidated API Gateway. Hence, the correct answer is: Set up Stage Variables for each release. The options that say: - Modify the Integration Response of the API Gateway to add different endpoints for each release. - Modify the Integration Request of the API Gateway to manage different endpoints for each release. are both incorrect because these two are primarily used to map the response or the request data to and from your backend. There is no way to use the Integration Request or Response as a variable that can be a part of the URL string of an HTTP integration for a method in your REST API. Moreover, these two can't be part of a custom API URL: <alpha|beta|rc|prod>.tutorialsdojo.com, which was mentioned in the scenario. The option that says: Use Layers to the underlying Lambda functions of the API Gateway is incorrect because this is only applicable if you want to configure your Lambda function to pull in additional code and content in the form of layers. Remember that a layer is just a ZIP archive that contains libraries, a custom runtime, or other dependencies. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-stages.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"An application performs various workflows and processes long-running tasks that take a long time to complete. Users are complaining that the application is unresponsive since the workflow substantially increases the time it takes to complete a user request. The development team is looking for a managed solution that can handle background tasks efficiently, scale automatically, and integrate seamlessly with the existing application deployed on Elastic Beanstalk. Which of the following is the BEST way to improve the performance of the application?",answers:[{text:"Use a multicontainer docker environment in Elastic Beanstalk to process the long-running tasks asynchronously.",isCorrect:!1},{text:"Use an Elastic Beanstalk worker environment to process the tasks asynchronously.",isCorrect:!0},{text:"Spawn a worker process locally in the EC2 instances and process the tasks asynchronously.",isCorrect:!1},{text:"Use an Amazon ECS Cluster with a Fargate launch type to process the tasks asynchronously.",isCorrect:!1}],explanation:"If your application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker environment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your application stays responsive under load. A long-running task is anything that substantially increases the time it takes to complete a request, such as processing images or videos, sending emails, or generating a ZIP archive. These operations can take only a second or two to complete, but a delay of a few seconds is a lot for a web request that would otherwise complete in less than 500 ms. One option is to spawn a worker process locally, return success, and process the task asynchronously. This works if your instance can keep up with all of the tasks sent to it. Under high load, however, an instance can become overwhelmed with background tasks and become unresponsive to higher-priority requests. If individual users can generate multiple tasks, the increase in load might not correspond to an increase in users, making it hard to scale out your web server tier effectively. To avoid running long-running tasks locally, you can use the AWS SDK for your programming language to send them to an Amazon Simple Queue Service (Amazon SQS) queue and run the process that performs them on a separate set of instances. You then design these worker instances to take items from the queue only when they have the capacity to run them, preventing them from becoming overwhelmed. Elastic Beanstalk worker environments simplify this process by managing the Amazon SQS queue and running a daemon process on each instance that reads from the queue for you. When the daemon pulls an item from the queue, it sends an HTTP POST request locally to http://localhost/ on port 80 with the contents of the queue message in the body. All that your application needs to do is perform the long-running task in response to the POST. You can configure the daemon to post to a different path, use a MIME type other than application/JSON, connect to an existing queue, or customize connections (maximum concurrent requests), timeouts, and retries. Hence, the best solution to meet the requirements of this scenario is to use an Elastic Beanstalk worker environment to process the tasks asynchronously. Spawning a worker process locally in the EC2 instances then processing the tasks asynchronously is incorrect. Although this is a valid solution, it is not scalable and hence, it's not the best one. Under high load, an instance can become overwhelmed with background tasks and become unresponsive to higher priority requests. This makes it hard to scale out your web server tier effectively. Using a multicontainer docker environment in Elastic Beanstalk to process the long-running tasks asynchronously is incorrect because this is primarily used to support multiple containers per Amazon EC2 instance with multicontainer Docker platform. This is not applicable when processing long-running tasks and it is not scalable since it's not using an SQS queue. Using an Amazon ECS Cluster with a Fargate launch type to process the tasks asynchronously is incorrect because Fargate just allows you to run your containerized applications without the need to provision and manage the backend infrastructure. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/"},{question:"A developer has an application that uses a Lambda function to process data from an Aurora MySQL DB Instance in a Virtual Private Cloud (VPC). The database throws a MySQL: ERROR 1040: Too many connections error whenever there is a surge in incoming traffic. Which is the most suitable solution for resolving the issue?",answers:[{text:"Increase the allocated memory of your function.",isCorrect:!1},{text:"Provision an RDS Proxy between the Lambda function and the RDS database instance",isCorrect:!0},{text:"Increase the concurrency limit of the Lambda function",isCorrect:!1},{text:"Increase the value of the max_connections parameter of the Aurora MySQL DB Instance.",isCorrect:!1}],explanation:"If a \"Too Many Connections\" error happens to a client connecting to a MySQL database, it means all available connections are in use by other clients. Opening a connection consumes resources on the database server. Since Lambda functions can scale to tens of thousands of concurrent connections, your database needs more resources to open and maintain connections instead of executing queries. The maximum number of connections a database can support is largely determined by the amount of memory allocated to it. Upgrading to a database instance with higher memory is a straightforward way of solving the problem. Another approach would be to maintain a connection pool that clients can reuse. This is where RDS Proxy comes in. RDS Proxy helps you manage a large number of connections from Lambda to an RDS database by establishing a warm connection pool to the database. Your Lambda functions interact with RDS Proxy instead of your database instance. It handles the connection pooling necessary for scaling many simultaneous connections created by concurrent Lambda functions. This allows your Lambda applications to reuse existing connections, rather than creating new connections for every function invocation. Thus, the correct answer is: Provision an RDS Proxy between the Lambda function and RDS database instance. The option that says: Increase the concurrency limit of the Lambda function is incorrect. The concurrency limit refers to the maximum requests AWS Lambda can handle simultaneously. Increasing the limit will allow for more requests to open a database connection, which could potentially worsen the problem. The option that says: Increase the value of the max_connections parameter of the Aurora MySQL DB Instance is incorrect. Although this may be a valid solution, it is not the most efficient since it simply increases the maximum number of connections that can be made to the database instance. Moreover, increasing the maximum number of connections alone, without considering the database size, may lead to other issues, such as slow response times, timeouts, and even crashes. The option that says: Increase the allocated memory of your function is incorrect. Increasing the Lambda function's memory can improve its performance, but it may not necessarily solve the underlying issue of the 'too many connections' error. This error is typically caused by a limit on the maximum number of connections the database can handle, so solutions that address the database's connection management, such as creating a connection pool using RDS Proxy, are more likely to be effective. References: https://aws.amazon.com/rds/proxy/ https://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/ Check out this Amazon RDS Cheat Sheet: https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/"},{question:"A DynamoDB table has several top-level attributes such as id, course_id, course_title, price, rating and many others. The database queries of your application returns all of the item attributes by default but you only want to fetch specific attributes such as the course_id and price per request. As the developer, how can you refactor your application to accomplish this requirement?",answers:[{text:"Use condition expressions",isCorrect:!1},{text:"Use filter expressions",isCorrect:!1},{text:"Use projection expression",isCorrect:!0},{text:"Use expression attribute names",isCorrect:!1}],explanation:'To read data from a table, you use operations such as GetItem, Query, or Scan. DynamoDB returns all of the item attributes by default. To get just some, rather than all of the attributes, use a projection expression. A projection expression is a string that identifies the attributes you want. To retrieve a single attribute, specify its name. For multiple attributes, the names must be comma-separated. The following AWS CLI example shows how to use a projection expression with a GetItemoperation. This projection expression retrieves a top-level scalar attribute (Description), the first element in a list (RelatedItems[0]), and a list nested within a map (ProductReviews.FiveStar). aws dynamodb get-item \\ --table-name ProductCatalog \\ --key \'{"Id":{"N":"1"}}\' \\ --projection-expression "Description, RelatedItems[0], ProductReviews.FiveStar" You can use any attribute name in a projection expression, provided that the first character is a-z or A-Z and the second character (if present) is a-z, A-Z, or 0-9. If an attribute name does not meet this requirement, you will need to define an expression attribute name as a placeholder. Therefore, using projection expression is the correct answer in this scenario. Using condition expressions is incorrect because this is primarily used to determine which items should be modified for data manipulation operations such as PutItem, UpdateItem, and DeleteItem calls. Using expression attribute names is incorrect because this is a placeholder that you use in a projection expression as an alternative to an actual attribute name. An expression attribute name must begin with a #, and be followed by one or more alphanumeric characters. Using filter expressions is incorrect because it simply determines which items (and not the attributes) within the Query results should be returned to you. All of the other results are discarded. Take note that the scenario says that you have to fetch specific attributes and not specific items. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.html'},{question:"A developer is building an e-commerce application which will be hosted in an ECS Cluster. To minimize the number of instances in use, she must select a strategy which will place tasks based on the least available amount of CPU or memory. Which of the following task placement strategy should the developer implement?",answers:[{text:"distinctInstance",isCorrect:!1},{text:"spread",isCorrect:!1},{text:"binpack",isCorrect:!0},{text:"random",isCorrect:!1}],explanation:"A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service. Amazon ECS supports the following task placement strategies: binpack - Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use. random - Place tasks randomly. spread - Place tasks evenly based on the specified value. Accepted values are attribute key-value pairs, instanceId, or host. The scenario states that the developer must select a task placement strategy which will place tasks based on the least available amount of CPU or memory. By using bin pack strategy with CPU as the field parameter, ECS is able to place tasks onto an instance with the least available amount of CPU first, before moving on to the other instances. Hence, the correct answer is to use the binpack task placement strategy. random is incorrect because this will place the tasks randomly, rather than placing the tasks to the instances based on the least available amount of CPU or memory. spread is incorrect because this will place tasks evenly to the instances based on a specified value. distinctInstance is incorrect because this is not a valid task placement strategy, but a task placement constraint. This is primarily used as a constraint to place each task on a different container instance. It can be specified when either running a task or creating a new service. References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html https://aws.amazon.com/blogs/compute/amazon-ecs-task-placement/ Check out this Amazon ECS Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/"},{question:"A mobile game has a serverless backend consisting of an API Gateway backed by Lambda functions and a DynamoDB table in provisioned capacity mode, where player data is stored. While the game has maintained a consistent level of traffic, recent growth in the player base has caused response times to slow down. To improve performance, the developer wants to reduce the number of database queries for data that rarely change. What approach can the developer take to achieve this goal cost-effectively and with less development overhead?",answers:[{text:"Switch the DynamoDB table’s capacity mode to On-demand.",isCorrect:!1},{text:"Set up an Amazon DynamoDB Accelerator (DAX) caching layer in front of the DynamoDB table.",isCorrect:!0},{text:"Create an Amazon MemoryDB for Redis database in front of the DynamoDB table to cache data.",isCorrect:!1},{text:"Use DynamoDB Session Handler to handle the saving and retrieval of player data.",isCorrect:!1}],explanation:"Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables without requiring developers to manage cache invalidation, data population, or cluster management. This will enable you to focus on building great applications for your customers without worrying about performance at scale. You do not need to modify application logic since DAX is compatible with existing DynamoDB API calls. You can enable DAX with just a few clicks in the AWS Management Console or using the AWS SDK. Just as with DynamoDB, you only pay for the capacity you provision. One of the key benefits of using DAX is that it works transparently with existing DynamoDB API calls, and the application code does not need to be modified to take advantage of the caching layer. This means that the developer can improve the performance of their DynamoDB applications without having to modify the code, making it easier and more cost-effective to improve application performance. Hence, the correct answer is: Set up an Amazon DynamoDB Accelerator (DAX) caching layer in front of the DynamoDB table. The option that says: Create an Amazon MemoryDB for Redis database in front of the DynamoDB table to cache data is incorrect. While this might work, it is not the most cost-effective approach because it involves setting up an entirely new database in addition to the existing DynamoDB table. Additionally, this method requires modifying the application code to use the Redis cache for data queries, which can be time-consuming. The option that says: Use DynamoDB Session Handler to handle the saving and retrieval of player data is incorrect because this is just a custom session handler for PHP and does not provide any caching functionality. It is not a suitable approach to reduce the number of database queries for data that rarely change. The option that says: Switch the DynamoDB table’s capacity mode to On-demand is incorrect. While it may help improve performance by automatically scaling the read capacity in response to traffic changes, it does not address the main objective of reducing the number of database queries for data that rarely change. References: https://aws.amazon.com/dynamodb/dax https://aws.amazon.com/caching/aws-caching/ Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"},{question:"A company has a microservices application that must be integrated with API Gateway. The developer must configure custom data mapping between the API Gateway and the microservices. In addition, the developer must specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. Which of the following integration types is the MOST suitable one to use in API Gateway to meet this requirement?",answers:[{text:"HTTP",isCorrect:!0},{text:"AWS",isCorrect:!1},{text:"HTTP_PROXY",isCorrect:!1},{text:"AWS_PROXY",isCorrect:!1}],explanation:"You can integrate an API method in your API Gateway with a custom HTTP endpoint of your application in two ways: - HTTP proxy integration - HTTP custom integration In your API Gateway console, you can define the type of HTTP integration of your resource by toggling the \"Proxy resource\" switch. With proxy integration, the setup is simple. You only need to set the HTTP method and the HTTP endpoint URI, according to the backend requirements, if you are not concerned with content encoding or caching. With custom integration, setup is more involved. In addition to the proxy integration setup steps, you need to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. API Gateway supports the following endpoint ports: 80, 443 and 1024-65535. Programmatically, you choose an integration type by setting the type property on the Integration resource. For the Lambda proxy integration, the value is AWS_PROXY. For the Lambda custom integration and all other AWS integrations, it is AWS. For the HTTP proxy integration and HTTP integration, the value is HTTP_PROXY and HTTP, respectively. For the mock integration, the type value is MOCK. Since the integration type that is being described in the scenario fits the definition of an HTTP custom integration, the correct answer in this scenario is to use the HTTP integration type. Hence, the correct answer is: HTTP. AWS is incorrect because this type is primarily used for Lambda custom integration. Since the scenario does not specify that the microservices are Lambda functions, the HTTP integration type is the most flexible and suitable for such a scenario. AWS_PROXY is incorrect because this type is primarily used for Lambda proxy integration. The scenario didn't mention that it uses a serverless application or Lambda. HTTP_PROXY is incorrect because this type is only used for HTTP proxy integration where you don't need to do data mapping for your request and response data. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/setup-http-integrations.html https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A developer is instructed to set up a new serverless architecture composed of AWS Lambda, API Gateway, and DynamoDB in a single stack. The new architecture should allow the developer to locally build, test, and debug serverless applications. Which of the following should the developer use to satisfy the above requirement?",answers:[{text:"AWS CloudFormation",isCorrect:!1},{text:"AWS Elastic Beanstalk",isCorrect:!1},{text:"AWS Serverless Application Model (AWS SAM)",isCorrect:!0},{text:"AWS Systems Manager",isCorrect:!1}],explanation:"The AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build serverless applications on AWS. It consists of the AWS SAM template specification that you use to define your serverless applications and the AWS SAM command line interface (AWS SAM CLI) that you use to build, test, and deploy your serverless applications. Because AWS SAM is an extension of AWS CloudFormation, you get the reliable deployment capabilities of AWS CloudFormation. You can define resources by using AWS CloudFormation in your AWS SAM template. Also, you can use the full suite of resources, intrinsic functions, and other template features that are available in AWS CloudFormation. You can use AWS SAM with a suite of AWS tools for building serverless applications. The AWS SAM CLI lets you locally build, test, and debug serverless applications that are defined by AWS SAM templates. The CLI provides a Lambda-like execution environment locally. It helps you catch issues upfront by providing parity with the actual Lambda execution environment. To step through and debug your code to understand what the code is doing, you can use AWS SAM with AWS toolkits like the AWS Toolkit for JetBrains, AWS Toolkit for PyCharm, AWS Toolkit for IntelliJ, and AWS Toolkit for Visual Studio Code. This tightens the feedback loop by making it possible for you to find and troubleshoot issues that you might run into in the cloud. Therefore, the most suitable service to use in this scenario is AWS Serverless Application Model (AWS SAM). AWS CloudFormation is incorrect. Although this service can certainly be used to deploy Lambda, API Gateway, DynamoDB, and other AWS resources of your serverless application, it doesn't have the capability to locally build, test, and debug your application like what AWS SAM has. In addition, AWS SAM is a more suitable service to use if you want to deploy and manage your serverless applications in AWS just as mentioned above. AWS Systems Manager is incorrect because this service is primarily used for managing resources in your AWS environment, not for building, testing, and debugging serverless applications. AWS Elastic Beanstalk is incorrect because this service is not suitable for deploying serverless applications. In addition, it doesn't have the capability to locally build, test, and debug your serverless applications as effectively as what AWS SAM can do. References: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html Check out this AWS Serverless Application Model Cheat Sheet: https://tutorialsdojo.com/aws-serverless-application-model-sam/ Comparison of AWS Services Cheat Sheets: https://tutorialsdojo.com/comparison-of-aws-services/"},{question:"A recruitment agency has a large collection of resumes stored in an Amazon S3 bucket. The agency wants to perform an analysis on these files, but for privacy compliance reasons, they need to ensure that certain personally identifiable information (PII) is redacted before being processed by their internal service. Which solution can meet the requirements in the most cost-effective way?",answers:[{text:"Use Amazon S3 Object Lambda to redact PII before it is returned to the application.",isCorrect:!0},{text:"Configure an Amazon S3 Access Point and set up an Amazon CloudFront distribution with a Lambda@Edge function to redact the PII as data is fetched from the S3 bucket.",isCorrect:!1},{text:"Implement a solution with AWS Glue to transform the data and redact PII before storing it in an S3 bucket.",isCorrect:!1},{text:"Use a Lambda function to create a redacted copy of each file in a separate S3 bucket. Then, set up an Amazon S3 Access Point to serve these files.",isCorrect:!1}],explanation:"S3 Object Lambda allows you to add your own code to S3 GET requests to modify and process data as it's being returned to an application. This feature is designed for use cases where data needs to be transformed on-the-fly without the need to store a transformed copy of the data. It's useful in scenarios like filtering rows, redacting confidential data, dynamically resizing images and other similar situations where data transformation or processing is required during data retrieval. In the scenario, when the internal service fetches a file from the S3 bucket, S3 Object Lambda will run a Lambda function to redact the PII from the data as it is being retrieved before it is returned to the application. This eliminates the need to create and store a separate, redacted copy of each resume, thereby saving on storage costs. Plus, since the redaction happens during data retrieval, there's no need to create a proxy for the internal service. This makes the solution efficient and cost-effective. Hence, the correct answer is: Use Amazon S3 Object Lambda to redact PII before it is returned to the application. The option that says: Configure an Amazon S3 Access Point and set up an Amazon CloudFront distribution with a Lambda@Edge function to redact the PII as data is fetched from the S3 bucket is incorrect. Lambda@Edge functions operate at the CDN edge locations, making them ideal for use cases that need low-latency responses to end users. However, in the scenario, the requirement is to redact PII before processing by an internal service, not necessarily to serve end users quickly. Moreover, Lambda@Edge is generally more expensive to run than a regular Lambda function. The option that says: Implement a solution with AWS Glue to transform the data and redact PII before storing it in the S3 bucket is incorrect. AWS Glue is an ETL service designed for complex transformations and analytics. With this approach, you'd create redacted copies of resumes, leading to increased storage costs. Also, using AWS Glue to merely redact PII might be overkill and less cost-effective compared to simpler solutions like S3 Object Lambda. The option that says: Use a Lambda function to create a redacted copy of each file in a separate S3 bucket. Then, set up an Amazon S3 Access Point to serve these files is incorrect. In this approach, storing redacted copies of data in S3 would increase storage costs. However, with S3 Object Lambda, on-the-fly redaction of PII becomes possible, eliminating the need for storing separate redacted copies of the data. References: https://aws.amazon.com/s3/features/object-lambda/ https://aws.amazon.com/blogs/aws/introducing-amazon-s3-object-lambda-use-your-code-to-process-data-as-it-is-being-retrieved-from-s3/ Check out this Amazon S3 Cheat Sheet: https://tutorialsdojo.com/amazon-s3/"},{question:"A developer has recently completed a new version of a serverless application that is ready to be deployed using AWS SAM. There is a requirement that the traffic should shift from the previous Lambda function to the new version in the shortest time possible, but you still don't want to shift traffic all-at-once immediately. Which deployment configuration is the MOST suitable one to use in this scenario?",answers:[{text:"CodeDeployDefault.HalfAtATime",isCorrect:!1},{text:"CodeDeployDefault.LambdaLinear10PercentEvery1Minute",isCorrect:!1},{text:"CodeDeployDefault.LambdaLinear10PercentEvery2Minutes",isCorrect:!1},{text:"CodeDeployDefault.LambdaCanary10Percent5Minutes",isCorrect:!0}],explanation:"If you use AWS SAM to create your serverless application, it comes built-in with CodeDeploy to help ensure safe Lambda deployments. There are various deployment preference types that you can choose from. For example: If you choose Canary10Percent10Minutes then 10 percent of your customer traffic is immediately shifted to your new version. After 10 minutes, all traffic is shifted to the new version. However, if your pre-hook/post-hook tests fail, or if a CloudWatch alarm is triggered, CodeDeploy rolls back your deployment. The following table outlines other traffic-shifting options that are available: - Canary: Traffic is shifted in two increments. You can choose from predefined canary options. The options specify the percentage of traffic that's shifted to your updated Lambda function version in the first increment, and the interval, in minutes, before the remaining traffic is shifted in the second increment. - Linear: Traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic that's shifted in each increment and the number of minutes between each increment. - All-at-once: All traffic is shifted from the original Lambda function to the updated Lambda function version at once. Hence, the CodeDeployDefault.LambdaCanary10Percent5Minutes option is correct because 10 percent of your customer traffic is immediately shifted to your new version. After 5 minutes, all traffic is shifted to the new version. This means that the entire deployment time will only take 5 minutes CodeDeployDefault.HalfAtATime is incorrect because this is only applicable for EC2/On-premises compute platform and not for Lambda. CodeDeployDefault.LambdaLinear10PercentEvery1Minute is incorrect because it will add 10 percent of the traffic linearly to the new version every minute. Hence, all traffic will be shifted to the new version only after 10 minutes CodeDeployDefault.LambdaLinear10PercentEvery2Minutes is incorrect because it will add 10 percent of the traffic linearly to the new version every 2 minutes. Hence, all traffic will be shifted to the new version only after 20 minutes. References: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html"},{question:"A developer is creating an analytics REST API service that is powered by API Gateway. Analysts from a separate AWS account must interact with the service through an IAM role. The IAM role already has a policy that grants permission to invoke the API. What else should the developer do to meet the requirement without too much overhead?",answers:[{text:"Create an API Key for the API. Attach a resource policy to the API that grants permission to the specified IAM role to invoke the GetAPIKeys action.",isCorrect:!1},{text:"Create a Cognito User Pool authorizer. Add the IAM role to the user pool. Authenticate the requester’s identity using Cognito. Ask the analysts to pass the token returned by Cognito in their request headers.",isCorrect:!1},{text:"Set AWS_IAM as the method authorization type for the API. Attach a resource policy to the API that grants permission to the specified IAM role to invoke the execute-api:Invoke action.",isCorrect:!0},{text:"Create a Lambda function authorizer for the API. In the Lambda function, write a logic that verifies the requester's identity by extracting the information from the context object.",isCorrect:!1}],explanation:'By using AWS_IAM as the method authorization type, it ensures that the API can only be accessed by IAM identities such as IAM users or IAM roles. Attaching a resource policy to the API that grants permission to the specified IAM role to invoke the execute-api:Invoke action allows the specified IAM role to make authorized requests to the API while denying access to any other unauthorized users or roles. {"Version": "2012-10-17","Statement": [{"Effect": "Allow","Principal": {"AWS": ["arn:aws:iam::account-id:role/Analyst"]},"Action": "execute-api:Invoke","Resource": ["execute-api:/stage/GET/reports"]}]} This combination of method authorization and resource policy provides an additional layer of security for the API. Hence, the correct answer in this scenario is to Set AWS_IAM as the method authorization type for the API. Attach a resource policy to the API that grants permission to the specified IAM role to invoke the execute-api:Invoke action. The option that says: Create an API Key for the API. Attach a resource policy to the API that grants permission to the specified IAM role to invoke the GetAPIKeys action is incorrect API Keys are just a way of identifying the calling parties that you trust, but they are not intended to be used to grant permissions to an IAM role. The option that says: Create a Lambda function authorizer for the API. In the Lambda function, write a logic that verifies the requester\'s identity by extracting the information from the context object is incorrect. While this may be possible, Lambda function authorizer is more suitable for custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML. Additionally, this approach requires you to write, test, and maintain custom authentication and authorization code, which can be complex and time-consuming. The option that says: Create a Cognito User Pool authorizer. Add the IAM role to the user pool. Authenticate the requester’s identity using Cognito. Ask the analysts to pass the token returned by Cognito in their request headers is incorrect. Adding a Cognito User Pool authorizer is unnecessary since the API will be accessed through an IAM role. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies-examples.html#apigateway-resource-policies-cross-account-example https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-authorization-flow.html Check out this AWS API Gateway Sheet: https://tutorialsdojo.com/amazon-api-gateway/'},{question:"A developer configured an Amazon API Gateway proxy integration named MyAPI to work with a Lambda function. However, when the API is being called, the developer receives a 502 Bad Gateway error. She tried invoking the underlying function, but it properly returned the result in XML format. What is the MOST likely root cause of this issue?",answers:[{text:"The API name of the Amazon API Gateway proxy is invalid.",isCorrect:!1},{text:"There is an incompatible output returned from a Lambda proxy integration backend.",isCorrect:!0},{text:"There has been an occasional out-of-order invocation due to heavy loads.",isCorrect:!1},{text:"The endpoint request timed-out.",isCorrect:!1}],explanation:'Amazon API Gateway Lambda proxy integration is a simple, powerful, and nimble mechanism to build an API with a setup of a single API method. The Lambda proxy integration allows the client to call a single Lambda function in the backend. The function accesses many resources or features of other AWS services, including calling other Lambda functions In Lambda proxy integration, when a client submits an API request, API Gateway passes the raw request as-is to the integrated Lambda function, except that the order of the request parameters is not preserved. This request data includes the request headers, query string parameters, URL path variables, payload, and API configuration data. The configuration data can include current deployment stage name, stage variables, user identity, or authorization context (if any). The backend Lambda function parses the incoming request data to determine the response that it returns. For API Gateway to pass the Lambda output as the API response to the client, the Lambda function must return the result in the following JSON format: { "isBase64Encoded": true|false, "statusCode": httpStatusCode, "headers": { "headerName": "headerValue", ... }, "body": "..."} Since the Lambda function returns the result in XML format, it will cause the 502 errors in the API Gateway. Hence, the correct answer is that there is an incompatible output returned from a Lambda proxy integration backend. The option that says: The API name of the Amazon API Gateway proxy is invalid is incorrect because there is nothing wrong with its MyAPI name. The option that says: There has been an occasional out-of-order invocation due to heavy loads is incorrect. Although this is a valid cause of a 502 error, the issue is most likely caused by the Lambda function\'s XML response instead of JSON. The option that says: The endpoint request timed-out is incorrect because this will likely result in 504 errors and not 502\'s. References: https://aws.amazon.com/premiumsupport/knowledge-center/malformed-502-api-gateway/ https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-simple-proxy-for-lambda-output-format https://docs.aws.amazon.com/apigateway/api-reference/handling-errors/ Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/'},{question:"To improve their information security management system (ISMS), a company recently released a new policy which requires all database credentials to be encrypted and be automatically rotated to avoid unauthorized access. Which of the following is the MOST appropriate solution to secure the credentials?",answers:[{text:"Create a secret in AWS Secrets Manager and enable automatic rotation of the database credentials.",isCorrect:!0},{text:"Enable IAM DB authentication which rotates the credentials by default.",isCorrect:!1},{text:"Create a parameter to the Systems Manager Parameter Store using the PutParameter API with a type of SecureString.",isCorrect:!1},{text:"Create an IAM Role which has full access to the database. Attach the role to the services which require access.",isCorrect:!1}],explanation:"AWS Secrets Manager is an AWS service that makes it easier for you to manage secrets. Secrets can be database credentials, passwords, third-party API keys, and even arbitrary text. You can store and control access to these secrets centrally by using the Secrets Manager console, the Secrets Manager command line interface (CLI), or the Secrets Manager API and SDKs. In the past, when you created a custom application that retrieves information from a database, you typically had to embed the credentials (the secret) for accessing the database directly in the application. When it came time to rotate the credentials, you had to do much more than just create new credentials. You had to invest time to update the application to use the new credentials. Then you had to distribute the updated application. If you had multiple applications that shared credentials and you missed updating one of them, the application would break. Because of this risk, many customers have chosen not to regularly rotate their credentials, which effectively substitutes one risk for another. Secrets Manager enables you to replace hardcoded credentials in your code (including passwords), with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure that the secret can't be compromised by someone examining your code, because the secret simply isn't there. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a schedule that you specify. This enables you to replace long-term secrets with short-term ones, which helps to significantly reduce the risk of compromise. Hence, creating a secret in AWS Secrets Manager and enabling automatic rotation of the database credentials is the most appropriate solution for this scenario. The option that says: Create a parameter to the Systems Manager Parameter Store using the PutParameter API with a type of SecureString is incorrect because, by default, Systems Manager Parameter Store doesn't rotate its parameters. The option that says: Enable IAM DB authentication which rotates the credentials by default is incorrect because this solution only enables the service to connect to Amazon RDS with IAM credentials. It doesn't have the capability to rotate the credentials like what AWS Secrets Manager does to its secrets. The option that says: Create an IAM Role which has full access to the database. Attach the role to the services which requires access is incorrect because although IAM Role is a preferred way to grant access to certain services, this solution doesn't rotate the keys/credentials. References: https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html https://aws.amazon.com/blogs/compute/sharing-secrets-with-aws-lambda-using-aws-systems-manager-parameter-store/ Check out this AWS Secrets Manager Cheat Sheet: https://tutorialsdojo.com/aws-secrets-manager/"},{question:"You developed a shell script which uses AWS CLI to create a new Lambda function. However, you received an InvalidParameterValueException after running the script. What is the MOST likely cause of this issue?",answers:[{text:"You have exceeded your maximum total code size per account.",isCorrect:!1},{text:"You provided an IAM role in the CreateFunction API which AWS Lambda is unable to assume.",isCorrect:!0},{text:"The resource already exists.",isCorrect:!1},{text:"The AWS Lambda service encountered an internal error.",isCorrect:!1}],explanation:"To create a Lambda function, you need a deployment package and an execution role. The deployment package contains your function code. The execution role grants the function permission to use AWS services, such as Amazon CloudWatch Logs for log streaming and AWS X-Ray for request tracing. You can use the CreateFunction API via the AWS CLI or the AWS SDK of your choice. A function has an unpublished version, and can have published versions and aliases. The unpublished version changes when you update your function's code and configuration. A published version is a snapshot of your function code and configuration that can't be changed. An alias is a named resource that maps to a version, and can be changed to map to a different version. The InvalidParameterValueException will be returned if one of the parameters in the request is invalid. For example, if you provided an IAM role in the CreateFunction API which AWS Lambda is unable to assume. Hence, this option is the most likely cause of the issue in this scenario. If you have exceeded your maximum total code size per account, the CodeStorageExceededException will be returned, which is why this option is incorrect. If the resource already exists, the ResourceConflictException will be returned and not InvalidParameterValueException. Therefore, this option is also incorrect. If the AWS Lambda service encountered an internal error, the ServiceException will be returned hence, this option is incorrect. References: https://docs.aws.amazon.com/lambda/latest/dg/API_CreateFunction.html https://docs.aws.amazon.com/cli/latest/reference/lambda/create-function.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"You want to update a Lambda function on your production environment and ensure that when you publish the updated version, you still have a quick way to roll back to the older version in case you encountered a problem. To prevent any sudden user interruptions, you want to gradually increase the traffic going to the new version. Which of the following implementation is the BEST option to use?",answers:[{text:"Use Route 53 weighted routing to two Lambda functions.",isCorrect:!1},{text:"Use ELB to route traffic to both Lambda functions.",isCorrect:!1},{text:"Use stage variables in your Lambda function.",isCorrect:!1},{text:"Use Traffic Shifting with Lambda Aliases.",isCorrect:!0}],explanation:"By default, an alias points to a single Lambda function version. When the alias is updated to point to a different function version, incoming request traffic in turn instantly points to the updated version. This exposes that alias to any potential instabilities introduced by the new version. To minimize this impact, you can implement the routing-config parameter of the Lambda alias that allows you to point to two different versions of the Lambda function and dictate what percentage of incoming traffic is sent to each version. For example, you can specify that only 2 percent of incoming traffic is routed to the new version while you analyze its readiness for a production environment, while the remaining 98 percent is routed to the original version. As the new version matures, you can gradually update the ratio as necessary until you have determined that the new version is stable. You can then update the alias to route all traffic to the new version. You can point an alias to a maximum of two Lambda function versions. In addition: - Both versions must have the same IAM execution role. - Both versions must have the same AWS Lambda Function Dead Letter Queues configuration, or no DLQ configuration. - When pointing an alias to more than one version, the alias cannot point to $LATEST. Hence, using Traffic Shifting for Lambda Aliases is the correct answer. Using Route 53 weighted routing to two Lambda functions is incorrect. Although you may configure 2 different endpoints for your Lambda versions and use Route 53 Weighted Routing, this is still not a manageable and convenient way of handling the failover of your serverless function. The best way is to use Lambda Aliases for the different versions of your function and do traffic shifting on these two versions. Using ELB to route traffic to both Lambda functions is incorrect because this is not the recommended way to gradually deploy the new version of your Lambda function. It is still best to use Lambda Aliases instead of an Application Load Balancer. Using stage variables in your Lambda function is incorrect because stage variables are primarily used in API Gateway and not in Lambda. Although this solution may work, you are still required to create an API Gateway and create a stage variable that will point to the new and old versions of the Lambda function. This entails extra configuration, compared with just doing traffic shifting in Lambda. References: https://docs.aws.amazon.com/lambda/latest/dg/aliases-intro.html https://docs.aws.amazon.com/lambda/latest/dg/lambda-traffic-shifting-using-aliases.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A developer is managing a distributed system that consists of an Application Load Balancer, an SQS queue, and an Auto Scaling group of EC2 instances. The system has been integrated with CloudFront to better serve clients worldwide. To enhance the security of the in-flight data, the developer was instructed to establish an end-to-end SSL connection between the origin and the end-users. Which TWO options will allow the developer to meet this requirement using CloudFront? (Select TWO.)",answers:[{text:"Configure your ALB to only allow traffic on port 443 using an SSL certificate from AWS Config.",isCorrect:!1},{text:"Set up an Origin Access Control (OAC) setting",isCorrect:!1},{text:"Configure the Viewer Protocol Policy to use HTTPS only",isCorrect:!0},{text:"Configure the Origin Protocol Policy to use HTTPS only",isCorrect:!0},{text:"Associate a Web ACL using AWS Web Application Firewall (WAF) with your CloudFront Distribution.",isCorrect:!1}],explanation:"For web distributions, you can configure CloudFront to require that viewers use HTTPS to request your objects, so connections are encrypted when CloudFront communicates with viewers. You can also configure CloudFront to use HTTPS to get objects from your origin, so connections are encrypted when CloudFront communicates with your origin. If you configure CloudFront to require HTTPS both to communicate with viewers and to communicate with your origin, here's what happens when CloudFront receives a request for an object. The process works basically the same way whether your origin is an Amazon S3 bucket or a custom origin such as an HTTP/S server: 1. A viewer submits an HTTPS request to CloudFront. There's some SSL/TLS negotiation here between the viewer and CloudFront. In the end, the viewer submits the request in an encrypted format. 2. If the object is in the CloudFront edge cache, CloudFront encrypts the response and returns it to the viewer, and the viewer decrypts it. 3. If the object is not in the CloudFront cache, CloudFront performs SSL/TLS negotiation with your origin and, when the negotiation is complete, forwards the request to your origin in an encrypted format. 4. Your origin decrypts the request, encrypts the requested object, and returns the object to CloudFront. 5. CloudFront decrypts the response, re-encrypts it, and forwards the object to the viewer. CloudFront also saves the object in the edge cache so that the object is available the next time it's requested. 6. The viewer decrypts the response. You can configure one or more cache behaviors in your CloudFront distribution to require HTTPS for communication between viewers and CloudFront. You also can configure one or more cache behaviors to allow both HTTP and HTTPS, so that CloudFront requires HTTPS for some objects but not for others. To implement this setup, you have to change the Origin Protocol Policy setting for the applicable origins in your distribution. If you're using the domain name that CloudFront assigned to your distribution, such as dtut0rial5d0j0.cloudfront.net, you change the Viewer Protocol Policy setting for one or more cache behaviors to require HTTPS communication. With this configuration, CloudFront provides the SSL/TLS certificate. Hence, the correct answers are: Configure the Origin Protocol Policy to use HTTPS only and Configure the Viewer Protocol Policy to use HTTPS only are correct answers in this scenario. The option that says: Configure your ALB to only allow traffic on port 443 using an SSL certificate from AWS Config is incorrect because you can't store a certificate in AWS Config. The option that says: Set up an Origin Access Control (OAC) setting is incorrect because this CloudFront feature only allows you to secure S3 origins by granting access to S3 buckets for designated CloudFront distributions. This method is applicable only to S3 origins and cannot be used to establish end-to-end SSL connections for other origins. The option that says: Associate a Web ACL using AWS Web Application Firewall (WAF) with your CloudFront Distribution is incorrect because AWS WAF is primarily used to protect your web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. This will not allow you to establish an SSL connection between your origin and your clients. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html#using-https-cloudfront-to-origin-certificate https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html Check out this Amazon CloudFront Cheat Sheet: https://tutorialsdojo.com/amazon-cloudfront/"},{question:"Your serverless AWS Lambda functions are integrated with Amazon API gateway using Lambda proxy integration. The API caching feature is enabled in the API Gateway with a TTL value of 300 seconds. A client would like to fetch the latest data from your endpoints every time a request is sent and invalidate the existing cache. What should the client do in order to get the latest data?",answers:[{text:"Override API caching by allowing the client to send requests to the endpoint directly.",isCorrect:!1},{text:"Have the client send a request with the Cache-Control: max-age=0 header.",isCorrect:!0},{text:"Have the client send a request with the Cached: false header.",isCorrect:!1},{text:"Modify cache TTL value to a shorter period.",isCorrect:!1}],explanation:"A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the Cache-Control: max-age=0 header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint. Modifying the TTL value for the cached data to a lower value is incorrect because there is still no guarantee that the client will submit a request after the cache has expired. Also, you will not be fully utilizing the purpose of API caching since new data will be fetched from the endpoint more often. The best solution for this scenario is to use the Cache-Control header instead. Allowing the client to access the endpoint directly is incorrect because the purpose of placing API Gateway in-front of your endpoints is to not expose your endpoints to the public and risk security issues. It also provides you the additional benefits of not burdening your endpoints with a massive number of requests and allowing developer-friendly data exchanges through APIs. Having the client send a request with the Cached: false header is incorrect because this is a custom header. The correct way is to configure the Cache-Control: max-age=0 header instead. Reference: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#override-api-gateway-stage-cache-for-method-cache Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A developer is moving a legacy web application from their on-premises data center to AWS. The application is used simultaneously by thousands of users, and their session states are stored in memory. The on-premises server usually reaches 100% CPU Utilization every time there is a surge in the number of people accessing the application. Which of the following is the best way to re-factor the performance and availability of the application's session management once it is migrated to AWS?",answers:[{text:"Use an ElastiCache for Redis cluster to store the user session state of the application.",isCorrect:!0},{text:"Store the user session state of the application using CloudFront.",isCorrect:!1},{text:"Use Sticky Sessions with Local Session Caching.",isCorrect:!1},{text:"Use an ElastiCache for Memcached cluster to store the user session state of the application.",isCorrect:!1}],explanation:"Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Built on open-source Redis and compatible with the Redis APIs, ElastiCache for Redis works with your Redis clients and uses the open Redis data format to store your data. Your self-managed Redis applications can work seamlessly with ElastiCache for Redis without any code changes. ElastiCache for Redis combines the speed, simplicity, and versatility of open-source Redis with manageability, security, and scalability from Amazon to power the most demanding real-time applications in Gaming, Ad-Tech, E-Commerce, Healthcare, Financial Services, and IoT. In order to address scalability and provide a shared data storage for sessions that can be accessible from any individual web server, you can abstract the HTTP sessions from the web servers themselves. A common solution to for this is to leverage an In-Memory Key/Value store such as Redis and Memcached. While Key/Value data stores are known to be extremely fast and provide sub-millisecond latency, the added network latency and added cost are the drawbacks. An added benefit of leveraging Key/Value stores is that they can also be utilized to cache any data, not just HTTP sessions, which can help boost the overall performance of your applications. With Redis, you can keep your data on disk with a point in time snapshot which can be used for archiving or recovery. Redis also lets you create multiple replicas of a Redis primary. This allows you to scale database reads and to have highly available clusters. Hence, the correct answer for this scenario is to use an ElastiCache for Redis cluster to store the user session state of the application. The option that says: Store the user session state of the application using CloudFront is incorrect because CloudFront is not suitable for storing user session data. It is primarily used as a content delivery network. The option that says: Use an ElastiCache for Memcached cluster to store the user session state of the application is incorrect. Although using ElastiCache is a viable answer, Memcached is not as highly available as Redis. The option that says: Use Sticky Sessions with Local Session Caching is incorrect. Although this is also a viable solution, it doesn't offer durability and high availability compared to a distributed session management solution. The best solution for this scenario is to use an ElastiCache for Redis cluster. References: https://aws.amazon.com/caching/session-management https://aws.amazon.com/elasticache/redis-vs-memcached/ https://aws.amazon.com/elasticache/redis/ Check out this Amazon Elasticache Cheat Sheet: https://tutorialsdojo.com/amazon-elasticache/"},{question:"A Software Engineer is developing an application that will be hosted on an Amazon EC2 instance and read messages from a standard Amazon SQS queue. The average time that it takes for the producers to send a new message to the queue is 10 seconds. Which of the following is the MOST efficient way for the application to query the new messages from the queue?",answers:[{text:"Configure the SQS queue to use Short Polling.",isCorrect:!1},{text:"Configure each message in the SQS queue to have a custom visibility timeout of 10 seconds.",isCorrect:!1},{text:"Configure an SQS Delay Queue with a value of 10 seconds.",isCorrect:!1},{text:"Configure the SQS queue to use Long Polling.",isCorrect:!0}],explanation:'Amazon SQS long polling is a way to retrieve messages from your Amazon SQS queues. While the regular short polling returns immediately, even if the message queue being polled is empty, long polling doesn’t return a response until a message arrives in the message queue or the long poll times out. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren\'t included in a response). This type of polling is suitable if the new messages that are being added to the SQS queue arrive less frequently. You can configure long polling to your SQS queue by simply setting the "Receive Message Wait Time" field to a value greater than 0. Hence, the correct answer is: Configure the SQS queue to use Long Polling. The option that says: Configure each message in the SQS queue to have a custom visibility timeout of 10 seconds is incorrect because a visibility timeout is typically used to prevent other consumers from processing the message again for a period of time. This is normally used if your application takes a long time to process and delete a message from the SQS queue. The option that says: Configure the SQS queue to use Short Polling is incorrect because it is inefficient to poll the queue every second if the average time that it takes for the producers to send a new message to the queue is 40 seconds. It is better to do Long Polling, which will query the queue every 15 or 20 seconds, considering that new messages are not being added every second. The option that says: Configure an SQS Delay Queue with a value of 10 seconds is incorrect because this is primarily configured if you want to postpone the delivery of new messages to the SQS queue for a number of seconds. Having this SQS configuration which sets the new messages to remain invisible to the consumers for a duration of the delay period, is not helpful in the given scenario. It is still better to use Long Polling instead of setting up a delay queue. References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html https://aws.amazon.com/sqs/faqs/ Check out this Amazon SQS Cheat Sheet: https://tutorialsdojo.com/amazon-sqs/ Comparison of AWS Services Cheat Sheets: https://tutorialsdojo.com/comparison-of-aws-services/'},{question:"A financial services company is developing a real-time fraud detection system for its payment processing application. Payment transaction events are sent via an API, and the system must analyze each transaction in real-time to identify and flag potentially fraudulent activities. The solution should support concurrent processing by multiple fraud detection models and monitoring systems while ensuring scalability, low-latency performance, and cost optimization. Which AWS service is best suited to meet these requirements?",answers:[{text:"Amazon Data Firehose",isCorrect:!1},{text:"Amazon Kinesis Agent",isCorrect:!1},{text:"Amazon Kinesis Data Streams",isCorrect:!0},{text:"Amazon Managed Streaming for Apache Kafka (Amazon MSK)",isCorrect:!1}],explanation:"Amazon Kinesis Data Streams is a fully managed, scalable service designed to handle large amounts of real-time data streaming. It enables the collection, processing, and analysis of real-time data at a massive scale, allowing applications to react to new information almost immediately. Data streams can be ingested from various sources, such as IoT devices, application logs, or payment transactions, making it ideal for use cases like fraud detection, real-time analytics, and monitoring. Kinesis Data Streams allows users to process data with multiple consumers simultaneously, supporting the parallel analysis by various fraud detection models and enabling real-time decision-making. Its low-latency capabilities ensure that the data is quickly available for processing, making it a great fit for applications that require instant responses, such as fraud prevention in payment systems. With its ability to scale dynamically based on the volume of incoming data, Kinesis Data Streams ensures cost optimization by allowing users to adjust their resources according to demand. The service automatically handles partitioning and replication of data, ensuring high availability and fault tolerance. Additionally, it integrates seamlessly with other AWS services like AWS Lambda for real-time processing, Amazon Redshift for data storage and analytics, and Amazon S3 for long-term data storage. Hence, the correct answer is: Amazon Kinesis Data Streams. The option that says: Amazon Data Firehose is incorrect because it is primarily used for loading streaming data into other AWS services (e.g., Amazon S3, Amazon Redshift, Amazon Elasticsearch) for storage or analysis. While it can stream data, it does not provide the real-time processing and concurrent model support that Kinesis Data Streams offers. Data Firehose is optimized for delivering data to storage destinations. It lacks fine-grained control over stream processing, which is necessary for fraud detection, where real-time analysis and immediate action are crucial. The option that says: Amazon Managed Streaming for Apache Kafka (Amazon MSK) is incorrect. While Apache Kafka is a powerful distributed streaming platform, it typically requires more operational overhead compared to Kinesis Data Streams. It’s more complex to set up, manage, and scale. For a real-time fraud detection system, simplicity and low-latency processing are essential, and Kinesis Data Streams provides a more streamlined solution with easier integration into the AWS environment. The option that says: Amazon Kinesis Agent is incorrect because it is a lightweight software that helps send log and event data from on-premises servers to Amazon Kinesis Data Streams or Data Firehose. However, it’s not a service for data streaming itself. It is only used for pushing data to a stream or Firehose from local sources but does not provide the real-time stream processing capabilities that Kinesis Data Streams does. References: https://docs.aws.amazon.com/streams/latest/dev/introduction.html https://aws.amazon.com/kinesis/data-streams/faqs/?nc=sn&loc=6 Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/"},{question:"A website hosted in AWS has a custom CloudWatch metric to track all HTTP server errors in the site every minute, which occurs intermittently. An existing CloudWatch Alarm has already been configured for this metric but you would like to re-configure this to properly monitor the application. The alarm should only be triggered when all three data points in the most recent three consecutive periods are above the threshold. Which of the following options is the MOST appropriate way to monitor the website based on the given threshold?",answers:[{text:"Set both the Evaluation Period and Datapoints to Alarm to 3.",isCorrect:!0},{text:"Use metric math in CloudWatch to properly compute the threshold.",isCorrect:!1},{text:"Use high-resolution metrics.",isCorrect:!1},{text:"Set both the Period and Datapoints to Alarm to 3.",isCorrect:!1}],explanation:"When you create an alarm, you specify three settings to enable CloudWatch to evaluate when to change the alarm state: - Period is the length of time to evaluate the metric or expression to create each individual data point for an alarm. It is expressed in seconds. If you choose one minute as the period, there is one datapoint every minute. - Evaluation Period is the number of the most recent periods, or data points, to evaluate when determining alarm state. - Datapoints to Alarm is the number of data points within the evaluation period that must be breaching to cause the alarm to go to the ALARM state. The breaching data points do not have to be consecutive, they just must all be within the last number of data points equal to Evaluation Period. In the following figure, the alarm threshold is set to three units. The alarm is configured to go to the ALARM state and both Evaluation Period and Datapoints to Alarm are 3. That is, when all three datapoints in the most recent three consecutive periods are above the threshold, the alarm goes to the ALARM state. In the figure, this happens in the third through fifth time periods. At period six, the value dips below the threshold, so one of the periods being evaluated is not breaching, and the alarm state changes to OK. During the ninth time period, the threshold is breached again, but for only one period. Consequently, the alarm state remains OK. Hence, the option that says: Set both the Evaluation Period and Datapoints to Alarm to 3 is the correct answer. The option that says: Use high-resolution metrics is incorrect because the scenario says that it only needs to monitor the HTTP server errors every minute, and not its sub-minute activity. If you set an alarm on a high-resolution metric, you can specify a high-resolution alarm with a period of 10 seconds or 30 seconds. Hence, this option is irrelevant in this scenario. The option that says: Set both the Period and Datapoints to Alarm to 3 is incorrect because you should set the Evaluation Period and not the Period setting. The option that says: Use metric math in CloudWatch to properly compute the threshold is incorrect because the Metric Math feature is only applicable for scenarios where you need to query multiple CloudWatch metrics or if you want to use math expressions to create new time series based on selected metrics. References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ConsoleAlarms.html Check out this Amazon CloudWatch Cheat Sheet: https://tutorialsdojo.com/amazon-cloudwatch/"},{question:"An application hosted in an Auto Scaling group of On-Demand EC2 instances is used to process data polled from an SQS queue, and the generated output is stored in an S3 bucket. To enhance security, you were tasked to ensure that all objects in the S3 bucket are encrypted at rest using server-side encryption with AWS KMS keys. Which of the following is required to properly implement this requirement?",answers:[{text:"Add a bucket policy which denies any s3:PutObject action unless the request includes the x-amz-server-side-encryption-aws-kms-key-id header.",isCorrect:!0},{text:"Add a bucket policy which denies any s3:PostObject action unless the request includes the x-amz-server-side-encryption-aws-kms-key-id header.",isCorrect:!1},{text:"Add a bucket policy which denies any s3:PostObject action unless the request includes the x-amz-server-side-encryption header.",isCorrect:!1},{text:"Add a bucket policy which denies any s3:PutObject action unless the request includes the x-amz-server-side-encryption header.",isCorrect:!1}],explanation:"Server-side encryption is about protecting data at rest. AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. AWS KMS uses KMS keys to encrypt your Amazon S3 objects. You use AWS KMS via the AWS Management Console or AWS KMS APIs to centrally create encryption keys, define the policies that control how keys can be used, and audit key usage to prove they are being used correctly. You can use these keys to protect your data in Amazon S3 buckets. The first time you add an SSE-KMS–encrypted object to a bucket in a region, a default KMS key is created for you automatically. This key is used for SSE-KMS encryption unless you select a KMS key that you created separately using AWS Key Management Service. Creating your own KMS key gives you more flexibility, including the ability to create, rotate, disable, and define access controls, and to audit the encryption keys used to protect your data. Amazon S3 supports bucket policies that you can use if you require server-side encryption for all objects that are stored in your bucket. For example, you can set a bucket policy that denies permission to upload an object (s3:PutObject) to everyone if the request does not include the x-amz-server-side-encryption header requesting server-side encryption with SSE-KMS. When you upload an object, you can specify the KMS key using the x-amz-server-side-encryption-aws-kms-key-id header which you can use to require a specific KMS key for object encryption. If the header is not present in the request, Amazon S3 assumes the default KMS key. Regardless, the KMS key ID that Amazon S3 uses for object encryption must match the KMS key ID in the policy, otherwise Amazon S3 denies the request. Therefore, the correct answer is: Add a bucket policy which denies any s3:PutObject action unless the request includes the x-amz-server-side-encryption-aws-kms-key-id header. The option that says: Add a bucket policy which denies any s3:PutObject action unless the request includes the x-amz-server-side-encryption header is incorrect. While this policy ensures that objects are encrypted, it does not enforce the use of AWS KMS keys. The x-amz-server-side-encryption header can typically specify different encryption methods, including SSE-S3 (Amazon S3 managed keys) or SSE-KMS (AWS KMS keys). Since the requirement is to use AWS KMS keys specifically, this option is not sufficient. The option that says: Add a bucket policy which denies any s3:PostObject action unless the request includes the x-amz-server-side-encryption header is incorrect because the correct action for uploading objects is s3:PutObject, not the s3:PostObject action. Additionally, it does not specifically enforce the use of AWS KMS keys. The option that says: Adding a bucket policy which denies any s3:PostObject action unless the request includes the x-amz-server-side-encryption-aws-kms-key-id header is incorrect because it uses the s3:PostObject action instead of the correct s3:PutObject action. Even though it primarily uses the AWS KMS keys, the incorrect action makes this policy ineffective for ensuring encryption during object uploads. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html https://docs.aws.amazon.com/AmazonS3/latest/userguide/specifying-kms-encryption.html Check out this Amazon S3 Cheat Sheet: https://tutorialsdojo.com/amazon-s3/"},{question:"A developer uses AWS SAM templates to deploy a serverless application. He needs to embed the application from the AWS Serverless Application Repository or from an S3 bucket as a nested application. Which of the following resource type is the most SUITABLE one that the developer should use?",answers:[{text:"AWS::Serverless::Function",isCorrect:!1},{text:"AWS::Serverless::Application",isCorrect:!0},{text:"AWS::Serverless::LayerVersion",isCorrect:!1},{text:"AWS::Serverless::Api",isCorrect:!1}],explanation:"A serverless application can include one or more nested applications. You can deploy a nested application as a stand-alone artifact or as a component of a larger application. As serverless architectures grow, common patterns emerge in which the same components are defined in multiple application templates. You can now separate out common patterns as dedicated applications, and then nest them as part of new or existing application templates. With nested applications, you can stay more focused on the business logic that's unique to your application. To define a nested application in your serverless application, use the AWS::Serverless::Application resource type. AWS::Serverless::Function is incorrect because this resource type describes configuration information for creating a Lambda function. You can describe any event source that you want to attach to the Lambda function—such as Amazon S3, Amazon DynamoDB Streams, and Amazon Kinesis Data Streams. AWS::Serverless::LayerVersion is incorrect because this resource type creates a Lambda layer version (LayerVersion) that contains library or runtime code that's needed by a Lambda function. When a serverless layer version is transformed, AWS SAM also transforms the logical ID of the resource so that old layer versions aren't automatically deleted by AWS CloudFormation when the resource is updated. AWS::Serverless::Api is incorrect because this resource type describes an API Gateway resource. It's useful for advanced use cases where you want full control and flexibility when you configure your APIs. For most scenarios, it is recommended that you create APIs by specifying this resource type as an event source of your AWS::Serverless::Function resource. References: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-template.html#serverless-sam-template-application https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-template-nested-applications.html"},{question:"A developer is planning to deploy a high-performance online trading application which requires a database that can scale globally and can handle frequent schema changes. The database should also support flexible schemas that enable faster and more iterative development. Which is the MOST suitable database service that you should use to achieve this requirement?",answers:[{text:"Amazon DynamoDB",isCorrect:!0},{text:"Amazon Aurora",isCorrect:!1},{text:"Amazon Redshift",isCorrect:!1},{text:"Amazon RDS",isCorrect:!1}],explanation:"NoSQL databases are a great fit for many modern applications such as mobile, web, and gaming that require flexible, scalable, high-performance, and highly functional databases to provide great user experiences. Flexibility: NoSQL databases generally provide flexible schemas that enable faster and more iterative development. The flexible data model makes NoSQL databases ideal for semi-structured and unstructured data. Scalability: NoSQL databases are generally designed to scale out by using distributed clusters of hardware instead of scaling up by adding expensive and robust servers. Some cloud providers handle these operations behind-the-scenes as a fully managed service. High-performance: NoSQL databases are optimized for specific data models (such as document, key-value, and graph) and access patterns that enable higher performance than trying to accomplish similar functionality with relational databases. Highly functional: NoSQL databases provide highly functional APIs and data types that are purpose-built for each of their respective data models. A relational database is known for having a rigid schema, with a lot of constraints and limits as to which (and what type of) data can be inserted or not. It is primarily used for scenarios where you have to support complex queries which fetch data across a number of tables. It is best for scenarios where you have complex table relationships but for use cases where you need to have a flexible schema, this is not a suitable database to use. For NoSQL, it is not as rigid as a relational database because you can easily add or remove rows or elements in your table/collection entry. It also has a more flexible schema because it can store complex hierarchical data within a single item which, unlike a relational database, does not entail changing multiple related tables. Hence, the best answer to be used here is a NoSQL database, like DynamoDB. When your business requires a low-latency response to high-traffic queries, taking advantage of a NoSQL system generally makes technical and economic sense. Amazon DynamoDB helps solve the problems that limit the relational system scalability by avoiding them. In DynamoDB, you design your schema specifically to make the most common and important queries as fast and as inexpensive as possible. Your data structures are tailored to the specific requirements of your business use cases. Therefore, the correct answer is Amazon DynamoDB for this scenario. Amazon RDS and Amazon Aurora are both incorrect because both of them are a type of relational database that doesn't provide flexible schemas, unlike DynamoDB. Although it can scale globally, it would entail a lot of configuration and setup to do so. Moreover, a relational database has a rigid schema which is not suitable for frequent schema changes. Amazon Redshift is incorrect because this is not a NoSQL database and is primarily used for OLAP systems. References: https://aws.amazon.com/nosql/ https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-general-nosql-design.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb RDS vs DynamoDB: https://tutorialsdojo.com/amazon-rds-vs-dynamodb/ Comparison of AWS Services Cheat Sheets: https://tutorialsdojo.com/comparison-of-aws-services-for-udemy-students/"},{question:"An application, which already uses X-Ray, generates thousands of trace data every hour. The developer wants to use a filter expression that will limit the results based on custom attributes or keys that he specifies. How should the developer refactor the application in order to filter the results in the X-Ray console?",answers:[{text:"Include the custom attributes as new segment fields in the segment document.",isCorrect:!1},{text:"Create a new sampling rule based on the custom attributes.",isCorrect:!1},{text:"Add the custom attributes as annotations in your segment document.",isCorrect:!0},{text:"Add the custom attributes as metadata in your segment document.",isCorrect:!1}],explanation:"Even with sampling, a complex application generates a lot of data. The AWS X-Ray console provides an easy-to-navigate view of the service graph. It shows health and performance information that helps you identify issues and opportunities for optimization in your application. For advanced tracing, you can drill down to traces for individual requests or use filter expressions to find traces related to specific paths or users. When you instrument your application, the X-Ray SDK records information about incoming and outgoing requests, the AWS resources used, and the application itself. You can add other information to the segment document as annotations and metadata. Annotations are simple key-value pairs that are indexed for use with filter expressions. Use annotations to record data that you want to use to group traces in the console, or when calling the GetTraceSummaries API. X-Ray indexes up to 50 annotations per trace. Metadata are key-value pairs with values of any type, including objects and lists, but that are not indexed. Use metadata to record data you want to store in the trace but don't need to use for searching traces. You can view annotations and metadata in the segment or subsegment details in the X-Ray console. Hence, adding the custom attributes as annotations in your segment document is the correct answer. Including the custom attributes as new segment fields in the segment document is incorrect because a segment field can't be used as a filter expression. You have to add the custom attributes as annotations to the segment document that you'll send to X-Ray, just as mentioned above. Creating a new sampling rule based on the custom attributes is incorrect because sampling is primarily used to ensure efficient tracing and to provide a representative sample of the requests that your application serves. Adding the custom attributes as metadata in your segment document is incorrect because metadata is primarily used to record custom data that you want to store in the trace but not for searching traces. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-annotations https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/"},{question:"A developer is working on an application which stores data to an Amazon DynamoDB table with the DynamoDB Streams feature enabled. He set up an event source mapping with DynamoDB Streams and AWS Lambda function to monitor any table changes then store the original data of the overwritten item in S3. When an item is updated, it should only send a copy of the item's previous value to an S3 bucket and maintain the new value in the DynamoDB table. Which StreamViewType is the MOST suitable one to use in the DynamoDB configuration to fulfill this scenario?",answers:[{text:"OLD_IMAGE",isCorrect:!0},{text:"KEYS_ONLY",isCorrect:!1},{text:"NEW_IMAGE",isCorrect:!1},{text:"NEW_AND_OLD_IMAGES",isCorrect:!1}],explanation:"DynamoDB Streams provides a time-ordered sequence of item level changes in any DynamoDB table. The changes are de-duplicated and stored for 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real time. Amazon DynamoDB is also integrated with AWS Lambda so that you can create triggers—pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables. When an item in the table is modified, StreamViewType determines what information are written to the stream for this table. Valid values for StreamViewType are KEYS_ONLY, NEW_IMAGE, OLD_IMAGE, and NEW_AND_OLD_IMAGES. For the OLD_IMAGE type, the entire item which has the previous value as it appeared before it was modified is written to the stream. Hence, this is the correct answer in this scenario. KEYS_ONLY is incorrect because it will only write the key attributes of the modified item to the stream. This choice is wrong since the question states that values should be copied as well. NEW_IMAGE is incorrect because it will configure the stream to write the entire item with its new value as it appears after it was modified. This choice is wrong since the stream should capture the item's pre-modified values. NEW_AND_OLD_IMAGES is incorrect because although it writes the new values of the item in the stream, it also includes the old one as well. Since this type will send both the new and the old item images of the item to the stream, this option is wrong. Remember that it should only send a copy of the item's previous value to the S3 bucket, and not the new value in the DynamoDB table. The most suitable one to use here is the OLD_IMAGE type. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/Streams.Lambda.html https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_StreamSpecification.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ AWS Lambda Integration with Amazon DynamoDB Streams: https://tutorialsdojo.com/aws-lambda-integration-with-amazon-dynamodb-streams/ Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"},{question:"You are building a distributed system using KMS where you need to encrypt data at a later time. An API must be called that returns only the encrypted copy of the data key which you will use for encryption. After an hour, you will decrypt the data key by calling the Decrypt API then using the returned plaintext data key to finally encrypt the data. Which is the MOST suitable KMS API that the system should use to securely implement the requirements described above?",answers:[{text:"Encrypt",isCorrect:!1},{text:"GenerateDataKey",isCorrect:!1},{text:"GenerateDataKeyWithoutPlaintext",isCorrect:!0},{text:"GenerateRandom",isCorrect:!1}],explanation:"The GenerateDataKeyWithoutPlaintext API generates a unique data key. This operation returns a data key that is encrypted under a KMS Key that you specify. GenerateDataKeyWithoutPlaintext is identical to GenerateDataKey except that it returns only the encrypted copy of the data key. Like GenerateDataKey, GenerateDataKeyWithoutPlaintext returns a unique data key for each request. The bytes in the key are not related to the caller or KMS key that is used to encrypt the data key. This operation is useful for systems that need to encrypt data at some point, but not immediately. When you need to encrypt the data, you call the Decrypt operation on the encrypted copy of the key. It's also useful in distributed systems with different levels of trust. For example, you might store encrypted data in containers. One component of your system creates new containers and stores an encrypted data key with each container. Then, a different component puts the data into the containers. That component first decrypts the data key, uses the plaintext data key to encrypt data, puts the encrypted data into the container, and then destroys the plaintext data key. In this system, the component that creates the containers never sees the plaintext data key. Hence, the correct answer is: GenerateDataKeyWithoutPlaintext GenerateDataKey is incorrect because this operation also returns a plaintext copy of the data key along with the copy of the encrypted data key under a KMS key that you specified. Take note that the scenario explicitly mentioned that the API must return only the encrypted copy of the data key which will be used later for encryption. Although this API can be used in this scenario, it is not recommended since the actual encryption process of the data happens at a later time and not in real-time. Encrypt is incorrect because this just encrypts plaintext into ciphertext by using a KMS key. This is primarily used to move encrypted data from one AWS region to another. GenerateRandom is incorrect because this just returns a random byte string that is cryptographically secure. This is not relevant in this scenario, as you have to use the GenerateDataKeyWithoutPlaintext API to properly implement the requirement. References: https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKeyWithoutPlaintext.html https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#data-keys Check out this AWS KMS Cheat Sheet: https://tutorialsdojo.com/aws-key-management-service-aws-kms/"},{question:"An online stock trading platform is hosted in an Auto Scaling group of EC2 instances with an Application Load Balancer in front to distribute the incoming traffic evenly. The developer must capture information about the IP traffic going to and from network interfaces in your VPC to comply with financial regulatory requirements. Which of the following options should the developer do to meet the requirement?",answers:[{text:"Use AWS Inspector to capture information about the IP traffic going to and from the network interfaces of your EC2 instances.",isCorrect:!1},{text:"Install and run the AWS X-Ray daemon to your EC2 instances using an instance metadata script.",isCorrect:!1},{text:"Create a flow log in your VPC.",isCorrect:!0},{text:"Use CloudTrail logs to track all API calls and capture information about the IP traffic going to and from your VPC.",isCorrect:!1}],explanation:"VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs and Amazon S3. After you've created a flow log, you can retrieve and view its data in the chosen destination. Flow logs can help you with a number of tasks; for example, to troubleshoot why specific traffic is not reaching an instance, which in turn helps you diagnose overly restrictive security group rules. You can also use flow logs as a security tool to monitor the traffic that is reaching your instance. CloudWatch Logs charges apply when using flow logs, whether you send them to CloudWatch Logs or to Amazon S3. Hence, you should create a flow log in your VPC to capture information about the IP traffic going to and from network interfaces in your VPC. Using CloudTrail logs to track all API calls and capture information about the IP traffic going to and from your VPC is incorrect. Although you can indeed use CloudTrail to track the API call, it can't capture information about the IP traffic of your VPC. Installing and running the AWS X-Ray daemon to your EC2 instances using an instance metadata script is incorrect because you have to use a user data script and not a metadata. Alternatively, you can instrument your application which is running in an EC2 instance to capture the client's IP address. However, it is much easier to just enable VPC Flow Logs to meet the requirement. Using AWS Inspector to capture information about the IP traffic going to and from the network interfaces of your EC2 instances is incorrect because this service is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It doesn't have the ability to capture IP traffic of your VPC. References: https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ec2.html https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html Check out these Amazon VPC and AWS X-Ray Cheat Sheets: https://tutorialsdojo.com/amazon-vpc/ https://tutorialsdojo.com/aws-x-ray/"},{question:"A Lambda function has been integrated with DynamoDB Streams as its event source. There has been a new version of the function that needs to be deployed using CodeDeploy where the traffic must be shifted in two increments. It should shift 10 percent of the incoming traffic to the new version in the first increment and then the remaining 90 percent should be deployed five minutes later. Which of the following deployment configurations is the MOST suitable to satisfy this requirement?",answers:[{text:"Linear",isCorrect:!1},{text:"Canary",isCorrect:!0},{text:"Rolling with additional batch",isCorrect:!1},{text:"All-at-once",isCorrect:!1}],explanation:"CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can use CodeDeploy. CodeDeploy supports the following deployment configurations: -In-place (for EC2/On-premises) - the application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. -Canary (for Lambda/ECS) - traffic is shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function or ECS task set in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment. -Linear (for Lambda/ECS) - traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment. -All-at-once (for Lambda/ECS) - all traffic is shifted from the original Lambda function or ECS task set to the updated function or task set all at once. In a Canary deployment configuration, the traffic is shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment. Hence, this is the correct answer which will satisfy the requirement for the given scenario. Linear is incorrect because this will cause the traffic to be shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment. All-at-once is incorrect because with this deployment configuration, the traffic is shifted from the original Lambda function to the updated Lambda function version all at once. Rolling with additional batch is incorrect because this is only applicable in Elastic Beanstalk and not for Lambda. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/"},{question:"A serverless application consisting of Lambda functions integrated with API Gateway, and DynamoDB processes ad hoc requests that its users send. Due to the recent spike in incoming traffic, some of your customers are complaining that they are getting HTTP 504 errors from time to time. Which of the following is the MOST likely cause of this issue?",answers:[{text:"The usage plan quota has been exceeded for the Lambda function.",isCorrect:!1},{text:"Since the incoming requests are increasing, the API Gateway automatically enabled throttling which caused the HTTP 504 errors.",isCorrect:!1},{text:"An authorization failure occurred between API Gateway and the Lambda function.",isCorrect:!1},{text:"API Gateway request has timed out because the underlying Lambda function has been running for more than 29 seconds.",isCorrect:!0}],explanation:"A gateway response is identified by a response type defined by API Gateway. The response consists of an HTTP status code, a set of additional headers that are specified by parameter mappings, and a payload that is generated by a non-VTL (Apache Velocity Template Language) mapping template. You can set up a gateway response for a supported response type at the API level. Whenever API Gateway returns a response of the type, the header mappings and payload mapping templates defined in the gateway response are applied to return the mapped results to the API caller. The following are the Gateway response types which are associated with the HTTP 504 error in API Gateway: INTEGRATION_FAILURE - The gateway response for an integration failed error. If the response type is unspecified, this response defaults to the DEFAULT_5XX type. INTEGRATION_TIMEOUT - The gateway response for an integration timed out error. If the response type is unspecified, this response defaults to the DEFAULT_5XX type. For the integration timeout, the range is from 50 milliseconds to 29 seconds for all integration types, including Lambda, Lambda proxy, HTTP, HTTP proxy, and AWS integrations. In this scenario, there is an issue where the users are getting HTTP 504 errors in the serverless application. This means the Lambda function is working fine at times but there are instances when it throws an error. Based on this analysis, the most likely cause of the issue is the INTEGRATION_TIMEOUT error since you will only get an INTEGRATION_FAILURE error if your AWS Lambda integration does not work at all in the first place. Hence, the root cause of this issue is that the API Gateway request has timed out because the underlying Lambda function has been running for more than 29 seconds. The option that says: Since the incoming requests are increasing, the API Gateway automatically enabled throttling which caused the HTTP 504 errors is incorrect because a large number of incoming requests will most likely produce an HTTP 502 or 429 error but not a 504 error. If executing the function would cause you to exceed a concurrency limit at either the account level (ConcurrentInvocationLimitExceeded) or function level (ReservedFunctionConcurrentInvocationLimitExceeded), Lambda may return a TooManyRequestsException as a response. For functions with a long timeout, your client might be disconnected during synchronous invocation while it waits for a response and returns an HTTP 504 error. The option that says: An authorization failure occurred between API Gateway and the Lambda function is incorrect because an authentication issue usually produces HTTP 403 errors and not 504s. The gateway response for authorization failures for missing authentication token error, invalid AWS signature error, or Amazon Cognito authentication problems is HTTP 403, which is why this option is unlikely to be the cause of this issue. The option that says: The usage plan quota has been exceeded for the Lambda function is incorrect. Although this is a possible root cause for this scenario, this option has the least chance to produce HTTP 504 errors. The scenario says that the issue happens from time to time and not all the time which suggests that this happens intermittently. If the usage plan indeed exceeded the quota, then the 504 error should always show up and not just from time to time. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html https://aws.amazon.com/about-aws/whats-new/2017/11/customize-integration-timeouts-in-amazon-api-gateway/ https://docs.aws.amazon.com/apigateway/latest/developerguide/supported-gateway-response-types.html Check out this API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"The users of a social media website must be authenticated using social identity providers such as Twitter, Facebook, and Google. Users can login to the site which will allow them to upload their selfies, memes, and other media files in an S3 bucket. As an additional feature, you should also enable guest user access to certain sections of the website. Which of the following should you do to accomplish this task?",answers:[{text:"Create an Identity Pool in Amazon Cognito and enable access to unauthenticated identities.",isCorrect:!0},{text:"Integrate AWS IAM Identity Center with your website.",isCorrect:!1},{text:"Create a custom identity broker which integrates with the AWS Security Token Service and supports unauthenticated access.",isCorrect:!1},{text:"Create a User Pool in Amazon Cognito and enable access to unauthenticated identities.",isCorrect:!1}],explanation:"Amazon Cognito provides authentication, authorization, and user management for your web and mobile apps. Your users can sign in directly with a username and password or through a third party such as Facebook, Amazon, or Google. The two main components of Amazon Cognito are user pools and identity pools. User pools are user directories that provide sign-up and sign-in options for your app users. Identity pools enable you to grant your users access to other AWS services. You can use identity pools and user pools separately or together. Amazon Cognito identity pools (federated identities) support user authentication through Amazon Cognito user pools, federated identity providers—including Amazon, Facebook, Google, and SAML identity providers—as well as unauthenticated identities. This feature also supports Developer Authenticated Identities (Identity Pools), which lets you register and authenticate users via your own back-end authentication process. Hence, the correct answer is: Create an Identity Pool in Amazon Cognito and enabling access to unauthenticated identities The option that says: Create a User Pool in Amazon Cognito and enable unauthenticated identities is incorrect because you should have created an Identity Pool instead. Take note that a User Pool doesn't have the option to enable unauthenticated identities. Moreover, you won't be able to provide your users access to upload their media files to S3 using a User Pool. The option that says: Create a custom identity broker which integrates with the AWS Security Token Service and supports unauthenticated access is incorrect because this is not a suitable solution in this scenario. You only need to build a custom identity broker application if your identity store is not compatible with SAML 2.0, which is required for identity federation. The option that says: Integrate AWS IAM Identity Center is incorrect because this is only used to help you manage access and permissions to custom applications that support Security Assertion Markup Language (SAML) 2.0 and commonly used third-party software as a service (SaaS) applications. This is primarily used for existing corporate identities and not for social identity providers. References: https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/ https://docs.aws.amazon.com/cognito/latest/developerguide/getting-started-with-identity-pools.html Check out this Amazon Cognito Cheat Sheet: https://tutorialsdojo.com/amazon-cognito/"},{question:"A developer is building a photo-sharing application that automatically enhances images uploaded by users to Amazon S3. When a user uploads an image, its S3 path is sent to an image-processing application hosted on AWS Lambda. The Lambda function applies the selected filter to the image and stores it back to S3. If the upload is successful, the application will return a prompt telling the user that the request has been accepted. The entire processing typically takes an average of 5 minutes to complete, which causes the application to become unresponsive. Which of the following is the MOST suitable and cost-effective option which will prevent the application from being unresponsive?",answers:[{text:"Use a combination of Lambda and Step Functions to orchestrate service components and asynchronously process the requests.",isCorrect:!1},{text:"Configure the application to asynchronously process the requests and change the invocation type of the Lambda function to Event.",isCorrect:!0},{text:"Configure the application to asynchronously process the requests and use the default invocation type of the Lambda function.",isCorrect:!1},{text:"Use AWS Serverless Application Model (AWS SAM) to allow asynchronous requests to your Lambda function.",isCorrect:!1}],explanation:"AWS Lambda supports synchronous and asynchronous invocation of a Lambda function. You can control the invocation type only when you invoke a Lambda function (referred to as on-demand invocation). The following examples illustrate on-demand invocations: - Your custom application invokes a Lambda function. - You manually invoke a Lambda function (for example, using the AWS CLI) for testing purposes. In both cases, you invoke your Lambda function using the Invoke operation, and you can specify the invocation type as synchronous or asynchronous. When you use AWS services as a trigger, the invocation type is predetermined for each service. You have no control over the invocation type that these event sources use when they invoke your Lambda function. In the Invoke API, you have 3 options to choose from for the InvocationType: RequestResponse (default) - Invoke the function synchronously. Keep the connection open until the function returns a response or times out. The API response includes the function response and additional data. Event - Invoke the function asynchronously. Send events that fail multiple times to the function's dead-letter queue (if it's configured). The API response only includes a status code. DryRun - Validate parameter values and verify that the user or role has permission to invoke the function. By configuring the application to asynchronously process requests by changing the invocation type of the Lambda function to \"Event,\" the function can run in the background without blocking the main application. When the processing is complete, Lambda can store it back to S3 and trigger another event, such as a notification to the user that the image is ready. Hence, the correct answer is to configure the application to asynchronously process the requests and change the invocation type of the Lambda function to Event. Configuring the application to asynchronously process the requests and use the default invocation type of the Lambda function is incorrect because this will invoke your Lambda function synchronously. The default invocation type is RequestResponse which invokes the function synchronously and keeps the connection open until the function returns a response or times out. Using AWS Serverless Application Model (AWS SAM) to allow asynchronous requests to your Lambda function is incorrect because AWS SAM just is an open-source framework that you can use to build serverless applications on AWS. Using a combination of Lambda and Step Functions to orchestrate service components and asynchronously process the requests is incorrect because the AWS Step Functions service just lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. Although this can be a valid solution, it is not cost-effective since the application does not have a lot of components to orchestrate. Lambda functions can effectively meet the requirements in this scenario without using Step Functions by processing the requests asynchronously. References: https://docs.aws.amazon.com/lambda/latest/dg/invocation-options.html https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A reporting application is hosted in AWS Elastic Beanstalk and uses Amazon DynamoDB as its database. If a user requests data, the application scans the entire table and returns the requested data. The table is expected to grow due to the surge of new users and the increase in requests for reports in the coming weeks. Which of the following should be done as a preparation to improve the application's performance with minimal cost? (Select TWO.)",answers:[{text:"Reduce page size",isCorrect:!0},{text:"Use DynamoDB Accelerator (DAX)",isCorrect:!1},{text:"Increase the Write Compute Unit (WCU) of the table",isCorrect:!1},{text:"Use Query operations instead",isCorrect:!0},{text:"Set the ScanIndexForward parameter to control the order of query results.",isCorrect:!1}],explanation:'In general, Scan operations are less efficient than other operations in DynamoDB. A Scan operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set. If possible, you should avoid using a Scan operation on a large table or index with a filter that removes many results. Also, as a table or index grows, the Scan operation slows. The Scan operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation. For faster response times, design your tables and indexes so that your applications can use Query instead of Scan. For tables, you can also consider using the GetItem and BatchGetItem APIs. Alternatively, you can refactor your application to use Scan operations in a way that minimizes the impact on your request rate. Instead of using a large Scan operation, you can use the following techniques to minimize the impact of a scan on a table\'s provisioned throughput. Reduce page size - Because a Scan operation reads an entire page (by default, 1 MB), you can reduce the impact of the scan operation by setting a smaller page size. The Scan operation provides a Limit parameter that you can use to set the page size for your request. Each Query or Scan request that has a smaller page size uses fewer read operations and creates a "pause" between each request. For example, suppose that each item is 4 KB and you set the page size to 40 items. A Query request would then consume only 20 eventually consistent read operations or 40 strongly consistent read operations. A larger number of smaller Query or Scan operations would allow your other critical requests to succeed without throttling. Isolate scan operations - DynamoDB is designed for easy scalability. As a result, an application can create tables for distinct purposes, possibly even duplicating content across several tables. You want to perform scans on a table that is not taking "mission-critical" traffic. Some applications handle this load by rotating traffic hourly between two tables—one for critical traffic, and one for bookkeeping. Other applications can do this by performing every write on two tables: a "mission-critical" table, and a "shadow" table. Hence, the correct answers are: - Use Query operations instead - Reduce page size The option that says: Using DynamoDB Accelerator (DAX) is incorrect. Although this will improve the scalability and read performance of the application, it simply adds a significant cost in maintaining your application. Using Query operations and reducing the page size of your query are the more cost-effective solutions in this scenario. The option that says: Set the ScanIndexForward parameter to control the order of query results is incorrect. While useful for ordering results, it does not improve the efficiency or performance of the underlying operation. Changing the order of scan results does not address the fundamental issue of scanning the entire table. The option that says: Increasing the Write Compute Unit (WCU) of the table is incorrect because the reporting application is primarily used for reading data and not for writing. In addition, increasing the WCU will increase the cost. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html#bp-query-scan-spikes https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/'},{question:"A company has created a private S3 bucket named tdojo. The Developer IAM role must be granted read access to all objects within this bucket. However, the QA IAM role should be restricted to accessing only the objects stored under the qa folder. Which S3 bucket policy will effectively implement the principle of least privilege access while satisfying the given requirements?",answers:[{text:'{ "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Principal": { "AWS": [ "arn:aws:iam::123456789123:role/Developer" ] }, "Action": [ "s3:GetObject", "s3:PutObject" ], "Resource": "arn:aws:s3:::tdojo/*" }, { "Effect": "Allow", "Principal": { "AWS": [ "arn:aws:iam::123456789123:role/QA" ] }, "Action": [ "s3:GetObject" ], "Resource": "arn:aws:s3:::tdojo/qa/*" } ]}',isCorrect:!1},{text:'{ "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Principal": { "AWS": [ "arn:aws:iam::123456789123:role/Developer", "arn:aws:iam::123456789123:role/QA" ] }, "Action": [ "s3:GetObject" ], "Resource": [ "arn:aws:s3:::tdojo/*", "arn:aws:s3:::tdojo/qa/*" ] } ]}',isCorrect:!1},{text:'{ "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Principal": { "AWS": [ "arn:aws:iam::123456789123:role/Developer" ] }, "Action": [ "s3:GetObject" ], "Resource": "arn:aws:s3:::tdojo/*" }, { "Effect": "Allow", "Principal": { "AWS": [ "arn:aws:iam::123456789123:role/QA" ] }, "Action": [ "s3:GetObject" ], "Resource": "arn:aws:s3:::tdojo/qa/*" } ]}',isCorrect:!0},{text:'{ "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Principal": { "AWS": [ "arn:aws:iam::123456789123:role/Developer" ] }, "Action": [ "s3:*" ], "Resource": "arn:aws:s3:::tdojo/*" }, { "Effect": "Allow", "Principal": { "AWS": [ "arn:aws:iam::123456789123:role/QA" ] }, "Action": [ "s3:GetObject" ], "Resource": "arn:aws:s3:::tdojo/qa/*" } ]}',isCorrect:!1}],explanation:'An S3 bucket policy is a JSON document that defines access controls for an Amazon S3 bucket. It allows you to grant or deny access to different AWS identities (such as IAM users and roles or federated users) for different S3 bucket actions (such as GetObject, PutObject, and ListBucket) on specific S3 objects or prefixes within a bucket. It can be used to control who can access your S3 data and how they can access it. An S3 bucket policy statement is composed of several elements, and the following are required to create a valid policy: - Effect: The effect can be Allow or Deny. - Action: The specific API action for which you are granting or denying permission. - Principal: The user, account, service, or other entity that is allowed or denied access to the bucket or objects within the bucket. - Resource: The resource that\'s affected by the action. You specify a resource using an Amazon Resource Name (ARN). In the scenario, you can create separate statements for the Developer and QA roles. This allows for granular control over access to the bucket and its contents. Each statement can have its own set of conditions, allowing for different permissions to be granted to different identities or groups under different circumstances. Additionally, separating statements can make it easier to read, understand and manage the policy as it becomes more complex. Hence, the correct answer is: { "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Principal": { "AWS": [ "arn:aws:iam::123456789123:role/Developer" ] }, "Action": [ "s3:GetObject" ], "Resource": "arn:aws:s3:::tdojo/*" }, { "Effect": "Allow", "Principal": { "AWS": [ "arn:aws:iam::123456789123:role/QA" ] }, "Action": [ "s3:GetObject" ], "Resource": "arn:aws:s3:::tdojo/qa/*" } ]} The policy below is not compliant with the principle of least privilege, as it uses a wildcard in the Action element. The scenario mentioned that the Developer IAM role must be given permission to read all objects in the bucket. Therefore, you have to explicitly mention the specific API to allow that operation, which is GetObject. { "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Principal": { "AWS": [ "arn:aws:iam::123456789123:role/Developer" ] }, "Action": [ "s3:*" ], "Resource": "arn:aws:s3:::tdojo/*" }, { "Effect": "Allow", "Principal": { "AWS": [ "arn:aws:iam::123456789123:role/QA" ] }, "Action": [ "s3:GetObject" ], "Resource": "arn:aws:s3:::tdojo/qa/*" } ]} The policy below does not follow the principle of least privilege since it grants excessive permissions to the Developer IAM role. Note that the Developer IAM role only needs read access. { "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Principal": { "AWS": [ "arn:aws:iam::123456789123:role/Developer" ] }, "Action": [ "s3:GetObject", "s3:PutObject" ], "Resource": "arn:aws:s3:::tdojo/*" }, { "Effect": "Allow", "Principal": { "AWS": [ "arn:aws:iam::123456789123:role/QA" ] }, "Action": [ "s3:GetObject" ], "Resource": "arn:aws:s3:::tdojo/qa/*" } ]} The IAM Policy shown below is not right as it grants the QA role access to read all objects within the S3 bucket, which is typically beyond its necessary permissions: { "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Principal": { "AWS": [ "arn:aws:iam::123456789123:role/Developer", "arn:aws:iam::123456789123:role/QA" ] }, "Action": [ "s3:GetObject" ], "Resource": [ "arn:aws:s3:::tdojo/*", "arn:aws:s3:::tdojo/qa/*" ] } ]} References: https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-iam-policies.html https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html Check out this Amazon S3 Cheat Sheet: https://tutorialsdojo.com/amazon-s3/'},{question:"An application hosted in a multicontainer Docker platform in Elastic Beanstalk uses DynamoDB to handle the session data of its users. These data are only used in a particular timeframe and the stale data can be deleted after the user logged out of the system. Which of the following is the most suitable way to delete the session data?",answers:[{text:"Delete the stale data by regularly performing a scan on the table.",isCorrect:!1},{text:"Enable TTL for the session data in the DynamoDB table.",isCorrect:!0},{text:"Use conditional writes to add the session data to the DynamoDB table and then automatically delete it based on the condition you specify.",isCorrect:!1},{text:"Use atomic counters to track the validity of the session data and delete once it becomes stale.",isCorrect:!1}],explanation:"Time To Live (TTL) for DynamoDB allows you to define when items in a table expire so that they can be automatically deleted from the database. TTL is provided at no extra cost as a way to reduce storage usage and reduce the cost of storing irrelevant data without using provisioned throughput. With TTL enabled on a table, you can set a timestamp for deletion on a per-item basis, allowing you to limit storage usage to only those records that are relevant. TTL is useful if you have continuously accumulated data that lose relevance after a specific time period. For example session data, event logs, usage patterns, and other temporary data. If you have sensitive data that must be retained only for a certain amount of time according to contractual or regulatory obligations, TTL helps you ensure that it is removed promptly and as scheduled. Therefore, the correct answer in this scenario is to: Enable TTL for the session data in the DynamoDB table. The option that says: Delete the stale data by regularly performing a scan on the table is incorrect because the Scan operation uses eventually consistent reads when accessing the data in a table and therefore, the result set might not include the changes to data in the table immediately before the operation began. This is an inefficient option that can simply be replaced by using TTL. The option that says: Use atomic counters to track the validity of the session data and deleting it once becomes stale is incorrect because atomic counters are primarily used in updating data and for scenarios where you want the updates to not be idempotent. The option that says: Use conditional writes to add the session data to the DynamoDB table and then automatically deleting it based on the condition you specify is incorrect because conditional writes are only helpful in cases where multiple users attempt to modify the same item. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"},{question:"A company offers a Generative Artificial Intelligence (AI) service exposed through a REST API managed by Amazon API Gateway. They recently rolled out a subscription tier where users receive API keys to access premium features. The company uses the CreateApiKey API for generating these keys. During testing, developers noticed that while existing users can access the service without issues, new premium subscribers get a 403 Forbidden error when using their API keys. What must be done to give new users access to the service?",answers:[{text:"Instruct users to send their API key in a custom header. In the integration request, adjust the mapping template to extract and evaluate this header to distinguish between free-tier and premium subscribers.",isCorrect:!1},{text:"Associate the API keys for the premium users with the intended usage plan using the CreateUsagePlanKey operation.",isCorrect:!0},{text:"Use the ImportApiKeys operation to import the premium users' keys, then apply the UpdateUsagePlan operation to set the new tier access.",isCorrect:!1},{text:"Use the UpdateAuthorizer operation to modify the authorization settings. Promote the changes to the production stage by calling the CreateDeployment operation.",isCorrect:!1}],explanation:"In Amazon API Gateway, API keys by themselves do not grant access to execute an API. They need to be associated with a usage plan, and that usage plan then determines which API stages and methods the API key can access. If the API key is not associated with a usage plan, it will not have permission to access any of the resources, which will result in a \"403 Forbidden\" error. In the given scenario, existing users can access the service, but new premium subscribers cannot. This indicates that while the API keys were created for new users, they might not have been associated with the appropriate usage plan. Hence, after generating an API key, it must be added to a usage plan by calling the CreateUsagePlanKey method. Hence, the correct answer is: Associate the API keys for the premium users with the intended usage plan using the CreateUsagePlanKey operation. The option that says: Use the ImportApiKeys operation to import the premium users' keys, then apply the UpdateUsagePlan operation to set the new tier access is incorrect. The importApiKeys API is primarily used for bulk importing API keys, not for associating them with a usage plan. Although the updateUsagePlan API modifies properties of a usage plan; it doesn't handle direct association of API keys. The option that says: Use the UpdateAuthorizer operation to modify the authorization settings. Promote the changes to the production stage by calling the CreateDeployment operation is incorrect. The updateAuthorizer operation is only used to modify the settings of an existing custom authorizer, which handles custom authorization logic for APIs. In the scenario, the issue is not related to custom authorization but rather to the association of API keys with a usage plan. The option that says: Instruct users to send their API key in a custom header. In the integration request, adjust the mapping template to extract and evaluate this header to distinguish between free-tier and premium subscribers is incorrect. Changing the way users provide their API key adds unnecessary complexity and won't solve the issue at hand. The problem isn't with how the API key is being sent but with the API key not having appropriate permissions because it's not associated with a usage plan. References: https://docs.aws.amazon.com/apigateway/latest/api/API_UpdateUsagePlan.html https://docs.aws.amazon.com/apigateway/latest/api/API_CreateApiKey.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A mobile game is using a DynamoDB table named GameScore that keeps track of users and scores. Each item in the table is identified by a partition key (UserId) and a sort key (GameTitle). The diagram below shows how the items in the table are organized: A developer wants to write a leaderboard application to display the top scores for each game. How can the developer meet the requirement in the MOST efficient manner?",answers:[{text:"Create a global secondary index. Assign the GameTitle attribute as the partition key and the TopScore attribute as the sort key.",isCorrect:!0},{text:"Use the Scan operation and filter the results based on a GameTitle value.",isCorrect:!1},{text:"Create a local secondary index. Assign the TopScore attribute as the partition key and the GameTitle attribute as the sort key.",isCorrect:!1},{text:"Create a local secondary index. Assign the GameTitle attribute as the partition key and the TopScore attribute as the sort key.",isCorrect:!1}],explanation:"Amazon DynamoDB provides fast access to items in a table by specifying primary key values. However, many applications might benefit from having one or more secondary (or alternate) keys available, to allow efficient access to data with attributes other than the primary key. To address this, you can create one or more secondary indexes on a table, and issue Query or Scan requests against these indexes. A secondary index is a data structure that contains a subset of attributes from a table, along with an alternate key to support Query operations. You can retrieve data from the index using a Query, in much the same way as you use Query with a table. A table can have multiple secondary indexes, which gives your applications access to many different query patterns. DynamoDB supports two types of secondary indexes: Global secondary index — an index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered \"global\" because queries on the index can span all of the data in the base table, across all partitions. Local secondary index — an index that has the same partition key as the base table, but a different sort key. A local secondary index is \"local\" in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value. To speed up queries on non-key attributes, you can create a global secondary index. A global secondary index contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table. The index key does not need to have any of the key attributes from the table; it doesn't even need to have the same key schema as a table. In this scenario, you could create a global secondary index named GameTitleIndex, with a partition key of GameTitle and a sort key of TopScore. Since the base table's primary key attributes are always projected into an index, the UserId attribute is also present. The following diagram shows what GameTitleIndex index would look like: Hence, the correct answer in this scenario is to: Create a global secondary index. Assign the GameTitle attribute as the partition key and the TopScore attribute as the sort key. Create a local secondary index. Assign the TopScore attribute as the partition key and the GameTitle attribute as the sort key is incorrect. You can't add a local secondary index to an existing table. Moreover, even if it's possible, making a query that returns the top scores for each game is impossible with the TopScore attribute as the partition key. When you issue a query, you must also specify a partition key. In this case, if you run a query with a partition key value of 500, the results might return different games with a score of 500 from various users. It does not tell if it's the highest score in that game. Create a local secondary index. Assign the GameTitle attribute as the partition key and the TopScore attribute as the sort key is incorrect because you can't add this index to an already existing table. Additionally, a local secondary index has the same partition key as the base table, but has a different sort key. It is \"local\" in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value. Use the Scan operation and filter the results based on a GameTitle value is incorrect. Technically, this also works but it is less efficient and slower compared to querying on secondary indexes. The Scan operation reads every item in a table. As the table grows, the slower the Scan operation would become. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ DynamoDB Scan vs Query: https://tutorialsdojo.com/dynamodb-scan-vs-query/"},{question:"A company selling smart security cameras uses an S3 bucket behind a CloudFront web distribution to store its static content, which it shares with customers worldwide. The company has recently released a new firmware update intended only for its premium customers, and unauthorized access should be denied with a user authentication process that has minimal latency. How can a developer refactor the current setup to achieve this requirement with the MOST efficient solution?",answers:[{text:"Use Signed URLs and Signed Cookies in CloudFront to distribute the firmware update file.",isCorrect:!1},{text:"Use Lambda@Edge and Amazon Cognito to authenticate and authorize premium customers to download the firmware update.",isCorrect:!0},{text:"Use the AWS Serverless Application Model (AWS SAM) and Amazon Cognito to authenticate the premium customers.",isCorrect:!1},{text:"Restrict access to the S3 bucket only to premium customers using an Origin Access Control (OAC).",isCorrect:!1}],explanation:"Lambda@Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency. With Lambda@Edge, you don't have to provision or manage infrastructure in multiple locations around the world. You pay only for the compute time you consume - there is no charge when your code is not running. With Lambda@Edge, you can enrich your web applications by making them globally distributed and improving their performance — all with zero server administration. Lambda@Edge runs your code in response to events generated by the Amazon CloudFront content delivery network (CDN). Just upload your code to AWS Lambda, which takes care of everything required to run and scale your code with high availability at an AWS location closest to your end user. You can use Lambda@Edge to help authenticate and authorize users for the premium pay-wall content on your website, filtering out unauthorized requests before they reach your origin infrastructure. For example, you can trigger a Lambda function to authorize each viewer request by calling authentication and user management service such as Amazon Cognito. Hence, the correct answer is: Use Lambda@Edge and Amazon Cognito to authenticate and authorize premium customers to download the firmware update. The option that says: Use the AWS Serverless Application Model (AWS SAM) and Amazon Cognito to authenticate the premium customers is incorrect because AWS SAM is just an open-source framework that you can use to build serverless applications on AWS. In this scenario, you have to integrate your CloudFront web distribution with Lambda@Edge, and you can do this without using AWS SAM. The option that says: Restrict access to the S3 bucket only to premium customers by using an Origin Access Control (OAC) is incorrect because OAC is primarily used to prevent your users from viewing your S3 files by simply using the direct S3 URL. The option that says: Use Signed URLs and Signed Cookies in CloudFront to distribute the firmware update file is incorrect. Although this solution provides a way to authenticate the premium users for the private content, the process of authentication has a significant latency in comparison to the Lambda@Edge solution. In this option, you have to refactor your application (which is deployed to a specific AWS region) to either create and distribute signed URLs to authenticated users or to send Set-Cookie headers that set signed cookies on the viewers for authenticated users. This will cause the latency, which could have been improved if the authentication logic resides on CloudFront edge locations using Lambda@Edge. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-at-the-edge.html https://aws.amazon.com/lambda/edge/ Check out these Amazon CloudFront and AWS Lambda Cheat Sheets: https://tutorialsdojo.com/amazon-cloudfront/ https://tutorialsdojo.com/aws-lambda/"},{question:"A developer is working with an AWS Serverless Application Model (AWS SAM) application composed of several AWS Lambda functions. The developer runs the application locally on his laptop using sam local commands. While testing, one of the functions returns Access denied errors. Upon investigation, the developer discovered that the Lambda function is using the AWS SDK to make API calls within a sandbox AWS account. Which combination of steps must the developer do to resolve the issue? (Select TWO)",answers:[{text:"Create an AWS SAM CLI configuration file at the root of the SAM project folder. Add the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables to it.",isCorrect:!1},{text:"Add the AWS credentials of the sandbox AWS account to the Globals section of the template.yml file and reference them in the AWS::Serverless::Function properties section of the Lambda function.",isCorrect:!1},{text:"Use the aws configure command with the --profile parameter to add a named profile with the sandbox AWS account's credentials.",isCorrect:!0},{text:"Run the function using sam local invoke with the --profile parameter.",isCorrect:!0},{text:"Run the function using sam local invoke with the --parameter-overrides parameter.",isCorrect:!1}],explanation:"AWS Lambda functions have an associated execution role that provides permissions to interact with other AWS services. However, when you run AWS Lambda functions locally using the SAM CLI, you're simulating the execution environment of the Lambda function but not replicating the AWS execution context, including the IAM execution role. This means that the function won't automatically assume any IAM execution role and instead will rely on the credentials stored in ~/.aws/credentials file. When testing locally with AWS SAM, you can specify a named profile from your AWS CLI configuration using the --profile parameter with the sam local invoke command. This will instruct the SAM CLI to use the credentials from the specified profile when invoking the Lambda function. You can run the aws configure with the --profile option to set the credentials for a named profile. In the scenario, the developer must first set up the sandbox AWS account's credentials using aws configure --profile sandbox. This creates a named profile 'sandbox' (note that you can use any name for the profile). For local testing with the SAM CLI, the developer can then specify this profile using the command sam local invoke --profile sandbox. This ensures that the locally executed Lambda function utilizes the correct credentials to access resources in the sandbox AWS account. Hence, the correct answers are: - Use the aws configure command with the --profile parameter to add a named profile with the sandbox AWS account's credentials. - Run the function using sam local invoke with the --profile parameter. The option that says: Create an AWS SAM CLI configuration file at the root of the SAM project folder. Add the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables to it is incorrect. The SAM CLI relies on the AWS credentials stored in the /.aws/credentials file, which can be set through the aws configure command. While it's technically possible to place application credentials in a configuration file, SAM CLI doesn't support sourcing AWS credentials from it for authentication. The option that says: Add the AWS credentials of the sandbox AWS account to the Globals section of the template.yml file and reference them in the AWS::Serverless::Function properties section of the Lambda function is incorrect. The Globals section in a SAM template.yaml is primarily used for setting properties that apply to all AWS resources of a certain type. It's not a storage location for AWS credentials. Moreover, the AWS::Serverless::Function resource property does not have fields for AWS credentials. Even if you were to add the credentials as environment variables, it still wouldn't grant the locally running function the permissions associated with those credentials. The option that says: Run the function using sam local invoke with the --parameter-overrides parameter is incorrect. The --parameter-overrides option is typically used to change template parameters during local testing. For instance, if you had a parameter in your SAM template for setting an environment variable, the --parameter-overrides option would allow you to test with different values for those parameters. Still, it does not interact with nor modify AWS credentials. References: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-cli-command-reference-sam-local-invoke.html https://aws.amazon.com/blogs/aws/aws-serverless-application-model-sam-command-line-interface-build-test-and-debug-serverless-apps-locally/ https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html Check out this AWS SAM Cheat Sheet: https://tutorialsdojo.com/aws-serverless-application-model-sam/"},{question:"A company operates an e-commerce website on Amazon Elastic Container Service (ECS) behind an Application Load Balancer (ALB). They've set their ALB as an origin for an Amazon CloudFront distribution. Users interact with the website via a custom domain linked to the CloudFront distribution, all maintained within a public hosted zone in Amazon Route 53. The company wants to display region-specific pricing tables to its users. For example, when a user from the UK visits the site, they should be redirected to https://tutorialsdojo.com/uk/, while users from the Philippines should view prices on https://tutorialsdojo.com/ph/ How can a developer incorporate this feature in the least amount of development overhead?",answers:[{text:"Forward the CloudFront-Viewer-Address header to the web server running on the ECS cluster. Implement a custom logic that matches the header's value against a GeoIP database to determine user location. Based on the resolved location, redirect users to the appropriate region-specific URL.",isCorrect:!1},{text:"Configure the Route 53 record to use the geolocation routing policy.",isCorrect:!1},{text:"Use AWS Web Application Firewall (WAF's) geo-matching rule to identify the user country and attach it to the ALB. Configure ALB listener rules with path conditions to route traffic based on the identified country.",isCorrect:!1},{text:"Implement a CloudFront function that returns the appropriate URL based on the CloudFront-Viewer-Country. Configure the distribution to trigger the function on Viewer request events.",isCorrect:!0}],explanation:"You can configure CloudFront to add specific HTTP headers to the requests that CloudFront receives from viewers and forwards to your origin or edge function. The values of these HTTP headers are based on the characteristics of the viewer or the viewer request. The headers provide information about the viewer's device type, IP address, geographic location, request protocol (HTTP or HTTPS), HTTP version, TLS connection details, and JA3 fingerprint. With these headers, your origin or your edge function can receive information about the viewer without the need for you to write your own code to determine this information. If your origin returns different responses based on the information in these headers, you can include them in the cache key so that CloudFront caches the responses separately. For example, your origin might respond with content in a specific language based on the country that the viewer is in or with content tailored to a specific device type. Your origin might also write these headers to log files, which you can use to determine information about where your viewers are, which device types they're on, and more. In the given scenario, when a user initiates a request to the website, a CloudFront function can be triggered to inspect the CloudFront-Viewer-Country header, which pinpoints the originating country of a user. CloudFront functions are lightweight Javascript code that you can use to manipulate web requests at the CloudFront edge locations. By using a CloudFront function triggered on \"Viewer request\" events, you can assess and act upon incoming requests even before they reach the origin or CloudFront retrieves a cached response. Here's a representation using a CloudFront function code snippet: Hence, the correct answer is: Implement a CloudFront function that returns the appropriate URL based on the CloudFront-Viewer-Country. Configure the distribution to trigger the function on Viewer request events. The option that says: Forward the CloudFront-Viewer-Address header to the web server running on the ECS cluster. Implement a custom logic that matches the header's value against a GeoIP database to determine user location. Based on the resolved location, redirect users to the appropriate region-specific URL is incorrect. While this approach is technically valid, it's more complex since you have to handle the IP-to-location translation on the backend and maintain an up-to-date GeoIP database. The option that says: Configure the Route 53 record to use the geolocation routing policy is incorrect. Route 53 geolocation routing is primarily used for directing traffic to specific resources based on user location for performance or regulatory reasons, not for content personalization based on geolocation. The option that says: Use AWS Web Application Firewall (WAF's) geo-matching rule to identify the user country and attach it to the ALB. Configure ALB listener rules with path conditions to route traffic based on the identified country is incorrect. AWS WAF's geo-matching rule identifies a user's country based on their IP. However, it's primarily designed to allow or block access, not for redirection. Even if used in conjunction with ALB, the ALB's listener rules can't inherently make routing decisions based on a viewer's geolocation References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/adding-cloudfront-headers.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-functions.html Check out this Amazon CloudFront Cheat Sheet: https://tutorialsdojo.com/amazon-cloudfront/"},{question:"A developer is building a web application which requires a multithreaded event-based key/value cache store that will cache result sets from database calls. You need to run large nodes with multiple cores for your cache layer and it should scale up or down as the demand on your system increases and decreases. Which of the following is the MOST suitable service that you should use?",answers:[{text:"AWS Greengrass",isCorrect:!1},{text:"Amazon ElastiCache for Redis",isCorrect:!1},{text:"Amazon ElastiCache for Memcached",isCorrect:!0},{text:"Amazon CloudFront",isCorrect:!1}],explanation:"Redis and Memcached are popular, open-source, in-memory data stores. Although they are both easy to use and offer high performance, there are important differences to consider when choosing an engine. Memcached is designed for simplicity while Redis offers a rich set of features that make it effective for a wide range of use cases. In this scenario, Redis can provide a much more durable and powerful cache layer to the prototype distributed system, however, you should take note of one keyword in the requirement: multithreaded. In terms of commands execution, Redis is mostly a single-threaded server. It is not designed to benefit from multiple CPU cores unlike Memcached, however, you can launch several Redis instances to scale out on several cores if needed. Memcached is a more suitable choice since the scenario specifies that the system will run large nodes with multiple cores or threads which Memcached can adequately provide. You can choose Memcached over Redis if you have the following requirements: - You need the simplest model possible. - You need to run large nodes with multiple cores or threads. - You need the ability to scale out and in, adding and removing nodes as demand on your system increases and decreases. - You need to cache objects, such as a database. This is why the most suitable answer to this scenario is Amazon ElastiCache for Memcached. Amazon ElastiCache for Redis is incorrect because it does not totally support a multithreaded architecture, unlike Memcached. Although Redis has more features compared with Memcached, the scenario requires that the cache layer is multithreaded. This is why Memcached is a more suitable cache engine to choose from instead of Redis. Amazon CloudFront is incorrect because it is primarily used as a Content Delivery Network (CDN) service which delivers your entire website, including dynamic, static, streaming, and interactive content using a global network of edge locations. AWS IoT Greengrass is incorrect because this service is primarily used to enable connected devices to run AWS Lambda functions, execute predictions based on machine learning models, keep device data in sync, and communicate with other devices securely even without an Internet connection. Hence, this is not a suitable option for this scenario. References: https://aws.amazon.com/elasticache/redis-vs-memcached https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/SelectEngine.html https://aws.amazon.com/caching/aws-caching/ Redis (cluster mode enabled vs disabled) vs Memcached: https://tutorialsdojo.com/redis-cluster-mode-enabled-vs-disabled-vs-memcached/ Comparison of AWS Services Cheat Sheets: https://tutorialsdojo.com/comparison-of-aws-services/"},{question:'A development team has started using AWS CloudFormation for deploying Lambda functions. Their project structure places the source code for the Lambda function locally within a directory named "tutorialsdojo". The lambda handler for the function is in a file called "app.js". Below is a snippet of their template. What is the next step in order for the template to be deployed using the aws cloudformation deploy CLI?',answers:[{text:"Package the Lambda function in a ZIP file. Specify the local path of the packaged file in the CodeUri property.",isCorrect:!1},{text:"Use the Fn::Base64 intrinsic function inline with the CodeUri property to encode the content of the app.js file.",isCorrect:!1},{text:"Use the aws cloudformation package command to upload the local artifacts of the Lambda function to an S3 bucket and produce a version of the template with references to the S3 URI of the file.",isCorrect:!0},{text:"Upload the app.js file to an S3 bucket. Update the CodeUri property in the template to point to the S3 URI of the file.",isCorrect:!1}],explanation:"The aws cloudformation package command packages the local artifacts (local paths) that your AWS CloudFormation template references. The command uploads local artifacts, such as source code for an AWS Lambda function or a Swagger file for an AWS API Gateway REST API, to an S3 bucket. The command returns a copy of your template, replacing references to local artifacts with the S3 location where the command uploaded the artifacts. Use this command to quickly upload local artifacts that might be required by your template. After you package your template's artifacts, run the aws cloudformation deploy command to deploy the returned template. Since we have local artifacts (source code for the AWS Lambda functions), we should use the package command. After running the package command, the modified template can now be deployed by using the deploy command. Hence, the correct answer is: Use the aws cloudformation package command to upload the local artifacts of the Lambda function to an S3 bucket and produce a version of the template with references to the S3 URI of the file. The option that says: Upload the app.js file to an S3 bucket. Update the CodeUri property in the template to point to the S3 URI of the file is incorrect. Simply uploading the app.js file to an S3 bucket without packaging it into a ZIP format is not adequate for deployment. AWS Lambda expects the deployment package to be in a ZIP format for the Node.js runtime. The option that says: Use the Fn::Base64 intrinsic function inline with the CodeUri property to encode the content of the app.js file is incorrect. Embedding the base64 encoded content of the app.js file within the template would result in deployment errors. Note that the CodeUri property specifically expects either a local path to the Lambda function code or an S3 URI pointing to the zipped code. The option that says: Package the Lambda function in a ZIP file. Specify the local path of the packaged file in the CodeUri property is incorrect. The CodeUri property expects either a local path to the directory containing the Lambda function source code or an S3 URI to the ZIP file. Specifying a local path to a ZIP file is wrong, as AWS CloudFormation wouldn't be able to directly deploy the function with the ZIP file. References: https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html https://docs.aws.amazon.com/cli/latest/reference/cloudformation/deploy/index.html Check out this AWS CloudFormation Cheat Sheet: https://tutorialsdojo.com/aws-cloudformation/"},{question:"A developer is managing a serverless application orchestrated by AWS Step Functions. One of the Lambda functions sends an API call to a third-party payment service, which takes some time to complete. The Step Functions workflow needs to pause while the service validates the payment. It should only resume after the service sends a notification to a webhook endpoint. Which combination of actions will fulfill the requirements in the most cost-effective manner? (Select Two)",answers:[{text:"Use a Wait State to pause the execution of the workflow. Configure the webhook handler to invoke the Lambda function synchronously.",isCorrect:!1},{text:"Configure the Lambda function task state to use the waitForTaskToken option. Retrieve the task token from the context object of the state machine and include it as part of the Lambda function’s payload body.",isCorrect:!0},{text:"Configure the webhook handler to call the SendTaskSuccess method after a successful notification.",isCorrect:!0},{text:"Set the invocation method of the Lambda function task state to asynchronous. Create an AWS SQS queue and configure the webhook handler to send the payment service’s response to the queue. Use a combination of Wait State and Choice State to poll the queue.",isCorrect:!1},{text:"Configure the webhook handler to call the SendTaskHeartbeat method after a successful notification.",isCorrect:!1}],explanation:"In AWS Step Functions, the waitForTaskToken option allows a task to be paused until an external system signals its completion. When a task is configured with this option, Step Functions generates a unique token, which can be retrieved from the context object of the state machine. This token, for instance, can be stored in a data store for reference. The diagram below depicts how waitForTaskToken is used for an SQS task state. An external system, such as a webhook handler can then reference the token and call the SendTaskSuccess or SendTaskFailure method to signal Step Functions to resume the workflow. When the workflow is in a paused state, you're not billed for the time the workflow is paused, making it a cost-effective method for awaiting external processes or events. Hence, the correct answers are: Configure the Lambda function task state to use the waitForTaskToken option. Retrieve the task token from the context object of the state machine and include it as part of the Lambda function’s payload body. Configure the webhook handler to call the SendTaskSuccess method after a successful notification. The option that says: Set the invocation method of the Lambda function task state to asynchronous. Create an AWS SQS queue and configure the webhook handler to send the payment service’s response to the queue. Use a combination of Wait State and Choice State to poll the queue is incorrect. While this solution may work, every iteration involving the Wait State and Choice State incurs a cost as a state transition. If the third-party service takes an unpredictable amount of time, the state machine could go through multiple cycles of waiting and checking the SQS queue, resulting in a higher cost. The option that says: Use a Wait State to pause the execution of the workflow. Configure the webhook handler to invoke the Lambda function synchronously is incorrect. A fixed Wait State is less cost-effective in scenarios where the waiting duration is unpredictable. If the third-party service finishes earlier than the wait duration, you're paying for unused time. If it takes longer, the workflow might proceed before the task is complete. The option that says: Configure the webhook handler to call the SendTaskHeartbeat method after a successful notification is incorrect because this method is simply used for keeping tasks alive and preventing them from timing out. It also does not signal completion. References: https://aws.amazon.com/blogs/compute/building-cost-effective-aws-step-functions-workflows/ https://docs.aws.amazon.com/step-functions/latest/dg/callback-task-sample-sqs.html https://docs.aws.amazon.com/step-functions/latest/dg/connect-to-resource.html Check out this AWS Step Functions Cheat Sheet: https://tutorialsdojo.com/aws-step-functions/"},{question:"A developer is building a Docker application using Amazon ECS. The application requires containers to maintain long-lived connections and access specific ports on the host container instance to send or receive traffic using port mapping. Which component of ECS should the developer configure to properly implement this task?",answers:[{text:"Task definition",isCorrect:!0},{text:"Service scheduler",isCorrect:!1},{text:"Container Agent",isCorrect:!1},{text:"Container instance",isCorrect:!1}],explanation:"Port mappings allow containers to access ports on the host container instance to send or receive traffic. Port mappings are specified as part of the container definition, which can be configured in the task definition. For task definitions that use the awsvpc network mode, you should only specify the containerPort. The hostPort can be left blank or it must be the same value as the containerPort. Port mappings on Windows use the NetNAT gateway address rather than localhost. There is no loopback for port mappings on Windows, so you cannot access a container's mapped port from the host itself. Hence, the correct answer is: Task Definition. The option that says: Service scheduler is incorrect because this only provides you the ability to run tasks manually (for batch jobs or single run tasks), with Amazon ECS placing tasks on your cluster for you. The service scheduler is ideally suited for long-running stateless services and applications but not for configuring port mappings. The option that says: Container instance is incorrect because this is just an Amazon EC2 instance running the Amazon ECS container agent and registered into a cluster. When you run tasks with Amazon ECS, your tasks using the EC2 launch type are placed on your active container instances. However, you can't manually configure the port mappings directly on your container instances but through task definitions. The option that says: Container Agent is incorrect because this only allows container instances to connect to your cluster. The Amazon ECS container agent is included in the Amazon ECS-optimized AMIs, but you can also install it on any Amazon EC2 instance that supports the Amazon ECS specification. As with the other incorrect options, you can't configure port mappings with this component. References: https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PortMapping.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#container_definition_portmappings Check out this Amazon ECS Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/"},{question:"A website is hosted in an Auto Scaling group of EC2 instances behind an Application Load Balancer. It also uses CloudFront with a default domain name to distribute its static assets and dynamic contents. However, the website has a poor search ranking as it doesn't use a secure HTTPS/SSL on its site. Which are the possible solutions that the developer can implement in order to set up HTTPS communication between the viewers and CloudFront? (Select TWO.)",answers:[{text:"Configure the ALB to use its default SSL/TLS certificate.",isCorrect:!1},{text:"Set the Viewer Protocol Policy to use HTTPS Only.",isCorrect:!0},{text:"Use a self-signed SSL/TLS certificate in the ALB which is stored in a private S3 bucket.",isCorrect:!1},{text:"Use a self-signed certificate in the ALB.",isCorrect:!1},{text:"Set the Viewer Protocol Policy to use Redirect HTTP to HTTPS.",isCorrect:!0}],explanation:"You can configure one or more cache behaviors in your CloudFront distribution to require HTTPS for communication between viewers and CloudFront. You also can configure one or more cache behaviors to allow both HTTP and HTTPS so that CloudFront requires HTTPS for some objects but not for others. If you're using the domain name that CloudFront assigned to your distribution, such as dtut0ria1sd0jo.cloudfront.net, you can change the Viewer Protocol Policy setting for one or more cache behaviors to require HTTPS communication by setting it as either Redirect HTTP to HTTPS, or HTTPS Only. If your origin is an Elastic Load Balancing load balancer, you can use a certificate provided by AWS Certificate Manager (ACM). You can also use a certificate that is signed by a trusted third-party certificate authority and imported into ACM. Note that you can't use a self-signed certificate for HTTPS communication between CloudFront and your origin. Hence, setting the Viewer Protocol Policy to use Redirect HTTP to HTTPS and setting the Viewer Protocol Policy to use HTTPS Only are the correct answers in this scenario. Using a self-signed SSL/TLS certificate in the ALB which is stored in a private S3 bucket is incorrect because you don't need to add an SSL certificate if you only require HTTPS for communication between the viewers and CloudFront. You should only do this if you require HTTPS between your origin and CloudFront. In addition, you can't use a self-signed certificate in this scenario even though it is stored in a private S3 bucket. You need to use either a certificate from ACM or a third-party certificate. Configuring the ALB to use its default SSL/TLS certificate is incorrect because there is no default SSL certificate in ELB, unlike what we have in CloudFront. Using a self-signed certificate in the ALB is incorrect because adding an SSL certificate in the ELB is not required. Moreover, you can't use a self-signed certificate in this scenario. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html#using-https-cloudfront-to-origin-certificate https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html Check out this Amazon CloudFront Cheat Sheet: https://tutorialsdojo.com/amazon-cloudfront/"},{question:"A web application is uploading large files, which are over 4 GB in size, to an Amazon S3 bucket called data.tutorialsdojo.com every 30 minutes. To minimize the time required for each upload, which of the following actions should be taken?",answers:[{text:"Use the Multipart upload API.",isCorrect:!0},{text:"Enable Transfer Acceleration in the bucket.",isCorrect:!1},{text:"Use the BatchWriteItem API.",isCorrect:!1},{text:"Use the Putltem API.",isCorrect:!1}],explanation:"Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Using multipart upload provides the following advantages: Improved throughput - You can upload parts in parallel to improve throughput. Quick recovery from any network issues - Smaller part size minimizes the impact of restarting a failed upload due to a network error. Pause and resume object uploads - You can upload object parts over time. Once you initiate a multipart upload, there is no expiry; you must explicitly complete or abort the multipart upload. Begin an upload before you know the final object size - You can upload an object as you are creating it. Hence, the correct answer is: Use the Multipart Upload API. The options that say: Use the BatchWriteItem API and Use the Putltem API are incorrect because these are primarily DynamoDB APIs and not S3. The option that says: Enable Transfer Acceleration in the bucket is incorrect because although Transfer Acceleration will typically reduce the upload time to S3, the bucket in the scenario won't be able to turn on this feature. Take note that the name of the bucket used for Transfer Acceleration must be DNS-compliant and must not contain periods (\".\"). References: https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html Check out this Amazon S3 Cheat Sheet: https://tutorialsdojo.com/amazon-s3/"},{question:"A Lambda function downloads the same 250 MB file between invocations and stores it in memory for processing. This leads to frequent timeouts and negatively impacts the performance of the serverless application. Which change should be made to resolve the issue most effectively?",answers:[{text:"Increase the memory allocation of the function.",isCorrect:!1},{text:"Store the file in the /tmp directory of the execution context and reuse it on succeeding invocations.",isCorrect:!0},{text:"Increase the ephemeral storage size of the function",isCorrect:!1},{text:"Increase the timeout of the function.",isCorrect:!1}],explanation:'When AWS Lambda executes your Lambda function, it provisions and manages the resources needed to run your Lambda function. When you create a Lambda function, you specify configuration information, such as the amount of memory and maximum execution time that you want to allow for your Lambda function. When a Lambda function is invoked, AWS Lambda launches an execution context based on the configuration settings you provide. The execution context is a temporary runtime environment that initializes any external dependencies of your Lambda function code, such as database connections or HTTP endpoints. This affords subsequent invocations better performance because there is no need to "cold-start" or initialize those external dependencies, as explained below. It takes time to set up an execution context and do the necessary "bootstrapping", which adds some latency each time the Lambda function is invoked. You typically see this latency when a Lambda function is invoked for the first time or after it has been updated because AWS Lambda tries to reuse the execution context for subsequent invocations of the Lambda function. After a Lambda function is executed, AWS Lambda maintains the execution context for some time in anticipation of another Lambda function invocation. In effect, the service freezes the execution context after a Lambda function completes and thaws the context for reuse if AWS Lambda chooses to reuse the context when the Lambda function is invoked again. Each execution context provides 512 MB - 10,240 MB of additional disk space in the /tmp directory. The directory content remains when the execution context is frozen, providing a transient cache that can be used for multiple invocations. You can add extra code to check if the cache has the data that you stored. Hence, the correct answer in this scenario is to Store the file in the /tmp directory of the execution context and reuse it on succeeding invocations. The option that says: Increase the memory allocation of the function is incorrect. The actual processing time may be reduced by allocating more memory, but there would still be a lot of time wasted in downloading the 250 MB file every time the function is invoked. The option that says: Increase the timeout of the function is incorrect because this doesn\'t solve the root cause of the problem. You may configure your function with a maximum timeout of 15 minutes. However, the fact still remains that the function repeatedly downloads the file at every invocation. The option that says: Increase the ephemeral storage size of the function is incorrect. This won\'t have any effect at all on solving the issue. The change must be made at the code level. Instead of storing the file directly on memory, storing it in the /tmp directory provides a more cost-effective and scalable solution, as the function can read and write the file from disk as needed. The /tmp directory also persists between invocations of the function, allowing it to access the file more quickly between invocations. References: https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html#function-configuration Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/'},{question:"A developer is refactoring a Lambda function that currently processes data using a public GraphQL API. There’s a new requirement to store query results in a database hosted in a VPC. The function has been configured with additional VPC-specific information, and the database connection has been successfully established. However, the engineer has discovered that the function can no longer connect to the internet after testing. Which of the following should the developer do to fix this issue? (Select TWO.)",answers:[{text:"Add a NAT gateway to your VPC.",isCorrect:!0},{text:"Ensure that the associated security group of the Lambda function allows outbound connections.",isCorrect:!0},{text:"Submit a limit increase request to AWS to raise the concurrent executions limit of your Lambda function.",isCorrect:!1},{text:"Set up elastic network interfaces (ENIs) to enable your Lambda function to connect securely to other resources within your private VPC.",isCorrect:!1},{text:"Configure your function to forward payloads that were not processed to a dead-letter queue (DLQ) using Amazon SQS.",isCorrect:!1}],explanation:"AWS Lambda uses the VPC information you provide to set up ENIs that allow your Lambda function to access VPC resources. Each ENI is assigned a private IP address from the IP address range within the subnets you specify but is not assigned any public IP addresses. Therefore, if your Lambda function requires Internet access (for example, to access AWS services that don't have VPC endpoints ), you can configure a NAT instance inside your VPC, or you can use the Amazon VPC NAT gateway. You cannot use an Internet gateway attached to your VPC since that requires the ENI to have public IP addresses. If your Lambda function needs Internet access, just as described in this scenario, do not attach it to a public subnet or to a private subnet without Internet access. Instead, attach it only to private subnets with Internet access through a NAT instance or add a NAT gateway to your VPC. You should also ensure that the associated security group of the Lambda function allows outbound connections. The option that says: Submit a limit increase request to AWS to raise the concurrent executions limit of your Lambda function is incorrect because the root cause of the problem is that the function cannot connect to public GraphQL APIs over the Internet. The scenario doesn't mention anything about a concurrency problem. The option that says: Configuring your function to forward payloads that were not processed to a dead-letter queue (DLQ) using Amazon SQS is incorrect because it will only improve the error handling of your Lambda function. The issue here is the Internet connectivity of your function and not its error handling hence, this option will not solve the problem. The option that says: Setting up elastic network interfaces (ENIs) to enable your Lambda function to connect securely to other resources within your private VPC is incorrect because this is already done automatically by AWS Lambda. It uses the VPC information you provide to automatically set up ENIs that allow your Lambda function to access VPC resources. You don't need to do this step in order for your Lambda function to be integrated with your VPC. References: https://docs.aws.amazon.com/lambda/latest/dg/vpc.html https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/ Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A web application is using an ElastiCache cluster that is suffering from cache churn. A developer needs to reconfigure the application so that data are retrieved from the database only in the event that there is a cache miss. Which pseudocode illustrates the caching strategy that the developer needs to implement?",answers:[{text:'get_item(item_id): item_value = database.query("SELECT * FROM Items WHERE id = ?", item_id) if item_value is None: item_value = cache.set(item_id, item_value) cache.add(item_id, item_value) return item_value',isCorrect:!1},{text:'get_item(item_id): item_value = cache.get(item_id) if item_value is not None: item_value = database.query("SELECT * FROM Items WHERE id = ?", item_id) cache.add(item_id, item_value) return item_value else: return item_value',isCorrect:!1},{text:"get_item(item_id, item_value): item_value = database.query(\"UPDATE Items WHERE id = ?\", item_id, item_value) cache.add(item_id, item_value) return 'ok'",isCorrect:!1},{text:'get_item(item_id): item_value = cache.get(item_id) if item_value is None: item_value = database.query("SELECT * FROM Items WHERE id = ?", item_id) cache.add(item_id, item_value) return item_value',isCorrect:!0}],explanation:'Lazy Loading is a caching strategy that loads data into the cache only when necessary. Here is how it works: - If the data exists in the cache and is current, ElastiCache returns the data to your application. This event is also called "Cache Hit". - If there is a "Cache Miss", or in other words, the data does not exist in the cache, or the data in the cache has expired, then your application requests the data from your data store, which returns the data to your application. Your application then writes the data received from the store to the cache so it can be more quickly retrieved the next time it is requested. In the scenario, to implement lazy loading, you must first check if the item is already in the cache using the "cache.get(item_id)" method. If the item is not in the cache (i.e. "item_value is None"), the code then queries the database for the item and stores it in the cache using the "cache.set(item_id, item_value)" method so that it can be retrieved faster next time. This way, the application is not querying the database every time the item is needed and instead uses the cached version of the item if it\'s available. Hence, the correct answer is the option that says: get_item(item_id): item_value = cache.get(item_id) if item_value is None: item_value = database.query("SELECT * FROM Items WHERE id = ?", item_id) cache.add(item_id, item_value) return item_value The option that says: get_item(item_id): item_value = database.query("SELECT * FROM Items WHERE id = ?", item_id) if item_value is None: item_value = cache.set(item_id, item_value) cache.add(item_id, item_value) return item_value is an incorrect implementation of lazy loading because it first queries the database and checks if the item is in the cache. The option that says: get_item(item_id): item_value = cache.get(item_id) if item_value is not None: item_value = database.query("SELECT * FROM Items WHERE id = ?", item_id) cache.add(item_id, item_value) return item_value else: return item_value does not implement lazy loading because the code is not utilizing the cache first and is querying the database every time the item is needed, this will make the application slow and inefficient. The option that says: get_item(item_id, item_value): item_value = database.query("UPDATE Items WHERE id = ?", item_id, item_value) cache.add(item_id, item_value) return \'ok\' is incorrect because this is an implementation of a write-through caching strategy where data is written to both the cache and the primary storage (such as a database). References: https://aws.amazon.com/caching/best-practices/ https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html Check out this Amazon Elasticache Cheat Sheet: https://tutorialsdojo.com/amazon-elasticache/'},{question:"You are deploying a serverless application composed of Lambda, API Gateway, CloudFront, and DynamoDB using CloudFormation. The AWS SAM syntax should be used to declare resources in your template which requires you to specify the version of the AWS Serverless Application Model (AWS SAM). Which of the following sections is required, aside from the Resources section, that should be in your CloudFormation template?",answers:[{text:"Parameters",isCorrect:!1},{text:"Transform",isCorrect:!0},{text:"Format Version",isCorrect:!1},{text:"Mappings",isCorrect:!1}],explanation:"For serverless applications (also referred to as Lambda-based applications), the optional Transform section specifies the version of the AWS Serverless Application Model (AWS SAM) to use. When you specify a transform, you can use AWS SAM syntax to declare resources in your template. The model defines the syntax that you can use and how it is processed. This section specifies one or more macros that AWS CloudFormation uses to process your template. The Transform section builds on the simple, declarative language of AWS CloudFormation with a powerful macro system. You can declare one or more macros within a template. AWS CloudFormation executes macros in the order that they are specified. When you create a change set, AWS CloudFormation generates a change set that includes the processed template content. You can then review the changes and execute the change set. AWS CloudFormation also supports the AWS::Serverless and AWS::Include transforms, which are macros hosted by AWS CloudFormation. AWS CloudFormation treats these transforms the same as any macros you create in terms of execution order and scope. Therefore, the Transform section should be the correct one to be added to your template. Mappings section is incorrect because this is just a literal mapping of keys and associated values that you can use to specify conditional parameter values, similar to a lookup table. Parameters section is incorrect because this only contains the values that will be passed to your template at runtime (when you create or update a stack). You can refer to parameters from the Resources and Outputs sections of the template, but this is not used to specify the AWS SAM version. Format Version section is incorrect because this just refers to the AWS CloudFormation template version that the template conforms to, and not the version of the AWS Serverless Application Model (AWS SAM) References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html https://aws.amazon.com/blogs/aws/cloudformation-macros/ Check out this AWS CloudFormation Cheat Sheet: https://tutorialsdojo.com/aws-cloudformation/ Check out this AWS Serverless Application Model Cheat Sheet: https://tutorialsdojo.com/aws-serverless-application-model-sam/"},{question:"An API gateway with a Lambda proxy integration takes a long time to complete its processing. There were also occurrences where some requests timed out. You want to monitor the responsiveness of your API calls as well as the underlying Lambda function. Which of the following CloudWatch metrics should you use to troubleshoot this issue? (Select TWO.)",answers:[{text:"IntegrationLatency",isCorrect:!0},{text:"CacheHitCount",isCorrect:!1},{text:"Latency",isCorrect:!0},{text:"Count",isCorrect:!1},{text:"CacheMissCount",isCorrect:!1}],explanation:"You can monitor API execution using CloudWatch, which collects and processes raw data from API Gateway into readable, near-real-time metrics. These statistics are recorded for a period of two weeks so that you can access historical information and gain a better perspective on how your web application or service is performing. By default, API Gateway metric data is automatically sent to CloudWatch in one-minute periods. The metrics reported by API Gateway provide information that you can analyze in different ways. The list below shows some common uses for the metrics. These are suggestions to get you started, not a comprehensive list. - Monitor the IntegrationLatency metrics to measure the responsiveness of the backend. - Monitor the Latency metrics to measure the overall responsiveness of your API calls. - Monitor the CacheHitCount and CacheMissCount metrics to optimize cache capacities to achieve a desired performance. Hence, the correct metrics that you have to use in this scenario are Latency and IntegrationLatency. Count is incorrect because this metric simply gets the total number of API requests in a given period. CacheMissCount is incorrect because this metric just gets the number of requests served from the backend in a given period when API caching is enabled. The Sum statistic represents this metric, namely, the total count of the cache misses in the given period. CacheHitCount is incorrect because this fetches the number of requests served from the API cache in a given period. The Sum statistic represents this metric, namely, the total count of the cache hits in the given period. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-metrics-and-dimensions.html https://docs.aws.amazon.com/apigateway/latest/developerguide/monitoring-cloudwatch.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway"},{question:"A developer has deployed a Lambda function that runs in DEV, UAT, and PROD environments. The function uses different parameters that varies based on the environment it is running in. The parameters are currently hardcoded in the function. Which action should the developer do to reference the appropriate parameters without modifying the code every time the environment changes?",answers:[{text:"Create individual Lambda Layers for each environment",isCorrect:!1},{text:"Publish three versions of the Lambda function. Assign the aliases DEV, UAT, and PROD to each version.",isCorrect:!1},{text:"Use environment variables to set the parameters per environment.",isCorrect:!0},{text:"Create a stage variable called ENV and invoke the Lambda function by its alias name.",isCorrect:!1}],explanation:"Environment variables for Lambda functions enable you to dynamically pass settings to your function code and libraries without making changes to your code. Environment variables are key-value pairs that you create and modify as part of your function configuration using either the AWS Lambda Console, the AWS Lambda CLI, or the AWS Lambda SDK. AWS Lambda then makes these key-value pairs available to your Lambda function code using standard APIs supported by the language, like process.env for Node.js functions. You can use environment variables to help libraries know what directory to install files in, where to store outputs, store connection and logging settings, and more. By separating these settings from the application logic, you don't need to update your function code when changing the function behavior based on different settings. Hence, the correct answer is: Use environment variables to set the parameters per environment. The option that says: Create a stage variable called ENV and invoke the Lambda function by its alias name is incorrect because the stage variable is a feature of API Gateway, not AWS Lambda. The option that says: Create individual Lambda Layers for each environment is incorrect because this feature is only used to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. The option that says: Publish three versions of the Lambda function. Assign the aliases DEV, UAT, and PROD to each version is incorrect because this is just like a pointer to a specific Lambda function version. By using aliases, you can access the Lambda function, which the alias is pointing to, without the caller knowing the specific version the alias is pointing to. References: https://docs.aws.amazon.com/lambda/latest/dg/env_variables.html https://docs.aws.amazon.com/lambda/latest/dg/lambda-configuration.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"}]},{id:"aws-developer-9",title:"AWS Certified Developer Associate Practice Exams 3",description:"Additional practice questions covering AWS development topics.",questions:[{question:"A tech company has a real-time traffic monitoring system which uses Amazon Kinesis Data Stream to collect data and a group of EC2 instances that consume and process the data stream. Your development team is responsible for adjusting the number of shards in the data stream to adapt to changes in the rate of data flow. Which of the following are correct regarding Kinesis resharding which your team should consider in managing the application? (Select TWO.)",answers:[{text:"You have to split the cold shards to decrease the capacity of the stream.",isCorrect:!1},{text:"You can decrease the stream's capacity by merging shards.",isCorrect:!0},{text:"You can increase the stream's capacity by splitting shards.",isCorrect:!0},{text:"The data records that are flowing to the parent shards will be lost when you reshard.",isCorrect:!1},{text:"You have to merge the hot shards to increase the capacity of the stream.",isCorrect:!1}],explanation:'Amazon Kinesis Data Streams is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. You can configure hundreds of thousands of data producers to continuously put data into a Kinesis data stream. Data will be available within milliseconds to your Amazon Kinesis applications, and those applications will receive data records in the order they were generated. The purpose of resharding in Amazon Kinesis Data Streams is to enable your stream to adapt to changes in the rate of data flow. You split shards to increase the capacity (and cost) of your stream. You merge shards to reduce the cost (and capacity) of your stream. One approach to resharding could be to split every shard in the stream—which would double the stream\'s capacity. However, this might provide more additional capacity than you actually need and therefore create unnecessary costs. You can also use metrics to determine which are your "hot" or "cold" shards, that is, shards that are receiving much more data, or much less data, than expected. You could then selectively split the hot shards to increase capacity for the hash keys that target those shards. Similarly, you could merge cold shards to make better use of their unused capacity. You can obtain some performance data for your stream from the Amazon CloudWatch metrics that Kinesis Data Streams publishes. However, you can also collect some of your own metrics for your streams. One approach would be to log the hash key values generated by the partition keys for your data records. Recall that you specify the partition key at the time that you add the record to the stream. Hence, the correct statements regarding Kinesis resharding are: - You can decrease the stream\'s capacity by merging shards - You can increase the stream\'s capacity by splitting shards The option that says: you have to merge the hot shards to increase the capacity of the stream is incorrect because a hot shard is the one that receives more data in the stream, which you should split rather than merge. The option that says: you have to split the cold shards to decrease the capacity of the stream is incorrect because a cold shard is the one that receives fewer data in the stream, which you should merge rather than split. The option that says: the data records that are flowing to the parent shards will be lost when you reshard is incorrect because the data records are actually re-routed to flow to the child shards based on the hash key values that the data-record partition keys map to. Hence, this is incorrect because the records are not lost. References: https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/ Kinesis Scaling, Resharding and Parallel Processing: https://tutorialsdojo.com/kinesis-scaling-resharding-and-parallel-processing/'},{question:"A developer monitors multiple sensors inside a data center which detects various environmental conditions that may affect their running servers. In the current architecture, the data is initially processed by an AWS Lambda function and then stored in a remote data warehouse. To make the system more durable and scalable, the developer plans to use an Amazon SQS FIFO queue to store the data, which will be polled by the Lambda function. There is a known issue with the sensor devices sending duplicate data intermittently. What action can the developer take to lessen the chances of processing duplicate messages?",answers:[{text:"Add a MessageDeduplicationId parameter to the SendMessage API request.",isCorrect:!0},{text:"Configure the Amazon SQS queue to automatically drop a duplicate message whenever it arrives within the message's VisibilityTimeout.",isCorrect:!1},{text:"Refactor the Lambda function to store the message's content and drop the incoming messages with similar content within a 5-minute period.",isCorrect:!1},{text:"Use an Amazon SQS Standard queue instead of a FIFO queue to avoid any duplicate messages.",isCorrect:!1}],explanation:"Amazon SQS FIFO First-In-First-Out queues are designed to enhance messaging between applications when the order of operations and events is critical or where duplicates can't be tolerated. Amazon SQS FIFO queues follow exactly-once processing. It introduces a parameter called Message Deduplication ID, which is the token used for deduplication of sent messages. Suppose a message with a particular message deduplication ID is sent successfully. In that case, any messages sent with the same message deduplication ID are accepted successfully but aren't delivered during the 5-minute deduplication interval. SQS remembers the MessageDeduplicationId values it’s seen for at least five minutes, which means deduplication Ids can only reduce, not completely eliminate, the chances of duplication occurring. For example, if a producer was unable to receive an acknowledgment after sending a message due to a network issue and then regains connection after 10 minutes and attempts to resend the message, there is a risk of duplication occurring. In this scenario, you can lessen the chances of the Lambda function processing duplicate messages by storing data in an SQS FIFO queue. You may provide a MessageDeduplicationId value so SQS can distinguish one message from another. Optionally, you may enable ContentBasedDeduplication to let SQS create an SHA-256 hash based on the message body and use it as the value for MessageDeduplicationId. Hence, in this scenario, the correct answer is to: Add a MessageDeduplicationId parameter to the SendMessage API request. Refactoring the Lambda function to store the message's content and dropping the incoming messages with similar content within a 5-minute period is incorrect because Lambda functions do not share data amongst themselves during a scale-up event. Therefore, if a function is processing a message and another function handles the succeeding message, it would not be able to compare if it is indeed a duplicate or not. You have to configure the SQS FIFO queue to use a Message Deduplication ID in order to avoid having duplicate messages. Configuring the Amazon SQS queue to automatically drop a duplicate message whenever it arrives within the message's VisibilityTimeout is incorrect because the visibility timeout is primarily used to prevent other consumers from processing the message again and not for detecting duplicate messages. Using an Amazon SQS Standard queue instead of a FIFO queue to avoid any duplicate messages is incorrect because using standard queues will actually introduce duplicate messages. Take note that FIFO queues help you avoid sending duplicates and not the Standard-type queue. References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html https://docs.amazonaws.cn/en_us/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html#FIFO-queues-exactly-once-processing https://aws.amazon.com/blogs/developer/how-the-amazon-sqs-fifo-api-works/ Check out this Amazon SQS Cheat Sheet: https://tutorialsdojo.com/amazon-sqs/"},{question:"A developer wants to use multi-factor authentication (MFA) to protect programmatic calls to specific AWS API operations like Amazon EC2 StopInstances. He needs to call an API where he can submit the MFA code that is associated with his MFA device. Using the temporary security credentials that are returned from the call, he can then make programmatic calls to API operations that require MFA authentication. Which API should the developer use to properly implement this security feature?",answers:[{text:"AssumeRoleWithWebIdentity",isCorrect:!1},{text:"AssumeRoleWithSAML",isCorrect:!1},{text:"GetFederationToken",isCorrect:!1},{text:"GetSessionToken",isCorrect:!0}],explanation:"The GetSessionToken API returns a set of temporary credentials for an AWS account or IAM user. The credentials consist of an access key ID, a secret access key, and a security token. Typically, you use GetSessionToken if you want to use MFA to protect programmatic calls to specific AWS API operations like Amazon EC2 StopInstances. MFA-enabled IAM users would need to call GetSessionToken and submit an MFA code that is associated with their MFA device. Using the temporary security credentials that are returned from the call, IAM users can then make programmatic calls to API operations that require MFA authentication. If you do not supply a correct MFA code, then the API returns an access denied error. Thus, the correct answer is to use the GetSessionToken API in this scenario. AssumeRoleWithWebIdentity is incorrect because this only returns a set of temporary security credentials for federated users who are authenticated through public identity providers such as Amazon, Facebook, Google, or OpenID, which were not mentioned in the scenario. This API does not support MFA. AssumeRoleWithSAML is incorrect because this just returns a set of temporary security credentials for users who have been authenticated via a SAML authentication response. This operation provides a mechanism for tying an enterprise identity store or directory to role-based AWS access without user-specific credentials or configuration. This API does not support MFA. GetFederationToken is incorrect because it does not support MFA. The appropriate STS API that the developer should use is GetSessionToken. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html#stsapi_comparison https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html Check out this AWS Identity & Access Management (IAM) Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"},{question:"A development team is working on an AWS Serverless Application Model (SAM) application with its source code hosted on GitHub. A newly recruited developer clones the repository and observes that the SAM template contains references to AWS Lambda functions with CodeUri pointing to local file paths. The developer has added a new Lambda function and must redeploy the updated version to Production. Which combination of steps must be taken to satisfy the requirement? (Select Two)",answers:[{text:"Execute sam publish to make the application available in the AWS Serverless Application Repository.",isCorrect:!1},{text:"Use the sam deploy command to deploy the application with a specified CloudFormation stack.",isCorrect:!0},{text:"Run sam init to initialize a new SAM project.",isCorrect:!1},{text:"Use the sam sync command to synchronize the local changes to the application in AWS.",isCorrect:!1},{text:"Execute sam build to resolve dependencies and construct deployment artifacts for all functions and layers in the SAM template.",isCorrect:!0}],explanation:"AWS SAM uses AWS CloudFormation as the underlying deployment mechanism. You can deploy your application by using AWS SAM command line interface (CLI) commands. You can also use other AWS services that integrate with AWS SAM to automate your deployments. When deploying a SAM application, it's vital to ensure that all components of the application are properly packaged and that all resources are provisioned correctly in the AWS environment. The sam build command serves this purpose by resolving any dependencies the application might have and constructing deployment artifacts for all functions and layers specified in the SAM template. This is especially important when the SAM template references local file paths, such as CodeUri pointing to local Lambda function codes. Once the application has been successfully built, the next step is to deploy it. The sam deploy command allows the application to be deployed using AWS CloudFormation, ensuring that all resources defined in the SAM template are provisioned and configured correctly in the target environment. Hence, the correct answers are: - Execute sam build to resolve dependencies and construct deployment artifacts for all functions and layers in the SAM template. - Use the sam deploy command to deploy the application with a specified CloudFormation stack. The option that says: Run sam init to initialize a new SAM project is incorrect because this command is simply used when creating a new SAM project, not to deploy an existing one. The option that says: Execute sam publish to make the application available in the AWS Serverless Application Repository is incorrect. This SAM CLI is used to publish applications to the AWS Serverless Application Repository, which is not a mandatory step for deploying SAM applications. The option that says: Use the sam sync command to synchronize the local changes to the application in AWS is incorrect. This command is typically used for quick syncing of local changes to AWS and is more suitable for rapid development testing. In a Production setting, a full deployment using sam deploy is more appropriate to ensure that all configurations and resources are correctly provisioned. References: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html https://docs.aws.amazon.com/serverlessrepo/latest/devguide/what-is-serverlessrepo.html Check out this AWS SAM Cheat Sheet: https://tutorialsdojo.com/aws-serverless-application-model-sam/"},{question:"An application is sending thousands of log files to an S3 bucket everyday. The request to retrieve the list of objects using the AWS CLI aws s3api list-objects command is timing out due to the high volume of data being fetched. In order to rectify this issue, you have to use pagination to control the number of results returned on your request. Which of the following parameters should you include in CLI command for this scenario? (Select TWO.)",answers:[{text:"--page-size",isCorrect:!0},{text:"--max-items",isCorrect:!0},{text:"--exclude",isCorrect:!1},{text:"--summarize",isCorrect:!1},{text:"--size-only",isCorrect:!1}],explanation:'For commands that can return a large list of items, the AWS Command Line Interface (AWS CLI) adds three options that you can use to control the number of items included in the output when the AWS CLI calls a service\'s API to populate the list. By default, the AWS CLI uses a page size of 1000 and retrieves all available items. If you see issues when running list commands on a large number of resources, the default page size of 1000 might be too high. This can cause calls to AWS services to exceed the maximum allowed time and generate a "timed out" error. You can use the --page-size option to specify that the AWS CLI request a smaller number of items from each call to the AWS service. The CLI still retrieves the full list, but performs a larger number of service API calls in the background and retrieves a smaller number of items with each call. This gives the individual calls a better chance of succeeding without a timeout. To include fewer items at a time in the AWS CLI output, use the --max-items option. The AWS CLI still handles pagination with the service as described above, but prints out only the number of items at a time that you specify. If the number of items output is fewer than the total number of items returned by the underlying API calls, the output includes a NextToken that you can pass to a subsequent command to retrieve the next set of items. Hence, the correct ones that you should include in the AWS CLI command are the --page-size and --max-items parameters. The --size-only parameter is incorrect because this just accepts a boolean value and is typically used along with "s3 sync" command. It makes the size of each key the only criteria to use to decide whether to sync from source to destination. The --exclude parameter is incorrect because it simply makes Amazon S3 exclude all files or objects that match a specified pattern from the result of the command. The --summary parameter is incorrect because this only displays the summary information (number of objects, total size) of objects returned from an "s3 ls" command. References: https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html https://docs.aws.amazon.com/cli/latest/reference/s3/index.html#cli-aws-s3'},{question:"An application has recently been migrated from an on-premises data center to a development Elastic Beanstalk environment. A developer will do iterative tests and therefore needs to deploy code changes and view them as quickly as possible. Which of the following options take the LEAST amount of time to complete the deployment?",answers:[{text:"Rolling with additional batch",isCorrect:!1},{text:"All at once",isCorrect:!0},{text:"Immutable",isCorrect:!1},{text:"Rolling",isCorrect:!1}],explanation:"In ElasticBeanstalk, you can choose from a variety of deployment methods: -All at once – Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time while the deployment occurs. This is the method that provides the least amount of time for deployment. -Rolling – Deploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch. -Rolling with additional batch – Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during the deployment process. -Immutable – Deploy the new version to a fresh group of instances by performing an immutable update. -Blue/Green - Deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly. Refer to the table below for the characteristics of each deployment method as well as the amount of time it takes to do the deployment, as seen in the Deploy Time column: Hence, the correct answer in this scenario is All at once. Rolling is incorrect because this will deploy the new version in batches only to existing instances, without provisioning new resources. Immutable is incorrect because this will deploy the new version to a fresh group of instances by performing an immutable update. Considering the time to deploy additional instances and installing the new version, this option does not provide the least amount of deployment time. Rolling with additional batch is incorrect because this just deploys the new version in batches. It first launches a new batch of instances to ensure full capacity during the deployment process then proceeds in deploying the new version. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/"},{question:"You are developing a serverless application in AWS composed of several Lambda functions and a DynamoDB database. The requirement is to process the requests asynchronously. Which of the following is the MOST suitable way to accomplish this?",answers:[{text:"Use the Invoke API to call the Lambda function and set the invocation type request parameter to RequestResponse.",isCorrect:!1},{text:"Use the InvokeAsync API to call the Lambda function and set the invocation type request parameter to RequestResponse.",isCorrect:!1},{text:"Use the Invoke API to call the Lambda function and set the invocation type request parameter to Event.",isCorrect:!0},{text:"Use the InvokeAsync API to call the Lambda function and set the invocation type request parameter to Event.",isCorrect:!1}],explanation:"AWS Lambda supports synchronous and asynchronous invocation of a Lambda function. You can control the invocation type only when you invoke a Lambda function (referred to as on-demand invocation). The following examples illustrate on-demand invocations: -Your custom application invokes a Lambda function. -You manually invoke a Lambda function (for example, using the AWS CLI) for testing purposes. In both cases, you invoke your Lambda function using the Invoke operation, and you can specify the invocation type as synchronous or asynchronous. When you use AWS services as a trigger, the invocation type is predetermined for each service. You have no control over the invocation type that these event sources use when they invoke your Lambda function. In the Invoke API, you have 3 options to choose from for the InvocationType: RequestResponse (default) - Invoke the function synchronously. Keep the connection open until the function returns a response or times out. The API response includes the function response and additional data. Event - Invoke the function asynchronously. Send events that fail multiple times to the function's dead-letter queue (if it's configured). The API response only includes a status code. DryRun - Validate parameter values and verify that the user or role has permission to invoke the function. Hence, the correct answer is the option that says: Use the Invoke API to call the Lambda function and set the invocation type request parameter to Event. The option that says: Use the InvokeAsync API to call the Lambda function and set the invocation type request parameter to RequestResponse is incorrect because the InvokeAsync API is already deprecated. In addition, using the RequestResponse type will invoke the Lambda function synchronously. The option that says: Use the Invoke API to call the Lambda function and set the invocation type request parameter to RequestResponse is incorrect because this is the default value of the invocation type that will invoke the Lambda function synchronously. The option that says: Use the InvokeAsync API to call the Lambda function and set the invocation type request parameter to Event is incorrect. Although it uses the correct invocation type, the InvokeAsync API that it uses is already deprecated. Reference: https://docs.aws.amazon.com/lambda/latest/dg/invocation-options.html https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A company is re-architecting its legacy application to use AWS Lambda and DynamoDB. The table is provisioned to have 10 read capacity units, and each item has a size of 4 KB. How many eventual and strong consistent read requests can the table handle per second?",answers:[{text:"10 strongly consistent reads and 10 eventually consistent reads per second",isCorrect:!1},{text:"10 strongly consistent reads and 20 eventually consistent reads per second",isCorrect:!0},{text:"20 strongly consistent reads and 10 eventually consistent reads per second",isCorrect:!1},{text:"5 strongly consistent reads and 20 eventually consistent reads per second",isCorrect:!1}],explanation:"One read request unit represents one strongly consistent read request, or two eventually consistent read requests, for an item up to 4 KB in size. Transactional read requests require 2 read request units to perform one read for items up to 4 KB. If you need to read an item that is larger than 4 KB, DynamoDB needs additional read request units. The total number of read request units required depends on the item size, and whether you want an eventually consistent or strongly consistent read. For example, suppose that you create a table with 10 provisioned read capacity units. This allows you to perform 10 strongly consistent reads per second, or 20 eventually consistent reads per second, for items up to 4 KB. Reading an item larger than 4 KB consumes more read capacity units. For example, a strongly consistent read of an item that is 8 KB (4 KB \xd7 2) consumes 2 read capacity units. An eventually consistent read on that same item consumes only 1 read capacity unit. Item sizes for reads are rounded up to the next 4 KB multiple. For example, reading a 3,500-byte item consumes the same throughput as reading a 4 KB item. To get the number of strong and eventual consistent read requests that your table can accommodate per second, you simply have to do the following steps: Step #1 Multiply the value of the provisioned RCU by 4 KB = 10 RCU x 4 KB = 40 Step #2 To get the number of strong consistency requests, just divide the result of step 1 by 4 KB = 40 / 4 KB = 10 strongly consistent read requests Step #3 To get the number of eventual consistency requests, just divide the result of step 1 by 2 KB =40 / 2 KB = 20 eventually consistent read requests Hence, the correct answer is that the table can handle 10 strongly consistent reads and 20 eventually consistent reads per second. 10 strongly consistent reads and 10 eventually consistent reads per second is incorrect. Although the former value is correct, the latter one is not. Take note that one strongly consistent read request is equivalent to two eventually consistent read request as these two consistency types are quite different from each other. 5 strongly consistent reads and 20 eventually consistent reads per second is incorrect. Although the latter value is correct, the former one is not. If the scenario says that it uses transaction read requests, then it is correct that it will provide 5 strongly consistent reads. However, it is explicitly mentioned to get both strong and eventual consistency. 20 strongly consistent reads and 10 eventually consistent reads per second is incorrect because it should be the other way around. The table can provide 10 strongly consistent reads and 20 eventually consistent reads per second. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ItemSizeCalculations.Reads Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ Calculating the Required Read and Write Capacity Unit for your DynamoDB Table: https://tutorialsdojo.com/calculating-the-required-read-and-write-capacity-unit-for-your-dynamodb-table/"},{question:"A company has a central data repository in Amazon S3 that needs to be accessed by developers belonging to different AWS accounts. The required IAM role has been created with the appropriate S3 permissions. Given that the developers mostly interact with S3 via APIs, which API should the developers call to use the IAM role?",answers:[{text:"GetSessionToken",isCorrect:!1},{text:"AssumeRoleWithSAML",isCorrect:!1},{text:"AssumeRoleWithWebIdentity",isCorrect:!1},{text:"AssumeRole",isCorrect:!0}],explanation:"A role specifies a set of permissions that you can use to access AWS resources. In that sense, it is similar to an IAM User. A principal (person or application) assumes a role to receive temporary permissions to carry out required tasks and interact with AWS resources. The role can be in your own account or any other AWS account. To assume a role, an application calls the AWS STS AssumeRole API operation and passes the ARN of the role to use. The operation creates a new session with temporary credentials. This session has the same permissions as the identity-based policies for that role. The given problem is an example of a scenario where cross-account access is required. For cross-account access, imagine that you own multiple accounts and need to access resources in a particular account. You could create long-term credentials in each account to access those resources. However, managing all those credentials and remembering which one can access which account can be time-consuming. Instead, you can create one set of long-term credentials in one account. Then use temporary security credentials to access all the other accounts by assuming roles in those accounts. Thus, the correct answer is AssumeRole. AssumeRoleWithWebIdentity is incorrect because this only returns a set of temporary security credentials for federated users who are authenticated through public identity providers such as Amazon, Facebook, Google, or OpenID, which were not mentioned in the scenario. The AssumeRole API is good for account access AssumeRoleWithSAML is incorrect because this just returns a set of temporary security credentials for users who have been authenticated via a SAML authentication response. This operation provides a mechanism for tying an enterprise identity store or directory to role-based AWS access without user-specific credentials or configuration. GetSessionToken is incorrect because this is primarily used to return a set of temporary credentials for an AWS account or IAM user only. References: https://aws.amazon.com/blogs/security/how-to-use-a-single-iam-user-to-easily-access-all-your-accounts-by-using-the-aws-cli/ https://aws.amazon.com/premiumsupport/knowledge-center/s3-instance-access-bucket/ https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html Check out this AWS Identity & Access Management (IAM) Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"},{question:"A developer is preparing the application specification (AppSpec) file in CodeDeploy, which will be used to deploy her Lambda functions to AWS. In the deployment, she needs to configure CodeDeploy to run a task before the traffic is shifted to the deployed Lambda function version. Which deployment lifecycle event should she configure in this scenario?",answers:[{text:"BeforeInstall",isCorrect:!1},{text:"BeforeAllowTraffic",isCorrect:!0},{text:"Start",isCorrect:!1},{text:"Install",isCorrect:!1}],explanation:"The content in the 'hooks' section of the AppSpec file varies, depending on the compute platform for your deployment. The 'hooks' section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts. The 'hooks' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment. An AWS Lambda hook is one Lambda function specified with a string on a new line after the name of the lifecycle event. Each hook is executed once per deployment. Following are descriptions of the hooks that are available for use in your AppSpec file. BeforeAllowTraffic – Use to run tasks before traffic is shifted to the deployed Lambda function version. AfterAllowTraffic – Use to run tasks after all traffic is shifted to the deployed Lambda function version. In a serverless Lambda function version deployment, event hooks run in the following order: ake note that Start, AllowTraffic, and End events in the deployment cannot be scripted, which is why they appear in gray in the above diagram. Hence, the correct answer is BeforeAllowTraffic. Start is incorrect because this deployment lifecycle event in Lambda cannot be scripted or used. In this scenario, the correct event that you should configure is the BeforeAllowTraffic event. BeforeInstall is incorrect because this event is only applicable for ECS, EC2 or On-Premises compute platforms and not for Lambda deployments. Install is incorrect because this uses the CodeDeploy agent to copy the revision files from the temporary location to the final destination folder of the EC2 or On-Premises server. This deployment lifecycle event is not available for Lambda as well as for ECS deployments. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-lambda https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-steps.html#deployment-steps-lambda Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/"},{question:"A company has a global multi-player game with a multi-master DynamoDB database topology which stores data in multiple AWS regions. You were assigned to develop a real-time data analytics application which will track and store the recent changes on all the tables from various regions. Only the new data of the recently updated item is needed to be tracked by your application. Which of the following is the MOST suitable way to configure the data analytics application to detect and retrieve the updated database entries automatically?",answers:[{text:"Enable DynamoDB Streams and set the value of StreamViewType to NEW_AND_OLD_IMAGE. Use Kinesis Adapter in the application to consume streams from DynamoDB.",isCorrect:!1},{text:"Enable DynamoDB Streams and set the value of StreamViewType to NEW_IMAGE. Use Kinesis Adapter in the application to consume streams from DynamoDB.",isCorrect:!0},{text:"Enable DynamoDB Streams and set the value of StreamViewType to NEW_IMAGE. Create a trigger in AWS Lambda to capture stream data and forward it to your application.",isCorrect:!1},{text:"Enable DynamoDB Streams and set the value of StreamViewType to NEW_AND_OLD_IMAGE. Create a trigger in AWS Lambda to capture stream data and forward it to your application.",isCorrect:!1}],explanation:"DynamoDB Streams provides a time-ordered sequence of item-level changes in any DynamoDB table. The changes are de-duplicated and stored for 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real time. The Kinesis Adapter is the recommended way to consume streams from DynamoDB for real-time processing. The DynamoDB Streams API is intentionally similar to that of Kinesis Streams, a service for real-time processing of streaming data at a massive scale. You can write applications for Kinesis Streams using the Kinesis Client Library (KCL). The KCL simplifies coding by providing useful abstractions above the low-level Kinesis Streams API. As a DynamoDB Streams user, you can leverage the design patterns found within the KCL to process DynamoDB Streams shards and stream records. To do this, you use the DynamoDB Streams Kinesis Adapter. The Kinesis Adapter implements the Kinesis Streams interface, so that the KCL can be used for consuming and processing records from DynamoDB Streams. When an item in the table is modified, StreamViewType determines what information is written to the stream for this table. Valid values for StreamViewType are: KEYS_ONLY - Only the key attributes of the modified item are written to the stream. NEW_IMAGE - The entire item, as it appears after it was modified, is written to the stream. OLD_IMAGE - The entire item, as it appeared before it was modified, is written to the stream. NEW_AND_OLD_IMAGES - Both the new and the old item images of the item are written to the stream. Hence, the correct answer is: Enable DynamoDB Streams and set the value of StreamViewType to NEW_IMAGE then use Kinesis Adapter in the application to consume streams from DynamoDB. The option that says: Enable DynamoDB Streams and set the value of StreamViewType to NEW_AND_OLD_IMAGE. Create a trigger in AWS Lambda to capture stream data and forward it to your application is incorrect. Using Lambda for real-time data analytics is not a suitable solution for this scenario since it reads records in batches. A more appropriate service to use is the Kinesis service. In addition, using the StreamViewType of NEW_AND_OLD_IMAGE is wrong since this will send both the old and the new values of the item. Remember that it is specifically mentioned in the scenario that only the new values should be tracked. The option that says: Enable DynamoDB Streams and set the value of StreamViewType to NEW_IMAGE. Create a trigger in AWS Lambda to capture stream data and forward it to your application is incorrect because just like what is mentioned above, it is better to use Kinesis instead of Lambda for the real-time data analytics application. The option that says: Enable DynamoDB Streams and set the value of StreamViewType to NEW_AND_OLD_IMAGE. Use Kinesis Adapter in the application to consume streams from DynamoDB is incorrect because this will send both the old and the new values of the item to the data analytics application. The correct StreamViewType to use here should be NEW_IMAGE. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.KCLAdapter.html https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_StreamSpecification.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ AWS Lambda Integration with Amazon DynamoDB Streams: https://tutorialsdojo.com/aws-lambda-integration-with-amazon-dynamodb-streams/"},{question:"A prototype application is hosted in an EC2 instance, which has an assigned IAM Role to store data from both the development and production S3 buckets. The instance also has AWS CLI access/secret key installed to handle other ad hoc tasks. You assigned a new IAM Role to the instance which has the permission to access the development bucket only. However, upon testing, the instance can still store files to both buckets. What is the MOST likely root cause of this issue?",answers:[{text:"Due to eventual consistency, you must wait 24 hours for the change to appear across all of AWS.",isCorrect:!1},{text:"The new IAM Role has an attached inline policy.",isCorrect:!1},{text:"The instance profile role of a running EC2 instance is static and can't be replaced at all.",isCorrect:!1},{text:"The application is still using the IAM role that is configured for the AWS CLI key.",isCorrect:!0}],explanation:"Applications that run on an EC2 instance must include AWS credentials in their AWS API requests. You could have your developers store AWS credentials directly within the EC2 instance and allow applications in that instance to use those credentials. But developers would then have to manage the credentials and ensure that they securely pass the credentials to each instance and update each EC2 instance when it's time to rotate the credentials. That's a lot of additional work. Instead, you can and should use an IAM role to manage temporary credentials for applications that run on an EC2 instance. When you use a role, you don't have to distribute long-term credentials (such as a username and password or access keys) to an EC2 instance. Instead, the role supplies temporary permissions that applications can use when they make calls to other AWS resources. When you launch an EC2 instance, you specify an IAM role to associate with the instance. Applications that run on the instance can then use the role-supplied temporary credentials to sign API requests. Using roles to grant permissions to applications that run on EC2 instances requires a bit of extra configuration. An application running on an EC2 instance is abstracted from AWS by the virtualized operating system. Because of this extra separation, an additional step is needed to assign an AWS role and its associated permissions to an EC2 instance and make them available to its applications. This extra step is the creation of an instance profile that is attached to the instance. The instance profile contains the role and can provide the role's temporary credentials to an application that runs on the instance. Those temporary credentials can then be used in the application's API calls to access resources and to limit access to only those resources that the role specifies. Note that only one role can be assigned to an EC2 instance at a time, and all applications on the instance share the same role and permissions. Using roles in this way has several benefits. Because role credentials are temporary and rotated automatically, you don't have to manage credentials, and you don't have to worry about long-term security risks. In addition, if you use a single role for multiple instances, you can make a change to that one role and the change is propagated automatically to all the instances. If you use the IAM console, the instance profile is managed for you and is mostly transparent to you. However, if you use the AWS CLI or API to create and manage the role and EC2 instance, then you must create the instance profile and assign the role to it as separate steps. Then, when you launch the instance, you must specify the instance profile name instead of the role name. In this scenario, the instance has both an attached IAM Role and AWS CLI. Although the Instance Profile role has been updated to only access the development environment, the application running in the instance might still use the access credentials or the old IAM Role that is attached in the AWS CLI. Take note that you have to configure both of your Instance Profile and AWS CLI in this scenario. Hence, the correct answer is that the application is still using the IAM role that is configured for the AWS CLI Key. The option that says: the new IAM Role has an attached inline policy is incorrect because an inline policy is just a policy that's embedded in a principal entity (a user, group, or role). This is irrelevant in this scenario as the main issue here is the profile/IAM Role that still exists and used, by the AWS CLI. The option that says: due to eventual consistency, you must wait 24 hours for the change to appear across all of AWS is incorrect because although there is eventual consistency in EC2, there is no exact time limit for the changes to be reflected, which is contrary to what this option is saying. Moreover, the existing AWS CLI profile would be a more compelling root cause in this scenario rather than what this option suggests. The option that says: the instance profile role of a running EC2 instance is static and can't be replaced at all is incorrect because the instance profile role can be changed anytime. You can remove the existing role and then add a different role to an instance profile. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html Check out this AWS Identity & Access Management (IAM) Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"},{question:"A serverless application, which is composed of multiple Lambda functions, has been deployed using AWS SAM. A developer was instructed to easily manage the deployments of the functions using CodeDeploy. When there is a new deployment, 10 percent of the incoming traffic should be shifted to the new version every 10 minutes until all traffic is shifted from the old version. What should the developer do to properly deploy the functions that satisfies this requirement?",answers:[{text:"Deploy the functions using a Linear deployment configuration.",isCorrect:!0},{text:"Deploy the functions using a Canary deployment configuration.",isCorrect:!1},{text:"Deploy the functions using an All-at-once deployment configuration.",isCorrect:!1},{text:"Deploy the functions using an Immutable deployment configuration.",isCorrect:!1}],explanation:"CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can use CodeDeploy. CodeDeploy supports the following deployment configurations: -In-place (for EC2/On-premises) - the application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. -Canary (for Lambda/ECS) - traffic is shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function or ECS task set in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment. -Linear (for Lambda/ECS) - traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment. -All-at-once (for Lambda/ECS) - all traffic is shifted from the original Lambda function or ECS task set to the updated function or task set all at once. In a Linear deployment configuration, the traffic will be shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment. Hence, the is the correct answer is: Deploy the functions using a Linear deployment configuration. The option that says: Deploy the functions using a Canary deployment configuration is incorrect because this will cause the traffic to be shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment. The option that says: Deploy the functions using an All-at-once deployment configuration is incorrect because, with this deployment configuration, the traffic is shifted from the original Lambda function to the updated Lambda function version all at once. The option that says: Deploy the functions using an Immutable deployment configuration is incorrect because this is only applicable in Elastic Beanstalk and not to Lambda. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/"},{question:"A developer is building the cloud architecture of an application which will be hosted in a large EC2 instance. The application will process the data and it will upload results to an S3 bucket. Which of the following is the SAFEST way to implement this architecture?",answers:[{text:"Use an IAM Inline Policy to grant the application the necessary permissions to upload data to S3.",isCorrect:!1},{text:"Use an IAM Role to grant the application the necessary permissions to upload data to S3.",isCorrect:!0},{text:"Install the AWS CLI then use it to upload the results to S3.",isCorrect:!1},{text:"Store the access keys in the instance then use the AWS SDK to upload the results to S3.",isCorrect:!1}],explanation:"Applications that run on an EC2 instance must include AWS credentials in their AWS API requests. You could have your developers store AWS credentials directly within the EC2 instance and allow applications in that instance to use those credentials. But developers would then have to manage the credentials and ensure that they securely pass the credentials to each instance and update each EC2 instance when it's time to rotate the credentials. That's a lot of additional work. Instead, you can and should use an IAM role to manage temporary credentials for applications that run on an EC2 instance. When you use a role, you don't have to distribute long-term credentials (such as a username and password or access keys) to an EC2 instance. Instead, the role supplies temporary permissions that applications can use when they make calls to other AWS resources. When you launch an EC2 instance, you specify an IAM role to associate with the instance. Applications that run on the instance can then use the role-supplied temporary credentials to sign API requests. Using roles to grant permissions to applications that run on EC2 instances requires a bit of extra configuration. An application running on an EC2 instance is abstracted from AWS by the virtualized operating system. Because of this extra separation, an additional step is needed to assign an AWS role and its associated permissions to an EC2 instance and make them available to its applications. This extra step is the creation of an instance profile that is attached to the instance. The instance profile contains the role and can provide the role's temporary credentials to an application that runs on the instance. Those temporary credentials can then be used in the application's API calls to access resources and to limit access to only those resources that the role specifies. Note that only one role can be assigned to an EC2 instance at a time, and all applications on the instance share the same role and permissions. Using roles in this way has several benefits. Because role credentials are temporary and rotated automatically, you don't have to manage credentials, and you don't have to worry about long-term security risks. In addition, if you use a single role for multiple instances, you can make a change to that one role and the change is propagated automatically to all the instances. Hence, using an IAM Role to grant the application the necessary permissions to upload data to S3 is the correct answer for this scenario as this provides the safest way to integrate your application hosted in EC2 and S3. Storing the access keys in the instance and then using the AWS SDK to upload the results to S3 is incorrect because this will expose the AWS access credentials to all users who have access to the EC2 instance. Since this option entails a security risk, this is incorrect as is not the safest method. Installing the AWS CLI then using it to upload the results to S3 is incorrect. Although this option is valid, this method also presents a security risk just as shown above. By default, an AWS CLI requires you to store the AWS access keys in your instance which will be used in executing the commands. Hence, this option is incorrect. Using an IAM Inline Policy to grant the application the necessary permissions to upload data to S3 is incorrect because inline policies are useful if you want to maintain a strict one-to-one relationship between a policy and the principal entity that it's applied to. This option doesn't provide a secure way of allowing the application that is hosted in EC2 to upload data to an S3 bucket. You should use an IAM Role instead. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html Check out this AWS Identity & Access Management (IAM) Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"},{question:"A company is heavily using a range of AWS services to host their enterprise applications. Currently, their deployment process still has a lot of manual steps which is why they plan to automate their software delivery process using continuous integration and delivery (CI/CD) pipelines in AWS. They will use CodePipeline to orchestrate each step of their release process and CodeDeploy for deploying applications to various compute platforms in AWS. In this architecture, which of the following are valid considerations when using CodeDeploy? (Select TWO.)",answers:[{text:"CodeDeploy can deploy applications to both your EC2 instances as well as your on-premises servers.",isCorrect:!0},{text:"The CodeDeploy agent communicates using HTTP over port 80.",isCorrect:!1},{text:"AWS Lambda compute platform deployments cannot use an in-place deployment type.",isCorrect:!0},{text:"CodeDeploy can deploy applications to EC2, AWS Lambda, and Amazon ECS only.",isCorrect:!1},{text:"You have to install and use the CodeDeploy agent installed on your EC2 instances and ECS cluster.",isCorrect:!1}],explanation:"CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can use CodeDeploy. CodeDeploy provides two deployment type options: In-place deployment: The application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete. Only deployments that use the EC2/On-Premises compute platform can use in-place deployments. AWS Lambda compute platform deployments cannot use an in-place deployment type. Blue/green deployment: The behavior of your deployment depends on which compute platform you use: - Blue/green on an EC2/On-Premises compute platform: The instances in a deployment group (the original environment) are replaced by a different set of instances (the replacement environment). If you use an EC2/On-Premises compute platform, be aware that blue/green deployments work with Amazon EC2 instances only. - Blue/green on an AWS Lambda compute platform: Traffic is shifted from your current serverless environment to one with your updated Lambda function versions. You can specify Lambda functions that perform validation tests and choose the way in which the traffic shift occurs. All AWS Lambda compute platform deployments are blue/green deployments. For this reason, you do not need to specify a deployment type. - Blue/green on an Amazon ECS compute platform: Traffic is shifted from the task set with the original version of a containerized application in an Amazon ECS service to a replacement task set in the same service. The protocol and port of a specified load balancer listener are used to reroute production traffic. During deployment, a test listener can be used to serve traffic to the replacement task set while validation tests are run. The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The CodeDeploy agent communicates outbound using HTTPS over port 443. It is also important to note that the CodeDeploy agent is required only if you deploy to an EC2/On-Premises compute platform. The agent is not required for deployments that use the Amazon ECS or AWS Lambda compute platform. Therefore, the valid considerations in CodeDeploy in this scenario are: - AWS Lambda compute platform deployments cannot use an in-place deployment type. - CodeDeploy can deploy applications to both your EC2 instances as well as your on-premises servers. The option that says: CodeDeploy can deploy applications to EC2, AWS Lambda, and Amazon ECS only is incorrect because it can also deploy to your on-premises servers. The option that says: The CodeDeploy agent communicates using HTTP over port 80 is incorrect because it is actually using HTTPS over port 443 and not HTTP. The option that says: You have to install and use the CodeDeploy agent installed on your EC2 instances and ECS cluster is incorrect. Although this statement is true for EC2 instances, it is wrong for the latter as the CodeDeploy agent is not required for deployments that use the Amazon ECS or AWS Lambda compute platform. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/ Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/"},{question:"Due to the popularity of serverless computing, your manager instructed you to share your technical expertise to the whole software development department of your company. You are planning to deploy a simple Node.js 'Hello World' Lambda function to AWS using CloudFormation. Which of the following is the EASIEST way of deploying the function to AWS?",answers:[{text:"Include your function source inline in the Code parameter of the AWS::Lambda::Function resource in the CloudFormation template.",isCorrect:!1},{text:"Include your function source inline in the ZipFile parameter of the AWS::Lambda::Function resource in the CloudFormation template.",isCorrect:!0},{text:"Upload the code in S3 then specify the S3Key and S3Bucket parameters under the AWS::Lambda::Function resource in the CloudFormation template.",isCorrect:!1},{text:"Upload the code in S3 as a ZIP file then specify the S3 path in the ZipFile parameter of the AWS::Lambda::Function resource in the CloudFormation template.",isCorrect:!1}],explanation:'To create a Lambda function, you first create a Lambda function deployment package, a .zip or .jar file consisting of your code and any dependencies. When creating the zip, include only the code and its dependencies, not the containing folder. You will then need to set the appropriate security permissions for the zip package. If you are using a CloudFormation template, you can configure the AWS::Lambda::Function resource which creates a Lambda function. To create a function, you need a deployment package and an execution role. The deployment package contains your function code. The execution role grants the function permission to use AWS services, such as Amazon CloudWatch Logs for log streaming and AWS X-Ray for request tracing. Under the AWS::Lambda::Function resource, you can use the Code property which contains the deployment package for a Lambda function. For all runtimes, you can specify the location of an object in Amazon S3. For Node.js and Python functions, you can specify the function code inline in the template. Changes to a deployment package in Amazon S3 are not detected automatically during stack updates. To update the function code, change the object key or version in the template. Hence, including your function source inline in the ZipFile parameter of the AWS::Lambda::Function resource in the CloudFormation template is the easiest way to deploy the Lambda function to AWS. Uploading the code in S3 then specifying the S3Key and S3Bucket parameters under the AWS::Lambda::Function resource in the CloudFormation template is incorrect. Although this is a valid deployment step, you still have to upload the code in S3 instead of just including the function source inline in the ZipFile parameter. Take note that the scenario explicitly mentions that you have to pick the easiest way. Including your function source inline in the Code parameter of the AWS::Lambda::Function resource in the CloudFormation template is incorrect because you should use the ZipFile parameter instead. Take note that the Code property is the parent property of the ZipFile parameter. Uploading the code in S3 as a ZIP file then specifying the S3 path in the ZipFile parameter of the AWS::Lambda::Function resource in the CloudFormation template is incorrect because contrary to its name, the ZipFile parameter directly accepts the source code of your Lambda function and not an actual zip file. If you include your function source inline with this parameter, AWS CloudFormation places it in a file named index and zips it to create a deployment package. This is the reason why it is called the "ZipFile" parameter. References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-function.html Check out this AWS CloudFormation Cheat Sheet: https://tutorialsdojo.com/aws-cloudformation/'},{question:"A leading commercial bank has an online banking portal that is hosted in an Auto Scaling group of EC2 instances with an Application Load Balancer in front to distribute the incoming traffic. The application has been instrumented, and the X-Ray daemon has been installed in all instances to allow debugging and troubleshooting using AWS X-Ray. In this architecture, from which source will AWS X-Ray fetch the client IP address?",answers:[{text:"From the X-Forwarded-Host header of the request.",isCorrect:!1},{text:"From the source IP of the IP packet.",isCorrect:!1},{text:"From the X-Forwarded-For header of the request.",isCorrect:!0},{text:"From the ipAddress query parameter of the request if it exists.",isCorrect:!1}],explanation:"AWS X-Ray receives data from services as segments. X-Ray then groups segments that have a common request into traces. X-Ray processes the traces to generate a service graph that provides a visual representation of your application. The compute resources running your application logic send data about their work as segments. A segment provides the resource's name, details about the request, and details about the work done. For example, when an HTTP request reaches your application, it can record the following data about: The host – hostname, alias or IP address The request – method, client address, path, user agent The response – status, content The work done – start and end times, subsegments Issues that occur – errors, faults and exceptions, including automatic capture of exception stacks. The X-Ray SDK gathers information from request and response headers, the code in your application, and metadata about the AWS resources on which it runs. You choose the data to collect by modifying your application configuration or code to instrument incoming requests, downstream requests, and AWS SDK clients. If a load balancer or other intermediary forwards a request to your application, X-Ray takes the client IP from the X-Forwarded-For header in the request instead of from the source IP in the IP packet. The client IP that is recorded for a forwarded request can be forged, so it should not be trusted. Hence, the correct answer in this scenario is that, AWS X-Ray will fetch the client IP address from the X-Forwarded-For header of the request. The option that says: from the X-Forwarded-Host header of the request is incorrect because this header is primarily used in identifying the original host where the request originated from and not the IP address. The option that says: from the ipAddress query parameter of the request if it exists is incorrect because query parameters are primarily used in the application layer and not for the network layer. Take note that AWS X-Ray uses the X-Forwarded-For header of the request and not any query parameter. The option that says: from the source IP of the IP packet is incorrect because AWS X-Ray uses the X-Forwarded-For header of the request and not the source IP of the IP packet since the application is using an Application Load Balancer. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html https://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/"},{question:"A company has a website hosted in a multicontainer Docker environment in Elastic Beanstalk. There is a requirement to integrate the website with API Gateway, where it simply passes client-submitted method requests to the backend. It is important that the client and backend interact directly with no intervention from API Gateway after the API method is set up, except for known issues such as unsupported characters. Which of the following integration types is the MOST suitable one to use to meet this requirement?",answers:[{text:"AWS",isCorrect:!1},{text:"HTTP_PROXY",isCorrect:!0},{text:"HTTP",isCorrect:!1},{text:"AWS_PROXY",isCorrect:!1}],explanation:'You can integrate an API method in your API Gateway with a custom HTTP endpoint of your application in two ways: - HTTP proxy integration - HTTP custom integration In your API Gateway console, you can define the type of HTTP integration of your resource by checking, or not checking, the "Configure as proxy resource" checkbox. For example, this API Resource configuration is a type of HTTP Proxy integration since the appropriate checkbox is ticked: With proxy integration, the setup is simple. You only need to set the HTTP method and the HTTP endpoint URI, according to the backend requirements, if you are not concerned with content encoding or caching. With custom integration, setup is more involved. In addition to the proxy integration setup steps, you need to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. API Gateway supports the following endpoint ports: 80, 443 and 1024-65535. Programmatically, you choose an integration type by setting the type property on the Integration resource. For the Lambda proxy integration, the value is AWS_PROXY. For the Lambda custom integration and all other AWS integrations, it is AWS. For the HTTP proxy integration and HTTP integration, the value is HTTP_PROXY and HTTP, respectively. For the mock integration, the type value is MOCK. Since the integration type that is being described in the scenario fits the definition of an HTTP proxy integration, the correct answer in this scenario is to use the HTTP_PROXY integration type. AWS is incorrect because this type is only used for Lambda custom integration. Take note that the scenario uses an application hosted in EC2 and not in Lambda. AWS_PROXY is incorrect because this type is primarily used for Lambda proxy integration. The scenario didn\'t mention that it uses a serverless application or Lambda. HTTP is incorrect because this type is only used for HTTP custom integration where you need to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/setup-http-integrations.html https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/'},{question:"You are developing a serverless application in AWS in which you have to control the code execution performance and costs of your Lambda functions. There is a requirement to increase the CPU available to your function in order to efficiently process records from an Amazon Kinesis data stream. Which of the following is the BEST way to meet this requirement?",answers:[{text:"Use Lambda@Edge.",isCorrect:!1},{text:"Configure the function to use unreserved account concurrency.",isCorrect:!1},{text:"Increase the concurrent execution limit of the function.",isCorrect:!1},{text:"Increase the allocated memory of the function.",isCorrect:!0}],explanation:"Lambda counts a request each time it starts executing in response to an event notification or invocation call, including test invokes from the console. You are charged based on the total number of requests processed across all of your Lambda functions. Duration is calculated from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 1ms. The price depends on the amount of memory you allocate to your function. In the AWS Lambda resource model, you choose the amount of memory you want for your function and are allocated proportional CPU power and other resources. An increase in memory size triggers an equivalent increase in CPU available to your function. Hence, the correct answer is: Increase the allocated memory of the function. The option that says: Configure the function to use unreserved account concurrency is incorrect because this configuration is primarily used for managing the number of simultaneous executions of your function as well as the capacity reservations for that concurrency level. The option that says: Increase the concurrent execution limit of the function is incorrect because this will just limit the number of simultaneous executions of your function and not increase the allocated CPU. The option that says: Use Lambda@Edge is incorrect because this is actually a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency. This will not increase the CPU of your function hence, this option does not meet the requirement. References: https://docs.aws.amazon.com/lambda/latest/dg/configuration-function-common.html https://aws.amazon.com/lambda/pricing/ Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A developer has instrumented an application using the X-Ray SDK to collect all data about the requests that an application serves. There is a new requirement to develop a custom debug tool which will enable them to view the full traces of their application without using the X-Ray console. What should the developer do to accomplish this task?",answers:[{text:"Use the BatchGetTraces API to get the list of trace IDs of the application and then retrieve the list of traces using GetTraceSummaries API.",isCorrect:!1},{text:"Use the GetServiceGraph API to get the list of trace IDs of the application and then retrieve the list of traces using GetTraceSummaries API.",isCorrect:!1},{text:"Use the GetTraceSummaries API to get the list of trace IDs of the application and then retrieve the list of traces using BatchGetTraces API.",isCorrect:!0},{text:"Use the GetGroup API to get the list of trace IDs of the application and then retrieve the list of traces using BatchGetTraces API.",isCorrect:!1}],explanation:"X-Ray compiles and processes segment documents to generate queryable trace summaries and full traces that you can access by using the GetTraceSummaries and BatchGetTraces APIs, respectively. In addition to the segments and subsegments that you send to X-Ray, the service uses information in subsegments to generate inferred segments and adds them to the full trace. Inferred segments represent downstream services and resources in the service map. In this scenario, the developer should use the GetTraceSummaries API to get the list of trace IDs of the application and then retrieve the list of traces using BatchGetTraces API in order to develop the custom debug tool The option that says: Use the GetGroup API to get the list of trace IDs of the application and then retrieving the list of traces using BatchGetTraces API is incorrect because the GetGroup API just retrieves the group resource details. The option that says: Use the GetServiceGraph API to get the list of trace IDs of the application and then retrieving the list of traces using GetTraceSummaries API is incorrect because the GetServiceGraph API just shows which services process the incoming requests, including the downstream services that they call as a result. In addition, you have to use the BatchGetTraces API instead of the GetTraceSummaries API to retrieve the list of traces. The option that says: Use the BatchGetTraces API to get the list of trace IDs of the application and then retrieving the list of traces using GetTraceSummaries API is incorrect because it should be the other way around. You have to use the GetTraceSummaries API to get the list of trace IDs of the application and then use its result as an input parameter to retrieve the list of traces using the BatchGetTraces API. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html https://docs.aws.amazon.com/xray/latest/api/API_BatchGetTraces.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/"},{question:"A Java web application built using AWS SDK for Java with a DynamoDB database is concurrently accessed by thousands of users during peak time. The application is highly write-intensive and there are a lot of incidents where it overwrites stale data from the DynamoDB table. How can you ensure your database writes are protected from being overwritten by other write operations that are occurring at the same time without affecting the application performance?",answers:[{text:"Implement optimistic locking with version number.",isCorrect:!0},{text:"Implement overly optimistic locking (OOL).",isCorrect:!1},{text:"Implement pessimistic locking with write locking.",isCorrect:!1},{text:"Implement pessimistic locking with read locking.",isCorrect:!1}],explanation:"Databases employ locking mechanisms to ensure that data is always updated to the latest version and is concurrent. There are multiple types of locking strategies that benefit different use cases. Some of these are: - Optimistic Locking - Pessimistic Locking - Overly Optimistic Locking Optimistic locking is a strategy to ensure that the client-side item that you are updating (or deleting) is the same as the item in DynamoDB. If you use this strategy, then your database writes are protected from being overwritten by the writes of others — and vice-versa. With optimistic locking, each item has an attribute that acts as a version number. If you retrieve an item from a table, the application records the version number of that item. You can update the item, but only if the version number on the server side has not changed. If there is a version mismatch, it means that someone else has modified the item before you did; the update attempt fails, because you have a stale version of the item. If this happens, you simply try again by retrieving the item and then attempting to update it. Optimistic locking prevents you from accidentally overwriting changes that were made by others; it also prevents others from accidentally overwriting your changes. Since the application is already using the AWS SDK for Java, it can support optimistic locking by simply adding the @DynamoDBVersionAttribute annotation to the objects. In the mapping class for your table, you designate one property to store the version number, and mark it using this annotation. When you save an object, the corresponding item in the DynamoDB table will have an attribute that stores the version number. The DynamoDBMapper assigns a version number when you first save the object, and it automatically increments the version number each time you update the item. Your update or delete requests will succeed only if the client-side object version matches the corresponding version number of the item in the DynamoDB table. Hence, implementing optimistic locking with version number is the correct answer in this scenario. Implementing pessimistic locking with read locking is incorrect because this type of locking can interrupt user operations. This is an approach where an entity is locked in the database for the entire time that it is in application memory (often in the form of an object). This can prevent certain users from reading, updating, or deleting an entry depending on the lock type. Implementing pessimistic locking with write locking is incorrect because just as explained above, pessimistic locking will significantly affect the performance of your application. Although it will ensure that your data writes are not overwritten on the fly, this type of locking will not meet the performance requirements mentioned in the scenario. Implementing overly optimistic locking (OOL) is incorrect because this strategy is completely inappropriate for multi-user systems since it is used for systems that have only one user or operation performing changes at a single time. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBContext.VersionSupport.html"},{question:"A company has an AWS account with only 2 Lambda functions, which process data and store the results in an S3 bucket. An Application Load Balancer is used to distribute the incoming traffic to the two Lambda functions as registered targets. You noticed that in peak times, the first Lambda function works with optimal performance but the second one is throttling the incoming requests. Which of the following is the MOST likely root cause of this issue?",answers:[{text:"The first function is using the unreserved account concurrency while the second function has been set with a concurrency execution limit of 1000.",isCorrect:!1},{text:"The concurrency execution limit provided to the first function is significantly higher than the second function.",isCorrect:!0},{text:"The concurrency execution limit provided to the first function is less than the second function.",isCorrect:!1},{text:"The first function is using the unreserved account concurrency while the second function has been set with a concurrency execution limit of 800.",isCorrect:!1}],explanation:"The unit of scale for AWS Lambda is a concurrent execution. However, scaling indefinitely is not desirable in all scenarios. For example, you may want to control your concurrency for cost reasons or to regulate how long it takes you to process a batch of events, or to simply match it with a downstream resource. To assist with this, Lambda provides a concurrent execution limit control at both the account level and the function level. The concurrent executions refers to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the concurrent execution count will differ depending on whether or not your Lambda function is processing events from a poll-based event source. If you create a Lambda function to process events from event sources that aren't poll-based (for example, Lambda can process every event from other sources, like Amazon S3 or API Gateway), each published event is a unit of work, in parallel, up to your account limits. Therefore, the number of invocations these event sources make influences the concurrency. If you set the concurrent execution limit for a function, the value is deducted from the unreserved concurrency pool. For example, if your account's concurrent execution limit is 1000 and you have 10 functions, you can specify a limit on one function at 200 and another function at 100. The remaining 700 will be shared among the other 8 functions. AWS Lambda will keep the unreserved concurrency pool at a minimum of 100 concurrent executions, so that functions that do not have specific limits set can still process requests. So, in practice, if your total account limit is 1000, you are limited to allocating 900 to individual functions. The scenario mentioned that an Application Load Balancer is used to distribute the incoming traffic to the two Lambda functions as registered targets, just as shown below: By default, an AWS account's concurrent execution limit is 1000 which will be shared by all Lambda functions. In this scenario, it is highly likely that the first function has more provisioned concurrency than the other one. Hence, the correct answer in this scenario is: the concurrency execution limit provided to the first function is significantly higher than the second function. The option that says: the first function is using the unreserved account concurrency while the second function has been set with a concurrency execution limit of 800 is incorrect because this will actually cause the first function to throttle the requests instead of the second one. The option that says: the concurrency execution limit provided to the first function is less than the second function is incorrect because what is really happening is the other way around: the concurrency execution limit provided to the first function is significantly higher than the second function, which is why the latter is throttling the requests. The option that says: the first function is using the unreserved account concurrency while the second function has been set with a concurrency execution limit of 1000 is incorrect because, in the first place, you cannot set a concurrency execution limit of 1000 since the maximum that you can allocate per function is only 900. Take note if you allocate the maximum concurrency execution to the second function, the unreserved account concurrency will actually just have a value of 100. References: https://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html https://docs.aws.amazon.com/lambda/latest/dg/scaling.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"An online forum requires a new table in DynamoDB named Thread in which the partition key is ForumName and the sort key is Subject. The following diagram shows how the items in the table would be organized: For reporting purposes, the application needs to find all of the threads that have been posted in a particular forum within the last three months. Which of the following is the MOST effective solution that you should implement?",answers:[{text:"Create a global secondary index and use the Query operation to utilize the LastPostDateTime attribute as the sort key.",isCorrect:!1},{text:"Add a local secondary index while creating the new Thread table. Use the Query operation to utilize the LastPostDateTime attribute as the sort key.",isCorrect:!0},{text:"Configure the application to Scan the entire Thread table and discard any posts that were not within the specified time frame.",isCorrect:!1},{text:"Configure the application to Query the entire Thread table and discard any posts that were not within the specified time frame.",isCorrect:!1}],explanation:'DynamoDB supports two types of secondary indexes: - Global secondary index — an index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered "global" because queries on the index can span all of the data in the base table, across all partitions. - Local secondary index — an index that has the same partition key as the base table, but a different sort key. A local secondary index is "local" in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value. A local secondary index maintains an alternate sort key for a given partition key value. A local secondary index also contains a copy of some or all of the attributes from its base table; you specify which attributes are projected into the local secondary index when you create the table. The data in a local secondary index is organized by the same partition key as the base table, but with a different sort key. This lets you access data items efficiently across this different dimension. For greater query or scan flexibility, you can create up to five local secondary indexes per table. Suppose that an application needs to find all of the threads that have been posted within the last three months. Without a local secondary index, the application would have to Scan the entire Thread table and discard any posts that were not within the specified time frame. With a local secondary index, a Query operation could use LastPostDateTime as a sort key and find the data quickly. In the provided scenario, you can create a local secondary index named LastPostIndex to meet the requirements. Note that the partition key is the same as that of the Thread table, but the sort key is LastPostDateTime as shown in the diagram below: Hence, the most effective solution in this scenario is to: Add a local secondary index while creating the new Thread table. Use the Query operation to utilize the LastPostDateTime attribute as the sort key in order to find the data quickly. Configuring the application to Scan the entire Thread table and discarding any posts that were not within the specified time frame is incorrect. Although this option is valid, this solution will consume a large amount of provisioned read-throughput and will take a significant amount of time to complete. This is not a scalable solution, and the time it takes to fetch the data will continue to increase as the table grows. Configuring the application to Query the entire Thread table and discarding any posts that were not within the specified time frame is incorrect because using the Query operation is not sufficient to meet this requirement. You have to create a local secondary index when you create the table to narrow down the results and improve the performance of your application. Creating a global secondary index and using the Query operation to utilize the LastPostDateTime attribute as the sort key is incorrect because using a local secondary index is a more appropriate solution to be used in this scenario. Take note that in this scenario, it is still using the same partition key (ForumName), but with an alternate sort key (LastPostDateTime), which warrants the use of a local secondary index. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html Amazon DynamoDB Overview: https://youtu.be/3ZOyUNIeorU Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/'},{question:"An application is hosted in Elastic Beanstalk, which is currently running in Java 7 runtime environment. A new version of the application is ready to be deployed, and the developer was tasked to upgrade the platform to Java 8 to accommodate the changes. All user traffic must be immediately directed to the new version. If problems arise, the developer should be able to quickly revert to the previous version. Which of the following is the MOST appropriate action that the developer should do to upgrade the platform?",answers:[{text:"Update the environment's platform version to Java 8.",isCorrect:!1},{text:"Perform a Traffic splitting deployment.",isCorrect:!1},{text:"Manually upgrade the Java runtime environment of the EC2 instances in the Elastic Beanstalk environment.",isCorrect:!1},{text:"Perform a Blue/Green Deployment.",isCorrect:!0}],explanation:"Elastic Beanstalk regularly releases new platform versions to update all Linux-based and Windows Server-based platforms. New platform versions provide updates to existing software components and support for new features and configuration options. You can use the Elastic Beanstalk console or the EB CLI to update your environment's platform version. Depending on the platform version you'd like to update to, Elastic Beanstalk recommends one of two methods for performing platform updates. Method 1 – Update your Environment's Platform Version - This is the recommended method when you're updating to the latest platform version, without a change in runtime, web server, or application server versions, and without a change in the major platform version. This is the most common and routine platform update. Method 2 – Perform a Blue/Green Deployment - This is the recommended method when you're updating to a different runtime, web server, or application server version or to a different major platform version. This is a good approach when you want to take advantage of new runtime capabilities or the latest Elastic Beanstalk functionality. Because AWS Elastic Beanstalk performs an in-place update when you update your application versions, your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment and then swap CNAMEs of the two environments to redirect traffic to the new version instantly. Blue/green deployments require that your environment runs independently of your production database if your application uses one. If your environment has an Amazon RDS DB instance attached to it, the data will not transfer over to your second environment and will be lost if you terminate the original environment. Hence, the correct answer is to perform a Blue/Green deployment to safely upgrade the application's runtime environment from Java 7 to Java 8. The option that says: Update the environment's platform version to Java 8 is incorrect because using this method is only recommended when you're updating to the latest platform version without a change in the runtime environment. The option that says: Manually upgrade the Java runtime environment of the EC2 instances in the Elastic Beanstalk environment is incorrect. Although this method will work, this entails a lot of configuration to implement compared with just performing a blue/green deployment. In addition, this method may introduce operational risk because the environment may go down while the developer is doing the updates manually. The option that says: Perform a Traffic splitting deployment is incorrect because the scenario requires all user traffic to be immediately directed towards the new version. Take note that in Traffic splitting, updates are released incrementally to a subset of users. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.platform.upgrade.html#using-features.platform.upgrade.bluegreen https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/ Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"},{question:"You have an application that reads an individual item from a DynamoDB table, modifies it locally, and submits the changes as a new entry to a separate table before proceeding onto the next item. The process is repeated for the next 100 entries, and it consumes a lot of time performing this entire process. Which strategy can be applied to your application in order to shorten the time needed to process all the necessary entries with MINIMAL configuration?",answers:[{text:"Use DynamoDB conditional writes.",isCorrect:!1},{text:"Deploy your application into a cluster of EC2 instances.",isCorrect:!1},{text:"Use DynamoDB's BatchGetItem and BatchWriteItem API operations.",isCorrect:!0},{text:"Modify your application to use multithreading.",isCorrect:!1}],explanation:"For applications that need to read or write multiple items, DynamoDB provides the BatchGetItem and BatchWriteItem operations. Using these operations can reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the individual read or write operations in parallel. Your applications benefit from this parallelism without having to manage concurrency or threading. The batch operations are essentially wrappers around multiple read or write requests. For example, if a BatchGetItem request contains five items, DynamoDB performs five GetItem operations on your behalf. Similarly, if a BatchWriteItem request contains two put requests and four delete requests, DynamoDB performs two PutItem and four DeleteItem requests. Hence, the correct answer is to use DynamoDB's BatchGetItem and BatchWriteItem API operations. Using DynamoDB conditional writes is incorrect because conditional writes are only helpful in cases where multiple users attempt to modify the same item. This is because write operations will succeed only if the item attributes meet one or more expected conditions. Otherwise, it returns an error. This method does not decrease the time it takes to process the entries. Modifying your application to use multithreading is incorrect because although refactoring your application to concurrently send multiple requests to the table is a valid solution, this entails a lot of effort to refactor your code in order to support multithreading. Without proper locking mechanisms, threads can mistakenly introduce redundancies hence, using the DynamoDB's batch API operations is still a better solution. Deploying your application into a cluster of EC2 instances is incorrect because you will need a solution that can track which application is currently handling which item. Although it could work if executed properly, it is not the simplest to do among the choices given. Take note that the scenario explicitly asks for a solution that you can implement with minimal configuration. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"},{question:"A serverless application is using API Gateway with a non-proxy Lambda Integration. A developer was tasked to expose a GET method on a new /getcourses resource to invoke the Lambda function, which will allow the consumers to fetch a list of online courses in JSON format. The consumers must include a query string parameter named courseType in their request to get the data. What is the MOST efficient solution that the developer should do to accomplish this requirement?",answers:[{text:"Configure the method response of the resource.",isCorrect:!1},{text:"Configure the integration request of the resource.",isCorrect:!1},{text:"Configure the integration response of the resource.",isCorrect:!1},{text:"Configure the method request of the resource.",isCorrect:!0}],explanation:"In Lambda non-proxy (or custom) integration, you can specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. For an AWS service action, you have the AWS integration of the non-proxy type only. API Gateway also supports the mock integration, where API Gateway serves as an integration endpoint to respond to a method request. The Lambda custom integration is a special case of the AWS integration, where the integration endpoint corresponds to the function-invoking action of the Lambda service. The Lambda proxy integration type (AWS_PROXY) lets an API method be integrated with the Lambda function invocation action with a flexible, versatile, and streamlined integration setup. This integration relies on direct interactions between the client and the integrated Lambda function. With this type of integration, also known as the Lambda proxy integration, you do not set the integration request or the integration response. API Gateway passes the incoming request from the client as the input to the backend Lambda function. In this scenario, you have to enforce the use of a required courseType query string parameter in the /getcourses resource in API Gateway. In order to do this, you can configure the method request of your resource just as shown in the diagram above. Hence, the correct answer is to configure the method request of the resource. Configuring the integration request of the resource is incorrect. Although configuring the integration request may also be valid, the client traffic will hit the method request first before it goes to the integration request down to the underlying Lambda function. This is why you should configure the method request first so it won't be necessary to check the required parameters in the Lambda integration. In addition, the integration request does not have the capability to enforce a request to include certain query string parameter nor enable API caching, unlike the method request. Configuring the integration response of the resource is incorrect because you have to use the method request. Configuring the response, either the method-type or the integration-type, is irrelevant in this scenario. Configuring the method response of the resource is incorrect because you should configure the method request instead. Take note that the scenario explicitly mentioned about the required query parameter which needs to be present before the processing can proceed. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-method-settings-method-request.html#setup-method-request-model https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-simple-proxy-for-lambda-input-format Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"You recently deployed an application to a newly created AWS account, which uses two identical Lambda functions to process ad-hoc requests. The first function processes incoming requests efficiently but the second one has a longer processing time even though both of the functions have exactly the same code. Based on your monitoring, the Throttles metric of the second function is greater than the first one in Amazon CloudWatch. Which of the following are possible solutions that you can implement to fix this issue? (Select TWO.)",answers:[{text:"Set the concurrency execution limit of the second function to 0.",isCorrect:!1},{text:"Configure the second function to use an unreserved account concurrency.",isCorrect:!1},{text:"Set the concurrency execution limit of both functions to 500.",isCorrect:!1},{text:"Decrease the concurrency execution limit of the first function.",isCorrect:!0},{text:"Set the concurrency execution limit of both functions to 450.",isCorrect:!0}],explanation:"The unit of scale for AWS Lambda is a concurrent execution. However, scaling indefinitely is not desirable in all scenarios. For example, you may want to control your concurrency for cost reasons or to regulate how long it takes you to process a batch of events, or to simply match it with a downstream resource. To assist with this, Lambda provides a concurrent execution limit control at both the account level and the function level. The concurrent executions refers to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the concurrent execution count will differ depending on whether or not your Lambda function is processing events from a poll-based event source. If you create a Lambda function to process events from event sources that aren't poll-based (for example, Lambda can process every event from other sources, like Amazon S3 or API Gateway), each published event is a unit of work, in parallel, up to your account limits. Therefore, the number of invocations these event sources make influences the concurrency. If you set the concurrent execution limit for a function, the value is deducted from the unreserved concurrency pool. For example, if your account's concurrent execution limit is 1000 and you have 10 functions, you can specify a limit on one function at 200 and another function at 100. The remaining 700 will be shared among the other 8 functions. AWS Lambda will keep the unreserved concurrency pool at a minimum of 100 concurrent executions, so that functions that do not have specific limits set can still process requests. So, in practice, if your total account limit is 1000, you are limited to allocating 900 to individual functions. By default, an AWS account's concurrent execution limit is 1000 which will be shared by all Lambda functions. In this scenario, it is highly likely that the first function has more provisioned concurrency than the other one. It is possible that the concurrency execution limit of the first function is set to a significantly high value (e.g. 900) and the second function is set to use the unreserved account concurrency which may only contain the last 100 units out of the AWS account's concurrent execution limit of 1000. Hence, the correct solutions in this scenario are: - Set the concurrency execution limit of both functions to 450 - Decrease the concurrency execution limit of the first function. Setting the concurrency execution limit of both functions to 500 is incorrect because by default, a newly created AWS account has a concurrent execution limit of only 1000. Take note that AWS Lambda will keep the unreserved concurrency pool at a minimum of 100 concurrent executions so that functions that do not have specific limits set can still process requests. Hence, you can only allocate a concurrent execution limit of 900 for a single Lambda function or 450 for two functions. Configuring the second function to use an unreserved account concurrency is incorrect because this may possibly be the current setting of this function, which is why the requests are being throttled. The total number of concurrent execution limits that you allocated to all Lambda functions affect the value of the unreserved concurrency limit. Since the second function is being throttled, it is highly likely that it is already using an unreserved account concurrency, which only has a low value since the units were already exhausted by the first function. Take note that the unreserved concurrency pool has a minimum value of 100 concurrent executions. Setting the concurrency execution limit of the second function to 0 is incorrect because this will throttle all future invocations of this function and will make the problem worse. References: https://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html https://docs.aws.amazon.com/lambda/latest/dg/scaling.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A developer is building an image processing utility using an AWS Lambda function. The function processes images in parallel using multiple threads to optimize performance. The images are stored in an Amazon S3 bucket and retrieved for processing. However, the function is not performing as efficiently as expected, with the processing time taking longer than anticipated, even when handling relatively small images. Which action should the developer modify to achieve better performance in the AWS Lambda function?",answers:[{text:"Increase the timeout setting of the Lambda function.",isCorrect:!1},{text:"Use AWS Step Functions to split tasks into smaller workflows.",isCorrect:!1},{text:"Optimize memory allocation for the Lambda function.",isCorrect:!0},{text:"Utilize Amazon S3 Transfer Acceleration for image uploads.",isCorrect:!1}],explanation:"AWS Lambda is a serverless compute service that allows developers to run code without provisioning or managing servers. It automatically scales based on the workload and charges only for the compute time consumed. Developers can use Lambda to execute code in response to events such as changes in data, HTTP requests, or system state changes, making it ideal for event-driven architectures. Lambda supports multiple programming languages and integrates seamlessly with other AWS services, enabling flexible and scalable application development. AWS Lambda functions operate within a highly available infrastructure and manage resources automatically, ensuring reliability and performance. Lambda can execute specific business logic by using triggers like S3 events, DynamoDB streams, or API Gateway, making it a key component for building modern, agile applications. AWS Lambda allows developers to configure memory allocation for 128 MB to 10,240 MB functions. This memory setting directly influences the CPU resources available to the function, as Lambda allocates CPU power proportionally to the configured memory. For instance, at 1,769 MB, a function has the equivalent of one vCPU. Increasing the memory allocation provides more RAM and enhances CPU capacity, which can lead to significant performance improvements for compute-intensive tasks. Hence, the correct answer is: Optimize memory allocation for the Lambda function. The option that says: Use AWS Step Functions to split tasks into smaller workflows is incorrect. AWS Step Functions are primarily used for orchestrating workflows and breaking down complex processes into smaller, manageable steps. However, this approach does not directly improve the execution performance of the Lambda function itself. The issue lies in the Lambda function's CPU resources, which Step Functions simply cannot address. While they can enhance task coordination, they typically do not optimize the speed of underlying tasks within a single function. The option that says: Increase the timeout setting of the Lambda function is incorrect. This option primarily focuses on extending the maximum runtime for the Lambda function. Increasing the timeout setting would allow the function to run longer but not address the underlying inefficiencies caused by insufficient memory or CPU resources. Timeout adjustments are typically useful for handling long-running tasks, not optimizing compute-intensive workloads. The option that says: Utilize Amazon S3 Transfer Acceleration for image uploads is incorrect. Amazon S3 Transfer Acceleration is designed to improve the upload and download speed of objects to and from S3 by using Amazon's global edge network. However, this feature is only relevant when data transfer speed between the client and S3 is a bottleneck. In this case, the issue lies with the processing of images within the Lambda function. Transfer Acceleration simply cannot influence the performance of compute tasks, as it is unrelated to the Lambda execution environment. References: https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtime-environment.html https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"Your team is developing a new feature on your application which is already hosted in Elastic Beanstalk. After several weeks, the new version of the application is ready to be deployed and you were instructed to handle the deployment. What is the correct way to deploy the new version to Elastic Beanstalk via the CLI?",answers:[{text:"Package your application as a zip file and deploy it using the aws elasticbeanstalk update-application command.",isCorrect:!1},{text:"Package your application as a tar file and deploy it using the eb deploy command.",isCorrect:!1},{text:"Package your application as a zip file and deploy it using the eb deploy command.",isCorrect:!0},{text:"Package your application as a tar file and deploy it using the aws elasticbeanstalk update-application command.",isCorrect:!1}],explanation:"The EB CLI is a command line interface for Elastic Beanstalk that provides interactive commands that simplify creating, updating and monitoring environments from a local repository. It is recommended that you use the EB CLI as part of your everyday development and testing cycle as an alternative to the AWS Management Console. You can tell the EB CLI to deploy a ZIP file or WAR file that you generate as part of a separate build process by adding the following lines to .elasticbeanstalk/config.yml in your project folder: deploy: artifact: path/to/buildartifact.zip If you configure the EB CLI in your Git repository, and you don't commit the artifact to source, use the --staged option to deploy the latest build: ~/eb$ eb deploy --staged Hence, packaging your application as a zip file and deploying it using the eb deploy command is the correct answer. Packaging your application as a tar file and deploying it using the eb deploy command is incorrect because tar is not supported. You can only deploy a ZIP or WAR file. Packaging your application as a tar file and deploying it using the aws elasticbeanstalk update-application command is incorrect because this CLI command just updates the specified properties of the application. This command does not allow you to upload packages to Elastic Beanstalk. Packaging your application as a zip file and deploying it using the aws elasticbeanstalk update-application command is incorrect because although you have a valid file type for your application bundle (ZIP), the CLI command that was used is wrong. Remember that the update-application command does not allow you to upload packages to Elastic Beanstalk. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-configuration.html#eb-cli3-artifact Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/ Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy: https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/ Comparison of AWS Services Cheat Sheets: https://tutorialsdojo.com/comparison-of-aws-services/"},{question:"Your manager assigned you a task of implementing server-side encryption with customer-provided encryption keys (SSE-C) to your S3 bucket, which will allow you to set your own encryption keys. Amazon S3 will manage both the encryption and decryption process using your key when you access your objects, which will remove the burden of maintaining any code to perform data encryption and decryption. To properly upload data to this bucket, which of the following headers must be included in your request?",answers:[{text:"x-amz-server-side-encryption-customer-key header only",isCorrect:!1},{text:"x-amz-server-side​-encryption​-customer-algorithm, x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers",isCorrect:!0},{text:"x-amz-server-side-encryption and x-amz-server-side-encryption-aws-kms-key-id headers",isCorrect:!1},{text:"x-amz-server-side-encryption, x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers",isCorrect:!1}],explanation:'Server-side encryption is about protecting data at rest. Using server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your own encryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it writes to disks, and decryption, when you access your objects. Therefore, you don\'t need to maintain any code to perform data encryption and decryption. The only thing you do is manage the encryption keys you provide. When you upload an object, Amazon S3 uses the encryption key you provide to apply AES-256 encryption to your data and removes the encryption key from memory. It is important to note that Amazon S3 does not store the encryption key you provide. Instead, it is stored in a randomly salted HMAC value of the encryption key in order to validate future requests. The salted HMAC value cannot be used to derive the value of the encryption key or to decrypt the contents of the encrypted object. That means, if you lose the encryption key, you lose the object. When you retrieve an object, you must provide the same encryption key as part of your request. Amazon S3 first verifies that the encryption key you provided matches, and then decrypts the object before returning the object data to you. When using server-side encryption with customer-provided encryption keys (SSE-C), you must provide encryption key information using the following request headers: x-amz-server-side-encryption-customer-algorithm - This header specifies the encryption algorithm. The header value must be "AES256". x-amz-server-side-encryption-customer-key - This header provides the 256-bit, base64-encoded encryption key for Amazon S3 to use to encrypt or decrypt your data. x-amz-server-side-encryption-customer-key-MD5 - This header provides the base64-encoded 128-bit MD5 digest of the encryption key according to RFC 1321. Amazon S3 uses this header for a message integrity check to ensure the encryption key was transmitted without error. Hence, the correct answer is: x-amz-server-side​-encryption​-customer-algorithm, x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers. The option that says: Including the x-amz-server-side-encryption and x-amz-server-side-encryption-aws-kms-key-id headers in the upload request is incorrect because these headers are primarily used in Server-Side Encryption with AWS KMS Keys (SSE-KMS) and not for Server-Side Encryption with Customer-Provided Keys (SSE-C). The option that says: Including the x-amz-server-side-encryption, x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers is incorrect because the x-amz-server-side-encryption header is not used in SSE-C encryption. This should be replaced with the x-amz-server-side​-encryption​-customer-algorithm header. The option that says: Including just the x-amz-server-side-encryption-customer-key header only is incorrect because you have to include the x-amz-server-side​-encryption​-customer-algorithm and x-amz-server-side-encryption-customer-key-MD5 headers as well to upload the objects to the S3 bucket with SSE-C encryption. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html#RESTObjectPUT-responses-examples https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/ Check out these Amazon S3 and AWS KMS Cheat Sheets: https://tutorialsdojo.com/amazon-s3/ https://tutorialsdojo.com/aws-key-management-service-aws-kms/'},{question:"A social media application is using DynamoDB to manage and store the session data of its users. As the number of users grew, the number of items in the table exponentially increased as well. You have to reduce storage usage and also reduce the cost of storing irrelevant data without using provisioned throughput to rectify this issue. Which of the following is the MOST cost-effective solution that you should implement?",answers:[{text:"Turn on Time To Live (TTL) in the table.",isCorrect:!0},{text:"Implement a Lazy Loading caching strategy to your application.",isCorrect:!1},{text:"Use a Lambda function with CloudWatch Events to schedule a purge of stale items in the table on a daily basis.",isCorrect:!1},{text:"Implement a Write-Through caching strategy in your application.",isCorrect:!1}],explanation:"Time To Live (TTL) for DynamoDB allows you to define when items in a table expire so that they can be automatically deleted from the database. TTL is provided at no extra cost as a way to reduce storage usage and reduce the cost of storing irrelevant data without using provisioned throughput. With TTL enabled on a table, you can set a timestamp for deletion on a per-item basis, allowing you to limit storage usage to only those records that are relevant. TTL is useful if you have continuously accumulated data that lose relevance after a specific time period. For example session data, event logs, usage patterns, and other temporary data. If you have sensitive data that must be retained only for a certain amount of time according to contractual or regulatory obligations, TTL helps you ensure that it is removed promptly and as scheduled. Hence, the correct answer is to: Turn on Time To Live (TTL) in the table. The option that says: Use a Lambda function with CloudWatch Events to schedule a purge of stale items in the table on a daily basis is incorrect. Although this solution can work, it entails a lot of configuration to implement and can incur an additional cost. The option that says: Implement a Write-Through caching strategy in your application is incorrect because this strategy simply adds data or updates data in the cache whenever data are written to the database, and not in the event of a cache miss. Since the scenario is about deleting the stale items in the table and not improving the cache performance, this option is incorrect. The option that says: Implement a Lazy Loading caching strategy to your application is incorrect because this strategy just loads data into the cache only when necessary. Just like the Write-Through caching strategy, this is not applicable in this scenario since the objective is to automatically expire and delete the stale session data in the DynamoDB table to improve the application performance. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"},{question:"A write-heavy data analytics application is using DynamoDB database which has global secondary index. Whenever the application is performing heavy write activities on the table, the DynamoDB requests return a ProvisionedThroughputExceededException. Which of the following is the MOST likely cause of this issue?",answers:[{text:"The provisioned write capacity for the global secondary index is greater than the write capacity of the base table.",isCorrect:!1},{text:"The provisioned write capacity for the global secondary index is less than the write capacity of the base table.",isCorrect:!0},{text:"The provisioned throughput exceeds the current throughput limit for your account.",isCorrect:!1},{text:"The rate of requests exceeds the allowed throughput.",isCorrect:!1}],explanation:"When you create a global secondary index on a provisioned mode table, you must specify read and write capacity units for the expected workload on that index. The provisioned throughput settings of a global secondary index are separate from those of its base table. A Query operation on a global secondary index consumes read capacity units from the index, not the base table. When you put, update, or delete items in a table, the global secondary indexes on that table are also updated; these index updates consume write capacity units from the index, not from the base table. For example, if you Query a global secondary index and exceed its provisioned read capacity, your request will be throttled. If you perform heavy write activity on the table but a global secondary index on that table has insufficient write capacity, then the write activity on the table will be throttled. To avoid potential throttling, the provisioned write capacity for a global secondary index should be equal or greater than the write capacity of the base table since new updates will write to both the base table and global secondary index. To view the provisioned throughput settings for a global secondary index, use the DescribeTable operation; detailed information about all of the table's global secondary indexes will be returned. Hence, the most likely cause of this issue is that the provisioned write capacity for the global secondary index is less than the write capacity of the base table. The option that says: The provisioned write capacity for the global secondary index is greater than the write capacity of the base table is incorrect because it should be the other way around, just as mentioned above. If the provisioned WCU of the global secondary index is greater than its base table then this issue is unlikely to happen. The option that says: The provisioned throughput exceeds the current throughput limit for your account is incorrect because this will only happen if DynamoDB returns a RequestLimitExceeded exception. The option that says: The rate of requests exceeds the allowed throughput is incorrect because this will only happen if DynamoDB returns a ThrottlingException. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html#GSI.ThroughputConsiderations https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.MessagesAndCodes Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"},{question:"An organization has a serverless application using AWS Lambda, Amazon API Gateway. Recently, the DevOps team discovered that the IAM roles associated with the Lambda functions had been manually modified. The organization must identify these unauthorized changes and ensure all resources are in sync with the CloudFormation stack. Which solution will help the company identify these changes?",answers:[{text:"Run a drift detection check on the CloudFormation stack.",isCorrect:!0},{text:"Analyze CloudWatch Logs to identify changes to the IAM role permissions.",isCorrect:!1},{text:"Use AWS Config to monitor updates made to the Lambda functions and IAM roles.",isCorrect:!1},{text:"Review CloudTrail logs to trace IAM role updates for the Lambda functions.",isCorrect:!1}],explanation:"CloudFormation drift detection is a feature that allows you to identify differences between the actual configuration of your AWS resources and their expected configuration as defined in your CloudFormation stack template. Some of its capabilities are: - Compares a stack's current state of resources with their expected state as defined in the CloudFormation template. - Identifies any discrepancies or \"drift\" between the actual and expected configurations of resources. - Assists in identifying potential issues affecting stack operations or overall infrastructure integrity. - Helps maintain consistency and compliance by detecting unintended or unauthorized changes outside of CloudFormation. Running a drift detection check on the CloudFormation stack would allow the organization to identify these unauthorized changes. CloudFormation would detect that the actual configuration of the IAM roles no longer matches the expected configuration defined in the stack template, and it would report these differences as drifts. By identifying the drifts, the organization can take appropriate actions to remediate the unauthorized changes and bring the resources back into compliance with the CloudFormation stack template. This could involve updating the IAM roles through CloudFormation or reverting the manual changes made to the roles. Additionally, CloudFormation's drift detection feature provides detailed information about the specific properties or attributes of the resources that have drifted, making it easier to understand the nature of the changes and take targeted actions to address them. Hence, the correct answer is: Run a drift detection check on the CloudFormation stack. The option that says: Use AWS Config to monitor updates made to the Lambda functions and IAM roles is incorrect. While AWS Config can track configuration changes and rule compliance for AWS resources, it wouldn't directly show which resources have deviated from their CloudFormation stack configuration. AWS Config might identify some changes, but it lacks the direct stack synchronization context provided by drift detection. The option that says: Review CloudTrail logs to trace IAM role updates for the Lambda functions is incorrect. AWS CloudTrail simply records API calls made within an AWS account, including changes made to IAM roles. Reviewing the CloudTrail logs, the organization can trace the API calls that modified the IAM roles associated with the Lambda functions. However, this approach alone does not identify whether the changes were authorized or unauthorized, nor does it ensure that all resources are in sync with the CloudFormation stack. The option that says: Analyze CloudWatch Logs to identify changes to the IAM role permissions is incorrect. CloudWatch Logs is primarily used for monitoring and collecting log data from various AWS services. It does not directly provide information about changes to IAM role permissions or help identify unauthorized changes in the context of the CloudFormation stack. References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-drift.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/detect-drift-stack.html Check out this CloudFormation Cheat Sheet: https://tutorialsdojo.com/aws-cloudformation/"},{question:"A developer is designing the cloud architecture of an internal application which will be used by about a hundred employees. She needs to ensure that the architecture is elastic enough to adequately match the supply of resources to the demand while maintaining its cost-effectiveness. Which of the following services can provide the MOST elasticity to the architecture? (Select TWO.)",answers:[{text:"Amazon RDS",isCorrect:!1},{text:"Amazon EC2 Spot Fleet",isCorrect:!0},{text:"Amazon DynamoDB",isCorrect:!0},{text:"AWS WAF",isCorrect:!1},{text:"Amazon CloudFront",isCorrect:!1}],explanation:"In the traditional data center-based model of IT, once the infrastructure is deployed, it typically runs whether it is needed or not, and all the capacity is paid for, regardless of how much it gets used. In the cloud, resources are elastic, meaning they can instantly grow or shrink to match the requirements of a specific application. Elasticity allows you to match the supply of resources—which cost money—to demand. Because cloud resources are paid for based on usage, matching needs to utilization is critical for cost optimization. Demand includes both external usage, such as the number of customers who visit a website over a given period, and internal usage, such as an application team using development and test environments. There are two basic types of elasticity: 1. Time-based2. Volume-based Time-based elasticity means turning off resources when they are not being used, such as a development environment that is needed only during business hours. Volume-based elasticity means matching scale to the intensity of demand, whether that’s compute cores, storage sizes, or throughput. By combining monitoring, tagging, and automation, you can get the most value out of your AWS resources and optimize costs. By taking advantage of volume-based elasticity, you can scale resources to match capacity. The best tool for accomplishing this task is Auto Scaling, which you can use to optimize performance by automatically increasing the number of EC2 instances during demand spikes and decreasing capacity during lulls to reduce costs. Auto Scaling is well-suited for applications that have stable demand patterns and for ones that experience hourly, daily, or weekly variability in usage. You can also use a combination of ELB and Auto Scaling to maximize the elasticity of your architecture. Beyond Auto Scaling for Amazon EC2, you can use Application Auto Scaling to automatically scale resources for other AWS services, including: - Amazon ECS - Amazon EC2 Spot Fleets - Amazon EMR clusters - Amazon AppStream 2.0 stacks and fleets - Amazon DynamoDB For Amazon EC2 Spot Fleets, it can either launch instances (scale out) or terminate instances (scale in), within the range that you choose, in response to one or more scaling policies. For Amazon DynamoDB, you can dynamically adjust provisioned throughput capacity in response to actual traffic patterns. This enables a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic without throttling. When the workload decreases, Application Auto Scaling decreases the throughput so that you don't pay for unused provisioned capacity. Hence, the correct answers are: - Amazon EC2 Spot Fleet. - Amazon DynamoDB. Amazon CloudFront is incorrect because this is primarily helpful for scaling out your application. Moreover, the scenario says that the internal application will only be used by about a hundred employees, which clearly doesn't warrant the use of a CDN or CloudFront. AWS WAF is incorrect because it only improves the security of your architecture and not its elasticity. Amazon RDS is incorrect. While Amazon RDS offers robust features and scalability, its cost structure can be higher due to the need for continuous instance uptime, storage, and backup costs. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html Check out these cheat sheets for Amazon EC2 and Amazon DyamoDB: https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/ https://tutorialsdojo.com/amazon-dynamodb/"},{question:"You are planning to create a DynamoDB table for your employee profile website. This will be used by the Human Resources department to easily view details about each employee. When choosing the partition key of the table, which of the following is the BEST attribute to use?",answers:[{text:"department_id since employees will fall in these departments.",isCorrect:!1},{text:"employee_id because each employee ID is unique.",isCorrect:!0},{text:"employee_name because this will speed up searching of records.",isCorrect:!1},{text:"position_id because this will help sort the records per department.",isCorrect:!1}],explanation:"When you create a table, in addition to the table name, you must specify the primary key of the table. The primary key uniquely identifies each item in the table so that no two items can have the same key. DynamoDB supports two different kinds of primary keys: 1. Partition key 2. Partition key and sort key Partition key – A simple primary key, composed of one attribute known as the partition key. DynamoDB uses the partition key's value as input to an internal hash function. The output from the hash function determines the partition (physical storage internal to DynamoDB) in which the item will be stored. In a table that has only a partition key, no two items can have the same partition key value. Partition key and sort key – Referred to as a composite primary key, this type of key is composed of two attributes. The first attribute is the partition key, and the second attribute is the sort key. DynamoDB uses the partition key value as input to an internal hash function. The output from the hash function determines the partition (physical storage internal to DynamoDB) in which the item will be stored. All items with the same partition key are stored together, in sorted order by sort key value. In a table that has a partition key and a sort key, it's possible for two items to have the same partition key value. However, those two items must have different sort key values. A composite primary key gives you additional flexibility when querying data. For example, if you provide only the value for Artist, DynamoDB retrieves all of the songs by that artist. To retrieve only a subset of songs by a particular artist, you can provide a value for Artist along with a range of values for SongTitle. Thus, in this scenario, the correct answer is to use employee_id because each employee ID is unique. Using high-cardinality attributes are recommended when creating primary partition keys. Examples of these unique attributes are email, employee_no, customerid, and so on. Both department_id and position_id are incorrect because these values are not unique per employee. Using employee_name is not recommended because in big organizations, somebody may share the same name as someone else. References: https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/ https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html#HowItWorks.CoreComponents.PrimaryKey Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"},{question:"A company is deploying the package of its Lambda function, which is compressed as a ZIP file, to AWS. However, they are getting an error in the deployment process because the package is too large. The manager instructed the developer to keep the deployment package small to make the development process much easier and more modularized. This should also help prevent errors that may occur when dependencies are installed and packaged with the function code. Which of the following options is the MOST suitable solution that the developer should implement?",answers:[{text:"Zip the deployment package again to further compress the zip file.",isCorrect:!1},{text:"Upload the deployment package to S3.",isCorrect:!1},{text:"Compress the deployment package as TAR file instead.",isCorrect:!1},{text:"Upload the other dependencies of your function as a separate Lambda Layer instead.",isCorrect:!0}],explanation:"You can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package. Layers let you keep your deployment package small, which makes development easier. You can avoid errors that can occur when you install and package dependencies with your function code. For Node.js, Python, and Ruby functions, you can develop your function code in the Lambda console as long as you keep your deployment package under 3 MB. A function can use up to 5 layers at a time. The total unzipped size of the function and all layers can't exceed the unzipped deployment package size limit of 250 MB. You can create layers, or use layers published by AWS and other AWS customers. Layers support resource-based policies for granting layer usage permissions to specific AWS accounts, AWS Organizations, or all accounts. Layers are extracted to the /opt directory in the function execution environment. Each runtime looks for libraries in a different location under /opt, depending on the language. Structure your layer so that function code can access libraries without additional configuration. Hence, the correct answer is to upload the other dependencies of your function as a separate Lambda Layer instead. Uploading the deployment package to S3 is incorrect. Although you can upload large deployment packages of over 50 MB in size via S3, your function will still be in a single layer. This doesn't meet the requirement of making the deployment package small and modularized. You have to use Lambda Layers instead. Zipping the deployment package again to further compress the zip file is incorrect because doing this will not significantly make the ZIP file smaller. Compressing the deployment package as TAR file instead is incorrect. Although it may decrease the size of the deployment package, it is still not enough to totally solve the issue. A compressed TAR file is not significantly smaller as compared to a ZIP file. References: https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html https://docs.aws.amazon.com/lambda/latest/dg/limits.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A data analytics company has installed sensors to track the number of people that goes to the mall. The data sets are collected in real-time by an Amazon Kinesis Data Stream which has a consumer that is configured to process data every other day and store the results to S3. Your team noticed that your S3 bucket is only receiving half of the data that is being sent to the Kinesis stream but after checking, you have verified that the sensors are properly sending the data to Amazon Kinesis in real-time without any issues. Which of the following is the MOST likely root cause of this issue?",answers:[{text:"The Amazon Kinesis Data Stream automatically deletes duplicate data.",isCorrect:!1},{text:"The Amazon Kinesis Data Stream has too many open shards.",isCorrect:!1},{text:"The sensors are having intermittent connection issues.",isCorrect:!1},{text:"By default, the data records are only accessible for 24 hours from the time they are added to a Kinesis stream.",isCorrect:!0}],explanation:"Amazon Kinesis Data Streams supports changes to the data record retention period of your stream. A Kinesis data stream is an ordered sequence of records meant to be written to and read from in real-time. Data records are therefore stored in shards in your stream temporarily. The time period from when a record is added to when it is no longer accessible is called the retention period. A Kinesis data stream stores record for up to 24 hours by default. You can increase the retention period up to 8760 hours (365 days) using the IncreaseStreamRetentionPeriod operation. You can decrease the retention period to a minimum of 24 hours using the DecreaseStreamRetentionPeriod operation. The request syntax for both operations includes the stream name and the retention period in hours. Finally, you can check the current retention period of a stream by calling the DescribeStream operation. Kinesis Data Streams supports changes to the data record retention period of your stream. A Kinesis data stream is an ordered sequence of records meant to be written to and read from in real-time. Data records are therefore stored in shards in your stream temporarily. In this scenario, the consumer of the data stream is configured to process the data every other day. Since the default data retention of the Kinesis data stream is only 24 hours, the data from the day before is already lost prior to the scheduled processing. Hence, the root cause of the problem in this scenario is that by default, the data records are only accessible for 24 hours from the time they are added to a Kinesis stream. The option that says: the sensors are having intermittent connection issues is incorrect because the sensors have been verified to be working properly, hence, this is not the root cause. The option that says: the Amazon Kinesis Data Stream has too many open shards is incorrect because having this configuration is irrelevant in this scenario as it just increases the data stream's rate of data flow. The option that says: the Amazon Kinesis Data Stream automatically deletes duplicate data is incorrect because Amazon Kinesis does not do this by default. If the sensors send two records with identical data, these will have unique sequence numbers in the stream. This does not have anything to do with data retention; hence, it is irrelevant in this scenario. Reference: http://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/"},{question:"A serverless application consisting of a Lambda function and a DynamoDB database is used to process Amazon S3 events. The Lambda function takes an average of three seconds to process the data and Amazon S3 publishes 10 events per second. What is the concurrent execution that the function will have?",answers:[{text:"13",isCorrect:!1},{text:"30",isCorrect:!0},{text:"3",isCorrect:!1},{text:"10",isCorrect:!1}],explanation:"Concurrent executions refers to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the concurrent execution count will differ depending on whether or not your Lambda function is processing events from a poll-based event source. If you create a Lambda function to process events from event sources that aren't poll-based (for example, Lambda can process every event from other sources, like Amazon S3 or API Gateway), each published event is a unit of work, in parallel, up to your account limits. Therefore, the number of invocations these event sources make influences the concurrency. You can use this formula to estimate the capacity used by your function: concurrent executions = invocations per second * average execution duration in seconds Supposed you have 10 events coming in per second and it takes 3 seconds for each event to be processed, then 10 instances of the function will be spawned to handle those requests. AWS Lambda won't be able to reuse those functions for 3 seconds. To accommodate the 2nd batch (another 10 events), AWS Lambda has to spawn another 10 instances of the function, which become busy for another 3 seconds. At the 3rd second, another batch comes in, invoking additional 10 functions. Suppose there's a continuous stream of events, the concurrent executions that you'll get at any given time would be 30. In this scenario, a Lambda function processes Amazon S3 events where the function takes on an average of three seconds to complete the processing (average execution duration in seconds) and Amazon S3 publishes 10 events per second (invocations per second). By using the formula given above: concurrent executions = 10 * 3 = 30 Hence, you will have 30 concurrent executions of your Lambda function. 3 is incorrect because the value of the concurrent executions is not equivalent to the average execution duration of your Lambda function in seconds. 10 is incorrect because the value of the concurrent executions is not equivalent to the number of invocations your Lambda function receives per second. 13 is incorrect because the value of the concurrent executions is not equivalent to the sum of the average execution duration and the number of invocations your Lambda function receives per second. References: https://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html https://docs.aws.amazon.com/lambda/latest/dg/scaling.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A developer is instrumenting an application that will be hosted in a large On-Demand EC2 instance in AWS. All of the downstream calls invoked by the application must be traced properly, including the AWS SDK calls. A user-defined data should also be present to expedite the troubleshooting process. Which of the following are valid considerations in AWS X-Ray that the developer should follow? (Select TWO.)",answers:[{text:"Set the namespace subsegment field to remote for AWS SDK calls and aws for other downstream calls.",isCorrect:!1},{text:"Set the namespace subsegment field to aws for AWS SDK calls and remote for other downstream calls.",isCorrect:!0},{text:"Set the metadata object with key-value pairs that you want X-Ray to index for search.",isCorrect:!1},{text:"Set the annotations object with any additional custom data that you want to store in the segment.",isCorrect:!1},{text:"Set the metadata object with any additional custom data that you want to store in the segment.",isCorrect:!0}],explanation:'A segment document conveys information about a segment to X-Ray. A segment document can be up to 64 kB and contain a whole segment with subsegments, a fragment of a segment that indicates that a request is in progress, or a single subsegment that is sent separately. You can send segment documents directly to X-Ray by using the PutTraceSegments API. X-Ray compiles and processes segment documents to generate queryable trace summaries and full traces that you can access by using the GetTraceSummaries and BatchGetTraces APIs, respectively. In addition to the segments and subsegments that you send to X-Ray, the service uses information in subsegments to generate inferred segments and adds them to the full trace. Inferred segments represent downstream services and resources in the service map. A subset of segment fields is indexed by X-Ray for use with filter expressions. For example, if you set the user field on a segment to a unique identifier, you can search for segments associated with specific users in the X-Ray console or by using the GetTraceSummaries API. Below are the optional subsegment fields: namespace - aws for AWS SDK calls; remote for other downstream calls. http - http object with information about an outgoing HTTP call. aws - aws object with information about the downstream AWS resource that your application called. error, throttle, fault, and cause - error fields that indicate an error occurred and that include information about the exception that caused the error. annotations - annotations object with key-value pairs that you want X-Ray to index for search. metadata - metadata object with any additional data that you want to store in the segment. subsegments - array of subsegment objects. precursor_ids - array of subsegment IDs that identify subsegments with the same parent that was completed prior to this subsegment. You can use the "metadata" field in the segment section to add custom data for your tracing. If you want to trace all the AWS SDK calls of your application, then you can add a subsegment and set the "namespace" field to "AWS". Alternatively, you can set the "namespace" value to "remote" if you want to trace other downstream calls. Hence, the valid considerations in this scenario are: - Set the namespace subsegment field to aws for AWS SDK calls and remote for other downstream calls. - Set the metadata object with any additional custom data that you want to store in the segment. Setting the annotations object with any additional custom data that you want to store in the segment is incorrect because this should be the metadata object and not the annotations object. Segments and subsegments can include an annotations object containing one or more fields that X-Ray indexes for use with filter expressions. Fields can have string, number, or Boolean values (no objects or arrays). X-Ray indexes up to 50 annotations per trace. Setting the namespace subsegment field to remote for AWS SDK calls and aws for other downstream calls is incorrect because this should be the other way around. You should set the namespace subsegment field to aws for AWS SDK calls and remote for other downstream calls Setting the metadata object with key-value pairs that you want X-Ray to index for search is incorrect because this should be the annotations object, and not the metadata object. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/ Instrumenting your Application with AWS X-Ray: https://tutorialsdojo.com/instrumenting-your-application-with-aws-x-ray/'},{question:"A developer is using API Gateway Lambda Authorizer to provide authentication for every API request and control access to your API. The requirement is to implement an authentication strategy which is similar to OAuth or SAML. Which of the following is the MOST suitable method that the developer should use in this scenario?",answers:[{text:"Request Parameter-based Authorization",isCorrect:!1},{text:"Cross-Account Lambda Authorizer",isCorrect:!1},{text:"AWS STS-based Authentication",isCorrect:!1},{text:"Token-based Authorization",isCorrect:!0}],explanation:"A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. When a client makes a request to one of your API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output. There are two types of Lambda authorizers: - A token-based Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token. - A request parameter-based Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of headers, query string parameters, stageVariables, and $context variables. It is possible to use an AWS Lambda function from an AWS account that is different from the one in which you created your Lambda authorizer function by using a Cross-Account Lambda Authorizer. Therefore, the correct answer in this scenario is to use a token-based authorization since this is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML. Requesting Parameter-based Lambda Authorization is incorrect because this does not use tokens to identify a caller but through a combination of headers, query string parameters, stageVariables, and $context variables. AWS STS-based authentication is incorrect because this is not a valid type of API Gateway Lambda Authorizer. Cross-Account Lambda Authorizer is incorrect because this just enables you to use an AWS Lambda function from a different AWS account as your API authorizer function. Moreover, this is not a valid Lambda authorizer type. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-lambda-authorizer-input.html https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-lambda-authorizer-cross-account-lambda-authorizer.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A Docker application hosted on an ECS cluster has encountered intermittent unavailability issues and timeouts. The lead DevOps engineer instructed you to instrument the application to detect where high latencies are occurring and to determine the specific services and paths impacting application performance. Which of the following steps should you take to accomplish this task properly? (Select TWO.)",answers:[{text:"Configure the port mappings and network mode settings in the container agent to allow traffic on TCP port 2000.",isCorrect:!1},{text:"Add the xray-daemon.config configuration file in your Docker image.",isCorrect:!1},{text:"Configure the port mappings and network mode settings in your task definition file to allow traffic on UDP port 2000.",isCorrect:!0},{text:"Manually install the X-Ray daemon to the instances via a user data script.",isCorrect:!1},{text:"Create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster.",isCorrect:!0}],explanation:"The AWS X-Ray SDK does not send trace data directly to AWS X-Ray. To avoid calling the service every time your application serves a request, the SDK sends the trace data to a daemon, which collects segments for multiple requests and uploads them in batches. Use a script to run the daemon alongside your application. To properly instrument your applications in Amazon ECS, you have to create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster. You can use port mappings and network mode settings in your task definition file to allow your application to communicate with the daemon container. The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon works in conjunction with the AWS X-Ray SDKs and must be running so that data sent by the SDKs can reach the X-Ray service. Hence, the correct steps to properly instrument the application is to create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster. In addition, you also have to configure the port mappings and network mode settings in your task definition file to allow traffic on UDP port 2000. Configuring the port mappings and network mode settings in the container agent to allow traffic on TCP port 2000 is incorrect because this should be done in the task definition and not in the container agent. Moreover, X-Ray is primarily using the UDP port 2000, so this should also be added alongside with the TCP port mapping. Manually installing the X-Ray daemon to the instances via a user data script is incorrect because this is only applicable if your application is hosted in an EC2 instance. Adding the xray-daemon.config configuration file in your Docker image is incorrect because this step is not suitable for ECS. The xray-daemon.config configuration file is primarily used in Elastic Beanstalk. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html https://docs.aws.amazon.com/xray/latest/devguide/scorekeep-ecs.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/ Instrumenting your Application with AWS X-Ray: https://tutorialsdojo.com/instrumenting-your-application-with-aws-x-ray/"},{question:"An application in your development account is running in an AWS Elastic Beanstalk environment which has an attached Amazon RDS database. You noticed that if you terminate the environment, it also brings down the database which hinders you from performing seamless updates with blue-green deployments. This also poses a critical security risk if the company decides to deploy the application in production. In this scenario, how can you decouple your database instance from your environment without having any data loss?",answers:[{text:"Use the blue / green deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance and delete the old environment.",isCorrect:!1},{text:"Use the blue / green deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance. Before terminating the old Elastic Beanstalk environment, remove its security group rule first before proceeding.",isCorrect:!0},{text:"Use a Canary deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and then create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance.",isCorrect:!1},{text:"Use a Canary deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance and delete the old environment.",isCorrect:!1}],explanation:"AWS Elastic Beanstalk provides support for running Amazon Relational Database Service (Amazon RDS) instances in your Elastic Beanstalk environment. This works great for development and testing environments. However, it isn't ideal for a production environment because it ties the lifecycle of the database instance to the lifecycle of your application's environment. If you haven't used a DB instance with your application before, try adding one to a test environment with the Elastic Beanstalk console first. This lets you verify that your application is able to read environment properties, construct a connection string, and connect to a DB instance before you add Amazon Virtual Private Cloud (Amazon VPC) and security group configuration to the mix. To decouple your database instance from your environment, you can run a database instance in Amazon RDS and configure your application to connect to it on launch. This enables you to connect multiple environments to a database, terminate an environment without affecting the database, and perform seamless updates with blue-green deployments. To allow the Amazon EC2 instances in your environment to connect to an outside database, you can configure the environment's Auto Scaling group with an additional security group. The security group that you attach to your environment can be the same one that is attached to your database instance, or a separate security group from which the database's security group allows ingress. You can connect your environment to a database by adding a rule to your database's security group that allows ingress from the autogenerated security group that Elastic Beanstalk attaches to your environment's Auto Scaling group. However, doing so creates a dependency between the two security groups. Subsequently, when you attempt to terminate the environment, Elastic Beanstalk will be unable to delete the environment's security group because the database's security group is dependent on it. Hence, the option that says: Use the blue/green deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance. Before terminating the old Elastic Beanstalk environment, remove its security group rule first before proceeding is the correct answer in this scenario. The option that says: Use the blue/green deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance and delete the old environment is incorrect. Although the deployment strategy being used here is valid, the existing security group rule is not yet removed, which hinders the deletion of the old environment. The option that says: Use a Canary deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance and delete the old environment is incorrect because there is no Canary deployment configuration in Elastic Beanstalk. This type of deployment strategy is usually used in Lambda. The option that says: Use a Canary deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and then create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance is incorrect because you should use a blue/green deployment strategy instead. This will also cause a data loss since the deletion protection for the database is not enabled. References: https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/ https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/"},{question:"A company has 5 different applications running on several On-Demand EC2 instances. The DevOps team is required to set up a graphical representation of the key performance metrics for each application. These system metrics must be available on a single shared screen for more effective and visible monitoring. Which of the following should the DevOps team do to satisfy this requirement using Amazon CloudWatch?",answers:[{text:"Set up a custom CloudWatch namespace with a unique metric name for each application.",isCorrect:!0},{text:"Set up a custom CloudWatch Event with a unique metric name for each application.",isCorrect:!1},{text:"Set up a custom CloudWatch Alarm with a unique metric name for each application.",isCorrect:!1},{text:"Set up a custom CloudWatch dimension with a unique metric name for each application.",isCorrect:!1}],explanation:"Amazon CloudWatch is basically a metrics repository. An AWS service—such as Amazon EC2—puts metrics into the repository, and you retrieve statistics based on those metrics. If you put your own custom metrics into the repository, you can retrieve statistics on these metrics as well. A namespace is a container for CloudWatch metrics. Metrics in different namespaces are isolated from each other so that metrics from different applications are not mistakenly aggregated into the same statistics. There is no default namespace. You must specify a namespace for each data point you publish to CloudWatch. You can specify a namespace name when you create a metric. These names must contain valid XML characters and be fewer than 256 characters in length. Possible characters are: alphanumeric characters (0-9A-Za-z), period (.), hyphen (-), underscore (_), forward slash (/), hash (#), and colon (:). The AWS namespaces typically use the following naming convention: AWS/service. For example, Amazon EC2 uses the AWS/EC2 namespace. Hence, the correct answer is: Set up a custom CloudWatch namespace with a unique metric name for each application. The option that says: Set up a custom CloudWatch Alarm with a unique metric name for each application is incorrect because a CloudWatch Alarm simply watches a single metric over a specified time period and performs one or more specified actions based on the value of the metric relative to a threshold over time. The option that says: Set up a custom CloudWatch Event with a unique metric name for each application is incorrect because a CloudWatch Event is primarily used to deliver a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources, and not for segregating metrics of various applications. The option that says: Set up a custom CloudWatch dimension with a unique metric name for each application is incorrect because a CloudWatch dimension is only a name/value pair that is part of the identity of a metric. You have to use a CloudWatch namespace instead. References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html#Namespace https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/viewing_metrics_with_cloudwatch.html https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/aws-services-cloudwatch-metrics.html Check out this Amazon CloudWatch Cheat Sheet: https://tutorialsdojo.com/amazon-cloudwatch/"},{question:"An online magazine is deployed in AWS and uses an Application Load Balancer, an Auto Scaling group of EC2 instances, and an RDS MySQL Database. Some of the readers are complaining about the website's sluggish performance when loading the articles. Upon checking, there is a high number of read operations in the database, which affects the website's performance. Which of the following actions should you take to resolve the issue with minimal code change?",answers:[{text:"Set up a multi-AZ deployments configuration in RDS.",isCorrect:!1},{text:"Launch a large ElastiCache Cluster as a database cache for RDS and apply the required code change.",isCorrect:!1},{text:"Upgrade the EC2 instances to a higher instance type.",isCorrect:!1},{text:"Create an RDS Read Replica instance and configure the application to use this for read queries.",isCorrect:!0}],explanation:"Amazon RDS Read Replicas provide enhanced performance and durability for the database (DB) instances. This feature makes it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read. You can reduce the load on your source DB instance by routing read queries from your applications to the read replica. Read replicas allow you to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Because read replicas can be promoted to master status, they are useful as part of a sharding implementation. To shard your database, add a read replica and promote it to master status, then, from each of the resulting DB Instances, delete the data that belongs to the other shard. Hence, the correct solution in this scenario is to: create an RDS Read Replica instance and configure the application to use this for read queries. The option that says: Launch a large ElastiCache Cluster as a database cache for RDS and apply the required code change is incorrect. Although this will improve the read performance of the application, this solution entails a lot of code changes in the application as compared with just using RDS Read Replicas. It is specifically mentioned in the scenario that you need to solve the issue with the minimal code change. The option that says: Set up a multi-AZ deployment configuration in RDS is incorrect because configuring a Multi-AZ RDS just improves the availability of the database but does not drastically improve the read performance. The more appropriate solution for this is to use Read Replicas instead. The option that says: Upgrade the EC2 instances to a higher instance type is incorrect because the issue lies with the database, not the application servers hosted in EC2 instances. References: https://aws.amazon.com/caching/database-caching/ https://aws.amazon.com/rds/details/read-replicas/ https://aws.amazon.com/elasticache/ Check out this Amazon RDS Cheat Sheet: https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/"},{question:"Using AWS SAM, a developer recently deployed a serverless application consisting of Lambda functions, API Gateway, Kinesis Data stream, and a DynamoDB table. The application has worked fine for a few days, but lately, there were a lot of ProvisionedThroughputExceeded exceptions being returned by DynamoDB. The developer also noticed that there's a sudden increase in read capacity units (RCU) usage whenever this issue happens. How should the developer refactor the application to find items based on primary key values and use the LEAST amount of RCU?",answers:[{text:"Use the Scan operation with eventual consistency reads.",isCorrect:!1},{text:"Use the Query operation with eventual consistency reads.",isCorrect:!0},{text:"Use the Query operation with strong consistency reads.",isCorrect:!1},{text:"Use the Scan operation with strong consistency reads.",isCorrect:!1}],explanation:"In general, Scan operations are less efficient than other operations in DynamoDB. A Scan operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set. A Scan operation reads every item in a table or a secondary index. By default, a Scan operation returns all of the data attributes for every item in the table or index. You can use the ProjectionExpression parameter so that Scan only returns some of the attributes rather than all of them. On the other hand, the Query operation finds items based on primary key values. You can query any table or secondary index that has a composite primary key (a partition key and a sort key). If possible, you should avoid using a Scan operation on a large table or index with a filter that removes many results. Also, as a table or index grows, the Scan operation slows. The Scan operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation. For faster response times, design your tables and indexes so that your applications can use Query instead of Scan. DynamoDB calculates the number of read capacity units consumed based on item size, not on the amount of data that is returned to an application. For this reason, the number of capacity units consumed will be the same whether you request all of the attributes (the default behavior) or just some of them (using a projection expression). The number will also be the same whether or not you use a filter expression. Hence, using the Query operation with eventual consistency reads is the correct answer. Using the Scan operation with eventual consistency reads is incorrect because it doesn't find items based on primary key values but by reading every item in a table. Using the Scan operation with strong consistency reads is incorrect because in addition to what is mentioned above about the Scan operation, using strong consistency will consume more RCU. Remember that the scenario requires a solution that uses the LEAST amount of RCU. Using the Query operation with strong consistency reads is incorrect because this consumes more RCU in comparison to eventual consistency. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Query.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ DynamoDB Scan vs Query: https://tutorialsdojo.com/dynamodb-scan-vs-query/"},{question:"A developer needs to configure the environment name, solution stack, and environment links of his application environment which will be hosted in Elastic Beanstalk. Which configuration file should the developer add in the source bundle to meet the above requirement?",answers:[{text:"env.yaml",isCorrect:!0},{text:"cron.yaml",isCorrect:!1},{text:"Dockerrun.aws.json",isCorrect:!1},{text:"env.config",isCorrect:!1}],explanation:"In Elastic Beanstalk, you can include a YAML formatted environment manifest in the root of your application source bundle to configure the environment name, solution stack and environment links to use when creating your environment. An environment manifest uses the same format as Saved Configurations. This file format includes support for environment groups. To use groups, specify the environment name in the manifest with a + symbol at the end. When you create or update the environment, specify the group name with --group-name (AWS CLI) or --env-group-suffix (EB CLI). The following example manifest defines a web server environment for the tutorialsdojo frontend application, with a link to a worker environment component that it is dependent upon. The manifest uses groups to allow creating multiple environments with the same source bundle: ~/tutorialsdojo/frontend/env.yaml AWSConfigurationTemplateVersion: 1.1.0.0SolutionStack: 64bit Amazon Linux 2015.09 v2.0.6 running Multi-container Docker 1.7.1 (Generic)OptionSettings: aws:elasticbeanstalk:command: BatchSize: '30' BatchSizeType: Percentage aws:elasticbeanstalk:sns:topics: Notification Endpoint: me@example.com aws:elb:policies: ConnectionDrainingEnabled: true ConnectionDrainingTimeout: '20' aws:elb:loadbalancer: CrossZone: true aws:elasticbeanstalk:environment: ServiceRole: aws-elasticbeanstalk-service-role aws:elasticbeanstalk:application: Application Healthcheck URL: / aws:elasticbeanstalk:healthreporting:system: SystemType: enhanced aws:autoscaling:launchconfiguration: IamInstanceProfile: aws-elasticbeanstalk-ec2-role InstanceType: t2.micro EC2KeyName: workstation-uswest2 aws:autoscaling:updatepolicy:rollingupdate: RollingUpdateType: Health RollingUpdateEnabled: trueTags: Cost Center: WebApp DevCName: front-A08G28LG+EnvironmentName: front+EnvironmentLinks: \"WORKERQUEUE\" : \"worker+\" Hence, using the env.yaml is the correct configuration file to be used in this scenario. Dockerrun.aws.json is incorrect because this configuration file is primarily used in multicontainer Docker environments that are hosted in Elastic Beanstalk. This can be used alone or combined with source code and content in a source bundle to create an environment on a Docker platform. env.config is incorrect because this is just a custom configuration file which is not readily available in Elastic Beanstalk. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle. The more appropriate configuration file to use here is the env.yaml which can help you configure the environment name, solution stack, and environment links to use when creating your environment. cron.yaml is incorrect because this configuration file is primarily used to define periodic tasks that add jobs to your worker environment's queue automatically at a regular interval. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-cfg-manifest.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/"},{question:"A developer has a set of EC2 instances that runs the Amazon Kinesis Client Library to process a data stream in AWS. Based on the custom metrics, it shows that the instances are maxing out their CPU Utilization, and there are insufficient Kinesis shards to handle the rate of data flowing through the stream. Which of the following is the BEST course of action that the developer should take to solve this issue and prevent this situation from re-occurring in the future?",answers:[{text:"Increase the number of instances up to the number of open shards.",isCorrect:!1},{text:"Increase the instance size to a larger type.",isCorrect:!1},{text:"Increase both the instance size and the number of open shards.",isCorrect:!0},{text:"Increase the number of shards.",isCorrect:!1}],explanation:"Resharding enables you to increase or decrease the number of shards in a stream in order to adapt to changes in the rate of data flowing through the stream. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that shard. It also tracks the shards in the stream using an Amazon DynamoDB table. When new shards are created as a result of resharding, the KCL discovers the new shards and populates new rows in the table. The workers automatically discover the new shards and create processors to handle the data from them. The KCL also distributes the shards in the stream across all the available workers and record processors. By increasing the instance size and number of shards in your Kinesis stream, the developer allows the instances to handle more record processors, which are running in parallel within the instance. It also allows the stream to properly accommodate the rate of data being sent in. The data capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the capacities of its shards. Hence, the correct answer is: Increase both the instance size and the number of open shards The option that says: Increase the instance size to a larger type is incorrect. Increasing the instance size alone is incorrect because the current number of Kinesis shards is not enough to accommodate the rate of data flowing through the stream. In this scenario, the best solution is to reshard or to increase the number of shards in the stream. The option that says: Increase the number of shards is incorrect. This is not enough because the instances cannot hold any more record processors, due to CPU Utilization maxing out. You should either increase the instance size or number of instances along with the increase of shards in this scenario. The option that says: Increase the number of instances up to the number of open shards is incorrect because this scenario requires resharding to adapt to changes in the rate of data flowing through the stream. Although adding new instances will improve the compute capacity, the rate of data flowing through the stream would still be low since the number of shards is still unchanged. References: https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html https://docs.aws.amazon.com/streams/latest/dev/building-consumers.html Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/ Kinesis Scaling, Resharding and Parallel Processing: https://tutorialsdojo.com/kinesis-scaling-resharding-and-parallel-processing/"},{question:"A developer is building a prototype microservices that are running as tasks in an Amazon ECS Cluster. His manager instructed him to define a task placement strategy which needs to be both cost and resource efficient. The task placement should minimize the number of instances in use which will keep the cost down since high availability is not much of a concern for this prototype. What should the developer implement to meet the above requirements?",answers:[{text:"Distribute tasks evenly across Availability Zones, and then re-distribute the tasks among EC2 instances based on the least available amount of CPU/memory within each Availability Zone.",isCorrect:!1},{text:"Distribute tasks evenly across all available EC2 instances using the spread task placement strategy.",isCorrect:!1},{text:"Distribute tasks among all registered EC2 instances based on the least available amount of CPU or memory using the binpack task placement strategy.",isCorrect:!0},{text:"Place tasks randomly using the random task placement strategy.",isCorrect:!1}],explanation:"The binpack strategy tries to fit your workloads in as few instances as possible. It gets its name from the bin packing problem where the goal is to fit objects of various sizes in the smallest number of bins. It is well suited to scenarios for minimizing the number of instances in your cluster, perhaps for cost savings, and lends itself well to automatic scaling for elastic workloads, to shut down instances that are not in use. A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service. Amazon ECS supports the following task placement strategies: binpack - Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use. random - Place tasks randomly. spread - Place tasks evenly based on the specified value. Accepted values are attribute key-value pairs, instanceId, or host. When you use the binpack strategy, you must also indicate if you are trying to make optimal use of your instances’ CPU or memory. This is done by passing an extra field parameter, which tells the task placement engine which parameter to use to evaluate how “full” your “bins” are. It then chooses the instance with the least available CPU or memory (depending on which you pick). If there are multiple instances with this CPU or memory remaining, it chooses randomly. By spreading tasks among your EC2 instances using the binpack strategy, you can minimize costs and resource consumption since this strategy maximizes available CPU/memory of your already running instances. Hence, the correct answer is: Distribute tasks among all registered EC2 instances based on the least available amount of CPU or memory using the binpack task placement strategy. The option that says: Distribute tasks evenly across all available EC2 instances using the spread task placement strategy is incorrect because this strategy is typically used to achieve high availability by making sure that multiple copies of a task are scheduled across multiple instances based on attributes such as Availability Zones. Since the scenario is focused on cost rather than availability, this option is clearly not suitable for this scenario. The option that says: Place tasks randomly using the random task placement strategy is incorrect. Random task placement just ensures tasks are run on instances with sufficient resources to complete them. Binpack has better cost-savings since it strategically places tasks in as few instances as possible. The option that says: Distribute tasks evenly across Availability Zones, and then re-distributing the tasks among EC2 instances based on the least available amount of CPU/memory within each Availability Zone is incorrect. Although it will meet the required task placement, this method will use more unnecessary EC2 instances. Take note that the scenario requires you to minimize the number of instances in use, which will keep the cost down. References: https://aws.amazon.com/blogs/compute/amazon-ecs-task-placement/ https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html Check out this Amazon ECS Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/"},{question:"A company has an AWS Amplify application, relying on Amazon Cognito for user authentication. Multi-factor authentication (MFA) is disabled for their User Pool. There has been a recent data breach in a popular website. The company is worried that attackers might exploit compromised email addresses and passwords to sign into their applications. For this reason, they want to enforce MFA only on users with suspicious login attempts. How can the company satisfy these requirements?",answers:[{text:"Enable the Time-based one-time password (TOTP) software token MFA for the User Pool",isCorrect:!1},{text:"Recreate the User Pool and enable SMS text message MFA.",isCorrect:!1},{text:"Create a subscription filter Lambda function that monitors for the CompromisedCredentialRisk metric from Advanced Security Metrics in CloudWatch Logs and triggers MFA when detected",isCorrect:!1},{text:"Enable Adaptive Authentication for the User Pool",isCorrect:!0}],explanation:"With adaptive authentication, you can configure your user pool to block suspicious sign-ins or add second factor authentication in response to an increased risk level. For each sign-in attempt, Amazon Cognito generates a risk score for how likely the sign-in request is to be from a compromised source. This risk score is based on factors that include device and user information. Adaptive authentication can turn on or require multi-factor authentication (MFA) for a user in your user pool when Amazon Cognito detects risk in a user's session, and the user hasn't yet chosen an MFA method. When you activate MFA for a user, they always receive a challenge to provide or set up a second factor during authentication, regardless of how you configured adaptive authentication. From your user's perspective, your app offers to help them set up MFA, and optionally, Amazon Cognito prevents them from signing in again until they have configured an additional factor. In the given scenario, by enabling Adaptive Authentication, the company can require MFA for users only when a sign-in attempt appears suspicious. This aligns with their objective to mitigate risks from the recent data breach without affecting all users. Hence, the correct answer is: Enable Adaptive Authentication for the User Pool. The option that says: Recreate the User Pool and enable SMS text message MFA is incorrect. This option is cumbersome since the users registered in the current User Pool will be lost. This means users would have to re-register or be re-imported into the new User Pool. Additionally, enabling MFA universally would challenge all users, not just those with suspicious login attempts. The option that says: Enable the Time-based one-time password (TOTP) software token MFA for the User Pool is incorrect. Enabling TOTP MFA would apply to all users, not just those with suspicious logins. This doesn't match the company's need to enforce MFA only for suspicious attempts. The option that says: Create a subscription filter Lambda function that monitors for the CompromisedCredentialRisk metric from Advanced Security Metrics in CloudWatch Logs and triggers MFA when detected is incorrect. This approach is not feasible because it operates outside of Cognito's authentication flow. Furthermore, in Cognito, MFA settings are applied at the User Pool level, not per user. References: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pool-settings-adaptive-authentication.html https://aws.amazon.com/blogs/security/how-to-use-new-advanced-security-features-for-amazon-cognito-user-pools/ Check out this Amazon Cognito Cheat Sheet: https://tutorialsdojo.com/amazon-cognito/"},{question:"You are developing an application that will use a Lambda function, which will be invoked asynchronously. The application will be implemented with exponential back-off that will handle failures so that the requests will be retried twice before the event is discarded. If the retries fail with an unexpected error, you have to direct unprocessed events to another service which will analyze the failure. Which of the following is the MOST suitable component that you should implement in the application architecture to meet the above requirement?",answers:[{text:"Delay Queue",isCorrect:!1},{text:"FIFO Queue",isCorrect:!1},{text:"Dead Letter Queue",isCorrect:!0},{text:"Amazon MQ",isCorrect:!1}],explanation:"Function invocation can result in an error for several reasons. Your code might raise an exception, time out, or run out of memory. The runtime executing your code might encounter an error and stop. You might run out of concurrency and be throttled. When an error occurs, your code might have run completely, partially, or not at all. In most cases, the client or service that invokes your function retries if it encounters an error, so your code must be able to process the same event repeatedly without unwanted effects. If your function manages resources or writes to a database, you need to handle cases where the same request is made several times. AWS Lambda directs events that cannot be processed to the specified Amazon SNS topic or Amazon SQS queue. Functions that don't specify a DLQ will discard events after they have exhausted their retries. You configure a DLQ by specifying the Amazon Resource Name TargetArn value on the Lambda function's DeadLetterConfig parameter. Hence, the setting up a Dead Letter Queue is the correct answer in this scenario. Delay Queue is incorrect because this just lets you postpone the delivery of new messages to a queue for a number of seconds. This is not relevant in this scenario since you can't use a delay queue within Lambda. FIFO Queue is incorrect because this is primarily used to enhance messaging between applications when the order of operations and events is critical, or where duplicates can't be tolerated. Although a DLQ is just a normal SQS queue, this option is still incorrect because you don't necessarily need a FIFO SQS queue since you can also use a Standard SQS queue to be the Dead Letter Queue of your Lambda function. Amazon MQ is incorrect because this is simply a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. References: https://docs.aws.amazon.com/lambda/latest/dg/dlq.html https://docs.aws.amazon.com/lambda/latest/dg/retries-on-errors.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A company is using AWS Organizations to manage its multiple AWS accounts which is being used by its various departments. To avoid security issues, it is of utmost importance to test the impact of service control policies (SCPs) on your IAM policies and resource policies before applying them. Which of the following services can you use to test and troubleshoot IAM and resource-based policies?",answers:[{text:"Systems Manager",isCorrect:!1},{text:"AWS Config",isCorrect:!1},{text:"Amazon Inspector",isCorrect:!1},{text:"IAM Policy Simulator",isCorrect:!0}],explanation:"The IAM policy simulator evaluates the policies that you choose and determines the effective permissions for each of the actions that you specify. The simulator uses the same policy evaluation engine that is used during real requests to AWS services. But the simulator differs from the live AWS environment in the following ways: - The simulator does not make an actual AWS service request, so you can safely test requests that might make unwanted changes to your live AWS environment. - Because the simulator does not simulate running the selected actions, it cannot report any response to the simulated request. The only result returned is whether the requested action would be allowed or denied. - If you edit a policy inside the simulator, these changes affect only the simulator. The corresponding policy in your AWS account remains unchanged. Check out this video on IAM Policy Simulator: https://youtu.be/1IIhVcXhvcE With the IAM policy simulator, you can test and troubleshoot IAM and resource-based policies in the following ways: - Test policies that are attached to IAM users, groups, or roles in your AWS account. If more than one policy is attached to the user, group, or role, you can test all the policies, or select individual policies to test. You can test which actions are allowed or denied by the selected policies for specific resources. - Test policies that are attached to AWS resources, such as Amazon S3 buckets, Amazon SQS queues, Amazon SNS topics, or Amazon S3 Glacier vaults. - If your AWS account is a member of an organization in AWS Organizations, then you can test the impact of service control policies (SCPs) on your IAM policies and resource policies. - Test new policies that are not yet attached to a user, group, or role by typing or copying them into the simulator. These are used only in the simulation and are not saved. Take note that you cannot type or copy a resource-based policy into the simulator. To use a resource-based policy in the simulator, you must include the resource in the simulation and select the checkbox to include that resource's policy in the simulation. - Test the policies with selected services, actions, and resources. For example, you can test to ensure that your policy allows an entity to perform the ListAllMyBuckets, CreateBucket, and DeleteBucket actions in the Amazon S3 service on a specific bucket. - Simulate real-world scenarios by providing context keys, such as an IP address or date, that are included in Condition elements in the policies being tested. - Identify which specific statement in a policy results in allowing or denying access to a particular resource or action. Hence, the correct answer is IAM Policy Simulator. AWS Config is incorrect because this is just a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Systems Manager is incorrect because this service just provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks across your AWS resources. Unlike IAM Policy Simulator, it can't be used to simulate your policies. Amazon Inspector is incorrect because it is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_iam_policy-sim-path-console.html Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"},{question:"A company has developed a Lambda function that will send status updates to a third-party provider for analytics. You need to schedule this function to run every 30 minutes. Which of the following is the MOST manageable and cost-effective way of setting up this task?",answers:[{text:"Enable scheduling on the AWS Console of your Lambda function. Define a schedule to run it at 30-minute intervals.",isCorrect:!1},{text:"Integrate Amazon EventBridge (Amazon CloudWatch Events) with Lambda, which will automatically trigger the function every 30 minutes.",isCorrect:!0},{text:"Use the Task Scheduler of your Windows PC to trigger the Lambda function every 30 minutes.",isCorrect:!1},{text:"Launch an EC2 instance that has a cron job that triggers the Lambda function every 30 minutes.",isCorrect:!1}],explanation:"Amazon EventBridge (Amazon CloudWatch Events) helps you respond to state changes in your AWS resources. When your resources change state, they automatically send events into an event stream. You can create rules that match selected events in the stream and route them to your AWS Lambda function to take action. For example, you can automatically invoke an AWS Lambda function to log the state of an EC2 instance or AutoScaling Group. You maintain event source mapping in Amazon CloudWatch Events by using a rule target definition. You can also create a Lambda function and direct AWS Lambda to execute it on a regular schedule. You can specify a fixed rate (for example, execute a Lambda function every hour or 15 minutes), or you can specify a Cron expression. Hence, the correct answer is: Integrate Amazon EventBridge (Amazon CloudWatch Events) with Lambda, which will automatically trigger the function every 30 minutes. The option that says: Launch an EC2 instance that has a cron job that triggers the Lambda function every 30 minutes is incorrect because provisioning a new instance incurs additional costs. There is also a possibility that the Lambda function will not be invoked in the event that the instance was stopped or terminated. The option that says: Use the Task Scheduler of your Windows PC to trigger the Lambda function every 30 minutes is incorrect because this setup is difficult to manage due to the fact that you are using your own computer to trigger the function. This may be the most cost-effective solution but it certainly is not the most manageable option. The best way is to integrate CloudWatch Events with Lambda. The option that says: Enable scheduling on the AWS Console of your Lambda function. Define a schedule to run it at 30-minute intervals is incorrect because there is no feature like this in Lambda. References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html Check out these Amazon CloudWatch and AWS Lambda Cheat Sheets: https://tutorialsdojo.com/amazon-cloudwatch/ https://tutorialsdojo.com/aws-lambda/"},{question:"An ECS Cluster has a running X-Ray Daemon that enables developers to easily debug and troubleshoot their application. However, the trace data being sent to AWS X-Ray is still not as detailed as your manager wants it to be. There is a new requirement that requires the application to provide more granular timing information and more details about its downstream calls to various AWS resources. What should you do to satisfy this requirement?",answers:[{text:"Use metadata",isCorrect:!1},{text:"Use annotations",isCorrect:!1},{text:"Use inferred segment",isCorrect:!1},{text:"Use subsegments",isCorrect:!0}],explanation:"A segment can break down the data about the work done into subsegments. Subsegments provide more granular timing information and details about downstream calls that your application made to fulfill the original request. A subsegment can contain additional details about a call to an AWS service, an external HTTP API, or an SQL database. You can even define arbitrary subsegments to instrument specific functions or lines of code in your application. For services that don't send their own segments like Amazon DynamoDB, X-Ray uses subsegments to generate inferred segments and downstream nodes on the service map. This lets you see all of your downstream dependencies, even if they don't support tracing or are external. Subsegments represent your application's view of a downstream call as a client. If the downstream service is also instrumented, the segment that it sends replaces the inferred segment generated from the upstream client's subsegment. The node on the service graph always uses information from the service's segment, if it's available, while the edge between the two nodes uses the upstream service's subsegment. Hence, the correct answer in this scenario is to use subsegments in your segment document. Using inferred segment is incorrect because this is the one generated by subsegments, which lets you see all of your downstream dependencies including the external ones even if they don't support tracing. The more appropriate solution in this scenario is to use subsegments instead. Using metadata is incorrect because this does not record the calls to AWS services and resources that are made by the application. Segments and subsegments can include a metadata object containing one or more fields with values of any type, including objects and arrays. Using annotations is incorrect because just like metadata, this also does not record the application's calls to your AWS services and resources. Segments and subsegments can include an annotations object containing one or more fields that X-Ray indexes for use with filter expressions. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html https://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html#api-segmentdocuments-subsegments Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/"},{question:"A developer wants to track the number of visitors on their website, which has a DynamoDB database. This is primarily used to give a rough idea on how many people visit the site whenever they launch a new advertisement, which means it can tolerate a slight overcounting or undercounting of website visitors. Which of the following will satisfy the requirement with MINIMAL configuration?",answers:[{text:"Use conditional writes to update the counter item in the DynamoDB table only if the item has a unique primary key and the new value is greater than the current value.",isCorrect:!1},{text:"Enable DynamoDB Streams to track the number of new visitors.",isCorrect:!1},{text:"Use atomic counters to increment the counter item in the DynamoDB table for every new visitor.",isCorrect:!0},{text:"Use conditional writes to update the counter item in the DynamoDB table and set the ReturnConsumedCapacity parameter to TOTAL.",isCorrect:!1}],explanation:"You can use the UpdateItem operation to implement an atomic counter — a numeric attribute that is incremented, unconditionally, without interfering with other write requests. (All write requests are applied in the order in which they were received). With an atomic counter, the updates are not idempotent. In other words, the numeric value will increment each time you call UpdateItem. You might use an atomic counter to keep track of the number of visitors to a website. In this case, your application would increment a numeric value, regardless of its current value. If an UpdateItemoperation should fail, the application could simply retry the operation. This would risk updating the counter twice, but you could probably tolerate a slight overcounting or undercounting of website visitors. An atomic counter would not be appropriate where overcounting or undercounting cannot be tolerated (For example, in a banking application). In this case, it is safer to use a conditional update instead of an atomic counter. Hence, using atomic counters to increment the counter item in the DynamDB table for every new visitor is the most suitable solution in this scenario. Using conditional writes to update the counter item in the DynamoDB table and set the ReturnConsumedCapacity parameter to TOTAL is incorrect because using conditional writes is not required for this scenario as the counter doesn't need to be idempotent. Remember that it is indicated that they can tolerate a slight overcounting or undercounting of website visitors. In addition, the ReturnConsumedCapacity parameter simply returns the total number of write capacity units consumed hence, it is irrelevant in this scenario. Using conditional writes to update the counter item in the DynamoDB table only if the item has a unique primary key and the new value is greater than the current value is incorrect because although this is a valid solution, it entails a lot of unnecessary configuration as compared to using an atomic counter. Enabling DynamoDB Streams to track the number of new visitors is incorrect because DynamoDB streams simply captures a time-ordered sequence of item-level modifications in the table. This is not suitable if you want to track the number of website visitors with minimal configuration. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.UpdateExpressions.html#Expressions.UpdateExpressions.SET.IncrementAndDecrement Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"},{question:"A multinational company uses Amazon EC2 Auto Scaling to maintain a fleet of EC2 instances behind an Application Load Balancer (ALB). The Amazon EC2 instances are configured with the Amazon CloudWatch agent to collect and publish custom metrics. However, the newly launched EC2 instances within the Auto Scaling group fail to send the metrics to Amazon CloudWatch. What changes are required to ensure the custom metrics are sent from all newly launched EC2 instances?",answers:[{text:"Attach the CloudWatchAgentServerPolicy policy to the IAM role specified in the EC2 Auto Scaling launch template to ensure proper permissions for the CloudWatch agent.",isCorrect:!0},{text:"Configure the IAM role for the EC2 instances with the CloudWatchAgentReadOnlyAccess policy to allow the CloudWatch agent to read default metrics and publish data.",isCorrect:!1},{text:"Add a user data script in the EC2 Auto Scaling launch template to install and start the CloudWatch agent during instance initialization.",isCorrect:!1},{text:"Attach the CloudWatchAgentAdminPolicy IAM policy to the IAM role specified in the EC2 Auto Scaling launch template to provide enhanced permissions for the CloudWatch agent.",isCorrect:!1}],explanation:"The CloudWatchAgentServerPolicy is an AWS-managed policy that grants the Amazon CloudWatch agent the necessary permission to collect and publish metrics from EC2 instances to CloudWatch. This policy allows actions such as cloudwatch:PutMetricData and logs:PutLogEvents, enabling the agent to send custom metrics and log data to CloudWatch. Attaching this policy to an IAM role associated with your EC2 instances ensures that the CloudWatch agent has the required effective permissions. An IAM role is an AWS Identity and Access Management (IAM) entity that defines a set of permissions for making AWS service requests. When you assign an IAM role to an EC2 instance, applications running on that instance can obtain temporary security credentials to interact with other AWS services without the need for embedding long-term credentials. This approach enhances security and simplifies credential management. To assign a role to an EC2 instance, users can create an instance profile that contains the role and then associate it with the instance during or after its launch. By combining the CloudWatchAgentServerPolicy with an appropriately configured IAM role, you enable the CloudWatch agent on your EC2 instances to collect and transmit custom metrics to CloudWatch seamlessly. This setup is essential for monitoring users' applications and infrastructure's performance and health, providing users with actionable insights to maintain optimal operations. Hence, the correct answer is: Attach the CloudWatchAgentServerPolicy policy to the IAM role specified in the EC2 Auto Scaling launch template to ensure proper permissions for the CloudWatch agent. The option that says: Add a user data script in the EC2 Auto Scaling launch template to install and start the CloudWatch agent during instance initialization is incorrect. While a user data script can automate the installation and startup of the CloudWatch agent, it only addresses the agent’s deployment process. This does not resolve the root cause of the issue, which is the lack of necessary IAM permissions for publishing custom metrics to Amazon CloudWatch. Even if the agent is installed and running, it requires the correct IAM role with the CloudWatchAgentServerPolicy policy attached to send metrics successfully. The option that says: Configure the IAM role for the EC2 instances with the CloudWatchAgentReadOnlyAccess policy to allow the CloudWatch agent to read default metrics and publish data is incorrect. The CloudWatchAgentReadOnlyAccess policy just provides permissions to view CloudWatch metrics and logs but does not grant the necessary permissions to publish custom metrics. This policy is intended for scenarios requiring read-only access to CloudWatch data and does not meet the requirements for the CloudWatch agent to send custom metrics to CloudWatch. The option that says: Attach the CloudWatchAgentAdminPolicy IAM policy to the IAM role specified in the EC2 Auto Scaling launch template to provide enhanced permissions for the CloudWatch agent is incorrect. Although the CloudWatchAgentAdminPolicy contains permissions for publishing custom metrics, it primarily provides excessive permissions, including administrative-level access to CloudWatch resources. This violates the principle of least privilege, which recommends granting only the permissions required for a task. References: https://docs.aws.amazon.com/aws-managed-policy/latest/reference/CloudWatchAgentServerPolicy.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html Check out these Amazon EC2,AWS Identity and Access Management ( IAM ) and Amazon CloudWatch Cheat Sheets: https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/ https://tutorialsdojo.com/aws-identity-and-access-management-iam/ https://tutorialsdojo.com/amazon-cloudwatch/"},{question:"A developer is instructed to collect data on the number of times that web visitors click the advertisement link of a popular news website. A database entry containing the count will be incremented for every click. Given that the website has millions of readers worldwide, your database should be configured to provide optimal performance to capture all the click events. What is the BEST service that the developer should implement in this scenario?",answers:[{text:"Take advantage of Amazon Aurora's performance speed and AUTO_INCREMENT feature for item updates.",isCorrect:!1},{text:"Use Amazon RDS for the database and setup SQL AUTO_INCREMENT on your tables.",isCorrect:!1},{text:"Launch an Amazon Redshift for the database and apply a step count of 1 for the IDENTITY column.",isCorrect:!1},{text:"Set up Amazon DynamoDB for the database and implement atomic counters for UpdateItem operation of the website counter.",isCorrect:!0}],explanation:"Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. Since fast performance is one of the requirements asked in the scenario, DynamoDB should be an option to consider. In DynamoDB, an item is a collection of attributes. Each attribute has a name and a value. An attribute value can be a scalar, a set, or a document type. DynamoDB provides four operations for basic create/read/update/delete (CRUD) functionality: PutItem – create an item. GetItem – read an item. UpdateItem – update an item. DeleteItem – delete an item. You can use the UpdateItem operation to implement an atomic counter—a numeric attribute that is incremented, unconditionally, without interfering with other write requests. With an atomic counter, the numeric value will increment each time you call UpdateItem. For example, you might use an atomic counter to keep track of the number of visitors to a website. In this case, your application would increment a numeric value, regardless of its current value. If an UpdateItem operation should fail, the application could simply retry the operation. This would risk updating the counter twice, but you could probably tolerate a slight overcounting or undercounting of website visitors. Hence, the correct answer is to setup Amazon DynamoDB for the database and implement atomic counters for the UpdateItem operation of the website counter. Using Amazon RDS for the database and setting up SQL AUTO_INCREMENT on your tables is incorrect because RDS is not scalable enough to handle millions of data being submitted by readers worldwide. Auto-increment allows a unique number to be generated automatically when a new record is inserted into a table. This is often the primary key field that we would like to be created automatically every time a new record is inserted. Since you would not want to add a new database entry for every link click and immediately consume all your storage space, it would be better to use DynamoDB's atomic counter instead. Launching an Amazon Redshift for the database and applying a step count of 1 for the IDENTITY column is incorrect because Redshift is more suited for data warehousing demands that need parallel execution capabilities and columnar storage types. Taking advantage of Amazon Aurora's performance speed and AUTO_INCREMENT feature for item updates is incorrect. Although Aurora is a scalable database service, using the AUTO_INCREMENT feature of SQL does not suit the scenario's requirement. Auto-increment simply allows a unique number to be generated automatically when a new record is inserted into a table. References: https://aws.amazon.com/dynamodb/ https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.AtomicCounters https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_TABLE_NEW.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"},{question:"You are hosting a website in an Amazon S3 bucket named tutorialsdojo and your users load the website using the http://tutorialsdojo.s3-website-us-east-1.amazonaws.com endpoint. You want to use JavaScript on the webpages that are stored in this bucket to be able to make authenticated GET and PUT requests. These requests are directed to the same bucket through the website.s3.amazonaws.com S3 API endpoint. However, you noticed that your web browser blocks the HTTP requests originating from your website. What should you do to rectify this issue?",answers:[{text:"Enable cross-account access.",isCorrect:!1},{text:"Enable Cross-Zone Load Balancing.",isCorrect:!1},{text:"Enable Cross-Region Replication (CRR).",isCorrect:!1},{text:"Enable Cross-origin resource sharing (CORS) configuration in the bucket.",isCorrect:!0}],explanation:"In the simplest case, your browser script makes a GET request for a resource from a server in another domain. Depending on the CORS configuration of that server, if the request is from a domain that's authorized to submit GET requests, the cross-origin server responds by returning the requested resource. If either the requesting domain or the type of HTTP request is not authorized, the request is denied. However, CORS makes it possible to preflight the request before actually submitting it. In this case, a preflight request is made in which the OPTIONS access request operation is sent. If the cross-origin server's CORS configuration grants access to the requesting domain, the server sends back a preflight response that lists all the HTTP request types that the requesting domain can make on the requested resource. Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources. To configure your bucket to allow cross-origin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support for each origin, and other operation-specific information. You can add up to 100 rules to the configuration. You add the XML document as the cors subresource to the bucket either programmatically or by using the Amazon S3 console as shown below: Therefore, you can solve the issue in the scenario by simply enabling cross-origin resource sharing (CORS) configuration in the bucket. Enabling cross-account access is incorrect because this is just a feature in IAM that allows you to provide access to your resources to an IAM User in another AWS account. Enabling Cross-Zone Load Balancing is incorrect because this is only used in ELB and not in S3. Enabling Cross-Region Replication (CRR) is incorrect because this is just a bucket-level configuration that enables automatic, asynchronous copying of objects across buckets in different AWS Regions. Reference: http://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/cors.html"},{question:"A company has a hybrid cloud architecture that connects its on-premises data center with AWS. The DevOps team has been tasked to set up the company's continuous integration and continuous delivery (CI/CD) systems. The application deployment to both Amazon EC2 instances and on-premises servers should also be automated. Which of the following AWS service should be used to accomplish this?",answers:[{text:"Amazon Kinesis",isCorrect:!1},{text:"AWS CodeDeploy",isCorrect:!0},{text:"AWS CodeBuild",isCorrect:!1},{text:"AWS CloudFormation",isCorrect:!1}],explanation:"CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can use CodeDeploy. When you deploy to an AWS Lambda compute platform, the deployment configuration specifies the way traffic is shifted to the new Lambda function versions in your application. Hence, the correct answer is AWS CodeDeploy. The option that says: AWS CodeBuild is incorrect. This service is mainly used for compiling source code, running tests, and producing software packages ready for deployment. While it is a crucial part of the CI/CD pipeline, it does not handle the deployment process itself. The option that says: AWS CloudFormation is incorrect because it only turns your infrastructure into code. This service does not deploy applications. The option that says: Amazon Kinesis is incorrect because this is just a data streaming service in AWS. This service does not deploy applications. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/"},{question:"In order to quickly troubleshoot their systems, your manager instructed you to record the calls that your application makes to all AWS services and resources. You developed a custom code that will send the segment documents directly to X-Ray by using the PutTraceSegments API. What should you include in your segment document to meet the above requirement?",answers:[{text:"tracing header",isCorrect:!1},{text:"metadata",isCorrect:!1},{text:"subsegments",isCorrect:!0},{text:"annotations",isCorrect:!1}],explanation:"A segment can break down the data about the work done into subsegments. Subsegments provide more granular timing information and details about downstream calls that your application made to fulfill the original request. A subsegment can contain additional details about a call to an AWS service, an external HTTP API, or an SQL database. You can even define arbitrary subsegments to instrument specific functions or lines of code in your application. For services that don't send their own segments like Amazon DynamoDB, X-Ray uses subsegments to generate inferred segments and downstream nodes on the service map. This lets you see all of your downstream dependencies, even if they don't support tracing, or are external. Subsegments represent your application's view of a downstream call as a client. If the downstream service is also instrumented, the segment that it sends replaces the inferred segment generated from the upstream client's subsegment. The node on the service graph always uses information from the service's segment, if it's available, while the edge between the two nodes uses the upstream service's subsegment. Hence, the correct answer in this scenario is to include subsegments in your segment document. Including tracing header is incorrect because this is added in the HTTP request header and not on the segment document. A tracing header (X-Amzn-Trace-Id) can originate from the X-Ray SDK, an AWS service, or the client request. Including metadata is incorrect because this does not record the calls to AWS services and resources that are made by the application. Segments and subsegments can include a metadata object containing one or more fields with values of any type, including objects and arrays. Including annotations is incorrect because just like metadata, this also does not record the application's calls to your AWS services and resources. Segments and subsegments can include an annotations object containing one or more fields that X-Ray indexes for use with filter expressions. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html https://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html#api-segmentdocuments-subsegments Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/ Instrumenting your Application with AWS X-Ray: https://tutorialsdojo.com/instrumenting-your-application-with-aws-x-ray/"},{question:"A mobile game is currently being developed and needs to have an authentication service. You need to use an AWS service which provides temporary AWS credentials for users who have been authenticated via their social media logins as well as for guest users who do not require any authentication. How can you BEST achieve this using AWS?",answers:[{text:"Use AWS Cognito User Pools then enable access to unauthenticated identities.",isCorrect:!1},{text:"Use AWS Cognito Identity Pools then enable access to unauthenticated identities.",isCorrect:!0},{text:"Use Amazon Cognito Sync.",isCorrect:!1},{text:"Use AWS IAM Identity Center.",isCorrect:!1}],explanation:'Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token. An identity pool is a store of user identity data specific to your account. Amazon Cognito identity pools enable you to create unique identities and assign permissions for users. Your identity pool can include: - Users in an Amazon Cognito user pool - Users who authenticate with external identity providers such as Facebook, Google, or a SAML-based identity provider - Users authenticated via your own existing authentication process With an identity pool, you can obtain temporary AWS credentials with permissions you define to directly access other AWS services or to access resources through Amazon API Gateway. Hence, the correct answer is: Use AWS Cognito Identity Pools, then enable access to unauthenticated identities. The option that says: Use AWS Cognito User Pools then enabling access to unauthenticated identities is incorrect because a user pool is just a user directory in Amazon Cognito. In addition, this doesn\'t enable the mobile game access to unauthenticated identities. You have to use an Identity Pool instead. The option that says: Use Amazon Cognito Sync is incorrect because this is just a client library that enables cross-device syncing of application-related user data. The option that says: Use AWS IAM Identity Center is incorrect because this service just makes it easy for you to centrally manage workforce access to multiple AWS accounts. It also does not allow any "guest" or unauthenticated access, unlike AWS Cognito. References: https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html https://docs.aws.amazon.com/cognito/latest/developerguide/getting-started-with-identity-pools.html Check out this Amazon Cognito Cheat Sheet: https://tutorialsdojo.com/amazon-cognito/'},{question:"Your development team is currently developing a financial application in AWS. One of the requirements is to create and control the encryption keys used to encrypt your data using the envelope encryption strategy to comply with the strict IT security policy of the company. Which of the following correctly describes the process of envelope encryption?",answers:[{text:"Encrypt plaintext data with a data key and then encrypt the data key with a top-level plaintext key.",isCorrect:!0},{text:"Encrypt plaintext data with a KMS key and then encrypt the KMS key with a top-level plaintext data key.",isCorrect:!1},{text:"Encrypt plaintext data with a data key and then encrypt the data key with a top-level encrypted key.",isCorrect:!1},{text:"Encrypt plaintext data with a KMS key and then encrypt the KMS key with a top-level encrypted data key.",isCorrect:!1}],explanation:"When you encrypt your data, your data is protected, but you have to protect your encryption key. One strategy is to encrypt it. Envelope encryption is the practice of encrypting plaintext data with a data key and then encrypting the data key under another key. You can even encrypt the data encryption key under another encryption key, and encrypt that encryption key under another encryption key. But, eventually, one key must remain in plaintext so you can decrypt the keys and your data. This top-level plaintext key encryption key is known as the root key. AWS KMS helps you protect your encryption keys by storing and managing them securely. Root keys stored in AWS KMS, known as AWS KMS keys, never leave the AWS KMS FIPS validated hardware security modules unencrypted. To use an AWS KMS key, you must call AWS KMS. Envelope encryption offers several benefits: Protecting data keys When you encrypt a data key, you don't have to worry about storing the encrypted data key, because the data key is inherently protected by encryption. You can safely store the encrypted data key alongside the encrypted data. Encrypting the same data under multiple keys Encryption operations can be time-consuming, particularly when the data being encrypted are large objects. Instead of re-encrypting raw data multiple times with different keys, you can re-encrypt only the data keys that protect the raw data. Combining the strengths of multiple algorithms In general, symmetric key algorithms are faster and produce smaller ciphertexts than public-key algorithms, but public-key algorithms provide inherent separation of roles and easier key management. Envelope encryption lets you combine the strengths of each strategy. Therefore, the correct answer is: Encrypt plaintext data with a data key and then encrypt the data key with a top-level plaintext key. The option that says: Encrypt plaintext data with a KMS key and then encrypt the KMS key with a top-level plaintext data key is incorrect because you have to encrypt your plaintext data with a data key and not a KMS key. Moreover, the top-level plaintext key should be the root key and not the data key. The option that says: Encrypt plaintext data with a KMS key and then encrypt the KMS key with a top-level encrypted data key is incorrect because plaintext data should be encrypted with a data key, not a KMS key. Also, the top-level key should be plaintext, not encrypted. The option that says: Encrypt plaintext data with a data key and then encrypt the data key with a top-level encrypted key is incorrect. While it is correct to encrypt plaintext data with a data key, the top-level key (root key) must be kept in plaintext and not be encrypted. References: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping https://docs.aws.amazon.com/kms/latest/developerguide/overview.html Check out this AWS Key Management Service (KMS) Cheat Sheet: https://tutorialsdojo.com/aws-key-management-service-aws-kms/"},{question:"A multinational e-commerce company hosts its product descriptions on an Amazon RDS database. All descriptions are originally written in English. Users can request on-demand translations via a Lambda function, which pulls the description and employs Amazon Translate's TranslateText API for the task. However, during sales of popular products, the surge in translation requests is stressing the RDS, causing increased response times. How can a developer improve the Lambda function's response time cost-effectively without performing database optimizations?",answers:[{text:"Store the results of the TranslateText API in an Amazon DynamoDB Accelerator (DAX) cluster.",isCorrect:!1},{text:"Update the Lambda function to use asynchronous invocation. Push the translation requests to an Amazon SQS queue and then process in batches.",isCorrect:!1},{text:"Use the /tmp storage in the Lambda function to cache recently translated product descriptions",isCorrect:!0},{text:"Use AWS Step Functions with a Parallel state to concurrently run multiple instances of the Lambda function for translation.",isCorrect:!1}],explanation:"When a Lambda function runs, AWS Lambda retains its execution context for potential subsequent invocations, allowing it to respond quickly without reinitializing the environment. This retention includes the /tmp directory, a writable temporary storage. Data saved in this directory persists across these retained invocations, making it useful for caching. In the scenario, after translating a product description, the result can be saved in /tmp. If the same translation is requested soon after, the Lambda function can retrieve the cached result instead of re-fetching from the database, improving response time. Hence, the correct answer is: Use the /tmp storage in the Lambda function to cache recently translated product descriptions. The option that says: Store the results of the TranslateText API in an Amazon DynamoDB Accelerator (DAX) cluster is incorrect because DAX is used to provide a fully managed, in-memory caching solution for Amazon DynamoDB, not for caching results from other AWS services like Amazon Translate. DAX is primarily designed to improve the performance of read-heavy DynamoDB workloads by caching data from DynamoDB tables in memory. The option that says: Use AWS Step Functions with a Parallel state to concurrently run multiple instances of the Lambda function for translation is incorrect. The AWS Step Functions Parallel state allows for the concurrent execution of multiple branches within a state machine. In this context, it could mean breaking down a single translation request into multiple smaller tasks, which doesn't make sense for translating a single product description. Such an approach could only complicate the process and potentially introduce additional delays. Additionally, running multiple Lambda instances concurrently can be more expensive. The option that says: Update the Lambda function to use asynchronous invocation. Push the translation requests to an Amazon SQS queue and then process in batches is incorrect. For on-demand translations, users expect translations to be near-instantaneous. Introducing SQS and batch processing would mean that the system would wait to accumulate several translation requests before processing them. This would add delay to the translation response time, potentially worsening the user experience. References: https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html#function-configuration Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A developer is creating a real-time auction app for second-hand cars using Kinesis Data Streams to ingest bids. The auction rules are as follows: A bid must be processed only once An EC2 instance consumer must process bids in the same order they were received. Which solution will meet the requirement?",answers:[{text:"Embed a unique ID in each bid record. Use Kinesis PutRecords API to write bids. Assign a timestamp-based value for the PartitionKey parameter.",isCorrect:!1},{text:"Replace the stream with an SQS FIFO queue and use the SendMessageBatch API to write bids. Provide a unique id in the MessageDeduplicationId parameter for each bid request.",isCorrect:!1},{text:"Embed a unique ID in each bid record. Use Kinesis PutRecord API to write bids. Assign a timestamp-based value for the SequenceNumberForOrdering parameter.",isCorrect:!0},{text:"Replace the stream with an SQS FIFO queue and use the SendMessage API to write bids. Provide a unique id in the MessageDeduplicationId parameter for each bid request.",isCorrect:!1}],explanation:"There are two API calls available for writing records to a Kinesis Data Stream: PutRecord or PutRecords. PutRecord writes a single record to the stream, while PutRecords writes multiple records to the stream in a batch. Kinesis Data Streams attempts to process all records in each PutRecords request. A single record failure does not stop the processing of subsequent records. As a result, PutRecords doesn't guarantee the ordering of records. If you need to read records in the same order they are written to the stream, use PutRecord along with the SequenceNumberForOrdering parameter. Furthermore, to handle duplicates, you can include a unique ID in each record that you write to the stream. The consumer application can then maintain a record of the unique IDs for the records that it has already processed. This record could be stored in an external database, like DynamoDB. When the consumer application receives a record from the stream, it can check its unique ID against the list of unique IDs that it has already processed. If the unique ID has already been processed, the consumer application can skip processing the record to avoid duplicates. Hence, the correct is: Embed a unique ID in each bid record. Use Kinesis PutRecord API to write bids. Assign a timestamp-based value for the SequenceNumberForOrdering parameter. The option that says: Embed a unique ID in each bid record. Use Kinesis PutRecords API to write bids. Assign a timestamp-based value for the PartitionKey parameter is incorrect because PutRecords does not guarantee the ordering of records. Although the SQS FIFO queue guarantees ordering and can prevent duplicates, it's not designed for real-time applications. Hence, the following options are incorrect: - Replace the stream with an SQS FIFO queue and use the SendMessage API to write bids. Provide a unique id in the MessageDeduplicationId parameter for each bid request. - Replace the stream with an SQS FIFO queue and use the SendMessageBatch API to write bids. Provide a unique id in the MessageDeduplicationId parameter for each bid request. References: https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecord.html https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html#kinesis-record-processor-duplicates-producer Check out this Amazon Kinesis Cheat sheet: https://tutorialsdojo.com/amazon-kinesis/"},{question:"A company has different AWS accounts, namely Account A, Account B, and Account C, which are used for their Development, Test, and Production environments respectively. A developer needs access to perform an audit whenever a new version of the application has been deployed to the Test (Account B) and production (Account C) environments. What is the MOST efficient way to provide the developer access to execute the specified task?",answers:[{text:"Set up AWS Organizations and attach a Service Control Policy to the developer to access the other accounts.",isCorrect:!1},{text:"Create separate identities and passwords for the developer on both the Test and Production accounts.",isCorrect:!1},{text:"Enable AWS multi-factor authentication (MFA) to the IAM User of the developer.",isCorrect:!1},{text:"Grant the developer cross-account access to the resources of Accounts B and C.",isCorrect:!0}],explanation:"You can grant your IAM users permission to switch to roles within your AWS account or to roles defined in other AWS accounts that you own. Imagine that you have Amazon EC2 instances that are critical to your organization. Instead of directly granting your users permission to terminate the instances, you can create a role with those privileges. Then allow administrators to switch to the role when they need to terminate an instance. Doing this adds the following layers of protection to the instances: - You must explicitly grant your users permission to assume the role. - Your users must actively switch to the role using the AWS Management Console or assume the role using the AWS CLI or AWS API. - You can add multi-factor authentication (MFA) protection to the role so that only users who sign in with an MFA device can assume the role. You can use a role to delegate access to resources that are in different AWS accounts that you own (Production and Testing). You can share resources in one account with users in a different account by setting up cross-account access. In this way, you don't need to create individual IAM users in each account and the users don't have to sign out of one account and sign into another in order to access resources that are in different AWS accounts. Hence, the most efficient answer in this scenario is to: Grant the developer cross-account access to the resources of Accounts B and C. The option that says: Create separate identities and passwords for the developer on both the Test and Production accounts is incorrect because although this is a valid option, it is not an efficient one since the developer will have to log in to each individual AWS account in order to access the resources. A better way to do this is to grant cross-account access. The option that says: Enable AWS multi-factor authentication (MFA) to the IAM User of the developer is incorrect because although this will improve the security, it still doesn't solve the access problem of the developer. This can be used in conjunction with cross-account access, but using this alone will not suffice. The option that says: Set up AWS Organizations and attach a Service Control Policy to the developer to access the other accounts is incorrect because although it will make the multiple AWS accounts of the company more manageable, it doesn't address the access requirement of the developer. Take note that SCPs are necessary, but not sufficient, for granting access for the accounts in your organization. You still need to attach IAM policies to users and roles in your organization's accounts to actually grant permissions to them. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html Check out this AWS Identity & Access Management (IAM) Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"},{question:"A developer is building an application that will be hosted in ECS and must be configured to run tasks and services using the Fargate launch type. The application will have four different tasks, each of which will access different AWS resources than the others. Which of the following is the MOST efficient solution that can provide your application in ECS access to the required AWS resources?",answers:[{text:"Create 4 different IAM Roles with the required permissions and attach them to each of the 4 ECS tasks.",isCorrect:!0},{text:"Create an IAM Group with all the required permissions and attach them to each of the 4 ECS tasks.",isCorrect:!1},{text:"Create 4 different Container Instance IAM Roles with the required permissions and attach them to each of the 4 ECS tasks.",isCorrect:!1},{text:"Create 4 different Service-Linked Roles with the required permissions and attach them to each of the 4 ECS tasks.",isCorrect:!1}],explanation:"By default, IAM users don't have permission to create or modify Amazon ECS resources or perform tasks using the Amazon ECS API. This means that they also can't do so using the Amazon ECS console or the AWS CLI. To allow IAM users to create or modify resources and perform tasks, you must create IAM policies. Policies grant IAM users permission to use specific resources and API actions. Then, attach those policies to the IAM users or groups that require those permissions. When you attach a policy to a user or group of users, it allows or denies the users permission to perform the specified tasks on the specified resources. Likewise, Amazon ECS container instances make calls to the Amazon ECS and Amazon EC2 APIs on your behalf, so they need to authenticate with your credentials. This authentication is accomplished by creating an IAM role for your container instances and associating that role with your container instances when you launch them. If you use an Elastic Load Balancing load balancer with your Amazon ECS services, calls to the Amazon EC2 and Elastic Load Balancing APIs are made on your behalf to register and deregister container instances with your load balancers. Hence, the most suitable solution in this scenario is: Create 4 different IAM Roles with the required permissions and attach them to each of the 4 ECS tasks. The option that says: Creating an IAM Group with all the required permissions and attaching them to each of the 4 ECS tasks is incorrect because you cannot directly attach an IAM Group to an ECS Task. Attaching an IAM Role is a more suitable solution in this scenario and not an IAM Group. The option that says: Creating 4 different Container Instance IAM Roles with the required permissions and attaching them to each of the 4 ECS tasks is incorrect because a Container Instance IAM Role only applies if you are using the EC2 launch type. Take note that the scenario says that the application will be using a Fargate launch type. The option that says: Creating 4 different Service-Linked Roles with the required permissions and attaching them to each of the 4 ECS tasks is incorrect because a service-linked role is a unique type of IAM role that is linked directly to Amazon ECS itself, not on the ECS task. Service-linked roles are predefined by Amazon ECS and include all the permissions that the service requires to call other AWS services on your behalf. References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_IAM_role.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html#create_task_iam_policy_and_role https://docs.aws.amazon.com/AmazonECS/latest/developerguide/instance_IAM_role.html Check out this Amazon ECS Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/"}]},{id:"aws-developer-10",title:"AWS Certified Developer Associate Practice Exams 4",description:"Additional practice questions covering AWS development topics.",questions:[{question:"A web application running in Amazon Elastic Beanstalk reads and writes a large number of related items in DynamoDB and processes each item one at a time. The network overhead of these transactions causes degradation in the application's performance. You were instructed by your manager to quickly refactor the application but without introducing major code changes such as implementing concurrency management or multithreading. Which of the following solutions is the EASIEST method to implement that will improve the application performance in a cost-effective manner?",answers:[{text:"Use DynamoDB Batch Operations API for GET, PUT, and DELETE operations.",isCorrect:!0},{text:"Upgrade the EC2 instances to a higher instance type.",isCorrect:!1},{text:"Enable DynamoDB Streams.",isCorrect:!1},{text:"Refactor the application to use DynamoDB transactional read and write APIs .",isCorrect:!1}],explanation:"For applications that need to read or write multiple items, DynamoDB provides the BatchGetItem and BatchWriteItem operations. Using these operations can reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the individual read or write operations in parallel. Your applications benefit from this parallelism without having to manage concurrency or threading. The batch operations are essentially wrappers around multiple read or write requests. For example, if a BatchGetItem request contains five items, DynamoDB performs five GetItem operations on your behalf. Similarly, if a BatchWriteItem request contains two put requests and four delete requests, DynamoDB performs two PutItem and four DeleteItem requests. In general, a batch operation does not fail unless all of the requests in the batch fail. For example, suppose you perform a BatchGetItemoperation but one of the individual GetItem requests in the batch fails. In this case, BatchGetItem returns the keys and data from the GetItemrequest that failed. The other GetItem requests in the batch are not affected. Hence, the correct answer is to use DynamoDB Batch Operations API for GET, PUT, and DELETE operations in this scenario. Upgrading the EC2 instances to a higher instance type is incorrect because the network overhead is the one that affects application performance and not the compute capacity. This is due to multiple read and write requests performed as single operations on DynamoDB, instead of a Batch operation. Enabling DynamoDB Streams is incorrect because a DynamoDB stream is just an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Apparently, this feature does not solve the application issue where there is a large volume of data being processed one by one, and not by batch. Refactoring the application to use DynamoDB transactional read and write APIs is incorrect because the Amazon DynamoDB transactions feature just simplifies the developer experience of making coordinated, all-or-nothing changes to multiple items both within and across tables. Transactions provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, enabling you to maintain data correctness in your applications easily. Take note that every transactional read and write API call consumes high RCU and WCUs, unlike eventual or strong consistency requests. Hence, this entails a significant increase in costs which contradicts the requirements of the scenario. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"},{question:"There is a requirement to improve the performance of your serverless application in AWS by increasing the allocated CPU available for your Lambda functions. Which of the following is the MOST appropriate solution that you should implement to meet this requirement?",answers:[{text:"Increase the memory configuration of your function.",isCorrect:!0},{text:"Use Lambda layers to optimize the performance of the Lambda function.",isCorrect:!1},{text:"Manually configure the CPU settings of the Lambda function to the maximum value.",isCorrect:!1},{text:"Configure the concurrency limit of your function to be the same as the account level concurrency limit.",isCorrect:!1}],explanation:"Lambda counts a request each time it starts executing in response to an event notification or invocation call, including test invokes from the console. You are charged based on the total number of requests processed across all of your Lambda functions. Duration is calculated from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 1ms. The price depends on the amount of memory you allocate to your function. In the AWS Lambda resource model, you choose the amount of memory you want for your function and are allocated proportional CPU power and other resources. An increase in memory size triggers an equivalent increase in CPU available to your function. Hence, the correct answer is to: Increase the memory configuration of your function. The option that says: Configuring the concurrency limit of your function to be the same as the account level concurrency limit is incorrect because this configuration is primarily used for managing the number of simultaneous executions of your function as well as the capacity reservations for that concurrency level. This will just limit the number of simultaneous executions of your function and not increase the allocated CPU. In addition, AWS Lambda will keep the unreserved concurrency pool at a minimum of 100 concurrent executions, so that functions that do not have specific limits set can still process requests. This means that you cannot configure the limit to be exactly the same as the account level limit. The option that says: Manually configuring the CPU settings of the Lambda function to the maximum value is incorrect because, in the first place, you cannot manually configure the CPU settings of your function. You have to increase the memory configuration of your function instead. The option that says: Use Lambda layers to optimize the performance of the Lambda function is incorrect. Lambda Layers is just an archive for any external dependencies or custom runtimes that you want to share between Lambda functions. You can speed up the deployment of your codes by moving external dependencies to a layer, however, this has a negligible impact on performance. References: https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html https://aws.amazon.com/lambda/pricing/ Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A cryptocurrency exchange portal has a key management service hosted in their on-premises data center, which stores encryption keys and uses an RSA asymmetric encryption algorithm. The company has recently implemented a hybrid cloud architecture in AWS and you were assigned to migrate the exchange portal to their cloud infrastructure. For security compliance, the keys should be stored in dedicated, third-party validated hardware security modules under your exclusive control. Which of the following is the BEST solution that you should implement to meet the above requirement?",answers:[{text:"Import the encryption keys from your on-premises key management service to AWS CloudHSM.",isCorrect:!0},{text:"Import the encryption keys from your on-premises key management service to AWS Secrets Manager as KMS Keys.",isCorrect:!1},{text:"Use AWS KMS to store and manage the encryption keys.",isCorrect:!1},{text:"Develop a custom key management service using the AWS Encryption SDK.",isCorrect:!1}],explanation:"AWS CloudHSM provides hardware security modules in AWS Cloud. A hardware security module (HSM) is a computing device that processes cryptographic operations and provides secure storage for cryptographic keys. When you use an HSM from AWS CloudHSM, you can perform a variety of cryptographic tasks: - Generate, store, import, export, and manage cryptographic keys, including symmetric keys and asymmetric key pairs. - Use symmetric and asymmetric algorithms to encrypt and decrypt data. - Use cryptographic hash functions to compute message digests and hash-based message authentication codes (HMACs). - Cryptographically sign data (including code signing) and verify signatures. - Generate cryptographically secure random data. You should consider using AWS CloudHSM instead of AWS KMS if you require: - Keys stored in dedicated, third-party validated hardware security modules under your exclusive control. - FIPS 140-2 compliance. - Integration with applications using PKCS#11, Java JCE, or Microsoft CNG interfaces. - High-performance in-VPC cryptographic acceleration (bulk crypto). Hence, the correct answer is to import the encryption keys from your on-premises key management service to AWS CloudHSM. Using AWS KMS to store and manage the encryption keys is incorrect. Although AWS KMS supports asymmetric encryption, it doesn't provide dedicated, third-party validated hardware security modules which are under your exclusive control. You have to use CloudHSM instead. Importing the encryption keys from your on-premises key management service to AWS Secrets Manager as KMS Keys is incorrect because you can't store KMS keys to AWS Secrets Manager. Developing a custom key management service using the AWS Encryption SDK is incorrect because this entails a lot of effort to implement. Moreover, the AWS Encryption SDK only encrypts your data using a symmetric key algorithm which doesn't comply with the requirements provided in the scenario. References: https://docs.aws.amazon.com/cloudhsm/latest/userguide/manage-keys.html https://aws.amazon.com/cloudhsm/faqs/ Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"},{question:"A company has a static website running in an Auto Scaling group of EC2 instances which they want to convert as a dynamic e-commerce web portal. One of the requirements is to use HTTPS to improve the security of their portal and also improve their search ranking as a reputable and secure site. A developer recently requested an SSL/TLS certificate from a third-party certificate authority (CA) which is ready to be imported to AWS. Which of the following services can the developer use to safely import the SSL/TLS certificate? (Select TWO.)",answers:[{text:"IAM certificate store",isCorrect:!0},{text:"AWS Certificate Manager",isCorrect:!0},{text:"Amazon Cognito",isCorrect:!1},{text:"CloudFront",isCorrect:!1},{text:"A private S3 bucket with versioning enabled",isCorrect:!1}],explanation:"To enable HTTPS connections to your website or application in AWS, you need an SSL/TLS server certificate. For certificates in a Region supported by AWS Certificate Manager (ACM), it is recommended that you use ACM to provision, manage, and deploy your server certificates. In unsupported Regions, you must use IAM as a certificate manager. ACM is the preferred tool to provision, manage, and deploy your server certificates. With ACM you can request a certificate or deploy an existing ACM or external certificate to AWS resources. Certificates provided by ACM are free and automatically renew. In a supported Region, you can use ACM to manage server certificates from the console or programmatically Use IAM as a certificate manager only when you must support HTTPS connections in a Region that is not supported by ACM. IAM securely encrypts your private keys and stores the encrypted version in IAM SSL certificate storage. IAM supports deploying server certificates in all Regions, but you must obtain your certificate from an external provider for use with AWS. You cannot upload an ACM certificate to IAM. Additionally, you cannot manage your certificates from the IAM Console. If you got your certificate from a third-party CA, import the certificate into ACM or upload it to the IAM certificate store. Hence, the correct answers are AWS Certificate Manager (ACM) and IAM certificate store. A private S3 bucket with versioning enabled is incorrect as S3 is not a suitable service to store the SSL certificate. You have to import it to either AWS Certificate Manager (ACM) or IAM certificate store. Amazon Cognito is incorrect because this is just a user identity and data synchronization service that helps you securely manage and synchronize app data for your users across their mobile devices. This service can't be used to store or import your SSL certificates. CloudFront is incorrect. Although you can upload certificates to CloudFront, it doesn't mean that you can import third-party SSL certificates on it. If you got your certificate from a third-party CA then you have to import the certificate into ACM or upload it to the IAM certificate store first. You would also not be able to export the certificate that you have loaded in CloudFront nor assign them to your EC2 or ELB instances as it would be tied to a single CloudFront distribution. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html https://aws.amazon.com/certificate-manager/ https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-procedures.html#cnames-and-https-uploading-certificates Check out this AWS Certificate Manager Cheat Sheet: https://tutorialsdojo.com/aws-certificate-manager/"},{question:"A serverless application, which uses a Lambda function integrated with API Gateway, provides data to a front-end application written in ReactJS. The users are complaining that they are getting HTTP 504 errors intermittently when they are using the application in peak times. The developer found no errors in the CloudWatch logs of the Lambda function. Which of the following is the MOST likely cause of this issue?",answers:[{text:"There is an authorization failure occurring between API Gateway and the Lambda function.",isCorrect:!1},{text:"The API Gateway automatically enabled throttling in peak times which caused the HTTP 504 errors.",isCorrect:!1},{text:"The memory allocated for the Lambda function is insufficient",isCorrect:!1},{text:"The underlying Lambda function has been running for more than 29 seconds causing the API Gateway request to time out.",isCorrect:!0}],explanation:"A gateway response is identified by a response type defined by API Gateway. The response consists of an HTTP status code, a set of additional headers that are specified by parameter mappings, and a payload that is generated by a non-VTL (Apache Velocity Template Language) mapping template. You can set up a gateway response for a supported response type at the API level. Whenever API Gateway returns a response of the type, the header mappings and payload mapping templates defined in the gateway response are applied to return the mapped results to the API caller. The following are the Gateway response types which are associated with the HTTP 504 error in API Gateway: INTEGRATION_FAILURE - The gateway response for an integration failed error. If the response type is unspecified, this response defaults to the DEFAULT_5XX type. INTEGRATION_TIMEOUT - The gateway response for an integration timed-out error. If the response type is unspecified, this response defaults to the DEFAULT_5XX type. For the integration timeout, the range is from 50 milliseconds to 29 seconds for all integration types, including Lambda, Lambda proxy, HTTP, HTTP proxy, and AWS integrations. In this scenario, there is an issue where the users are getting HTTP 504 errors in the serverless application. This means the Lambda function is working fine at times, but there are instances when it throws an error. Based on this analysis, the most likely cause of the issue is the INTEGRATION_TIMEOUT error since you will only get an INTEGRATION_FAILURE error if your AWS Lambda integration does not work at all in the first place. Hence, the correct answer is: The underlying Lambda function has been running for more than 29 seconds causing the API Gateway request to time out. The option that says: The memory allocated for the Lambda function is insufficient is incorrect. The fact that no errors were found in the CloudWatch Logs suggests that the function is not the bottleneck. The option that says: The API Gateway automatically enabled throttling in peak times which caused the HTTP 504 errors is incorrect because a large number of incoming requests will most likely produce an HTTP 502 or 429 error but not a 504 error. If executing the function would cause you to exceed a concurrency limit at either the account level (ConcurrentInvocationLimitExceeded) or function level (ReservedFunctionConcurrentInvocationLimitExceeded), Lambda may return a TooManyRequestsException as a response. For functions with a long timeout, your client might be disconnected during synchronous invocation while it waits for a response and returns an HTTP 504 error. The option that says: There is an authorization failure occurring between API Gateway and the Lambda function is incorrect because an authentication issue usually produces HTTP 403 errors and not 504s. The gateway response for authorization failures for missing authentication token errors, invalid AWS signature errors, or Amazon Cognito authentication problems is HTTP 403, which is why this option is unlikely to cause this issue. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/limits.html https://aws.amazon.com/about-aws/whats-new/2017/11/customize-integration-timeouts-in-amazon-api-gateway/ https://docs.aws.amazon.com/apigateway/latest/developerguide/supported-gateway-response-types.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A developer is creating a React application whose source code is hosted in GitHub. To help ensure proper functionality and identify any UI issues before going live, the developer must perform end-to-end (E2E) testing using Cypress. Which combination of actions should the developer take? (Select Two)",answers:[{text:"Update the amplify.yml file with appropriate configuration settings for Cypress.",isCorrect:!0},{text:"Update the amplifyconfiguration.json with appropriate configuration settings for Cypress.",isCorrect:!1},{text:"Include the location of the Cypress configuration file in the aws-exports.js file.",isCorrect:!1},{text:"Create an application in AWS Amplify Studio. Clone the application’s source code in a local environment and run amplify pull --appId APP_ID --envName ENV_NAME",isCorrect:!1},{text:"Connect the Github repository to AWS Amplify Hosting",isCorrect:!0}],explanation:"AWS Amplify is a set of purpose-built tools and features that enables frontend web and mobile developers to quickly and easily build full-stack applications on AWS. Amplify provides two services: -Amplify Hosting - provides a git-based workflow for hosting full-stack serverless web apps with continuous deployment. -Amplify Studio - a visual development environment that simplifies the creation of scalable, full-stack web and mobile apps. Use can use Amplify Studio to build your frontend UI with a set of ready-to-use UI components, create an app backend with AWS resources, and then connect the two together. Amplify Hosting provides deep integration with Cypress for End-to-End (E2E) testing, allowing developers to generate a UI report for their tests. To add Cypress tests to your application, you can update the build settings in the amplify.yml configuration file, which will enable Amplify to run the tests during the build process. Hence, the correct answers are: - Connect the Github repository to AWS Amplify Hosting - Update the amplify.yml file with appropriate configuration settings for Cypress. The option that says: Create an application in AWS Amplify Studio. Clone the application’s source code in a local environment and run amplify pull --appId APP_ID --envName ENV_NAME is incorrect. This is not necessary, as you will not be recreating the application from scratch. The option that says: Include the location of the Cypress configuration file in the aws-exports.js file is incorrect. The aws-exports.js is simply a configuration file containing information for an Amplify application's region, user pool, or identity pool. You can't include build settings for Cypress in this file. The option that says: Update the amplifyconfiguration.json with appropriate configuration settings for Cypress is incorrect because this file is just the same as aws-exports.js, but for Android and iOS projects. References: https://docs.aws.amazon.com/amplify/latest/userguide/welcome.html https://aws.amazon.com/blogs/mobile/running-end-to-end-cypress-tests-for-your-fullstack-ci-cd-deployment-with-amplify-console/ https://amplify-sns.workshop.aws/en/80_e2e_test/00_-cypress.html"},{question:"A developer runs a shell script that uses the aws s3 cp CLI to upload a large file to an S3 bucket. The S3 bucket is configured with Server-side encryption with AWS Key Management Service (SSE-KMS). An Access Denied error always shows up whenever the developer uploads a file with a size of 100 GB or more. However, whenever he uploads a smaller file, the request succeeds. Which of the following are possible reasons why this issue is happening? (Select TWO.)",answers:[{text:"The developer does not have the kms:Encrypt permission.",isCorrect:!1},{text:"The AWS CLI S3 commands perform a multipart upload when the file is large.",isCorrect:!0},{text:"The developer's IAM permission has an attached inline policy that restricts him from uploading a file to S3 with a size of 100 GB or more.",isCorrect:!1},{text:"The maximum size that can be encrypted in KMS is only 100 GB.",isCorrect:!1},{text:"The developer does not have the kms:Decrypt permission.",isCorrect:!0}],explanation:"If you are getting an Access Denied error when trying to upload a large file to your S3 bucket with an upload request that includes an AWS KMS key, then you have to confirm that you have permission to perform kms:Decrypt actions on the AWS KMS key that you're using to encrypt the object. To perform a multipart upload with encryption using an AWS KMS key, the requester must have permission to the kms:Decrypt and kms:GenerateDataKey* actions on the key. These permissions are required because Amazon S3 must decrypt and read data from the encrypted file parts before it completes the multipart upload. Hence, the correct answers in this scenario are: - The AWS CLI S3 commands perform a multipart upload when the file is large. - The developer does not have the kms:Decrypt permission. The option that says: The developer does not have the kms:Encrypt permission is incorrect. This permission is not necessary for you to have as it's typically used for direct encryption requests with the KMS key and is only applicable for small data up to 4KB. Amazon S3 operates differently; it generates a data key from your KMS key to encrypt files. Behind the scenes, Amazon S3 actually generates a data key from your KMS key to encrypt files. The option that says: The developer's IAM permission has an attached inline policy that restricts him from uploading a file to S3 with a size of 100 GB or more is incorrect because inline policies are just policies that you create, manage, and embed directly into a single user, group, or role. This is unlikely to happen as there is no direct way to restrict a user from uploading a file with a specific size constraint. The option that says: The maximum size that can be encrypted in KMS is only 100 GB is incorrect because there is no such limitation in KMS. References: https://aws.amazon.com/premiumsupport/knowledge-center/s3-large-file-encryption-kms-key/ https://docs.aws.amazon.com/cli/latest/userguide/cli-services-s3-commands.html#using-s3-commands-managing-objects Check out this AWS Key Management Service (KMS) Cheat Sheet: https://tutorialsdojo.com/aws-key-management-service-aws-kms/"},{question:"A company has an application that is using CloudFront to serve their static contents to their users around the globe. They are receiving a number of bad reviews from their customers lately because it takes a lot of time to log into their website. Sometimes, their users are also getting HTTP 504 errors which is why the developer was instructed to fix this problem immediately. Which of the following combination of options should the developer use together to set up a cost-effective solution for this scenario? (Select TWO.)",answers:[{text:"Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your Lambda functions to execute the authentication process in AWS locations closer to the users.",isCorrect:!0},{text:"Launch your application to multiple AWS regions to serve your global users. Use a Route 53 record with latency routing policy to route incoming traffic to the region with the best latency to the user.",isCorrect:!1},{text:"Add a Cache-Control max-age directive to your objects in CloudFront and specify the longest practical value for max-age to increase the cache hit ratio of your CloudFront distribution.",isCorrect:!1},{text:"Configure an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses.",isCorrect:!0},{text:"Launch your application to multiple and geographically disperse VPCs on various AWS regions then create a transit VPC to easily connect all your resources. Use several Lambda functions in each region using the AWS Serverless Application Model (SAM) service to improve the overall application performance.",isCorrect:!1}],explanation:"Lambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points: - After CloudFront receives a request from a viewer (viewer request) - Before CloudFront forwards the request to the origin (origin request) - After CloudFront receives the response from the origin (origin response) - Before CloudFront forwards the response to the viewer In the given scenario, you can use Lambda@Edge to allow your Lambda functions to customize the content that CloudFront delivers and to execute the authentication process in AWS locations closer to the users. In addition, you can set up an origin failover by creating an origin group with two origins with one as the primary origin and the other as the second origin, which CloudFront automatically switches to when the primary origin fails. This will alleviate the occasional HTTP 504 errors that users are experiencing. Hence, the correct answers in this scenario are: - Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your Lambda functions to execute the authentication process in AWS locations closer to the users. - Configure an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses. The option that says: Launch your application to multiple AWS regions to serve your global users. Use a Route 53 record with latency routing policy to route incoming traffic to the region with the best latency to the user is incorrect. Although this may resolve the performance issue, this solution entails a significant implementation cost since you have to deploy your application to multiple AWS regions. Remember that the scenario asks for a solution that will improve the performance of the application with minimal cost. The option that says: Launch your application to multiple and geographically disperse VPCs on various AWS regions then create a transit VPC to easily connect all your resources. Use several Lambda functions in each region using the AWS Serverless Application Model (SAM) service to improve the overall application performance is incorrect. Although setting up multiple VPCs across various regions which are connected with a transit VPC is valid, this solution still entails higher setup and maintenance costs. A more cost-effective option would be to use Lambda@Edge instead. The option that says: Add a Cache-Control max-age directive to your objects in CloudFront and specify the longest practical value for max-age to increase the cache hit ratio of your CloudFront distribution is incorrect because improving the cache hit ratio for the CloudFront distribution is irrelevant in this scenario. You can improve your cache performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content. However, take note that the problem in the scenario is the slow authentication process of your global users and not just the caching of the static objects. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html Check out these Amazon CloudFront and AWS Lambda Cheat Sheets: https://tutorialsdojo.com/amazon-cloudfront/ https://tutorialsdojo.com/aws-lambda/"},{question:"An application architect manages several AWS accounts for staging, testing, and production environments, which are used by several development teams. For application deployments, the developers use the similar base CloudFormation template for their applications. Which of the following can allow the developer to effectively manage the updates on this template across all AWS accounts with minimal effort?",answers:[{text:"Update the stacks on multiple AWS accounts using CloudFormation StackSets.",isCorrect:!0},{text:"Define and manage stack instances on multiple AWS Accounts using CloudFormation Stack Instances.",isCorrect:!1},{text:"Create and manage stacks on multiple AWS accounts using CloudFormation Change Sets.",isCorrect:!1},{text:"Use AWS CodePipeline to automate the deployment of CloudFormation templates across multiple accounts.",isCorrect:!1}],explanation:"AWS CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified regions. A stack set lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. All the resources included in each stack are defined by the stack set's AWS CloudFormation template. As you create the stack set, you specify the template to use, as well as any parameters and capabilities that the template requires. Hence, the correct solution in this scenario is to update the stacks on multiple AWS accounts using CloudFormation StackSets. After you've defined a stack set, you can create, update, or delete stacks in the target accounts and regions you specify. When you create, update, or delete stacks, you can also specify operational preferences, such as the order of regions in which you want the operation to be performed, the failure tolerance beyond which stack operations stop, and the number of accounts in which operations are performed on stacks concurrently. Remember that a stack set is a regional resource so if you create a stack set in one region, you cannot see it or change it in other regions. The option that says: Creating and managing stacks on multiple AWS accounts using CloudFormation Change Sets is incorrect because Change Sets only allow you to preview how proposed changes to a stack might impact your running resources. In this scenario, the most suitable way to meet the requirement is to use StackSets. The option that says: Defining and managing stack instances on multiple AWS Accounts using CloudFormation Stack Instances is incorrect because a stack instance is simply a reference to a stack in a target account within a region. Remember that a stack instance is associated with one stack set which is why this is just one of the components of CloudFormation StackSets. The option that says: Use AWS CodePipeline to automate the deployment of CloudFormation templates across multiple accounts is incorrect. AWS CodePipeline can automate the deployment process, but it is primarily a CI/CD tool. While it can be configured to deploy CloudFormation templates, it does not inherently provide the same level of centralized management for multiple accounts and regions as StackSets does. References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-getting-started.html Check out this AWS CloudFormation Cheat Sheet: https://tutorialsdojo.com/aws-cloudformation/"},{question:"Your team is developing a serverless application, which is composed of multiple Lambda functions which process data from an SQS queue and stores the results to an RDS database. To comply with the strict IT policy of the company, you were instructed to configure these functions to share the same connection string that should be properly secured and encrypted. What should you do to protect, encrypt, and share your database credentials in AWS?",answers:[{text:"Store the database credentials as environment variables with KMS encryption which will be shared by the Lambda functions.",isCorrect:!1},{text:"Use AWS Systems Manager Parameter Store as a Secure String Parameter.",isCorrect:!0},{text:"Use IAM DB Authentication in RDS to allow encrypted connections from each Lambda function.",isCorrect:!1},{text:"Encrypt the database credentials and store them in an S3 bucket which the Lambda functions can fetch.",isCorrect:!1}],explanation:"AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. You can store values as plain text or encrypted data. You can then reference values by using the unique name that you specified when you created the parameter. Parameter Store offers the following benefits and features: - Use a secure, scalable, hosted secrets management service (No servers to manage). - Improve your security posture by separating your data from your code. - Store configuration data and secure strings in hierarchies and track versions. - Control and audit access at granular levels. - Configure change notifications and trigger automated actions. - Tag parameters individually, and then secure access from different levels, including operational, parameter, Amazon EC2 tag, or path levels. - Reference AWS Secrets Manager secrets by using Parameter Store parameters. Hence, the correct solution for this scenario is to use AWS Systems Manager Parameter Store as a Secure String Parameter. Encrypting the database credentials and storing them in an S3 bucket which the Lambda functions can fetch is incorrect because it is a security risk to store sensitive database passwords and credentials in S3, even though the data is encrypted. A more suitable and secure way is to use the AWS Secrets Manager or the Systems Manager Parameter Store. Storing the database credentials as environment variables with KMS encryption which will be shared by the Lambda functions is incorrect because even though the credentials will be encrypted, these environment variables will only be used by an individual Lambda function and cannot be shared. Using IAM DB Authentication in RDS to allow encrypted connections from each Lambda function is incorrect. While using IAM DB Authentication can significantly enhance security for database connections in AWS environments, it's more about controlling access rather than managing the sharing of encrypted credentials like a database connection string across multiple applications or services. Additionally, this method is limited to certain types of databases supported by AWS, such as Amazon RDS for MySQL and PostgreSQL, and might not be available for all database engines. Lastly, in the scenario where Lambda functions need to share and securely access a connection string, other AWS services like the Systems Manager Parameter Store would be more appropriate to meet all the specified needs. References: https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html https://aws.amazon.com/blogs/compute/sharing-secrets-with-aws-lambda-using-aws-systems-manager-parameter-store/ Check out this AWS Systems Manager Cheat Sheet: https://tutorialsdojo.com/aws-systems-manager/"},{question:"You are developing an application that continuously collects data about player-game interactions and feeds the real-time data into your gaming platform. There is a requirement to make the system highly scalable to accommodate the sudden influx of gamers that will use the platform. Which AWS service will help you achieve this?",answers:[{text:"AWS Kinesis Data Stream",isCorrect:!0},{text:"AWS Elastic Map Reduce",isCorrect:!1},{text:"AWS Redshift",isCorrect:!1},{text:"AWS DynamoDB",isCorrect:!1}],explanation:"Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more. All other options are incorrect because these services do not provide real-time processing, unlike Kinesis. References: https://aws.amazon.com/kinesis/data-streams/ https://aws.amazon.com/kinesis/data-streams/faqs/ Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/"},{question:"A company is developing an online system that lets patients schedule appointments with their preferred doctors at medical centers all over the country. The company uses Amazon DynamoDB as its primary database. The DynamoDB Streams feature is enabled on the DynamoDB table to capture all changes made to the booking data. A Lambda function integrated with Amazon EventBridge (Amazon CloudWatch Events) is used to process the data stream every 36 hours and then store the results in an S3 bucket. There are a lot of updated items in DynamoDB that are not sent to the S3 bucket, even though there are no errors in the logs. Which of the following is the MOST appropriate solution for this issue?",answers:[{text:"Set the value of StreamViewType parameter in DynamoDB Streams to NEW_IMAGE.",isCorrect:!1},{text:"Set the value of StreamViewType parameter in DynamoDB Streams to NEW_AND_OLD_IMAGES.",isCorrect:!1},{text:"Decrease the interval of running your function to 24 hours.",isCorrect:!0},{text:"Increase the interval of running your function to 48 hours.",isCorrect:!1}],explanation:'A DynamoDB stream is an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attribute(s) of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the "before" and "after" images of modified items. All data in DynamoDB Streams is subject to a 24-hour lifetime. You can retrieve and analyze the last 24 hours of activity for any given table; however, data older than 24 hours is susceptible to trimming (removal) at any moment. If you disable a stream on a table, the data in the stream will continue to be readable for 24 hours. After this time, the data expires and the stream records are automatically deleted. Note that there is no mechanism for manually deleting an existing stream; you just need to wait until the retention limit expires (24 hours), and all the stream records will be deleted. Hence, the correct answer is to decrease the interval of running your function to 24 hours because in DynamoDB Streams, data older than 24 hours is susceptible to trimming (removal) at any moment. Increasing the interval of running your function to 48 hours is incorrect because this will actually make the problem worse. Considering that the data in DynamoDB Streams only lasts for 24 hours, you should actually decrease the interval of running your function and not further increase it. Setting the value of StreamViewType parameter in DynamoDB Streams to NEW_AND_OLD_IMAGES is incorrect because this just configures DynamoDB to write both the new and the old item images of the item to the stream. Setting the Stream View Type is actually irrelevant in this scenario since the problem is about missing data and not missing old or new values of the items. Setting the value of StreamViewType parameter in DynamoDB Streams to NEW_IMAGE is incorrect because this just configures the DynamoDB to write only the new image of the item to the stream. Just as mentioned above, the root cause of the issue is the length of the interval of running the Lambda function and not the DynamoDB Streams configuration. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_StreamSpecification.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ AWS Lambda Integration with Amazon DynamoDB Streams: https://tutorialsdojo.com/aws-lambda-integration-with-amazon-dynamodb-streams/'},{question:"A company is developing a real-time analytics platform that allows users to submit data through the Amazon API Gateway. The data is processed by AWS Lambda and stored in Amazon S3. However, during high-traffic periods, users experience timeouts despite the Lambda function completing tasks on time. The company needs to analyze API Gateway metrics in Amazon CloudWatch to determine the cause of these timeouts. Which two API Gateway metrics in Amazon CloudWatch should be reviewed to troubleshoot the delay issues? (Select TWO.)",answers:[{text:"5XXError",isCorrect:!1},{text:"Count",isCorrect:!1},{text:"IntegrationLatency",isCorrect:!0},{text:"Latency",isCorrect:!0},{text:"4XXError",isCorrect:!1}],explanation:"Amazon API Gateway is a fully managed service for creating, deploying, and managing APIs at scale. It is an intermediary between clients and backend services, offering features like authorization, traffic management, and monitoring. API Gateway integrates with Amazon CloudWatch to provide metrics that help diagnose performance issues, analyze traffic patterns, and ensure reliable operation. API Gateway metrics include Latency, which measures the total time taken to process a request, and IntegrationLatency, which tracks the time API Gateway spends communicating with the backend service. Metrics like 4XXError and 5XXError provide insights into client-side and server-side errors, helping pinpoint request or integration failures. Count tracks the total requests, offering visibility into traffic patterns and usage trends. Additionally, CacheHitCount and CacheMissCount are useful for understanding the effectiveness of caching; high cache hits reduce backend load and improve response times, while cache misses indicate requests that bypassed the cache and were processed by the backend. By monitoring these metrics in CloudWatch and setting alarms on critical thresholds, organizations can optimize API performance, troubleshoot issues, and maintain high availability. For more insights, enabling logging in CloudWatch Logs can provide detailed data for debugging and root cause analysis. Hence, the correct answers are: - IntegrationLatency. - Latency. The option that says: 4XXError is incorrect. This metric primarily captures client-side errors, such as unauthorized access or invalid requests, and does not reflect delays or timeouts in API Gateway. Since the scenario focuses on analyzing the root cause of timeouts during high traffic, monitoring client-side errors is simply not relevant for identifying latency issues. The option that says: Count is incorrect. While this metric tracks the total number of requests received by API Gateway, it only provides insights into traffic volume. It does not indicate latency, processing delays, or timeouts. Although it is useful for understanding traffic trends, it is not directly helpful in diagnosing performance issues. The option that says: 5XXError is incorrect because this metric tracks server-side errors, such as integration failures or configuration issues, which are not evident in this scenario. The Lambda function completes within its specified time, and delays are observed at the API Gateway. Thus, monitoring server-side errors would simply not address the identified latency problem. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-metrics-and-dimensions.html https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A web application is currently deployed on an On-Demand Linux EC2 instance that connects to an Amazon RDS database. Users have frequently reported that the application crashes intermittently. The support team has reviewed the logs in CloudWatch but has been unable to identify the root cause. To enhance troubleshooting, the team needs to monitor additional metrics, including memory utilization, swap usage, and the count of idle and active processes running on the instance. Which solution would be the MOST appropriate to implement in this situation?",answers:[{text:"Install the Amazon CloudWatch Logs agent to the EC2 instance.",isCorrect:!0},{text:"Install the AWS X-Ray daemon on the EC2 instance.",isCorrect:!1},{text:"Use detailed monitoring in CloudWatch.",isCorrect:!1},{text:"Use AWS CloudShell to consolidate all metrics in a single dashboard.",isCorrect:!1}],explanation:"You can use the CloudWatch agent to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. The agent supports both Windows Server and Linux, and enables you to select the metrics to be collected, including sub-resource metrics such as per-CPU core. Aside from the usual metrics, it also tracks the memory, swap, and disk space utilization metrics of your server. Hence, the most suitable solution to use in this scenario is to: Install the Amazon CloudWatch Logs agent to the EC2 instance. The option that says: Use AWS CloudShell to consolidate all metrics in a single dashboard is incorrect because this service is simply a command-line interface used for managing AWS resources from a terminal. It is not a monitoring tool and does not collect or display EC2 instance metrics. The option that says: Using detailed monitoring in CloudWatch is incorrect because this will typically send metric data of the EC2 instance to CloudWatch in 1-minute periods instead of 5-minute intervals. The option that says: Install the AWS X-Ray daemon on the EC2 instance is incorrect. Although this is helpful for troubleshooting your application, it does not have the capability to track the memory and swap usage of the instance. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html Check out these Amazon EC2 and CloudWatch Cheat Sheets: https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/ https://tutorialsdojo.com/amazon-cloudwatch/"},{question:"You are developing a new batch job for the enterprise application suite in your company, which is hosted in an Auto Scaling group of EC2 instances behind an ELB. The application is using an S3 bucket configured with Server-Side Encryption with AWS KMS Keys (SSE-KMS). The batch job must upload files to the bucket using the default AWS KMS key to protect the data at rest. What should you do to satisfy this requirement with the LEAST amount of configuration?",answers:[{text:"Include the x-amz-server-side-encryption header with a value of aws:kms as well as the x-amz-server-side-encryption-aws-kms-key-id header containing the ID of the default AWS KMS key in your upload request.",isCorrect:!1},{text:"Include the x-amz-server-side​-encryption​-customer-algorithm, x-amz-server-side-encryption-customer-key, and x-amz-server-side-encryption-customer-key-MD5 headers with appropriate values in the upload request.",isCorrect:!1},{text:"Include the x-amz-server-side-encryption header with a value of AES256 in your upload request.",isCorrect:!1},{text:"Include the x-amz-server-side-encryption header with a value of aws:kms in your upload request.",isCorrect:!0}],explanation:"Server-side encryption is about protecting data at rest. AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. AWS KMS uses KMS keys to encrypt your Amazon S3 objects. You use AWS KMS via the AWS Management Console or AWS KMS APIs to centrally create encryption keys, define the policies that control how keys can be used, and audit key usage to prove they are being used correctly. You can use these keys to protect your data in Amazon S3 buckets. The first time you add an SSE-KMS–encrypted object to a bucket in a region, a default KMS key is created for you automatically. This key is used for SSE-KMS encryption unless you select a KMS key that you created separately using AWS Key Management Service. Creating your own KMS key gives you more flexibility, including the ability to create, rotate, disable, and define access controls and audit the encryption keys used to protect your data. To upload an object to the S3 bucket, which uses SSE-KMS, you have to send a request with an x-amz-server-side-encryption header with the value of aws:kms. There's also an optional x-amz-server-side-encryption-aws-kms-key-id header, which specifies the ID of the AWS KMS master encryption key that was used for the object. The Amazon S3 API also supports encryption context with the x-amz-server-side-encryption-context header. When you upload an object, you can specify the KMS key using the x-amz-server-side-encryption-aws-kms-key-id header. If the header is not present in the request, Amazon S3 assumes the default KMS key. Regardless, the KMS key ID that Amazon S3 uses for object encryption must match the KMS key ID in the policy, otherwise, Amazon S3 denies the request. Hence, the correct is: Include the x-amz-server-side-encryption header with a value of aws:kms in your upload request. The option that says: Including the x-amz-server-side​-encryption​-customer-algorithm, x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers with appropriate values in the upload request is incorrect because these headers are only required if your S3 bucket is using Server-Side Encryption with Customer-Provided Keys (SSE-C). The option that says: Including the x-amz-server-side-encryption header with a value of aws:kms as well as the x-amz-server-side-encryption-aws-kms-key-id header containing the ID of the default AWS KMS key in your upload request is incorrect. Although this is a valid option, you actually don't need to add the x-amz-server-side-encryption-aws-kms-key-id header if you will be using the default AWS KMS key. Take note that the scenario explicitly mentioned to provide a solution with the LEAST amount of configuration. The option that says: Including the x-amz-server-side-encryption header with a value of AES256 in your upload request is incorrect because the value should be set as aws:kms instead. The value of AES256 is only applicable for SSE-S3 and SSE-C. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html#RESTObjectPUT-responses-examples https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/ Check out these Amazon S3 and AWS KMS Cheat Sheets: https://tutorialsdojo.com/amazon-s3/ https://tutorialsdojo.com/aws-key-management-service-aws-kms/"},{question:"A company is developing a distributed system which will use a Lambda function that will be invoked asynchronously. In the event of failure, the function must be retried twice before sending the unprocessed events to an Amazon SQS queue through the use of Dead Letter Queue (DLQ). Which of the following is the correct way to implement a DLQ in Lambda?",answers:[{text:"Specify the Amazon Resource Name of the SQS Queue in the Lambda function's DeadLetterConfig parameter.",isCorrect:!0},{text:"Specify the AWS Service Namespace of the SQS Queue in the Lambda function's DeadLetterConfig parameter.",isCorrect:!1},{text:"Specify the AWS Service Namespace of the SQS Queue in the AWS::Lambda::Function resource of the CloudFormation template that you'll use for deploying the function.",isCorrect:!1},{text:"Specify the Amazon Resource Name of the SQS Queue in the Transform section of the AWS SAM template that you'll use for deploying the function.",isCorrect:!1}],explanation:"Any Lambda function invoked asynchronously is retried twice before the event is discarded. If the retries fail and you're unsure why, use Dead Letter Queues (DLQ) to direct unprocessed events to an Amazon SQS queue or an Amazon SNS topic to analyze the failure. AWS Lambda directs events that cannot be processed to the specified Amazon SNS topic or Amazon SQS queue. Functions that don't specify a DLQ will discard events after they have exhausted their retries. Hence, the correct answer is to specify the Amazon Resource Name of the SQS Queue in the Lambda function's DeadLetterConfig parameter. Specifying the AWS Service Namespace of the SQS Queue in the AWS::Lambda::Function resource of the CloudFormation template that you'll use for deploying the function is incorrect because you have to use the Amazon Resource Name (ARN) of the SQS Queue and not the service namespace. Moreover, you have to configure this in the Lambda function's DeadLetterConfig parameter and not in the CloudFormation template. Specifying the AWS Service Namespace of the SQS Queue in the Lambda function's DeadLetterConfig parameter is incorrect because you have to specify the ARN of the queue and not its service namespace. Specifying the Amazon Resource Name of the SQS Queue in the Transform section of the AWS SAM template that you'll use for deploying the function is incorrect. Although it is right to use the Amazon Resource Name of the SQS Queue, you still have to set this in the Lambda function's DeadLetterConfig parameter and not on the AWS SAM template. References: https://docs.aws.amazon.com/lambda/latest/dg/dlq.html https://docs.aws.amazon.com/lambda/latest/dg/retries-on-errors.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A developer will be building a game data feed application which will continuously collect data about player-game interactions and feed the data into your gaming platform. The application uses the Kinesis Client Library to process the data stream from the Amazon Kinesis Data Streams and stores the data to Amazon DynamoDB. It is required that the system should have enough shards and EC2 instances in order to handle failover and adequately process the amount of data coming in and out of the stream. Which of the following ratio of the number of Kinesis shards to EC2 worker instances should the developer implement to achieve the above requirement in the most cost-effective and highly available way?",answers:[{text:"4 shards : 8 instances",isCorrect:!1},{text:"6 shards : 1 instance",isCorrect:!1},{text:"4 shards : 2 instances",isCorrect:!0},{text:"1 shard : 6 instances",isCorrect:!1}],explanation:"A stream is composed of one or more shards, each of which provides a fixed unit of capacity. The total capacity of the stream is the sum of the capacities of its shards. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that shard. It also tracks the shards in the stream using an Amazon DynamoDB table. Typically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances. Since the question requires the system to smoothly process streaming data, a fair number of shards and instances are required. By launching 4 shards, the stream will have more capacity for reading and writing data. By launching 2 instances, each instance will focus on processing two shards. It also provides high availability in the event that one instance goes down. Therefore, the ratio of 4 shards : 2 instances is the correct answer. The 1 shard : 6 instances ratio is incorrect because having just one shard for the stream will be insufficient and in the event that your incoming data rate increases, this single shard will not be able to handle the load. The 6 shards : 1 instance ratio is incorrect because having just one instance to process multiple shards will be insufficient since the processing capacity of your system will be severely limited. You have to allocate more instances in proportion to the number of open shards in your data stream. Moreover, a single instance is not a highly available option since the application doesn't have a backup instance to process the shards in the event of an outage. The 4 shards : 8 instances ratio is incorrect because launching more instances than the number of open shards will not improve the processing of the stream as it is only useful for failure standby purposes. Take note that each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. In addition, this option is not the most cost-effective choice as well. References: https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/"},{question:"A Lambda function is over 80 MB in size, which exceeds the deployment package size limit for direct uploads. You want to refactor the function to pull in additional code and other dependencies from another source, which will reduce the size of the deployment. Which feature of Lambda should you use in order to implement the above task?",answers:[{text:"Layers",isCorrect:!0},{text:"Execution Context",isCorrect:!1},{text:"Alias",isCorrect:!1},{text:"Environment Variable",isCorrect:!1}],explanation:"You can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package. Layers let you keep your deployment package small, which makes development easier. You can avoid errors that can occur when you install and package dependencies with your function code. For Node.js, Python, and Ruby functions, you can develop your function code in the Lambda console as long as you keep your deployment package under 3 MB. A function can use up to 5 layers at a time. The total unzipped size of the function and all layers can't exceed the unzipped deployment package size limit of 250 MB. You can create layers, or use layers published by AWS and other AWS customers. Layers support resource-based policies for granting layer usage permissions to specific AWS accounts, AWS Organizations, or all accounts. Layers are extracted to the /opt directory in the function execution environment. Each runtime looks for libraries in a different location under /opt, depending on the language. Structure your layer so that function code can access libraries without additional configuration. Hence, the correct answer is to use Lambda Layers. Alias is incorrect because this is just like a pointer to a specific Lambda function version. This will not enable your function to pull in additional code and other dependencies from another source. Execution Context is incorrect because this is a temporary runtime environment that initializes any external dependencies of your Lambda function code, such as database connections or HTTP endpoints. Environment Variable is incorrect because this just enables you to dynamically pass settings to your function code and libraries, without making changes to your code. References: https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html https://docs.aws.amazon.com/lambda/latest/dg/limits.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A mobile game has a DynamoDB table named TutorialsDojoScores which keeps track of the users and their respective scores. Each item in the table is identified by the FighterId attribute as its partition key and the FightTitle attribute as the sort key. A developer needs to retrieve data from non-key attributes of the table named DojoTopScores and DojoDateTime attributes. Which type of index should the developer add in the table to speed up queries on non-key attributes?",answers:[{text:"Primary Index",isCorrect:!1},{text:"Global Secondary Index",isCorrect:!0},{text:"Local Secondary Index",isCorrect:!1},{text:"Sparse Index",isCorrect:!1}],explanation:'Amazon DynamoDB provides fast access to items in a table by specifying primary key values. However, many applications might benefit from having one or more secondary (or alternate) keys available, to allow efficient access to data with attributes other than the primary key. To address this, you can create one or more secondary indexes on a table, and issue Query or Scan requests against these indexes. A secondary index is a data structure that contains a subset of attributes from a table, along with an alternate key to support Query operations. You can retrieve data from the index using a Query, in much the same way as you use Query with a table. A table can have multiple secondary indexes, which gives your applications access to many different query patterns. It is considered "global" because queries on the index can span all of the data in the base table, across all partitions. To speed up queries on non-key attributes, you can create a global secondary index. A global secondary index contains a selection of attributes from the base table, but they are organized by a primary key that is different from that of the table. The index key does not need to have any of the key attributes from the table; it doesn\'t even need to have the same key schema as a table. Hence, the correct answer in this scenario is to add a Global Secondary Index. Sparse index is incorrect because parse indexes are only useful for queries over a small subsection of a table. For any item in a table, DynamoDB writes a corresponding index entry only if the index sort key value is present in the item. If the sort key doesn\'t appear in every table item, the index is said to be "sparse". Local Secondary Index is incorrect because this is used for queries which use the same partition key value, and in addition, you can\'t add this index to an already existing table. A local secondary index has the same partition key as the base table, but has a different sort key. It is "local" in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value. Primary Index is incorrect because this one actually refers to the partition key, which is the FighterId attribute in this scenario. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/'},{question:'A developer has recently released a new Lambda function which calculates accruals, interests, and other financial data. This function must have a streamlined integration setup with API Gateway. The requirement is to pass the incoming request from the client as the input to the backend Lambda function, via HTTPS, in the following format: { "resource": "Resource path", "path": "Path parameter", "httpMethod": "Incoming request\'s method name" "headers": {String containing incoming request headers} "multiValueHeaders": {List of strings containing incoming request headers} "queryStringParameters": {query string parameters } "multiValueQueryStringParameters": {List of query string parameters} "pathParameters": {path parameters} "stageVariables": {Applicable stage variables} "requestContext": {Request context, including authorizer-returned key-value pairs} "body": "A JSON string of the request payload." "isBase64Encoded": "A boolean flag to indicate if the applicable request payload is Base64-encode"} Which of the following options is the MOST appropriate method to use to meet this requirement?',answers:[{text:"Lambda custom integration",isCorrect:!1},{text:"HTTP Proxy integration",isCorrect:!1},{text:"HTTP custom integration",isCorrect:!1},{text:"Lambda proxy integration",isCorrect:!0}],explanation:"You choose an API integration type according to the types of integration endpoint you work with and how you want data to pass to and from the integration endpoint. For a Lambda function, you can have two types of integration: - Lambda proxy integration - Lambda custom integration In Lambda proxy integration, the setup is simple. If your API does not require content encoding or caching, you only need to set the integration's HTTP method to POST, the integration endpoint URI to the ARN of the Lambda function invocation action of a specific Lambda function, and the credential to an IAM role with permissions to allow API Gateway to call the Lambda function on your behalf. In Lambda non-proxy (or custom) integration, in addition to the proxy integration setup steps, you also specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. For an AWS service action, you have the AWS integration of the non-proxy type only. API Gateway also supports the mock integration, where API Gateway serves as an integration endpoint to respond to a method request. The Lambda custom integration is a special case of the AWS integration, where the integration endpoint corresponds to the function-invoking action of the Lambda service. The Lambda proxy integration type (AWS_PROXY) lets an API method be integrated with the Lambda function invocation action with a flexible, versatile, and streamlined integration setup. This integration relies on direct interactions between the client and the integrated Lambda function. With this type of integration, also known as the Lambda proxy integration, you do not set the integration request or the integration response. API Gateway passes the incoming request from the client as the input to the backend Lambda function. The integrated Lambda function takes the input of this format and parses the input from all available sources, including request headers, URL path variables, query string parameters, and applicable body. The function returns the result following this output format. This is the preferred integration type to call a Lambda function through API Gateway and is not applicable to any other AWS service actions, including Lambda actions other than the function-invoking action. Hence, Lambda proxy integration is correct as it matches the description depicted in the scenario. Lambda custom integration is incorrect because this type requires you to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. HTTP custom integration is incorrect because this type is only used where you need to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. Take note that the scenario uses an application hosted in Lambda which is why you have to use Lambda integration instead. HTTP proxy integration is incorrect because the scenario uses an application hosted in Lambda which is why you have to use Lambda integration instead. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-simple-proxy-for-lambda-input-format Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A company has a latency-sensitive service running on AWS Fargate, which is fronted by an Application Load Balancer (ALB). A CloudFront distribution uses the ALB as its origin and presents a custom domain for clients to access the service. The service authenticates requests by validating the JSON Web Token (JWT) obtained from the Authorization header sent by clients. Lately, there has been a significant influx of login attempts from unauthenticated users, which increases the CPU utilization of the Fargate tasks. Which solution would reduce the load on the Fargate tasks in the most operationally efficient manner?",answers:[{text:"Create a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.",isCorrect:!0},{text:"Create a Lambda@Edge function for JWT validation. Attach it to the Origin Response event of the CloudFront distribution.",isCorrect:!1},{text:"Create a Lambda function that performs JWT validation. Configure the ALB to route login requests to the Lambda function.",isCorrect:!1},{text:"Enable auto-scaling on the Fargate tasks.",isCorrect:!1}],explanation:"CloudFront Functions allows you to write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. This feature is designed for operations that can be processed with low latency at the edge locations of AWS, such as: - Cache key normalization – You can transform HTTP request attributes (headers, query strings, cookies, even the URL path) to create an optimal cache key, which can improve your cache hit ratio. - Header manipulation – You can insert, modify, or delete HTTP headers in the request or response. For example, you can add a True-Client-IP header to every request. - Status code modification and body generation – You can evaluate headers and respond back to viewers with customized content. - URL redirects or rewrites – You can redirect viewers to other pages based on information in the request or rewrite all requests from one path to another. - Request authorization – You can validate hashed authorization tokens, such as JSON web tokens (JWT), by inspecting authorization headers or other request metadata. When you associate a CloudFront function with a CloudFront distribution, it allows CloudFront to intercept requests and responses at CloudFront edge locations. CloudFront functions can only be invoked during two specific events: when CloudFront receives a request from a viewer (viewer request) and before CloudFront returns the response to the viewer (viewer response). In the given scenario, by attaching the validation function to the Viewer Request event, requests can be authenticated right when they hit the CloudFront cache and before they are forwarded to your AWS Fargate service. This method not only helps in reducing unnecessary traffic to your Fargate tasks but also improves overall latency for valid requests, as they don't have to wait behind unauthenticated requests being processed by your backend infrastructure. In addition, because CloudFront Functions operate at the edge locations of AWS's infrastructure, they are highly scalable and can handle a very high number of requests per second. Hence, the correct answer is: Create a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution. The option that says: Create a Lambda function that performs JWT validation. Configure the ALB to route login requests to the Lambda function is incorrect. This option doesn't eliminate the problem of unauthenticated requests reaching the backend infrastructure. All requests, regardless of authentication, will still need to traverse the network to reach the ALB and Lambda, which is not operationally efficient and could increase latency. The option that says: Enable auto-scaling on the Fargate tasks is incorrect. While auto-scaling could help handle the increased load by adding more tasks as demand increases, it is a reactive measure and not a long-term solution. It doesn't prevent unauthenticated requests from consuming resources on Fargate. The option that says: Create a Lambda@Edge function for JWT validation. Attach it to the Origin Response event of the CloudFront distribution is incorrect. The Origin Response event is triggered after the request has been processed by your origin (in this case, the ALB) and the origin has returned a response to CloudFront. Validating the JWT at this stage would not reduce the number of unauthenticated requests reaching your Fargate service. It's too late in the request processing flow to prevent unauthenticated requests from consuming resources on your backend infrastructure. Moreover, this could also introduce errors in the CloudFront flow. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cloudfront-functions.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/example-function-validate-token.html Check out this Amazon CloudFront Cheat Sheet: https://tutorialsdojo.com/amazon-cloudfront/"},{question:"A mobile game has a serverless backend in AWS which is composed of Lambda, API Gateway, and DynamoDB. It writes 100 items per second to the DynamoDB table and the size is 1.5 KB per item. The table has a provisioned WCU of 100 but the write requests are still being throttled by DynamoDB. What is the MOST suitable solution in order to rectify this throttling issue?",answers:[{text:"Implement database caching with an ElastiCache cluster.",isCorrect:!1},{text:"Use strong consistency in the write operations.",isCorrect:!1},{text:"Increase the WCU to 200.",isCorrect:!0},{text:"Enable DynamoDB Accelerator (DAX).",isCorrect:!1}],explanation:"A write capacity unit (WCU) represents one write per second, for an item up to 1 KB in size. For example, suppose that you create a table with 10 write capacity units. This allows you to perform 10 writes per second, for items up to 1 KB in size per second. Item sizes for writes are rounded up to the next 1 KB multiple. For example, writing a 500-byte item consumes the same throughput as writing a 1 KB item. If you need to write an item that is larger than 1 KB, DynamoDB must consume additional write capacity units. Transactional write requests require 2 write capacity units to perform one write per second for items up to 1 KB. The total number of write capacity units required depends on the item size. For example, if your item size is 2 KB, you require 2 write capacity units to sustain one standard write request per second or 4 write capacity units for a transactional write request. Suppose that you want to write 100 items per second to your table, and that the items are 512 bytes in size. Each write requires one provisioned write capacity unit. First, you have to round up your item size to the nearest whole number per KB. Then to determine the write capacity unite per item, you need to divide the item size of the operation by 1 KB. Once you got the value, you just simply have to multiply it with the number of write request per second (1 x 100) hence, the WCU that you should provision is 100. Here are the 3 steps to get the total number of WCU needed for your application. In this scenario, you are storing 100 items to a DynamoDB table every second, where each item is 1.5 KB in size. To get the WCU, you just have to follow these 3 simple steps below: Step #1 Get the Average Item Size round up by 1 KB Average Item Size = 1.5 KB = 2 KB (Rounded up) Step #2 Get the WCU per Item by dividing the Average Item Size by 1 KB Divide the Average Item Size by 1KB and round up the result: = 2 KB / 1 KB = 2 WCU per Item Step #3 Multiply the WCU per item to the number of items to be written per second = 2 WCU per item \xd7 100 writes per second = 200 WCU Hence, the correct answer is to increase the WCU to 200 because your DynamoDB table only got 100 WCU, which is causing the issue. Enabling DynamoDB Accelerator (DAX) is incorrect. Although it will significantly improve the read performance of the mobile app, the issue with the throttling write requests will still persist. You have to increase the WCU to properly address the issue in this scenario. Implementing database caching with an ElastiCache cluster is incorrect because just as mentioned above, the main issue here is the write performance of the DynamoDB table and not its read performance. Using strong consistency in the write operations is incorrect because a strong consistency model is primarily used for read operations and not for writes. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ItemSizeCalculations.Writes https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"},{question:"A single docker container environment is hosted in Elastic Beanstalk. Your manager instructed you to ensure that the compute resources maintain full capacity during deployments to avoid any degradation of the service or possible down time. Which of the following deployment methods should you use to satisfy the given requirement? (Select TWO.)",answers:[{text:"Rolling",isCorrect:!1},{text:"Rolling with additional batch",isCorrect:!0},{text:"Immutable",isCorrect:!0},{text:"Canary",isCorrect:!1},{text:"All at once",isCorrect:!1}],explanation:"In ElasticBeanstalk, you can choose from a variety of deployment methods: All at once – Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time while the deployment occurs. Rolling – Deploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch. Rolling with additional batch – Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during the deployment process. Immutable – Deploy the new version to a fresh group of instances by performing an immutable update. Blue/Green - Deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly. To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a rolling deployment with an additional batch. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances. Immutable environment updates are an alternative to rolling updates. Immutable environment updates ensure that configuration changes that require replacing instances are applied efficiently and safely. If an immutable environment update fails, the rollback process requires only terminating an Auto Scaling group. A failed rolling update, on the other hand, requires performing an additional rolling update to roll back the changes. To perform an immutable environment update, Elastic Beanstalk creates a second, temporary Auto Scaling group behind your environment's load balancer to contain the new instances. First, Elastic Beanstalk launches a single instance with the new configuration in the new group. This instance serves traffic alongside all of the instances in the original Auto Scaling group that are running the previous configuration. Hence, Immutable and Rolling with additional batch are the correct deployment methods to be used in this scenario. All at once is incorrect because this will deploy the new version to all existing instances immediately and will not create new EC2 instances. Hence, it is possible that there would be a degradation of the service since some instances would be unavailable during the deployment process. Rolling is incorrect because this will deploy the new version in batches only to existing instances, without provisioning new resources. The compute capacity of the environment would still be compromised in this method. Canary is incorrect because this type of deployment method is not readily available in Elastic Beanstalk, but primarily to Lambda. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/ Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"},{question:"A developer is working on an application that will process files encrypted with a data key generated from a KMS key. The application needs to decrypt the files locally before it can proceed with the processing of the files. Which of the following are valid and secure steps in decrypting data? (Select TWO.)",answers:[{text:"Use the Decrypt operation to decrypt the encrypted data key.",isCorrect:!0},{text:"Use the Decrypt operation to decrypt the plaintext data key.",isCorrect:!1},{text:"Use the encrypted data key to decrypt data locally, then erase the encrypted data key from memory.",isCorrect:!1},{text:"Use the plaintext data key to decrypt data locally, then erase the encrypted data key from memory.",isCorrect:!1},{text:"Use the plaintext data key to decrypt data locally, then erase the plaintext data key from memory.",isCorrect:!0}],explanation:"When you encrypt your data, it is protected, but you have to protect your encryption key. One strategy is to encrypt it. Envelope encryption is the practice of encrypting plaintext data with a data key and then encrypting the data key under another key. You can even encrypt the data encryption key under another encryption key, and encrypt that encryption key under another encryption key. But, eventually, one key must remain in plaintext so you can decrypt the keys and your data. This top-level plaintext key encryption key is known as the root key. AWS KMS helps you to protect your encryption keys by storing and managing them securely. Root keys stored in AWS KMS, known as AWS KMS keys, never leave the AWS KMS FIPS validated hardware security modules unencrypted. To use an AWS KMS key, you must call AWS KMS. It is recommended that you use the following pattern to encrypt data locally in your application: 1. Use the GenerateDataKey operation to get a data encryption key. 2. Use the plaintext data key (returned in the Plaintext field of the response) to encrypt data locally, then erase the plaintext data key from memory. 3. Store the encrypted data key (returned in the CiphertextBlob field of the response) alongside the locally encrypted data. To decrypt data locally: 1. Use the Decrypt operation to decrypt the encrypted data key. The operation returns a plaintext copy of the data key. 2. Use the plaintext data key to decrypt data locally, then erase the plaintext data key from memory. The option that says: Use the Decrypt operation to decrypt the plaintext data key is incorrect because there is no need to decrypt a plaintext, or 'unencrypted', data key. The correct way is to use the Decrypt operation to decrypt the encrypted data key. The option that says: Use the plaintext data key to decrypt data locally, then erase the encrypted data key from memory is incorrect. Although this is a valid option for the encryption process, you still must erase the plaintext data key instead of the encrypted one. Take note that you are asked to choose valid and secure steps as per the scenario. The option that says: Use the encrypted data key to decrypt data locally, then erase the encrypted data key from memory is incorrect because you cannot decrypt data using an encrypted data key. You have to decrypt the encrypted data key first and use the plaintext copy of the data key to decrypt the files. References: https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping Check out these Amazon S3 and AWS KMS Cheat Sheets: https://tutorialsdojo.com/amazon-s3/ https://tutorialsdojo.com/aws-key-management-service-aws-kms/"},{question:"An EBS-backed EC2 instance has been recently reported to contain a malware that could spread to your other instances. To fix this security vulnerability, you will need to attach its root EBS volume to a new EC2 instance which hosts a security program that can scan viruses, worms, Trojan horses, or spyware. What steps would you take to detach the root volume from the compromised EC2 instance?",answers:[{text:"Unmount the volume from the OS and then detach.",isCorrect:!1},{text:"Detach the volume from the AWS Console. AWS takes care of unmounting the volume for you.",isCorrect:!1},{text:"Stop the instance then detach the volume.",isCorrect:!0},{text:"Unmount the volume, stop the instance, and then detach.",isCorrect:!1}],explanation:"You can detach an Amazon EBS volume from an instance explicitly or by terminating the instance. However, if the instance is running, you must first unmount the volume from the instance. If an EBS volume is the root device of an instance, you must stop the instance before you can detach the volume. The options that say unmount the volume from the OS and then detach and unmount the volume, stop the instance, and then detach are both incorrect because you can’t unmount the root volume on a running instance. The option that says: Detach the volume from the AWS Console. AWS takes care of unmounting the volume for you is incorrect because unmounting the volume is not managed by AWS. Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-detaching-volume.html Check out this Amazon EC2 Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/"},{question:"A developer is working on an online game based on a popular movie, which may have a few users in its first few weeks of release. However, it is expected to grow and reach millions of concurrent users, with terabytes or more of new data generated per day. The database must seamlessly handle hundreds of thousands of reads and writes per second. Which of the following would be the MOST ideal data store to choose for this application?",answers:[{text:"Amazon RDS",isCorrect:!1},{text:"Amazon S3",isCorrect:!1},{text:"Amazon DynamoDB",isCorrect:!0},{text:"Amazon Redshift",isCorrect:!1}],explanation:"Today's applications have more demanding requirements than ever before. For example, an online game might start out with just a few users and a very small amount of data. However, if the game becomes successful, it can easily outstrip the resources of the underlying database management system. It is not uncommon for web-based applications to have hundreds, thousands, or millions of concurrent users, with terabytes or more of new data generated per day. Databases for such applications must handle tens (or hundreds) of thousands of reads and writes per second. Amazon DynamoDB is well-suited for these kinds of workloads. As a developer, you can start with a small amount of provisioned throughput and gradually increase it as your application becomes more popular. DynamoDB scales seamlessly to handle very large amounts of data and very large numbers of users. Amazon RDS is incorrect because this is a type of SQL database, and it can't scale seamlessly to handle very large amounts of data and the number of users, as compared to DynamoDB. Using Amazon Aurora database could also provide scalability, which may be at par with DynamoDB, but this will entail the additional cost of setting up several replicas in various AWS regions. Amazon Redshift is incorrect because this is a columnar-based database, which is more appropriate for online analytical processing (OLAP) applications. Amazon S3 is incorrect because this is primarily used for object storage and not as a database. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html https://aws.amazon.com/products/databases/ Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ Amazon DynamoDB Overview: https://youtu.be/3ZOyUNIeorU?si=ogJGx2E_ZfMFJQvc"},{question:"A leading technology company is building a serverless application in AWS using the C++ programming language. The application will use DynamoDB as its data store, Lambda as its compute service, and API Gateway as its API Proxy. You are tasked to handle the deployment of the compute resources to AWS. Which of the following steps should you implement to properly deploy the serverless application?",answers:[{text:"Use AWS Serverless Application Model (AWS SAM) to deploy the Lambda function.",isCorrect:!1},{text:"Upload the deployment package to S3 and then use CloudFormation to deploy Lambda function with a reference to the S3 URL of the package.",isCorrect:!1},{text:"Create a Lambda function with the C++ code and directly upload it to AWS.",isCorrect:!1},{text:"Create a new layer which contains the Custom Runtime for C++ and then launch a Lambda function which uses that runtime.",isCorrect:!0}],explanation:"You can implement an AWS Lambda runtime in any programming language. A runtime is a program that runs a Lambda function's handler method when the function is invoked. You can include a runtime in your function's deployment package in the form of an executable file named bootstrap. A runtime is responsible for running the function's setup code, reading the handler name from an environment variable, and reading invocation events from the Lambda runtime API. The runtime passes the event data to the function handler, and posts the response from the handler back to Lambda. Your custom runtime runs in the standard Lambda execution environment. It can be a shell script, a script in a language that's included in Amazon Linux, or a binary executable file that's compiled in Amazon Linux. Hence, the correct answer in this scenario is to create a new layer which contains the Custom Runtime for C++ and then launch a Lambda function which uses that runtime. Uploading the deployment package to S3 and then using CloudFormation to deploy Lambda function with a reference to the S3 URL of the package is incorrect because you have to implement a Custom Runtime in order to execute the C++ code. Take note that this programming language is not natively supported yet in Lambda, which is why the use of a Custom Runtime is essential. Creating a Lambda function with the C++ code and directly uploading it to AWS is incorrect because there is a 50 MB deployment package size limit in Lambda if you'll directly upload the package. Just as mentioned above, you have to implement a Custom Runtime for this scenario. Using AWS Serverless Application Model (AWS SAM) to deploy the Lambda function is incorrect because using SAM alone is not enough to run the C++ code in Lambda. You have to use a Custom Runtime. References: https://docs.aws.amazon.com/lambda/latest/dg/runtimes-custom.html https://docs.aws.amazon.com/lambda/latest/dg/runtimes-walkthrough.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A developer is using API Gateway Lambda Authorizer to securely authenticate the API requests to their web application. The authentication process should be implemented using a custom authorization scheme which accepts header and query string parameters from the API caller. Which of the following methods should the developer use to properly implement the above requirement?",answers:[{text:"Amazon Cognito User Pools Authorizer",isCorrect:!1},{text:"Token-based Authorization",isCorrect:!1},{text:"Request Parameter-based Authorization",isCorrect:!0},{text:"Cross-Account Lambda Authorizer",isCorrect:!1}],explanation:"A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. When a client makes a request to one of your API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output. There are two types of Lambda authorizers: - A token-based Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token. - A request parameter-based Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of headers, query string parameters, stageVariables, and $context variables. It is possible to use an AWS Lambda function from an AWS account that is different from the one in which you created your Lambda authorizer function, by using a Cross-Account Lambda Authorizer. Therefore, the correct answer is to use a Request Parameter-based Authorization as it uses a combination of headers, query string parameters, stageVariables, and $context variables for authentication. Amazon Cognito User Pools Authorizer is incorrect because this is just an alternative to using IAM roles and policies or Lambda authorizers. An Amazon Cognito user pool is primarily used to control who can access your API in Amazon API Gateway using identity token or access token, and not header and query string parameters from the API caller. Token-based authorization is incorrect because this is only suitable if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML. Cross-Account Lambda Authorizer is incorrect because this just enables you to use an AWS Lambda function from a different AWS account as your API authorizer function. Moreover, this is not a valid Lambda authorizer type. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-lambda-authorizer-input.html https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-lambda-authorizer-cross-account-lambda-authorizer.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"You are using AWS Serverless Application Model (AWS SAM) to build and deploy applications in your serverless infrastructure. Your manager instructed you to create a CloudFormation template that includes your SAM script and other service configurations. This template will be used to launch a similar infrastructure in another region. What should you do in order to accomplish this task?",answers:[{text:"Add a Mappings section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.",isCorrect:!1},{text:"Add a Parameters section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.",isCorrect:!1},{text:"Add a Resources section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.",isCorrect:!1},{text:"Add a Transform section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.",isCorrect:!0}],explanation:"AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications that run in AWS. You create a template that describes all the AWS resources that you want, and AWS CloudFormation takes care of provisioning and configuring those resources for you. A Cloudformation template is a JSON- or YAML-formatted text file that describes your AWS infrastructure. The template includes several sections for you to define your infrastructure code. For serverless applications (also referred to as Lambda-based applications), the Transform section specifies the version of the AWS Serverless Application Model (AWS SAM) to use. When you specify a transform, you can use AWS SAM syntax to declare resources in your template. The model defines the syntax that you can use and how it is processed. More specifically, the AWS::Serverless transform, which is a macro hosted by AWS CloudFormation, takes an entire template written in the AWS Serverless Application Model (AWS SAM) syntax and transforms and expands it into a compliant AWS CloudFormation template. In the following example, the template uses AWS SAM syntax to simplify the declaration of a Lambda function and its execution role. Transform: AWS::Serverless-2016-10-31Resources: MyServerlessFunctionLogicalID: Type: AWS::Serverless::Function Properties: Handler: index.handler Runtime: nodejs8.10 CodeUri: 's3://testBucket/mySourceCode.zip' Hence, the correct answer is: Add a Transform section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use. The option that says: Add a Parameters section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use is incorrect because this is just the part of the template that contains values to pass to your template at runtime when you create or update a stack. The option that says: Add a Resources section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use. is incorrect because this is primarily used to specify the stack resources and their properties. The option that says: Add a Mappings section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use is incorrect because this just lists a mapping of keys and associated values that you can use to specify conditional parameter values, similar to a lookup table. You can match a key to a corresponding value by using the Fn::FindInMap intrinsic function in the Resources and Outputs sections. References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-aws-serverless.html Check out this AWS CloudFormation Cheat Sheet: https://tutorialsdojo.com/aws-cloudformation/ Check out this AWS Serverless Application Model Cheat Sheet: https://tutorialsdojo.com/aws-serverless-application-model-sam/"},{question:"You are using an AWS Lambda function to process records in an Amazon Kinesis Data Streams stream which has 100 active shards. The Lambda function takes an average of 10 seconds to process the data and the stream is receiving 50 new items per second. Which of the following statements are TRUE regarding this scenario?",answers:[{text:"The Lambda function will throttle the incoming requests due to the excessive number of Kinesis shards.",isCorrect:!1},{text:"There will be at most 100 Lambda function invocations running concurrently.",isCorrect:!0},{text:"The Kinesis shards must be merged to increase the data capacity of the stream as well as the concurrency execution of the Lambda function.",isCorrect:!1},{text:"The Lambda function has 500 concurrent executions.",isCorrect:!1}],explanation:"You can use an AWS Lambda function to process records in an Amazon Kinesis data stream. With Kinesis, you can collect data from many sources and process them with multiple consumers. Lambda supports standard data stream iterators and HTTP/2 stream consumers. Lambda reads records from the data stream and invokes your function synchronously with an event that contains stream records. Lambda reads records in batches and invokes your function to process records from the batch. Concurrent executions refers to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the it will differ depending on whether or not your Lambda function is processing events from a poll-based event source. For Lambda functions that process Kinesis or DynamoDB streams, the number of shards is the unit of concurrency. If your stream has 100 active shards, there will be at most 100 Lambda function invocations running concurrently. This is because Lambda processes each shard’s events in sequence. Hence, the correct answer in this scenario is that: there will be at most 100 Lambda function invocations running concurrently. The option that says: the Lambda function has 500 concurrent executions is incorrect because the number of concurrent executions for poll-based event sources is different from push-based event sources. This number of concurrent executions would have been correct if the Lambda function is integrated with a push-based even source such as API Gateway or Amazon S3 Events. Remember that the Kinesis and Lambda integration is using a poll-based event source, which means that the number of shards is the unit of concurrency for the function. The option that says: the Lambda function will throttle the incoming requests due to the excessive number of Kinesis shards is incorrect because, by default, AWS Lambda will automatically scale the function's concurrency execution in response to increased traffic, up to your concurrency limit. Moreover, having 100 shards is not excessive at all as long as there is a sufficient number of workers or consumers of the stream. The option that says: the Kinesis shards must be merged to increase the data capacity of the stream as well as the concurrency execution of the Lambda function is incorrect because, in the first place, you have to split the shards in order to increase the data capacity of the stream and not merge them. Since the Lambda function is using a poll-based event source mapping for Kinesis, the number of shards is the unit of concurrency for the function. References: https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html https://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html https://docs.aws.amazon.com/lambda/latest/dg/scaling.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A commercial bank is developing an online auction application with a DynamoDB database that will allow customers to bid for real estate properties from the comforts of their homes. The application should allow the minimum acceptable price established by the bank prior to the auction. The opening bid entered by the staff must be at least the minimum bid and the new bids submitted by the customers should be greater than the current bid. The application logic has already been implemented but the DynamoDB database calls should also be tailored to meet the requirements. Which of the following is the MOST effective solution that will satisfy the requirement in this scenario?",answers:[{text:"Configure the database calls of the application to use conditional updates and conditional writes with a condition expression that will check if the new bid submitted by the customer is greater than the current bid.",isCorrect:!0},{text:"Enable DynamoDB Transactions to automatically check the minimum acceptable price as well as the current and new bid price.",isCorrect:!1},{text:"Use DynamoDB Streams and a Lambda function to track the current bid price and compare against all of the new bids submitted by the customers.",isCorrect:!1},{text:"Use an optimistic locking strategy in your database calls to ensure that the new bid submitted by the customer is greater than the current bid.",isCorrect:!1}],explanation:"By default, the DynamoDB write operations (PutItem, UpdateItem, DeleteItem) are unconditional: each of these operations will overwrite an existing item that has the specified primary key. DynamoDB optionally supports conditional writes for these operations. A conditional write will succeed only if the item attributes meet one or more expected conditions. Otherwise, it returns an error. Conditional writes are helpful in many situations. For example, you might want a PutItem operation to succeed only if there is not already an item with the same primary key. Or you could prevent an UpdateItem operation from modifying an item if one of its attributes has a certain value. Conditional writes are helpful in cases where multiple users attempt to modify the same item. Therefore, configuring the database calls of the application to use conditional updates and conditional writes with a condition expression that will check if the new bid submitted by the customer is greater than the current bid is the most suitable solution in this scenario. Using DynamoDB Streams and a Lambda function to track the current bid price and compare against all of the new bids submitted by the customers is incorrect because DynamoDB Streams is primarily used to capture a time-ordered sequence of item-level modifications in the table. Although you can use a Lambda function that checks if the current and new bid prices are correct, this solution is cumbersome to implement as opposed to using conditional writes. Using an optimistic locking strategy in your database calls to ensure that the new bid submitted by the customer is greater than the current bid is incorrect because the Optimistic locking strategy simply ensures that the client-side item that you are updating (or deleting) is the same as the item in DynamoDB. It doesn't have the capability to specify certain conditions to meet the requirement. Enabling DynamoDB Transactions to automatically check the minimum acceptable price as well as the current and new bid price is incorrect because the DynamoDB Transactions feature simply provides developers atomicity, consistency, isolation, and durability (ACID) across one or more tables within a single AWS account and region. Although this can be used in building applications that require coordinated inserts, deletes, or updates to multiple items as part of a single logical business operation, the implementation of this solution entails a lot of work. Using conditional updates and conditional writes is still a more effective solution in this scenario instead of using DynamoDB Transactions. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ AWS Lambda Integration with Amazon DynamoDB Streams: https://tutorialsdojo.com/aws-lambda-integration-with-amazon-dynamodb-streams/"},{question:"You are developing a Node.js application which uses a DynamoDB database. The architecture should be designed to allow you to query over a single partition of the table, as specified by the partition key value in the query. It should also support both eventually consistent and strongly consistent reads. What should you do to satisfy this requirement?",answers:[{text:"Add a local secondary index before the table is created.",isCorrect:!0},{text:"Add a global secondary index before the table is created.",isCorrect:!1},{text:"Add a local secondary index after the table has been created.",isCorrect:!1},{text:"Add a global secondary index after the table has been created.",isCorrect:!1}],explanation:'A local secondary index maintains an alternate sort key for a given partition key value. It also contains a copy of some or all of the attributes from its base table; you specify which attributes are projected into the local secondary index when you create the table. The data in a local secondary index is organized by the same partition key as the base table, but with a different sort key. This lets you access data items efficiently across this different dimensions. For greater query or scan flexibility, you can create up to five local secondary indexes per table. A local secondary index (LSI) is "local" in the sense that every partition of an LSI is scoped to a base table partition that has the same partition key value. It lets you query over a single partition, as specified by the partition key value in the query. When you query a local secondary index, you can choose either eventual consistency or strong consistency. Queries or scans on a local secondary index consume read capacity units from the base table. When you write to a table, its local secondary indexes are also updated; these updates consume write capacity units from the base table. Take note that local secondary indexes are created at the same time you create a table. You cannot add a local secondary index to an existing table, nor can you delete any local secondary indexes that currently exist. Hence, the correct option in this scenario is to add a local secondary index before the table is created. Adding a local secondary index after the table has been created is incorrect because as mentioned above, you cannot add a local secondary index to an existing table. This index must be created at the same time you create your table. Adding a global secondary index before the table is created is incorrect because a global secondary index (GSI) is primarily used if you want to query over the entire table, across all partitions. GSI only supports eventual consistency and not strong consistency. You have to use a local secondary index instead. Adding a global secondary index after the table has been created is incorrect because you should use a local secondary index (LSI) instead of a global secondary index (GSI) just as mentioned above. Although you can add a GSI to an already existing table, unlike LSI, this option is still not sufficient to meet the specified requirement. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/'},{question:"An application experiences a sluggish response whenever there is a surge in requests involving read queries. The developer has already attempted to improve performance by optimizing the queries. However, the problem still persists even after applying the change. The application is hosted in an Amazon ECS Cluster and uses a MySQL database backed by Amazon RDS. Which of the following could the developer do to resolve the performance issue? (Select TWO.)",answers:[{text:"Implement database caching using Amazon ElastiCache.",isCorrect:!0},{text:"Implement a Multi-AZ deployment configuration for the RDS DB instance.",isCorrect:!1},{text:"Cache the database response using Amazon CloudFront.",isCorrect:!1},{text:"Replace the database with Amazon MemoryDB for Redis",isCorrect:!1},{text:"Set up read replicas for the RDS database instance and route read queries to these replicas.",isCorrect:!0}],explanation:"Amazon RDS Read Replicas provide enhanced performance and durability for the database (DB) instances. This feature makes it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. You can reduce the load on your source DB instance by routing read queries from your applications to the read replica. Read replicas allow you to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Because read replicas can be promoted to master status, they are useful as part of a sharding implementation. To shard your database, add a read replica and promote it to master status, then, from each of the resulting DB Instances, delete the data that belongs to the other shard. In-memory data caching can be one of the most effective strategies to improve your overall application performance and reduce your database costs. Caching can be applied to any type of database, including relational databases such as Amazon RDS or NoSQL databases such as Amazon DynamoDB, MongoDB, and Apache Cassandra. The best part of caching is that it’s minimally invasive to implement, and by doing so, your application performance regarding both scale and speed is dramatically improved. Amazon ElastiCache offers fully managed Redis and Memcached. Seamlessly deploy, run, and scale popular open-source compatible in-memory data stores. Build data-intensive apps or improve the performance of your existing apps by retrieving data from high throughput and low latency in-memory data stores. Hence, the correct answers in this scenario are: - Set up read replicas for the RDS database instance and route read queries to these replicas. - Implement database caching using Amazon ElastiCache. The option that says: Replace the database with Amazon MemoryDB for Redis is incorrect because Redshift is primarily used for online analytics processing applications (OLAP) and as a data warehouse. Hence, this will not improve the read performance of your application. The option that says: Cache the database response using Amazon CloudFront is incorrect. Although CloudFront can provide caching and for CDN, it is not suitable to be used for database caching. Using Read Replicas and ElastiCache are more appropriate features to be used in this scenario. The option that says: Implement a Multi-AZ deployment configuration for the RDS DB instance is incorrect because configuring a Multi-AZ RDS just improves the availability of the database but does not drastically improve the read performance, which Read Replicas can provide. References: https://aws.amazon.com/caching/database-caching/ https://aws.amazon.com/rds/details/read-replicas/ https://aws.amazon.com/elasticache/ Check out these Amazon RDS and Elasticache Cheat Sheets: https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/ https://tutorialsdojo.com/amazon-elasticache/ Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"},{question:"A developer has been instructed to configure Cross-Region Replication (CRR) to their S3 bucket as part of the company's disaster recovery plan. She is using the put-bucket-replication AWS CLI to enable CRR on the bucket but it fails whenever she attempts to issue the command. However, the same command works for the other S3 buckets. Which of the following options is the MOST likely reason for this issue?",answers:[{text:"S3 Transfer Acceleration is not enabled in the bucket.",isCorrect:!1},{text:"Versioning is not enabled in the bucket.",isCorrect:!0},{text:"Amazon S3 Object Lock is enabled in the bucket.",isCorrect:!1},{text:"The bucket should be configured as a static web hosting first.",isCorrect:!1}],explanation:"Cross-region replication (CRR) enables automatic, asynchronous copying of objects across buckets in different AWS Regions. Buckets configured for cross-region replication can be owned by the same AWS account or by different accounts. Cross-region replication is enabled with a bucket-level configuration. You add the replication configuration to your source bucket. To enable the cross-region replication feature in S3, the following items should be met: - The source and destination buckets must have versioning enabled. - The source and destination buckets must be in different AWS Regions. - Amazon S3 must have permissions to replicate objects from that source bucket to the destination bucket on your behalf. Hence, the most likely root cause for this scenario is that the versioning is not enabled in the bucket. The option that says: Amazon S3 Object Lock is enabled in the bucket is incorrect because this feature simply enables you to store objects using a write-once-read-many (WORM) model. You can use it to prevent an object from being deleted or overwritten for a fixed amount of time or indefinitely, but it will not affect your cross-region replication configuration. The option that says: The bucket should be configured as a static web hosting first is incorrect because this feature won't affect the cross-region replication in S3. This is primarily used to make your S3 bucket a static website, as its name implies. The option that says: S3 Transfer Acceleration is not enabled in the bucket is incorrect because this just enables fast, easy, and secure transfers of files to and from your bucket. Just like the other options, it does not affect cross-region replication in any way. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html https://docs.aws.amazon.com/AmazonS3/latest/dev/crr-how-setup.html Check out this Amazon S3 Cheat Sheet: https://tutorialsdojo.com/amazon-s3/"},{question:"A global financial company has hundreds of users from all over the world who regularly upload terabytes of transactional data to a centralized Amazon S3 bucket. Users from different parts of the globe are experiencing delays in uploading data, which in turn affects processing times. The goal is to improve data throughput and ensure consistently fast data transfer to the S3 bucket regardless of the user's location. Which of the following should be used to satisfy the above requirement?",answers:[{text:"S3 Transfer Acceleration",isCorrect:!0},{text:"AWS Transfer for SFTP",isCorrect:!1},{text:"Amazon CloudFront",isCorrect:!1},{text:"AWS Direct Connect",isCorrect:!1}],explanation:"Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3 bucket. Transfer Acceleration leverages Amazon CloudFront’s globally distributed AWS Edge Locations. As data arrives at an AWS Edge Location, data is routed to your Amazon S3 bucket over an optimized network path. Hence, the correct answer is: S3 Transfer Acceleration. AWS Transfer for SFTP is incorrect because this is just a fully managed service that enables the transfer of files directly into and out of Amazon S3 using the Secure File Transfer Protocol (SFTP) which is also known as Secure Shell (SSH) File Transfer Protocol. It does not provide a fast, easy, and secure way to transfer files over long distances between your client and your Amazon S3 bucket. AWS Direct Connect is incorrect because you have users all around the world and not just on your on-premises data center. Direct Connect would be too costly and is definitely not suitable for this purpose. Amazon CloudFront is incorrect because this service is primarily used to serve static content and not as a transfer accelerator going to or from Amazon S3. CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. References: http://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration-examples.html Check out this Amazon S3 Cheat Sheet: https://tutorialsdojo.com/amazon-s3/ Comparison of AWS Services Cheat Sheets: https://tutorialsdojo.com/comparison-of-aws-services/"},{question:"A developer is building an online game in AWS which will be using a NoSQL database with DynamoDB. Each player data has an average size of 3.5 KB and it is expected that the game will send 150 eventually consistent read requests per second. How may Read Capacity Units (RCU) should the developer provision to the table?",answers:[{text:"150",isCorrect:!1},{text:"300",isCorrect:!1},{text:"75",isCorrect:!0},{text:"600",isCorrect:!1}],explanation:"One read request unit represents one strongly consistent read request, or two eventually consistent read requests, for an item up to 4 KB in size. Transactional read requests require 2 read request units to perform one read for items up to 4 KB. If you need to read an item that is larger than 4 KB, DynamoDB needs additional read request units. The total number of read request units required depends on the item size, and whether you want an eventually consistent or strongly consistent read. For example, suppose that you create a table with 10 provisioned read capacity units. This allows you to perform 10 strongly consistent reads per second, or 20 eventually consistent reads per second, for items up to 4 KB. Reading an item larger than 4 KB consumes more read capacity units. For example, a strongly consistent read of an item that is 8 KB (4 KB \xd7 2) consumes 2 read capacity units. An eventually consistent read on that same item consumes only 1 read capacity unit. Item sizes for reads are rounded up to the next 4 KB multiple. For example, reading a 3,500-byte item consumes the same throughput as reading a 4 KB item. To get the number of RCU required to handle 150 eventually consistent read requests with an average item size of 3.5 KB, you simply have to do the following steps: Step #1 Get the Average Item Size by rounding up to 4 KB = 3.5 KB = 4 KB (rounded up) Step #2 Get the RCU per Item by dividing the Average Item Size by 8 KB = 4 KB / 8 KB = 0.5 Step #3 Multiply the RCU per item to the number of items to be written per second = 150 x 0.5 = 75 eventually consistent read requests Hence, the correct answer is 75. 150 is incorrect because this is the value for strongly consistent read requests based on the given RCU. Take note that for Step #2, you have to divide the Average Item Size by 8 KB and not by 4 KB, if you are calculating for eventual consistency. 300 is incorrect because this is the value for transactional read requests based on the given RCU. Take note that for Step #2, you have to divide the Average Item Size by 8 KB and not by 2 KB, if you are calculating for eventual consistency. 600 is incorrect because this is the value for transactional write requests, which is irrelevant since only the RCU is provided in the scenario. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ItemSizeCalculations.Reads Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ Calculating the Required Read and Write Capacity Unit for your DynamoDB Table: https://tutorialsdojo.com/calculating-the-required-read-and-write-capacity-unit-for-your-dynamodb-table/"},{question:"A company decided to re-use the same Lambda function for multiple stages of their API, but the function should read data from a different Amazon DynamoDB table depending on which stage is being called. In order to accomplish this, they instructed the developer to pass configuration parameters to a Lambda function through mapping templates in API Gateway. Which of the following is the MOST suitable solution that the developer should use to meet this requirement?",answers:[{text:"Use Stage Variables.",isCorrect:!0},{text:"Set up traffic shifting with Lambda Aliases.",isCorrect:!1},{text:"Create environment variables in the Lambda function.",isCorrect:!1},{text:"Set up an API Gateway Private Integration to the Lambda function.",isCorrect:!1}],explanation:"Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates. For example, you can define a stage variable in a stage configuration, and then set its value as the URL string of an HTTP integration for a method in your REST API. Later, you can reference the URL string using the associated stage variable name from the API setup. This way, you can use the same API setup with a different endpoint at each stage by resetting the stage variable value to the corresponding URLs. You can also access stage variables in the mapping templates, or pass configuration parameters to your AWS Lambda or HTTP backend. With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints. For example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, http://tutorialsdojo.com). In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage, and calls a different web host (for example, beta.tutorialsdojo.com). Similarly, stage variables can be used to specify a different AWS Lambda function name for each stage in your API. You can also use stage variables to pass configuration parameters to a Lambda function through your mapping templates. For example, you may want to re-use the same Lambda function for multiple stages in your API, but the function should read data from a different Amazon DynamoDB table depending on which stage is being called. In the mapping templates that generate the request for the Lambda function, you can use stage variables to pass the table name to Lambda. Stage variables are not applied to the security definitions section of the API specification. For example, you cannot use different Amazon Cognito user pools for different stages. Hence, the correct answer in this scenario is to use stage variables. Setting up an API Gateway Private Integration to the Lambda function is incorrect because this is just used to expose your HTTP/HTTPS resources behind an Amazon VPC to allow access to clients outside of your VPC. Creating environment variables in the Lambda function is incorrect because using an environment variable alone is not enough to meet this requirement. You have to integrate your Lambda function with API Gateway by using a stage variable, along with a proper mapping configuration. Setting up traffic shifting with Lambda Aliases is incorrect because this is just like a pointer to a specific Lambda function version. By using aliases, you can access the Lambda function which the alias is pointing to, without the caller knowing the specific version the alias is pointing to. Take note that we are not talking about different versions of the Lambda functions here as the scenario explicitly mentioned that we have passed the configuration parameters from API Gateway to the Lambda function. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html https://docs.aws.amazon.com/apigateway/latest/developerguide/amazon-api-gateway-using-stage-variables.html https://docs.aws.amazon.com/lambda/latest/dg/lambda-configuration.html Check out this Amazon API Gateway and AWS Lambda Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/ https://tutorialsdojo.com/aws-lambda/"},{question:"A company has a development team that’s heavily relying on AWS CodeBuild, and CodeDeploy. The management would like to further automate its CI/CD process. They requested a system that monitors the status of each code change, from the moment it's committed through to its deployment. Which of the following AWS services will help you achieve this?",answers:[{text:"AWS Fault Injection Simulator",isCorrect:!1},{text:"AWS CodePipeline",isCorrect:!0},{text:"Amazon CodeGuru",isCorrect:!1},{text:"AWS Elastic Beanstalk",isCorrect:!1}],explanation:'AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define. This makes it a good choice for automating your CI/CD process and centrally monitoring application activity. Moreover, AWS CodePipeline integrates with AWS CloudWatch, which provides a reliable, scalable, and flexible monitoring solution. You can create dashboards in CloudWatch to centrally monitor application activity and manage day-to-day development tasks. The option that says: AWS Fault Injection Simulator is incorrect because this is just a managed service that is commonly used in chaos engineering, and not for application development. It enables you to perform fault injection experiments on your AWS workloads to improve the performance and resiliency of your applications. The option that says: Elastic Beanstalk is incorrect because it is an orchestration service to quickly deploy and manage applications in AWS. The option that says: Amazon CodeGuru is incorrect because this is simply a developer tool that provides intelligent recommendations to improve the quality of your codebase and for identifying an application\'s most "expensive" lines of code in terms of resource intensiveness, CPU performance, and code efficiency. References: https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html https://aws.amazon.com/codepipeline/ Check out this AWS CodePipeline Cheat Sheet: https://tutorialsdojo.com/aws-codepipeline/'},{question:"A clickstream application uses Amazon Kinesis Data Stream for real-time processing. PutRecord API calls are being used by the producer to send data to the stream. However, there are cases where the producer intermittently restarted while doing the processing, which resulted in sending the same data twice to the stream. This inadvertently causes duplication of entries in the data stream, which affects the processing of the consumers. Which of the following should you implement to resolve this issue?",answers:[{text:"Split shards of the data stream.",isCorrect:!1},{text:"Merge shards of the data stream.",isCorrect:!1},{text:"Embed a primary key within the record.",isCorrect:!0},{text:"Add more shards.",isCorrect:!1}],explanation:"There are two primary reasons why records may be delivered more than one time to your Amazon Kinesis Data Streams application: producer retries and consumer retries. Your application must anticipate and appropriately handle processing individual records multiple times. Consider a producer that experiences a network-related timeout after it makes a call to PutRecord, but before it can receive an acknowledgment from Amazon Kinesis Data Streams. The producer cannot be sure if the record was delivered to Kinesis Data Streams. Assuming that every record is important to the application, the producer would have written to retry the call with the same data. If both PutRecord calls on that same data were successfully committed to Kinesis Data Streams, then there will be two Kinesis Data Streams records. Although the two records have identical data, they also have unique sequence numbers. Applications that need strict guarantees should embed a primary key within the record to remove duplicates later when processing. Note that the number of duplicates due to producer retries is usually low compared to the number of duplicates due to consumer retries. Hence, the correct answer in this scenario is to embed a primary key within the record to remove duplicates later when processing. Adding more shards is incorrect because this is not a suitable solution for handling duplicate records in the Kinesis data stream. This is primarily used to increase the rate of data flowing through the stream. Splitting shards of the data stream is incorrect because this is used to increase the capacity of the stream and not to avoid any duplicate data. Merging shards of the data stream is incorrect because this is primarily used to make better use of the unused capacity in the stream and to save on costs. References: https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-duplicates.html https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/"},{question:"A developer wants to deploy a REST API using the CloudFormation template shown below: Which changes should be done so that the newly created API endpoint can be referenced to other stacks?",answers:[{text:"Include the Export property in the original template's Outputs section. Then use the Fn::ImportValue function in other templates to retrieve the exported value.",isCorrect:!0},{text:"Specify HelloWorldApias parameter when using the Fn::ImportValue function in other templates.",isCorrect:!1},{text:"Add the AWS::Include transform in the original template to directly import the HelloWorldFunction resource to other templates.",isCorrect:!1},{text:"Include the Export property in the original template's Outputs section. Then use the Ref function in other templates to retrieve the exported value.",isCorrect:!1}],explanation:"To share information between stacks, export a stack's output values. Other stacks that are in the same AWS account and region can import the exported values. For example, you might have a single networking stack that exports the IDs of a subnet and security group for public web servers. Stacks with a public webserver can easily import those networking resources. You don't need to hard code resource IDs in the stack's template or pass IDs as input parameters. To export a stack's output value, use the Export field in the Output section of the stack's template. To import those values, use the Fn::ImportValue function in the template for the other stacks. In this scenario, we can expose the API endpoint to other stacks by adding the Export property in the Outputs section. In the example below, we use 'SimpleAPI' as the name of the value to be exported: To reference the endpoint's value in other templates, simply use the Fn::ImportValue function and specify SimpleAPI as its parameter. Hence, the correct answer is: Include the Export property in the original template's Outputs section. Then use the Fn::ImportValue function in other templates to retrieve the exported value. The option that says: Include the Export property in the original template's Outputs section. Then use the Ref function in other templates to retrieve the exported value is incorrect. The Ref function is not capable of returning resource/parameter values from other templates. The option that says: Specify HelloWorldApi as the parameter when using the Fn::ImportValue function in other templates is incorrect. Fn::ImportValue is not a standalone command. It cannot be used to directly reference values from the Outputs section. You must first declare the name of the resource to be exported using the Export property. The option that says: Add the AWS::Include transform in the original template to directly import the HelloWorldFunction resource to other templates is incorrect. The AWS::Include transform is used for referencing templates stored in Amazon S3. This allows you to reuse resources in other CloudFormation templates. Keep in mind that in this scenario, you only need to solve the issue of exporting the API endpoint in other templates. References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputs-section-structure.html Check out this AWS CloudFormation Cheat Sheet: https://tutorialsdojo.com/aws-cloudformation/"},{question:"The current application deployment process of a company is tedious and is prone to errors. They asked a developer to set up CodeDeploy as their deployment service, which can automate their application deployments on their hybrid cloud architecture. Which of the following deployment types does CodeDeploy support? (Select TWO.)",answers:[{text:"Rolling deployments to ECS.",isCorrect:!1},{text:"In-place deployments to AWS Lambda.",isCorrect:!1},{text:"Blue/green deployments to ECS.",isCorrect:!0},{text:"In-place deployments to on-premises servers",isCorrect:!0},{text:"Blue/green deployments to on-premises servers.",isCorrect:!1}],explanation:"CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can use CodeDeploy. CodeDeploy provides two deployment type options: In-place deployment: The application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete. Only deployments that use the EC2/On-Premises compute platform can use in-place deployments. AWS Lambda compute platform deployments cannot use an in-place deployment type. Blue/green deployment: The behavior of your deployment depends on which compute platform you use: - Blue/green on an EC2/On-Premises compute platform: The instances in a deployment group (the original environment) are replaced by a different set of instances (the replacement environment). If you use an EC2/On-Premises compute platform, be aware that blue/green deployments work with Amazon EC2 instances only. - Blue/green on an AWS Lambda compute platform: Traffic is shifted from your current serverless environment to one with your updated Lambda function versions. You can specify Lambda functions that perform validation tests and choose the way in which the traffic shift occurs. All AWS Lambda compute platform deployments are blue/green deployments. For this reason, you do not need to specify a deployment type. - Blue/green on an Amazon ECS compute platform: Traffic is shifted from the task set with the original version of a containerized application in an Amazon ECS service to a replacement task set in the same service. The protocol and port of a specified load balancer listener are used to reroute production traffic. During deployment, a test listener can be used to serve traffic to the replacement task set while validation tests are run. The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The CodeDeploy agent communicates outbound using HTTPS over port 443. It is also important to note that the CodeDeploy agent is required only if you deploy to an EC2/On-Premises compute platform. The agent is not required for deployments that use the Amazon ECS or AWS Lambda compute platform. Therefore, the supported deployment types in CodeDeploy are: - Blue/green deployments to ECS - In-place deployments to on-premises servers Rolling deployments to ECS is incorrect because only blue/green deployment is allowed if you used the AWS CodeDeploy service to deploy the new version of your application to ECS. Take note that in CodeDeploy, only the EC2/On-Premises compute platform can use both in-place deployments and blue/green deployment. For Lambda and ECS, you can only do a blue/green deployment in CodeDeploy. This type of deployment is actually done in Elastic Beanstalk for Multicontainer docker environment which implicitly uses ECS. In-place deployments to AWS Lambda is incorrect because AWS Lambda compute platform deployments cannot use an in-place deployment type. Blue/green deployments to on-premises servers is incorrect because, in CodeDeploy, blue/green deployments only work with Amazon EC2 instances only. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/ Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/ Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy: https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/ Comparison of AWS Services Cheat Sheets: https://tutorialsdojo.com/comparison-of-aws-services/"},{question:"An application is hosted in Elastic Beanstalk with an ElastiCache cluster that acts as a database cache layer for accessing its data in DynamoDB. It is currently configured to write the data to the cache only if there is a cache miss, which causes the data in the cache to become stale. A developer is instructed to ensure that the data in the cache is always current and to minimize wasted space in the cluster by automatically deleting the data that are never read. What is the BEST way to implement this to satisfy the given requirement?",answers:[{text:"Use a Write Through caching strategy.",isCorrect:!1},{text:"Implement a Write Through caching strategy in the application and enable TTL in Elasticache.",isCorrect:!0},{text:"Implement Lazy Loading in the application in conjunction with the Write Through caching strategy.",isCorrect:!1},{text:"Use a Lazy Loading caching strategy.",isCorrect:!1}],explanation:"The write-through strategy adds data or updates data in the cache whenever data is written to the database. With this strategy, the data in the cache is never stale since the data in the cache is updated every time it is written to the database. This is why the data in the cache is always current. One of its disadvantages is that, since most data are never read in the cluster, you are actually wasting resources and disk space. This issue can be rectified by simply adding TTL to minimize wasted space. Hence, the correct answer is to implement a Write-Through caching strategy in the application and enable TTL. Implementing Lazy Loading in the application in conjunction with the Write Through caching strategy is incorrect. Although the Write Through caching strategy can ensure that the cache always have the current data, the Lazy Loading strategy is not helpful in minimizing wasted space in the cluster. This combination is only helpful in scenarios where you have missing data which continues to be missing until it is added or updated on the database. Using a Write-Through caching strategy is incorrect. Although the caching strategy will meet the first requirement of ensuring that the data in the cache is always current, it fails the second requirement of minimizing wasted space in the cluster as this strategy does not automatically delete the data that are never read. You should use a combination of Write-Through and TTL instead. Using a Lazy Loading caching strategy is incorrect because it is just a strategy to load data into the cache only when necessary. It doesn't meet the requirement of the scenario, which is to not have stale data in the cache. References: https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html#Strategies.WriteThrough https://aws.amazon.com/caching/implementation-considerations/ Check out this Amazon Elasticache Cheat Sheet: https://tutorialsdojo.com/amazon-elasticache/"},{question:"You are developing an online learning platform using Lambda, Elastic Beanstalk, and DynamoDB. There is a requirement that whenever a new customer is added to the DynamoDB table, it will invoke a Lambda function that sends a welcome email to the customer. Which of the following is the MOST suitable solution that you should use to implement this feature?",answers:[{text:"Enable DynamoDB Transactions and configure it as the event source for the Lambda function.",isCorrect:!1},{text:"Use Amazon EventBridge (Amazon CloudWatch Events) to track all new data in your table and configure it as the event source for the Lambda function.",isCorrect:!1},{text:"Enable DynamoDB Streams and configure it as the event source for the Lambda function.",isCorrect:!0},{text:"Use Amazon Kinesis Data Streams to track all new data in your table and configure it as the event source for the Lambda function.",isCorrect:!1}],explanation:"Amazon DynamoDB is integrated with AWS Lambda so that you can create triggers—pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables. If you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records. The Lambda function can perform any actions you specify, such as sending a notification or initiating a workflow. For example, you can write a Lambda function to simply copy each stream record to persistent storage, such as Amazon Simple Storage Service (Amazon S3), to create a permanent audit trail of write activity in your table. Or suppose you have a mobile gaming app that writes to a GameScores table. Whenever the TopScore attribute of the GameScores table is updated, a corresponding stream record is written to the table's stream. This event could then trigger a Lambda function that posts a congratulatory message on a social media network. (The function would simply ignore any stream records that are not updates to GameScores or that do not modify the TopScore attribute.) Hence, the correct answer is: Enable DynamoDB Streams and configure it as the event source for the Lambda function. The option that says: Use Amazon EventBridge (Amazon CloudWatch Events) to track all new data in your table and configure it as the event source for the Lambda function is incorrect because the Amazon EventBridge (Amazon CloudWatch Events) service does not have the capability to track any new inserts or updates on the DynamoDB table. Although Amazon EventBridge (Amazon CloudWatch Events) delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources, it cannot provide tracking of the DynamoDB's table activities. The option that says: Enable DynamoDB Transactions and configure it as the event source for the Lambda function is incorrect because Amazon DynamoDB transactions just simplifies the developer experience of making coordinated, all-or-nothing changes to multiple items both within and across tables. This feature is primarily used to provide atomicity, consistency, isolation, and durability (ACID) in DynamoDB, enabling you to maintain data correctness in your applications easily. The option that says: Use Amazon Kinesis Data Streams to track all new data in your table and configure it as the event source for the Lambda function is incorrect because using DynamoDB Streams is a better option than Kinesis. In addition, Kinesis does not have the capability to immediately track any new inserts or updates on the DynamoDB table. References: https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html Check out these AWS Lambda and Amazon DynamoDB Cheat Sheets: https://tutorialsdojo.com/aws-lambda/ https://tutorialsdojo.com/amazon-dynamodb/ AWS Lambda Integration with Amazon DynamoDB Streams: https://tutorialsdojo.com/aws-lambda-integration-with-amazon-dynamodb-streams/"},{question:"A startup has an urgent requirement to deploy their new NodeJS application to AWS. You were assigned to perform the deployment to a service where you don't need to worry about the underlying infrastructure that runs the application. The service must also automatically handle provisioning, load balancing, scaling, and application health monitoring. Which service will you use to easily deploy and manage the application?",answers:[{text:"AWS CloudFormation",isCorrect:!1},{text:"AWS Elastic Beanstalk",isCorrect:!0},{text:"AWS CodeDeploy",isCorrect:!1},{text:"AWS SAM",isCorrect:!1}],explanation:"With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. You can also perform most deployment tasks, such as changing the size of your fleet of Amazon EC2 instances or monitoring your application, directly from the Elastic Beanstalk web interface (console). To use Elastic Beanstalk, you create an application, upload an application version in the form of an application source bundle (for example, a Java .war file) to Elastic Beanstalk, and then provide some information about the application. Elastic Beanstalk automatically launches an environment and creates and configures the AWS resources needed to run your code. After your environment is launched, you can then manage your environment and deploy new application versions. Hence, the correct answer in this scenario is Elastic Beanstalk. AWS CloudFormation is incorrect. Although the CloudFormation service provides deployment capabilities, you will still have to design a custom template that contains the required AWS resources for your application needs. Hence, this will require more time to complete instead of just directly using Elastic Beanstalk. AWS SAM is incorrect because the AWS Serverless Application Model (AWS SAM) is just an open-source framework that you can use to build serverless applications on AWS. You can't host your application in AWS, unlike Elastic Beanstalk, and it does not automatically handle the details of capacity provisioning, load balancing, scaling, and application health monitoring. AWS CodeDeploy is incorrect because this is primarily used for deployment and not as an orchestration service for your applications. AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/ Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy: https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/"},{question:'The read and write operations to an Amazon DynamoDB table are throttled, causing errors in a stateful application that maintains user sessions. Despite checking Amazon CloudWatch metrics, the consumed capacity units have not exceeded the provisioned capacity. Upon further investigation, it is found that a "hot partition" is being accessed more frequently than others by downstream services. What should be done to resolve this issue with MINIMAL cost? (Select TWO.)',answers:[{text:"Use DynamoDB Accelerator (DAX).",isCorrect:!1},{text:"Increase the amount of read or write capacity for your table.",isCorrect:!1},{text:"Refactor your application to distribute your read and write operations as evenly as possible across your table.",isCorrect:!0},{text:"Implement read sharding to distribute workloads evenly.",isCorrect:!1},{text:"Implement error retries and exponential backoff.",isCorrect:!0}],explanation:'Partitions are usually throttled when they are accessed by your downstream applications much more frequently than other partitions (that is, a "hot" partition), or when workloads rely on short periods of time with high usage (a "burst" of read or write activity). This problem can be more pronounced in stateful applications, where maintaining session data or transactions may cause repeated access to the same partition. To avoid hot partitions and throttling, you must optimize your table and partition structure. DynamoDB adaptive capacity automatically boosts throughput capacity to high-traffic partitions. However, each partition is still subject to the hard limit. This means that adaptive capacity can\'t solve larger issues with your table or partition design. To avoid hot partitions and throttling, optimize your table and partition structure. To solve this issue, consider one or more of the following solutions: - Increase the amount of read or write capacity for your table to anticipate short-term spikes or bursts in read or write operations. If you decide later you don\'t need the additional capacity, decrease it. Take note that Before deciding on how much to increase read or write capacity, consider the best practices in designing your partition keys. - Implement error retries and exponential backoff. This technique improves reliability in stateful applications by allowing retries after progressively longer wait times between errors. AWS SDKs have built-in support for this logic. - Distribute your read operations and write operations as evenly as possible across your table. A hot partition can degrade the overall performance, particularly for stateful applications that repeatedly access certain keys. - Implement a caching solution, such as DynamoDB Accelerator (DAX) or Amazon ElastiCache. For stateful workloads with frequent reads to session or static data, caching can significantly reduce database access. Hence, the correct answers are: - Implement error retries and exponential backoff. - Refactor your application to distribute your read and write operations as evenly as possible across your table. The option that says: Using a DynamoDB Accelerator (DAX) is incorrect. Although it can reduce read load by caching frequently accessed items, it will simply incurs additional costs to maintain the DAX cluster. Since the issue can be addressed by improving workload distribution without extra cost, DAX is not a cost-effective solution for this scenario. The option that says: Increasing the amount of read or write capacity for your table is incorrect because, while it can alleviate throttling by temporarily handling higher traffic, it is an expensive solution that contradicts the minimal-cost requirement. Scaling read and write capacity (RCU and WCU) can lead to significant increases in operational costs, especially for stateful applications with continuous usage patterns. The option that says: Implementing read sharding to distribute workloads evenly is incorrect because the issue primarily involves write operations in a stateful workload. Instead, write sharding should be used to better distribute writes across the partition key space. This can be achieved by appending either random or calculated suffixes to partition key values, which spreads the data more evenly across partitions. References: https://aws.amazon.com/premiumsupport/knowledge-center/throttled-ddb/ https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-sharding.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/'},{question:"A small retail business hired a developer to replace their spreadsheet-based inventory tracking system. They want an automated system that does the following: Process an inventory replenishment when a stock level goes below a certain threshold Send a notification to the business owner about the inventory and replenishment status. The inventory data has already been migrated to an Amazon DynamoDB table. Which architectural pattern should the developer adopt to meet the requirements?",answers:[{text:"An Event-Driven pattern using DynamoDB Streams for capturing inventory changes, Lambda function for executing business logic, and Amazon SNS for push notifications.",isCorrect:!0},{text:"A Scheduled pattern using Amazon EventBridge Scheduler for checking inventory levels, Lambda function for executing business logic, and Amazon SNS for push notifications.",isCorrect:!1},{text:"A Fan-out pattern where Amazon SNS broadcasts orders, triggering Lambda functions for business logic execution and Amazon SNS for push notifications.",isCorrect:!1},{text:"A Batch pattern with Amazon SQS for queuing changes, Lambda function for executing business logic, and Amazon SNS for email notifications.",isCorrect:!1}],explanation:"An event-driven architecture uses events to trigger and communicate between decoupled services and is common in modern applications built with microservices. An event is a change in state or an update, like an item being placed in a shopping cart on an e-commerce website. Events can either carry the state (the item purchased, its price, and a delivery address) or events can be identifiers (a notification that an order was shipped). Event-driven architectures have three key components: event sources/producers, event routers, and event consumers. A producer publishes an event to the router, which filters and pushes the events to consumers. Producer services and consumer services are decoupled, which allows them to be scaled, updated, and deployed independently. In the scenario, the DynamoDB table is the source of events. Updates made to an inventory item are tracked by DynamoDB Streams. Leveraging an Event-Driven pattern, a Lambda function can immediately be triggered by this change in the stream. The function can then execute the required business logic for inventory replenishment and send a notification to an Amazon SNS topic to promptly notify the business owner about the inventory status. This immediate response ensures timely replenishment actions and notifications. Hence, the correct answer is: An Event-Driven pattern using DynamoDB Streams for capturing inventory changes, Lambda function for executing business logic, and Amazon SNS for push notifications. The option that says: A Scheduled pattern using Amazon EventBridge Scheduler for checking inventory levels, Lambda function for executing business logic, and Amazon SNS for push notifications is incorrect. With this approach, if an item's stock goes below the threshold immediately after a check, the system won't be aware of it until the next scheduled check, making it not a suitable pattern for replenishing inventory in a timely manner. The option that says: A Batch pattern with Amazon SQS for queuing changes, Lambda function for executing business logic, and Amazon SNS for email notifications is incorrect because this pattern is designed to process a group of records collectively. It means that immediate action might not be taken when a single item's stock goes below the threshold. Instead, it would wait for a batch of records to be queued up, making it not ideal for a scenario where timely action on individual stock items is critical. The option that says: A Fan-out pattern where Amazon SNS broadcasts orders, triggering Lambda functions for business logic execution and Amazon SNS for push notifications is incorrect. This pattern is primarily suited for scenarios where one event needs to initiate multiple independent actions in parallel. For instance, converting a video clip in different formats. In contrast, the scenario requires succeeding actions that are dependent: after processing the stock level, a notification is sent if a certain condition is met. References: https://aws.amazon.com/what-is/eda/ https://docs.aws.amazon.com/lambda/latest/operatorguide/event-driven-architectures.html https://aws.amazon.com/blogs/architecture/lets-architect-designing-event-driven-architectures/ Check out these Cheat Sheets for AWS Lambda, Amazon DynamoDB, and Amazon SNS: https://tutorialsdojo.com/aws-lambda/ https://tutorialsdojo.com/amazon-dynamodb/ https://tutorialsdojo.com/amazon-sns/"},{question:"A developer is designing an application which will be hosted in ECS and uses an EC2 launch type. You need to group your container instances by certain attributes such as Availability Zone, instance type, or custom metadata. After you have defined a group of container instances, you will need to customize Amazon ECS to place tasks on container instances based on the group you specified. Which of the following ECS features provides you with expressions that you can use to group container instances by a specific attribute?",answers:[{text:"Task Placement Constraints",isCorrect:!1},{text:"Task Placement Strategies",isCorrect:!1},{text:"Cluster Query Language",isCorrect:!0},{text:"Task Groups",isCorrect:!1}],explanation:"When a task that uses the EC2 launch type is launched, Amazon ECS must determine where to place the task based on the requirements specified in the task definition, such as CPU and memory. Similarly, when you scale down the task count, Amazon ECS must determine which tasks to terminate. You can apply task placement strategies and constraints to customize how Amazon ECS places and terminates tasks. Task placement strategies and constraints are not supported for tasks using the Fargate launch type. By default, Fargate tasks are spread across Availability Zones. Cluster queries are expressions that enable you to group objects. For example, you can group container instances by attributes such as Availability Zone, instance type, or custom metadata. You can add custom metadata to your container instances, known as attributes. Each attribute has a name and an optional string value. You can use the built-in attributes provided by Amazon ECS or define custom attributes. After you have defined a group of container instances, you can customize Amazon ECS to place tasks on container instances based on group. Running tasks manually is ideal in certain situations. For example, suppose that you are developing a task but you are not ready to deploy this task with the service scheduler. Perhaps your task is a one-time or periodic batch job that does not make sense to keep running or restart when it finishes. Hence, the correct ECS feature which provides you with expressions that you can use to group container instances by a specific attribute is Cluster Query Language. Task Group is incorrect because this is just a set of related tasks. This does not provide expressions that enable you to group objects. All tasks with the same task group name are considered as a set when performing spread placement. Task Placement Constraint is incorrect because it is just a rule that is considered during task placement. Although it uses cluster queries when you are placing tasks on container instances based on a specific expression, it does not provide the actual expressions which are used to group those container instances. Task Placement Strategies is incorrect because this is just an algorithm for selecting instances for task placement or tasks for termination. References: https://aws.amazon.com/blogs/compute/amazon-ecs-task-placement/ https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cluster-query-language.html Check out this Amazon ECS Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/"},{question:"A developer is designing a multitiered system which utilizes various AWS resources. The application will be hosted in Elastic Beanstalk, which uses an RDS database and an S3 bucket that is configured to use Server-Side Encryption with Customer-Provided Encryption Keys (SSE-C). In this configuration, Amazon S3 does not store the encryption key you provide but instead, stores a randomly salted hash-based message authentication code (HMAC) value of the encryption key in order to validate future requests. Which of the following is a valid consideration that the developer should keep in mind when implementing this architecture?",answers:[{text:"The salted HMAC value can be used to derive the value of the encryption key.",isCorrect:!1},{text:"The salted HMAC value can be used to decrypt the contents of the encrypted object.",isCorrect:!1},{text:"If you lose the encryption key, the salted HMAC value can be used to decrypt the object.",isCorrect:!1},{text:"If you lose the encryption key, you lose the object.",isCorrect:!0}],explanation:"Server-side encryption is about protecting data at rest. Using server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your own encryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it writes to disks, and decryption, when you access your objects. Therefore, you don't need to maintain any code to perform data encryption and decryption. The only thing you do is manage the encryption keys you provide. When you upload an object, Amazon S3 uses the encryption key you provide to apply AES-256 encryption to your data and removes the encryption key from memory. It is important to note that Amazon S3 does not store the encryption key you provide. Instead, it is stored in a randomly salted HMAC value of the encryption key in order to validate future requests. The salted HMAC value cannot be used to derive the value of the encryption key or to decrypt the contents of the encrypted object. That means, if you lose the encryption key, you lose the object. When you retrieve an object, you must provide the same encryption key as part of your request. Amazon S3 first verifies that the encryption key you provided matches, and then decrypts the object before returning the object data to you. Hence, the valid consideration that the developer should keep in mind when implementing this architecture is: if you lose the encryption key, you lose the object. The option that says: the salted HMAC value can be used to derive the value of the encryption key is incorrect because the salted HMAC is just used to validate future encryption requests. It cannot be used to derive the value of the encryption key or to decrypt the contents of the encrypted object. The option that says: the salted HMAC value can be used to decrypt the contents of the encrypted object is incorrect because just as mentioned above, the HMAC cannot be used to derive the value of the encryption key or to decrypt the contents of the encrypted object. The option that says: if you lose the encryption key, the salted HMAC value can be used to decrypt the object is incorrect because if you lose the encryption key, you lose the object. You cannot use the salted HMAC value to decrypt the object. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html#RESTObjectPUT-responses-examples https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/ Check out these Amazon S3 and AWS KMS Cheat Sheets: https://tutorialsdojo.com/amazon-s3/ https://tutorialsdojo.com/aws-key-management-service-aws-kms/"},{question:"There is a requirement to postpone the delivery of new messages to an SQS queue for a number of seconds. You must configure the queue to ensure that any messages that you send remain invisible to consumers for a duration of time specified. Which of the following SQS feature should you use to meet this requirement?",answers:[{text:"Delay Queue",isCorrect:!0},{text:"Short Polling",isCorrect:!1},{text:"Visibility Timeouts",isCorrect:!1},{text:"Long Polling",isCorrect:!1}],explanation:"Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes Delay queues are similar to visibility timeouts because both features make messages unavailable to consumers for a specific period of time. The difference between the two is that, for delay queues, a message is hidden when it is first added to queue, whereas for visibility timeouts a message is hidden only after it is consumed from the queue. To set delay seconds on individual messages rather than on an entire queue, use message timers to allow Amazon SQS to use the message timer's DelaySeconds value instead of the delay queue's DelaySeconds value. Hence, the correct answer is to use a Delay Queue. Short Polling is incorrect because this is just the default configuration of SQS that queries only a subset of its servers (based on a weighted random distribution), to determine whether any messages are available for a response. Visibility Timeouts is incorrect because, with this configuration, a message is hidden only after it is consumed from the queue, and not before. Take note that the difference between the two is that, for delay queues, a message is hidden when it is first added to the queue, whereas for visibility timeouts a message is hidden only after it is consumed from the queue. Long Polling is incorrect because this just helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-delay-queue.html Check out this Amazon SQS Cheat Sheet: https://tutorialsdojo.com/amazon-sqs/"},{question:'You were recently hired as a developer for a leading insurance firm in Asia which has a hybrid cloud architecture with AWS. The project that was assigned to you involves setting up a static website using Amazon S3 with a CORS configuration as shown below: <?xml version="1.0" encoding="UTF-8"?><CORSConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/"> <CORSRule> <AllowedOrigin>https://tutorialsdojo.com</AllowedOrigin> <AllowedMethod>GET</AllowedMethod> <AllowedMethod>PUT</AllowedMethod> <AllowedMethod>POST</AllowedMethod> <AllowedMethod>DELETE</AllowedMethod> <AllowedHeader>*</AllowedHeader> <ExposeHeader>ETag</ExposeHeader> <ExposeHeader>x-amz-meta-custom-header</ExposeHeader> <MaxAgeSeconds>3600</MaxAgeSeconds> </CORSRule></CORSConfiguration> Which of the following statements are TRUE with regards to this S3 configuration? (Select TWO.)',answers:[{text:"This configuration authorizes the user to perform actions on the S3 bucket.",isCorrect:!1},{text:"It allows a user to view, add, remove or update objects inside the S3 bucket from the domain tutorialsdojo.com.",isCorrect:!0},{text:"All HTTP Methods are allowed.",isCorrect:!1},{text:"This will cause the browser to cache the response of the preflight OPTIONS request for 1 hour.",isCorrect:!0},{text:"The request will fail if the x-amz-meta-custom-header header is not included.",isCorrect:!1}],explanation:"Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources. To configure your bucket to allow cross-origin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support each origin, and other operation-specific information. You can add up to 100 rules to the configuration. You add the XML document as the cors subresource to the bucket either programmatically or by using the Amazon S3 console as shown below: A CORS configuration is an XML file that contains a series of rules within a <CORSRule>. A configuration can have up to 100 rules. A rule is defined by one of the following tags: AllowedOrigin - Specifies domain origins that you allow to make cross-domain requests. AllowedMethod - Specifies a type of request you allow (GET, PUT, POST, DELETE, HEAD) in cross-domain requests. AllowedHeader - Specifies the headers allowed in a preflight request. Below are some of the CORSRule elements: MaxAgeSeconds - Specifies the amount of time in seconds (in this example, 3000) that the browser caches an Amazon S3 response to a preflight OPTIONS request for the specified resource. By caching the response, the browser does not have to send preflight requests to Amazon S3 if the original request will be repeated. ExposeHeader - Identifies the response headers (in this example, x-amz-server-side-encryption, x-amz-request-id, and x-amz-id-2) that customers are able to access from their applications (for example, from a JavaScript XMLHttpRequest object). Hence, the correct answers in this scenario are: - It allows a user to view, add, remove or update objects inside the S3 bucket from the domain tutorialsdojo.com - This will cause the browser to cache an Amazon S3 response of a preflight OPTIONS request for 1 hour The option that says: the request will fail if the x-amz-meta-custom-header header is not included is incorrect because the ExposeHeader element refers to the header that will be exposed to the response and not a constraint for the request. The option that says: this configuration authorizes the user to perform actions on the S3 bucket is incorrect because this configuration actually does the opposite. It doesn't authorize the user to perform actions on the S3 bucket. The option that says: all HTTP Methods are allowed is incorrect because the configuration didn't include the HEAD HTTP method. References: http://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/cors.html"},{question:"An online role-playing video game requires cross-device syncing of application-related user data. It must synchronize the user profile data across mobile devices without requiring your own backend. When the device is online, it should synchronize data and notify other devices immediately that an update is available. Which of the following is the most suitable feature that you have to use to meet this requirement?",answers:[{text:"Amazon Cognito Sync",isCorrect:!0},{text:"Amazon Cognito User Pools",isCorrect:!1},{text:"AWS Device Farm",isCorrect:!1},{text:"Amazon Cognito Identity Pools",isCorrect:!1}],explanation:"Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status. When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available. Amazon Cognito lets you save end-user data in datasets containing key-value pairs. This data is associated with an Amazon Cognito identity, so that it can be accessed across logins and devices. To sync this data between the Amazon Cognito service and an end user’s devices, invoke the synchronize method. Each dataset can have a maximum size of 1 MB. You can associate up to 20 datasets with an identity. The Amazon Cognito Sync client creates a local cache for the identity data. Your app talks to this local cache when it reads and writes keys. This guarantees that all of your changes made on the device are immediately available on the device, even when you are offline. When the synchronize method is called, changes from the service are pulled to the device, and any local changes are pushed to the service. At this point, the changes are available to other devices to synchronize. Amazon Cognito automatically tracks the association between identity and devices. Using the push synchronization, or push sync, feature, you can ensure that every instance of a given identity is notified when identity data changes. Push sync ensures that whenever the sync store data changes for a particular identity, all devices associated with that identity receive a silent push notification informing them of the change. Hence, the correct answer is to Amazon Cognito Sync. Amazon Cognito User Pools is incorrect because this is just a user directory which allows your users to sign in to your web or mobile app through Amazon Cognito. Amazon Cognito Identity Pools is incorrect because this simply enables you to create unique identities for your users and federate them with identity providers where you can obtain temporary, limited-privilege AWS credentials to access other AWS services. AWS Device Farm is incorrect because this is only an app testing service that lets you test and interact with your Android, iOS, and web apps on many devices at once, or reproduce issues on a device in real-time. References: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html https://docs.aws.amazon.com/cognito/latest/developerguide/push-sync.html Check out this Amazon Cognito Cheat Sheet: https://tutorialsdojo.com/amazon-cognito/"},{question:"The developer has built a real-time IoT device monitoring application that leverages Amazon Kinesis Data Stream to ingest data. The application uses several EC2 instances for processing. Recently, the developer has observed a steady increase in the rate of data flowing into the stream, indicating that the stream's capacity must be scaled up to sustain optimal performance. What should the developer do to increase the capacity of the stream?",answers:[{text:"Split every shard in the stream.",isCorrect:!0},{text:"Merge every shard in the stream.",isCorrect:!1},{text:"Integrate Amazon Data Firehose with the Amazon Kinesis Data Stream to increase the capacity of the stream.",isCorrect:!1},{text:"Upgrade the instance type of the EC2 instances.",isCorrect:!1}],explanation:'Amazon Kinesis Data Streams is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. You can configure hundreds of thousands of data producers to continuously put data into a Kinesis data stream. Data will be available within milliseconds to your Amazon Kinesis applications, and those applications will receive data records in the order they were generated. The purpose of resharding in Amazon Kinesis Data Streams is to enable your stream to adapt to changes in the rate of data flow. You split shards to increase the capacity (and cost) of your stream. You merge shards to reduce the cost (and capacity) of your stream. One approach to resharding could be to split every shard in the stream—which would double the stream\'s capacity. However, this might provide more additional capacity than you actually need and, therefore, create unnecessary costs. You can also use metrics to determine which are your "hot" or "cold" shards, that is, shards that are receiving much more data or much less data than expected. You could then selectively split the hot shards to increase capacity for the hash keys that target those shards. Similarly, you could merge cold shards to make better use of their unused capacity. You can obtain some performance data for your stream from the Amazon CloudWatch metrics that Kinesis Data Streams publishes. However, you can also collect some of your own metrics for your streams. One approach would be to log the hash key values generated by the partition keys for your data records. Recall that you specify the partition key at the time that you add the record to the stream. Hence, the correct answer is: Split every shard in the stream. The option that says: Upgrade the instance type of the EC2 instances is incorrect. Although it will improve the processing time of the data in the stream, it will not increase the capacity of the stream itself. You have to reshard the stream in order to increase or decrease its capacity, and not just upgrade the EC2 instances which process the data in the stream. The option that says: Merge every shard in the stream is incorrect because merging shards will actually decrease the capacity of the stream rather than increase it. This is only useful if you want to save costs or if the data stream is underutilized, which are both not indicated in the scenario. The option that says: Integrate Amazon Data Firehose with the Amazon Kinesis Data Stream to increase the capacity of the stream is incorrect because Data Firehose just provides a way to reliably transform and load streaming data into data stores and analytics tools. This method will not increase the capacity of the stream as it doesn\'t mention anything about resharding. References: https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/ Kinesis Scaling, Resharding and Parallel Processing: https://tutorialsdojo.com/kinesis-scaling-resharding-and-parallel-processing/'},{question:"A company is using OpenAPI, which is also known as Swagger, for the API specifications of their REST web services that are hosted on their on-premises data center. They want to migrate their system to AWS using Lambda and API Gateway. In line with this, you are instructed to create a new API and populate it with the resources and methods from their Swagger definition. Which of the following is the EASIEST way to accomplish this task?",answers:[{text:"Import their Swagger or OpenAPI definitions to API Gateway using the AWS Console.",isCorrect:!0},{text:"Use AWS SAM to migrate and deploy the company's web services to API Gateway.",isCorrect:!1},{text:"Create models and templates for request and response mappings based on the company's API definitions.",isCorrect:!1},{text:"Use CodeDeploy to migrate and deploy the company's web services to API Gateway.",isCorrect:!1}],explanation:"You can use the API Gateway Import API feature to import a REST API from an external definition file into API Gateway. Currently, the Import API feature supports OpenAPI v2.0 and OpenAPI v3.0 definition files. You can update an API by overwriting it with a new definition or merge a definition with an existing API. You specify the options using a mode query parameter in the request URL. You can paste a Swagger API definition in the AWS Console to create a new API and populate it with the resources and methods from your Swagger or OpenAPI definition, just as shown below: You can also import your Swagger definition through the AWS CLI and SDKs. Hence, the correct answer in this scenario is to import their Swagger or OpenAPI definitions to API Gateway using the AWS Console. Using CodeDeploy to migrate and deploy the company's web services to API Gateway is incorrect because using CodeDeploy alone is not enough to deploy new custom APIs. This is mainly used in conjunction with AWS SAM where you can add deployment preferences to manage the way traffic is shifted during an AWS Lambda application deployment. Using AWS SAM to migrate and deploy the company's web services to API Gateway is incorrect. Although using AWS SAM is the preferred way to deploy your serverless application, it is not the easiest way to import the Swagger API definitions file. As mentioned above, you can simply import Swagger or OpenAPI files directly to AWS. Creating models and templates for request and response mappings based on the company's API definitions is incorrect because this is primarily done for API Gateway integration to other services and not for importing API definitions file. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-import-api.html https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-from-example.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A developer is tasked with automating the deployment of a new microservice in an ECS cluster using AWS CodeDeploy. The developer is writing the AppSpec file to instruct CodeDeploy on how to handle the deployment. Which sets of properties are REQUIRED in the resources section to successfully deploy the microservice? (Select THREE.)",answers:[{text:"ContainerName",isCorrect:!0},{text:"NetworkConfiguration",isCorrect:!1},{text:"targetversion",isCorrect:!1},{text:"ContainerPort",isCorrect:!0},{text:"alias",isCorrect:!1},{text:"TaskDefinition",isCorrect:!0}],explanation:"An AppSpec file is a YAML or JSON formatted file used by AWS CodeDeploy to manage and direct the deployment process. It provides instructions regarding how the deployment service should handle application content and lifecycle event hooks. Depending on the type of deployment, the content and structure of the AppSpec file will differ, tailored to the specific requirements and nature of each service. For ECS deployments, the resources section specifies the Amazon ECS service to deploy and has the following structure: Out of these properties, only the TaskDefinition, ContainerName, ContainerPort are required by CodeDeploy for ECS deployments. TaskDefinition - This is the task definition for the Amazon ECS service to deploy. It is specified with the ARN of the task definition. ContainerName - This is the name of the Amazon ECS container that contains your Amazon ECS application. It must be a container specified in your Amazon ECS task definition. ContainerPort - This is the port on the container where traffic will be routed. Hence, the correct answers are: - TaskDefinition - ContainerName - ContainerPort targetversion is incorrect. This property is specifically related to Lambda deployments when using CodeDeploy. It specifies which version of a Lambda function to route traffic to. However, for ECS deployments, it's not a required property in the AppSpec file. alias is incorrect because this property is only relevant to Lambda deployments using CodeDeploy. In AWS Lambda, an alias is a pointer to a specific Lambda function version. In the context of ECS deployments, this property is not required in the AppSpec file. NetworkConfiguration is incorrect. While it can be included in the AppSpec file for ECS deployments using CodeDeploy, it's just optional. If not specified, the ECS service's current network configuration is used. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-resources.html https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-example.html#appspec-file-example-ecs Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/"},{question:"A batch application is hosted in an Auto Scaling group of On-Demand EC2 instances which consumes and processes the messages from an SQS queue. The system works well but there are times that the consumers process the same message twice. Upon investigation, you found out that if the consumer takes a long time to process the message, that exact same message becomes available again to other consumers, which causes duplicate processing. Which of the following is the BEST solution that the developer should implement to meet this requirement?",answers:[{text:"Postpone the delivery of new messages by using a delay queue.",isCorrect:!1},{text:"Configure the queue to use long polling by setting the Receive Message Wait Time parameter to a value greater than 0.",isCorrect:!1},{text:"Set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue.",isCorrect:!0},{text:"Configure the queue to use short polling by setting the WaitTimeSeconds parameter of the ReceiveMessage request to 0.",isCorrect:!1}],explanation:"The visibility timeout is a period of time during which Amazon SQS prevents other consuming components from receiving and processing a message. When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn't automatically delete the message. Because Amazon SQS is a distributed system, there's no guarantee that the consumer actually receives the message (for example, due to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must delete the message from the queue after receiving and processing it. Immediately after the message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The maximum is 12 hours. The visibility timeout begins when Amazon SQS returns a message. During this time, the consumer processes and deletes the message. However, if the consumer fails before deleting the message and your system doesn't call the DeleteMessage action for that message before the visibility timeout expires, the message becomes visible to other consumers and the message is received again. If a message must be received only once, your consumer should delete it within the duration of the visibility timeout. Every Amazon SQS queue has the default visibility timeout setting of 30 seconds. You can change this setting for the entire queue. Typically, you should set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue. When receiving messages, you can also set a special visibility timeout for the returned messages without changing the overall queue timeout. Hence, the best solution in this scenario is to set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue. Configuring the queue to use short polling by setting the WaitTimeSeconds parameter of the ReceiveMessage request to 0 is incorrect. Although the implementation steps for short polling is accurate, this is not enough to keep other consumers from processing the undeleted message that became available again in the queue. This is just the default configuration of SQS that queries only a subset of its servers (based on a weighted random distribution), to determine whether any messages are available for a response. Hence, this is irrelevant in this scenario. Configuring the queue to use long polling by setting the Receive Message Wait Time parameter to a value greater than 0 is incorrect. Although the implementation steps for long polling is accurate, this configuration just helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). A more appropriate solution in this scenario is to configure the visibility timeout of the messages. Postponing the delivery of new messages by using a delay queue is incorrect. Although a visibility timeout and delay queue are almost the same, there are still some key differences between these two in the scenario which warrants the use of the former rather than the latter. For delay queues, a message is hidden when it is first added to queue, whereas for visibility timeouts, a message is hidden only after it is consumed from the queue which is what the scenario depicts. References: https://aws.amazon.com/sqs/faqs/ https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html Check out this Amazon SQS Cheat Sheet: https://tutorialsdojo.com/amazon-sqs/"},{question:"You are managing an application which is composed of an SQS queue and an Auto Scaling group of EC2 instances. Recently, your customers are complaining that there are a lot of incidents where their orders are being erroneously sent twice. What should you do to rectify this problem?",answers:[{text:"Use a FIFO (First-In-First-Out) Queue by disabling the content-based deduplication.",isCorrect:!1},{text:"Use a Standard Queue and provide the Message Group ID for each message.",isCorrect:!1},{text:"Use a Standard Queue and provide the Message Deduplication ID for each message.",isCorrect:!1},{text:"Use a FIFO (First-In-First-Out) Queue and provide the Message Deduplication ID for each message.",isCorrect:!0}],explanation:"Unlike standard queues, FIFO queues don't introduce duplicate messages. FIFO queues help you avoid sending duplicates to a queue. If you retry the SendMessage action within the 5-minute deduplication interval, Amazon SQS doesn't introduce any duplicates into the queue. To configure deduplication, you must do one of the following: - Enable content-based deduplication. This instructs Amazon SQS to use a SHA-256 hash to generate the message deduplication ID using the body of the message - but not the attributes of the message. - Explicitly provide the message deduplication ID (or view the sequence number) for the message. The message deduplication ID is the token used for deduplication of sent messages. If a message with a particular message deduplication ID is sent successfully, any messages sent with the same message deduplication ID are accepted successfully but aren't delivered during the 5-minute deduplication interval. Message deduplication applies to an entire queue, not to individual message groups. Amazon SQS continues to keep track of the message deduplication ID even after the message is received and deleted. Hence, the correct answer in this scenario is to use a FIFO (First-In-First-Out) Queue and provide the Message Deduplication ID for each message. Using a FIFO (First-In-First-Out) Queue by disabling the content-based deduplication is incorrect. Although the use of FIFO queue is valid, it is wrong to disable the content-based deduplication. This should be enabled to avoid duplicate messages in the queue. Using a Standard Queue and providing the Message Group ID for each message is incorrect because you should use a FIFO queue instead to avoid duplicate messages. Using a Standard Queue and providing the Message Deduplication ID for each message is incorrect. Although it is a valid answer to provide the Message Deduplication ID, this feature can't be enabled for Standard Queues. You have to use the FIFO queues instead for this scenario. References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html#FIFO-queues-exactly-once-processing https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html Check out this Amazon SQS Cheat Sheet: https://tutorialsdojo.com/amazon-sqs/"},{question:"You are developing a high-traffic online stocks trading application, which will be hosted in an ECS Cluster and will be accessed by thousands of investors for intraday stocks trading. Each task of the cluster should be evenly placed across multiple Availability Zones to avoid any service disruptions. Which of the following is the MOST suitable placementStrategy configuration that you should use in your task definition?",answers:[{text:'"placementStrategy": [ { "field": "memory", "type": "binpack" }]',isCorrect:!1},{text:'"placementStrategy": [ { "type": "random" }]',isCorrect:!1},{text:'"placementStrategy": [ { "field": "instanceId", "type": "spread" }]',isCorrect:!1},{text:'"placementStrategy": [ { "field": "attribute:ecs.availability-zone", "type": "spread" }]',isCorrect:!0}],explanation:'When a task that uses the EC2 launch type is launched, Amazon ECS must determine where to place the task based on the requirements specified in the task definition, such as CPU and memory. Similarly, when you scale down the task count, Amazon ECS must determine which tasks to terminate. You can apply task placement strategies and constraints to customize how Amazon ECS places and terminates tasks. By default, tasks are randomly placed with RunTask or spread across Availability Zones with CreateService. Spread is typically used to achieve high availability by making sure that multiple copies of a task are scheduled across multiple instances based on attributes such as Availability Zones. A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service. Amazon ECS supports the following task placement strategies: binpack - Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use. random - Place tasks randomly. spread - Place tasks evenly based on the specified value. Accepted values are attribute key-value pairs, instanceId, or host. The spread strategy, contrary to the binpack strategy, tries to put your tasks on as many different instances as possible. It is typically used to achieve high availability and mitigate risks, by making sure that you don’t put all your task-eggs in the same instance-baskets. Spread across Availability Zones, therefore, is the default placement strategy used for services. When using the spread strategy, you must also indicate a field parameter. It is used to indicate the bins that you are considering. The accepted values are instanceID, host, or a custom attribute key:value pairs such as attribute:ecs.availability-zone to balance tasks across zones. There are several AWS attributes that start with the ecs prefix, but you can be creative and create your own attributes. Hence, the task placement configuration which has a value of "field": "attribute:ecs.availability-zone", "type": "spread" is correct, because this is using the appropriate strategy for task placement. The configuration which has a value of "field": "instanceId", "type": "spread" is incorrect because although it is using a spread task placement strategy, it distributes tasks evenly across all instances and not to Availability Zones. The configuration which has a value of "field": "memory", "type": "binpack" is incorrect because this is using a strategy that bin packs tasks based on memory. Take note that the scenario is asking for a configuration which will evenly place the tasks across multiple Availability Zones, and not based on memory. The configuration which has a value of "type": "random" is incorrect because this will just place the tasks on instances randomly. You have to use the spread task placement strategy instead to meet the requirements of the scenario. References: https://aws.amazon.com/blogs/compute/amazon-ecs-task-placement/ https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html Check out this Amazon ECS Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/'},{question:"A newly hired developer has been instructed to debug an application. She tried to access the X-Ray console to view service maps and segments but her current access is insufficient. Which of the following is the MOST appropriate managed policy that should be granted to the developer?",answers:[{text:"AmazonS3ReadOnlyAccess",isCorrect:!1},{text:"AWSXrayFullAccess",isCorrect:!1},{text:"AWSXRayDaemonWriteAccess",isCorrect:!1},{text:"AWSXrayReadOnlyAccess",isCorrect:!0}],explanation:"You can use AWS Identity and Access Management (IAM) to grant X-Ray permissions to users and compute resources in your account. IAM controls access to the X-Ray service at the API level to enforce permissions uniformly, regardless of which client (console, AWS SDK, AWS CLI) your users employ. To use the X-Ray console to view service maps and segments, you only need read permissions. To enable console access, add the AWSXrayReadOnlyAccess managed policy to your IAM user. For local development and testing, create an IAM user with read and write permissions. Generate access keys for the user and store them in the standard AWS SDK location. You can use these credentials with the X-Ray daemon, the AWS CLI, and the AWS SDK. To deploy your instrumented app to AWS, create an IAM role with write permissions and assign it to the resources running your application. AWSXRayDaemonWriteAccess includes permission to upload traces and some read permissions as well to support the use of sampling rules. The read and write policies do not include permission to configure encryption key settings and sampling rules. Use AWSXrayFullAccess to access these settings or add configuration APIs in a custom policy. For encryption and decryption with a customer managed key that you create, you also need permission to use the key. Hence, the AWSXrayReadOnlyAccess managed policy is the most appropriate one to grant to the developer in order for her to access the X-Ray console. This also abides with the standard security advice of granting least privilege, or granting only the permissions required to perform a task. AWSXRayDaemonWriteAccess is incorrect because this policy is more suitable if you want to grant permission to upload traces to X-Ray. AWSXrayFullAccess is incorrect. Although this can provide the required access to the developer, it does not abide with the standard security advice of granting least privilege. Hence, this is not the most appropriate policy to use. AmazonS3ReadOnlyAccess is incorrect because this policy just provides the instance permission to download the X-Ray daemon from Amazon S3. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-permissions.html https://docs.aws.amazon.com/xray/latest/devguide/security.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/"},{question:"A software engineer is developing a serverless application which will use a DynamoDB database. One of the requirements is that each write request should return the total number of write capacity units consumed, with subtotals for the table and any secondary indexes that were affected by the operation. What should be done to accomplish this feature?",answers:[{text:"Add the ReturnValues parameter with a value of TOTAL in every write request.",isCorrect:!1},{text:"Add the ReturnConsumedCapacity parameter with a value of TOTAL in every write request.",isCorrect:!1},{text:"Add the ReturnValues parameter with a value of INDEXES in every write request.",isCorrect:!1},{text:"Add the ReturnConsumedCapacity parameter with a value of INDEXES in every write request.",isCorrect:!0}],explanation:"To create, update, or delete an item in a DynamoDB table, use one of the following operations: - PutItem - UpdateItem - DeleteItem For each of these operations, you need to specify the entire primary key, not just part of it. For example, if a table has a composite primary key (partition key and sort key), you must supply a value for the partition key and a value for the sort key. To return the number of write capacity units consumed by any of these operations, set the ReturnConsumedCapacity parameter to one of the following: TOTAL — returns the total number of write capacity units consumed. INDEXES — returns the total number of write capacity units consumed, with subtotals for the table and any secondary indexes that were affected by the operation. NONE — no write capacity details are returned. (This is the default.) Hence, the correct answer is to add the ReturnConsumedCapacity parameter with a value of INDEXES in every write request. Adding the ReturnValues parameter with a value of INDEXES in every write request is incorrect because you should use a ReturnConsumedCapacity parameter instead. Adding the ReturnConsumedCapacity parameter with a value of TOTAL in every write request is incorrect because this will not return the consumed WCU subtotals for the table and any secondary indexes that were affected by the operation just as what is required by the application. You have to use INDEXES instead. Adding the ReturnValues parameter with a value of TOTAL in every write request is incorrect because you should use a ReturnConsumedCapacity parameter instead. In addition, the value of the parameter is also incorrect as it doesn't return the consumed WCU subtotals for the table and any secondary indexes that were affected by the operation. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.WritingData https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_PutItem.html#API_PutItem_RequestParameters Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"},{question:"For application deployments, a company is using CloudFormation templates, which are regularly updated to map the latest AMI IDs. A developer was assigned to automate the process since the current set up takes a lot of time to execute on a regular basis. Which of the following is the MOST suitable solution that the developer should implement to satisfy this requirement?",answers:[{text:"Integrate CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments.",isCorrect:!1},{text:"Set up your Systems Manager State Manager to store the latest AMI IDs and integrate it with your CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template.",isCorrect:!1},{text:"Integrate AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments.",isCorrect:!1},{text:"Set up CloudFormation with Systems Manager Parameter Store to retrieve the latest AMI IDs for your template. Whenever you decide to update the EC2 instances, call the update-stack API in CloudFormation in your CloudFormation template.",isCorrect:!0}],explanation:"You can use the existing Parameters section of your CloudFormation template to define Systems Manager parameters, along with other parameters. Systems Manager parameters are a unique type that is different from existing parameters because they refer to actual values in the Parameter Store. The value for this type of parameter would be the Systems Manager (SSM) parameter key instead of a string or other value. CloudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation. If the parameter referenced in the template does not exist in Systems Manager, there will be synchronous validation error that will be thrown. Also, if you have defined any parameter value validations (AllowedValues, AllowedPattern, etc.) for Systems Manager parameters, they will be performed against SSM keys which are given as input values for template parameters, not actual values stored in Systems Manager. Parameters stored in Systems Manager are mutable. Any time you use a template containing Systems Manager parameters to create/update your stacks, CloudFormation uses the values for these Systems Manager parameters at the time of the create/update operation. So, as parameters are updated in Systems Manager, you can have the new value of the parameter take effect by just executing a stack update operation. The Parameters section in the output for Describe API will show an additional ‘ResolvedValue’ field that contains the resolved value of the Systems Manager parameter that was used for the last stack operation. Hence, the correct answer is to set up CloudFormation with Systems Manager Parameter Store to retrieve the latest AMI IDs for your template. Whenever you decide to update the EC2 instances, call the update-stack API in CloudFormation in your CloudFormation template. The option that says: Set up your Systems Manager State Manager to store the latest AMI IDs and integrate it with your CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template is incorrect because the Systems Manager State Manager service simply automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. This can't be used as a parameter store that refers to the latest AMI of your application. The option that says: Integrate AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments is incorrect because AWS Service Catalog is not suitable in this scenario since this service just allows organizations to create and manage catalogs of IT services that are approved for use on AWS. In addition, AWS Config is simply a service that enables you to assess, audit, and evaluate the configurations of your AWS resources, which clearly is irrelevant in this case as the developer won't be able to use this to store the latest AMI IDs. The option that says: Integrate CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments is incorrect because, just as mentioned above, the AWS Service Catalog just allows organizations to create and manage catalogs of IT services that are approved for use on AWS. A more appropriate solution for this scenario would be to use the Systems Manager Parameter Store to retrieve the latest AMI IDs for your template. References: https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/ https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html Check out this AWS Systems Manager Cheat Sheet: https://tutorialsdojo.com/aws-systems-manager/"},{question:"A financial company has a cryptocurrency application that has been hosted in Elastic Beanstalk for a couple of months. Recently, the application's performance has been degrading, so you decided to check the CPU and memory utilization of the underlying EC2 instances in CloudWatch. You can see the CPU utilization of the instances but not the memory utilization. Which of the following is the MOST likely cause of this issue?",answers:[{text:"The .ebextensions/xray-daemon.config file in Elastic Beanstalk is missing.",isCorrect:!1},{text:"The detailed monitoring is not enabled in CloudWatch.",isCorrect:!1},{text:"X-Ray Daemon is not installed on the EC2 instances.",isCorrect:!1},{text:"CloudWatch does not track memory utilization by default.",isCorrect:!0}],explanation:"Amazon CloudWatch agent enables you to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. The agent supports both Windows Server and Linux and allows you to select the metrics to be collected, including sub-resource metrics such as per-CPU core. Metrics are data about the performance of your systems. By default, several services provide free metrics for resources (such as Amazon EC2 instances, Amazon EBS volumes, and Amazon RDS DB instances). You can also enable detailed monitoring on some resources, such as your Amazon EC2 instances, or publish your own application metrics. Amazon CloudWatch can load all the metrics in your account (both AWS resource metrics and application metrics that you provide) for search, graphing, and alarms. However, take note that CloudWatch does not monitor the memory, swap, and disk space utilization of your instances. If you need to track these metrics, you can install a CloudWatch agent in your EC2 instances. Hence, the correct answer is: CloudWatch does not track memory utilization by default. The option that says: The detailed monitoring is not enabled in CloudWatch is incorrect because this will just send metric data for your instance to CloudWatch in 1-minute periods, but not including the memory utilization. The option that says: X-Ray Daemon is not installed to the EC2 instances is incorrect because X-Ray is primarily used for troubleshooting applications and not to monitor the actual EC2 instances. Even if the X-Ray Daemon is installed and is running in the instance, it will still not send the memory utilization metrics to CloudWatch. The option that says: The .ebextensions/xray-daemon.config file in Elastic Beanstalk is missing is incorrect because just like what is mentioned above, X-Ray will not send the memory utilization of the EC2 instance. References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html Check out these Amazon EC2 and CloudWatch Cheat Sheets: https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/ https://tutorialsdojo.com/amazon-cloudwatch/"},{question:"A company has an AWS account with an ID of 061218980612 and has a centralized Java web application hosted in AWS Elastic Beanstalk that is used by different departments. The developer used the iam create-account-alias --account-alias finance-dept AWS CLI command to create a user-friendly identifier for the finance department. For faster troubleshooting, the application must also be configured to easily trace all its downstream requests, such as Apache HTTP requests, AWS SDK requests, and SQL queries made using a JDBC driver. The ability to send traces to multiple different tracing backends without having to re-instrument the application code is required as well. Which of the following options is the MOST suitable solution that the developer implements?",answers:[{text:"Use the https://061218980612.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure the Amazon CloudWatch Evidently to trace all the downstream API calls.",isCorrect:!1},{text:"Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.",isCorrect:!0},{text:"Use the https://finance-dept.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the downstream API calls.",isCorrect:!1},{text:"Use the https://finance-dept.aws.amazon.com/console sign-in page URL for the AWS account. Install and configure the AWS X-Ray auto-instrumentation Java agent to trace all the downstream API calls.",isCorrect:!1}],explanation:"The SDKs included with X-Ray are part of a tightly integrated instrumentation solution offered by AWS. The AWS Distro for OpenTelemetry is part of a broader industry solution in which X-Ray is only one of many tracing solutions. You can implement end-to-end tracing in X-Ray using either approach, but it’s important to understand the differences in order to determine the most useful approach for you. It is recommended to instrument your application with the AWS Distro for OpenTelemetry if you need the following: -The ability to send traces to multiple different tracing backends without having to re-instrument your code -Support for a large number of library instrumentations for each language, maintained by the OpenTelemetry community -Fully managed Lambda layers that package everything you need to collect telemetry data without requiring code changes when using Java, Python, or Node.js Conversely, it is recommended to choose an X-Ray SDK for instrumenting your application if you need the following: -A tightly integrated single-vendor solution -Integration with X-Ray centralized sampling rules, including the ability to configure sampling rules from the X-Ray console and automatically use them across multiple hosts, when using Node.js, Python, Ruby, or .NET An account alias substitutes for an account ID in the web address for your account. You can create and manage an account alias from the AWS Management Console, AWS CLI, or AWS API. Your sign-in page URL has the following format by default: https://Your_AWS_Account_ID.signin.aws.amazon.com/console/ If you create an AWS account alias for your AWS account ID, your sign-in page URL looks like the following example. https://Your_Alias.signin.aws.amazon.com/console/ The original URL containing your AWS account ID remains active and can be used after you create your AWS account alias. For example, the following create-account-alias command creates the alias tutorialsdojo for your AWS account: aws iam create-account-alias --account-alias tutorialsdojo Hence, for this scenario, the correct answer is: Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls. The option that says: Use the https://061218980612.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure the Amazon CloudWatch Evidently to trace all the downstream API calls is incorrect because Amazon CloudWatch Evidently is not capable of tracing any API calls. This particular service is used to safely validate your new features by serving them to a specified percentage of your users while you roll out the feature. The option that says: Use the https://finance-dept.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the downstream API calls is incorrect because the AWS Identity and Access Management (IAM) Roles Anywhere is mainly used to bridge the trust model of IAM and Public Key Infrastructure (PKI) but not for tracing the downstream call. The model connects the role, the IAM Roles Anywhere service principal, and identities encoded in X509 certificates, that are issued by a Certificate Authority (CA). The option that says: Use the https://finance-dept.aws.amazon.com/console sign-in page URL for the AWS account. Install and configure the AWS X-Ray auto-instrumentation Java agent to trace all the downstream API calls is incorrect. Although it is right that the AWS X-Ray auto-instrumentation agent for Java is capable of providing a tracing solution that instruments your Java web applications with minimal development effort, it still doesn't have the ability to send traces to multiple different tracing backends without having to re-instrument the application. A more suitable option is to set up the AWS Distro for OpenTelemetry. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-instrumenting-your-app.html#xray-instrumenting-choosing https://docs.aws.amazon.com/IAM/latest/UserGuide/console_account-alias.html https://docs.aws.amazon.com/cli/latest/reference/iam/create-account-alias.html Check out this AWS IAM and AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/ https://tutorialsdojo.com/aws-x-ray/"},{question:"A developer is designing a multi-threaded e-commerce application that will be reading and writing data on a DynamoDB table. There will be a lot of people who will use the application to update the price of items in the table at the same time. The application should prevent an update operation from modifying an item if one of its attributes has a certain value. Which of the following is the most suitable solution that the developer should use in this application?",answers:[{text:"Use batch operations.",isCorrect:!1},{text:"Use optimistic locking and conditional writes.",isCorrect:!0},{text:"Use pessimistic locking and conditional writes.",isCorrect:!1},{text:"Use atomic counters.",isCorrect:!1}],explanation:"Optimistic locking is a strategy to ensure that the client-side item that you are updating (or deleting) is the same as the item in DynamoDB. Optimistic concurrency depends on checking a value upon save to ensure that it has not changed. If you use this strategy, then your database writes are protected from being overwritten by the writes of others — and vice-versa. By default, the DynamoDB write operations (PutItem, UpdateItem, DeleteItem) are unconditional: each of these operations will overwrite an existing item that has the specified primary key. DynamoDB optionally supports conditional writes for these operations. A conditional write will succeed only if the item attributes meet one or more expected conditions. Otherwise, it returns an error. Conditional writes are helpful in many situations. For example, you might want a PutItem operation to succeed only if there is not already an item with the same primary key. Or you could prevent an UpdateItem operation from modifying an item if one of its attributes has a certain value. Conditional writes are helpful in cases where multiple users attempt to modify the same item. In the AWS SDK for PHP, there is a PessimisticLockingStrategy class for DynamoDB. This locking strategy uses pessimistic locking (similar to how the native PHP session handler works) to ensure that sessions are not edited while another process is reading/writing to it. Pessimistic locking can be expensive and can increase latencies, especially in cases where the user can access the session more than once at the same time (e.g. ajax, iframes, or multiple browser tabs). Hence, using a combination of optimistic locking and conditional writes is the correct answer in this scenario. Using a combination of pessimistic concurrency and conditional writes is incorrect as this will just prevent a value from being updated by locking the item or row in the database. This can block users from reading, updating, or deleting an entry depending on the lock type which is not suitable for the multithreaded application. You have to use optimistic locking strategy and conditional writes instead. Using batch operations is incorrect because this will just reduce the number of network round trips when reading or writing multiple items from your application to DynamoDB. This will not improve the concurrency of your multithreaded application. Using atomic counters is incorrect because this is just a numeric attribute that is unconditionally incremented without interfering with other write requests. The more suitable solution to use in this solution is conditional writes. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBContext.VersionSupport.htm https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html https://docs.aws.amazon.com/aws-sdk-php/v2/api/class-Aws.DynamoDb.Session.LockingStrategy.PessimisticLockingStrategy.html Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"},{question:"A developer has enabled API Caching on his application endpoints in Amazon API Gateway. For testing purposes, he wants to fetch the latest data, and not the cache data, whenever he sends a GET request with a Cache-Control: max-age=0 header to a specific resource. Which of the following policies will allow him to invalidate the cache for requests?",answers:[{text:'{ "Version": "2012-10-17", "Statement": [ { "Effect": "Deny", "Action": [ "execute-api:*" ], "Resource": [ "arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier" ] } ]}',isCorrect:!1},{text:'{ "Version": "2012-10-17", "Statement": [ { "Effect": "Deny", "Action": [ "execute-api:InvalidateCache" ], "Resource": [ "arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier" ] } ]}',isCorrect:!1},{text:'{ "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Action": [ "execute-api:Invoke" ], "Resource": ["arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier" ] } ]}',isCorrect:!1},{text:'{ "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Action": [ "execute-api:InvalidateCache" ], "Resource": [ "arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier" ] } ]}',isCorrect:!0}],explanation:'You can enable API caching in Amazon API Gateway to cache your endpoint\'s responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled. A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the Cache-Control: max-age=0 header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint. To grant permission for a client, attach a policy of the following format to an IAM execution role for the user: { "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Action": [ "execute-api:InvalidateCache" ], "Resource": [ "arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier" ] } ]} Hence, the option which has the same policy shown above is the correct answer since attaching the policy that allows the action "execute-api:InvalidateCache" to your IAM execution role will allow the API Gateway execution service to invalidate the cache results for requests on the specified resource (or resources). In the list of options, you have to take a look at the values of the "Effect" and "Action" fields. The option which has a policy of "Effect": "Deny", "Action": ["execute-api:InvalidateCache"] is incorrect because this will deny any request to invalidate cache results in API Gateway. The option which has a policy of "Effect": "Deny", "Action": ["execute-api:*"] is incorrect because this will deny all requests to your API Gateway such as API invocation, cache invalidation, and all other actions. The option which has a policy of "Effect": "Allow", "Action": ["execute-api:Invoke"] is incorrect because this will just allow API invocation requests in API Gateway and not cache invalidation requests. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#override-api-gateway-stage-cache-for-method-cache https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-control-access-using-iam-policies-to-invoke-api.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/'},{question:"A developer is currently building a scalable microservices architecture where complex applications are decomposed into smaller, independent services. Docker will be used as its application container to provide an optimal way of running small, decoupled services. The developer should also have fine-grained control over the custom application architecture. Which of the following services is the MOST suitable one to use?",answers:[{text:"EC2",isCorrect:!1},{text:"ECS",isCorrect:!0},{text:"Elastic Beanstalk",isCorrect:!1},{text:"AWS SAM",isCorrect:!1}],explanation:"Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type. You can also use Elastic Beanstalk to host Docker applications in AWS. It is an application management platform that helps customers easily deploy and scale web applications and services. It keeps the provisioning of building blocks (e.g., EC2, RDS, Elastic Load Balancing, Auto Scaling, CloudWatch), deployment of applications, and health monitoring abstracted from the user so they can just focus on writing code. You simply specify which container images are to be deployed, the CPU and memory requirements, the port mappings, and the container links. Elastic Beanstalk will automatically handle all the details such as provisioning an Amazon ECS cluster, balancing load, auto-scaling, monitoring, and placing your containers across your cluster. Elastic Beanstalk is ideal if you want to leverage the benefits of containers but just want the simplicity of deploying applications from development to production by uploading a container image. You can work with Amazon ECS directly if you want more fine-grained control for custom application architectures. Hence, the correct answer in this scenario is ECS. Elastic Beanstalk is incorrect. Although it can be used to host Docker applications, it is ideal to be used if you want the simplicity of deploying applications from development to production by uploading a container image. It does not provide fine-grained control for custom application architectures unlike ECS. AWS SAM is incorrect because the AWS Serverless Application Model (AWS SAM) is just an open-source framework that you can use to build serverless applications on AWS and not to host Docker applications. EC2 is incorrect. Although you can run Docker in your EC2 instances, it does not provide a highly scalable, fast, container management service in comparison to ECS. Take note that in itself, EC2 is not scalable and should be paired with Auto Scaling and ELB. References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html https://aws.amazon.com/ecs/faqs/ Check out this Amazon ECS Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/"}]},{id:"aws-developer-11",title:"AWS Certified Developer Associate Practice Exams 5",description:"Additional practice questions covering AWS development topics.",questions:[{question:"A developer is writing a CloudFormation template which will be used to deploy a simple Lambda function to AWS. The function to be deployed is made in Python with just 3 lines of codes which can be written inline in the template. Which parameter of the AWS::Lambda::Function resource should the developer use to place the Python code in the template?",answers:[{text:"Code",isCorrect:!1},{text:"CodeUri",isCorrect:!1},{text:"ZipFile",isCorrect:!0},{text:"Handler",isCorrect:!1}],explanation:'To create a Lambda function you first create a Lambda function deployment package, a .zip or .jar file consisting of your code and any dependencies. When creating the zip, include only the code and its dependencies, not the containing folder. You will then need to set the appropriate security permissions for the zip package. If you are using a CloudFormation template, you can configure the AWS::Lambda::Function resource which creates a Lambda function. To create a function, you need a deployment package and an execution role. The deployment package contains your function code. The execution role grants the function permission to use AWS services, such as Amazon CloudWatch Logs for log streaming and AWS X-Ray for request tracing. Under the AWS::Lambda::Function resource, you can use the Code property which contains the deployment package for a Lambda function. For all runtimes, you can specify the location of an object in Amazon S3. For Node.js and Python functions, you can specify the function code inline in the template. Changes to a deployment package in Amazon S3 are not detected automatically during stack updates. To update the function code, change the object key or version in the template. Hence, the ZipFile parameter to is the correct one to be used in this scenario, which will allow the developer to place the python code inline in the template. If you include your function source inline with this parameter, AWS CloudFormation places it in a file named index and zips it to create a deployment package. This is the reason why it is called the "ZipFile" parameter, and not because it accepts zip files. The Handler parameter is incorrect because this is not a valid property of AWS::Lambda::Function resource but of the AWS::Serverless::Function resource in AWS SAM. In addition, this parameter is primarily used to specify the name of the handler, which is just a function in your code that AWS Lambda can invoke when the service executes your code. The Code parameter is incorrect because you should use the ZipFile parameter instead. Take note that the Code property is the parent property of the ZipFile parameter. The CodeUri parameter is incorrect because this is not a valid property of AWS::Lambda::Function resource but of the AWS::Serverless::Function resource in AWS SAM. This parameter accepts the S3 URL of your code and not the actual code itself. References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-function.html Check out this AWS CloudFormation Cheat Sheet: https://tutorialsdojo.com/aws-cloudformation/'},{question:"A developer has a Node.js function running in AWS Lambda. Currently, the code initializes a database connection to an Amazon RDS database every time the Lambda function is executed, and closes the connection before the function ends. What feature in AWS Lambda will allow the developer to reuse the already existing database connection instead of initializing it each time the function is run?",answers:[{text:"Environment variables",isCorrect:!1},{text:"AWS Lambda is not capable of maintaining existing database connections due to its transient data store.",isCorrect:!1},{text:"Event source mapping",isCorrect:!1},{text:"Execution context",isCorrect:!0}],explanation:'When AWS Lambda executes your Lambda function, it provisions and manages the resources needed to run your Lambda function. When you create a Lambda function, you specify configuration information, such as the amount of memory and maximum execution time that you want to allow for your Lambda function. When a Lambda function is invoked, AWS Lambda launches an execution context based on the configuration settings you provide. The execution context is a temporary runtime environment that initializes any external dependencies of your Lambda function code, such as database connections or HTTP endpoints. This affords subsequent invocations better performance because there is no need to "cold-start" or initialize those external dependencies. After a Lambda function is executed, AWS Lambda maintains the execution context for some time in anticipation of another Lambda function invocation. In effect, the service freezes the execution context after a Lambda function completes, and thaws the context for reuse, if AWS Lambda chooses to reuse the context when the Lambda function is invoked again. Hence, the correct answer is: Execution context. Environment variables is incorrect because these are just variables that enable you to dynamically pass settings to your function code and libraries, without making changes to your code. Environment variables are key-value pairs that you create and modify as part of your function configuration. Event source mapping is incorrect because an event source is just an entity that publishes events and is integrated with your Lambda function. Supported event sources are the AWS services that can be preconfigured to work with AWS Lambda. The configuration is referred to as event source mapping, which maps an event source to a Lambda function. It enables automatic invocation of your Lambda function when events occur. The option that says: AWS Lambda is not capable of maintaining existing database connections due to its transient data store is incorrect because Lambda actually is capable of doing this using the execution context just as discussed above. References: https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html#function-code Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/'},{question:"An e-commerce application, which is hosted in an ECS Cluster, contains the connection string of an external database and other sensitive configuration files. Since the application accepts credit card payments, the company has to meet strict security compliance which requires that the database credentials are encrypted and periodically rotated. Which of the following should you do to comply to the requirements?",answers:[{text:"Store the database credentials in an encrypted ecs.config configuration file.",isCorrect:!1},{text:"Store the database credentials in AWS Secrets Manager and enable rotation.",isCorrect:!0},{text:"Store the database credentials as a secure string parameter in Systems Manager Parameter Store.",isCorrect:!1},{text:"Store the database credentials in an encrypted dockerrun.aws.json configuration file.",isCorrect:!1}],explanation:"Amazon ECS enables you to inject sensitive data into your containers by storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters and then referencing them in your container definition. This feature is supported by tasks using both the EC2 and Fargate launch types. Secrets can be exposed to a container in the following ways: - To inject sensitive data into your containers as environment variables, use the secrets container definition parameter. - To reference sensitive information in the log configuration of a container, use the secretOptions container definition parameter. You can configure AWS Secrets Manager to automatically rotate the secret for a secured service or database. Secrets Manager already natively knows how to rotate secrets for supported Amazon RDS databases. However, Secrets Manager also can enable you to rotate secrets for other databases or third-party services. Because each service or database can have a unique way of configuring its secrets, Secrets Manager uses a Lambda function that you can customize to work with whatever database or service that you choose. You customize the Lambda function to implement the service-specific details of how to rotate a secret. AWS Secrets Manager is the correct answer for this scenario because it can provide both the required encryption as well as the ability to periodically rotate the secrets. Storing the database credentials as a secure string parameter in Systems Manager Parameter Store is incorrect. Although this service can encrypt your sensitive database credentials, it doesn't have the capability to periodically rotate your secrets, unlike AWS Secrets Manager. Storing the database credentials in an encrypted ecs.config configuration file is incorrect because this file is primarily used to store the environment variables of the Amazon ECS container agent. Storing the database credentials in an encrypted dockerrun.aws.json configuration file is incorrect because this file is just an Elastic Beanstalk–specific JSON file that describes how to deploy a set of Docker containers as an Elastic Beanstalk application. This is primarily used in a multicontainer Docker environment and it is not suitable for storing sensitive database credentials, which requires periodic rotation. References: https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-securestring.html https://aws.amazon.com/systems-manager/faq/ Check out these AWS Systems Manager and Secrets Manager Cheat Sheets: https://tutorialsdojo.com/aws-systems-manager/ https://tutorialsdojo.com/aws-secrets-manager/"},{question:"A developer is utilizing AWS X-Ray to generate a visual representation of the requests flowing through their enterprise web application. Since the application interacts with multiple services, all requests must be traced in X-Ray, including any downstream calls made to AWS resources. Which of the following actions should the developer implement for this scenario?",answers:[{text:"Install AWS X-Ray on the different services that communicate with the application including the AWS resources that the application calls.",isCorrect:!1},{text:"Use AWS X-Ray SDK to upload a trace segment by executing PutTraceSegments API.",isCorrect:!1},{text:"Use X-Ray SDK to generate segment documents with subsegments and send them to the X-Ray daemon, which will buffer them and upload to the X-Ray API in batches.",isCorrect:!0},{text:"Pass multiple trace segments as a parameter of PutTraceSegments API.",isCorrect:!1}],explanation:"You can send trace data to X-Ray in the form of segment documents. A segment document is a JSON formatted string that contains information about the work that your application does in service of a request. Your application can record data about the work that it does itself in segments or work that uses downstream services and resources in subsegments. A segment document can be up to 64 kB and contain a whole segment with subsegments, a fragment of a segment that indicates that a request is in progress, or a single subsegment that is sent separately. You can send segment documents directly to X-Ray by using the PutTraceSegments API. An alternative is, instead of sending segment documents to the X-Ray API, you can send segments and subsegments to an X-Ray daemon, which will buffer them and upload to the X-Ray API in batches. The X-Ray SDK sends segment documents to the daemon to avoid making calls to AWS directly. This is the correct option among the choices. Hence, using X-Ray SDK to generate segment documents with subsegments and sending them to the X-Ray daemon, which will buffer them and upload to the X-Ray API in batches is the correct answer in this scenario. Using AWS X-Ray SDK to upload a trace segment by executing PutTraceSegments API is incorrect because you should upload the segment documents with subsegments instead. A trace segment is just a JSON representation of a request that your application serves. Installing AWS X-Ray on the different services that communicate with the application including the AWS resources that the application calls is incorrect because you cannot run a trace on the application and the services at the same time as this will produce two different results. You simply have to send the segment documents with subsegments to get the information about downstream calls that your application makes to AWS resources. Passing multiple trace segments as a parameter of PutTraceSegments API is incorrect because, contrary to the API's name, you have to upload segment documents and not trace segments. The API has a single parameter: TraceSegmentDocuments, that takes a list of JSON segment documents. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html https://docs.aws.amazon.com/xray/latest/devguide/xray-api-sendingdata.html https://docs.aws.amazon.com/xray/latest/api/API_PutTraceSegments.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/"},{question:"A developer is launching a Lambda function that requires access to a MySQL RDS instance that is in a private subnet. Which of the following is the MOST secure way to achieve this?",answers:[{text:"Expose an endpoint of your RDS to the Internet using an Elastic IP.",isCorrect:!1},{text:"Move your RDS instance to a public subnet.",isCorrect:!1},{text:"Configure the Lambda function to connect to your VPC.",isCorrect:!0},{text:"Ensure that the Lambda function has a proper IAM permission to access RDS.",isCorrect:!1}],explanation:"You can configure a Lambda function to connect to a virtual private cloud (VPC) in your account. Use Amazon Virtual Private Cloud (Amazon VPC) to create a private network for resources such as databases, cache instances, or internal services. Connect your function to the VPC to access private resources during execution. AWS Lambda runs your function code securely within a VPC by default. However, to enable your Lambda function to access resources inside your private VPC, you must provide additional VPC-specific configuration information that includes VPC subnet IDs and security group IDs. AWS Lambda uses this information to set up elastic network interfaces (ENIs) that enable your function to connect securely to other resources within your private VPC. The following diagram guides you through a decision tree as to whether you should use a VPC (Virtual Private Cloud): Don't put your Lambda function in a VPC unless you have to. There is no benefit outside of using this to access resources you cannot expose publicly, like a private Amazon Relational Database instance. Services like Amazon OpenSearch Service can be secured over IAM with access policies, so exposing the endpoint publicly is safe and wouldn't require you to run your function in the VPC to secure it. Hence, configuring the Lambda function to connect to your VPC is the correct answer for this scenario. Ensuring that the Lambda function has proper IAM permission to access RDS is incorrect. Even though you grant the necessary IAM permissions to the Lambda function to access RDS, the function would still not be able to connect to RDS since there is no established connection between Lambda and the private subnet of your VPC. Exposing an endpoint of your RDS to the Internet using an Elastic IP is incorrect because this is not the most secure way of granting access to your Lambda function. It will be able to connect to RDS but so will the billions of people on the public Internet. Moving your RDS instance to a public subnet is incorrect because this is an unnecessary change and not a best practice from a security perspective. You only need to configure your Lambda function to your VPC so it can connect to the RDS in the private subnet. If you move your RDS instance to a public subnet, it will introduce a critical security flaw to your entire architecture since your database will become accessible publicly. References: https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html#lambda-vpc https://docs.aws.amazon.com/lambda/latest/dg/welcome.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A company has recently developed a containerized application that uses a multicontainer Docker platform which supports multiple containers per instance. They need a service that automatically handles tasks such as provisioning of the resources, load balancing, auto-scaling, monitoring, and placing the containers across the cluster. Which of the following services provides the EASIEST way to accomplish the above requirement?",answers:[{text:"Lambda",isCorrect:!1},{text:"ECS",isCorrect:!1},{text:"Elastic Beanstalk",isCorrect:!0},{text:"EKS",isCorrect:!1}],explanation:"You can create docker environments that support multiple containers per Amazon EC2 instance with multicontainer Docker platform for Elastic Beanstalk. Elastic Beanstalk uses Amazon Elastic Container Service (Amazon ECS) to coordinate container deployments to multicontainer Docker environments. Amazon ECS provides tools to manage a cluster of instances running Docker containers. Elastic Beanstalk takes care of Amazon ECS tasks including cluster creation, task definition and execution. AWS Elastic Beanstalk is an application management platform that helps customers easily deploy and scale web applications and services. It keeps the provisioning of building blocks (e.g., EC2, RDS, Elastic Load Balancing, Auto Scaling, CloudWatch), deployment of applications, and health monitoring abstracted from the user so they can just focus on writing code. You simply specify which container images are to be deployed, the CPU and memory requirements, the port mappings, and the container links. Elastic Beanstalk will automatically handle all the details such as provisioning an Amazon ECS cluster, balancing load, auto-scaling, monitoring, and placing your containers across your cluster. Elastic Beanstalk is ideal if you want to leverage the benefits of containers but just want the simplicity of deploying applications from development to production by uploading a container image. You can work with Amazon ECS directly if you want more fine-grained control for custom application architectures. Hence, the correct answer in this scenario is Elastic Beanstalk. ECS is incorrect. Although it can host Docker applications, it doesn't automatically handle all the details such as resource provisioning, balancing load, auto-scaling, monitoring, and placing your containers across your cluster, unlike Elastic Beanstalk. Take note that even though you can use Service Auto Scaling in ECS, you still have to enable and configure it. Elastic Beanstalk still provides the easiest way to accomplish the requirements. Lambda is incorrect because this is primarily used for serverless applications and not for Docker or any other containerized applications. EKS is incorrect because Amazon EKS just provides you an easy way to run Kubernetes on AWS without needing to install and operate your own Kubernetes clusters. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html https://aws.amazon.com/ecs/faqs/ Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/"},{question:"A company is legally obligated to keep transaction records containing Personally Identifiable Information (PII) for a duration of five years. These records are stored in Amazon S3. To handle data redaction, the company has developed Lambda functions with naming conventions starting as RedactPII-[role], where <role> represents different roles. The company wants to provide varying levels of redaction based on each role, ensuring each user only sees the necessary data. Only a single copy of the records should be maintained. Which combination of actions will achieve the given requirements? (Select THREE.)",answers:[{text:"Create an S3 Access Point for each user role.",isCorrect:!0},{text:"Use the GetObjectLegalHold API to retrieve the redacted data.",isCorrect:!1},{text:"Set up an S3 event notification to invoke the corresponding RedactPII-[role] function in response to GET requests.",isCorrect:!1},{text:"Use the GetObject API to retrieve the redacted data",isCorrect:!0},{text:"Configure an S3 Object Lambda Access Point for each S3 Access Point name. Associate the RedactPII-[role] Lambda functions with the corresponding S3 Object Lambda Access Point.",isCorrect:!0},{text:"Set up S3 Replication for the bucket.",isCorrect:!1}],explanation:"S3 Object Lambda allows you to add your own code to S3 GET requests to modify and process data as it's being returned to an application. This feature is designed for use cases where data needs to be transformed on the fly without the need to store a transformed copy of the data. It's useful in scenarios like filtering rows, redacting confidential data, dynamically resizing images, and other similar situations where data transformation or processing is required during data retrieval. In the scenario, we have to create dedicated S3 Object Lambda Access Points, with each user having access to their own unique Access Point. Each of these Access Points is associated with the corresponding RedactPII-[role] Lambda function. On the client side, users issue standard GetObject requests to their specific S3 Object Lambda Access Point, allowing them to retrieve redacted data tailored to their role. This setup ensures that each user can only access the data associated with their role while maintaining the requirement of a single copy of the records in the S3 bucket. Hence, the correct answers are: - Create an S3 Access Point for each user role. - Configure an S3 Object Lambda Access Point for each S3 Access Point name. Associate the RedactPII-[role] Lambda functions with the corresponding S3 Object Lambda Access Point. - Use the GetObject API to retrieve the redacted data The option that says: Set up an S3 event notification to invoke the corresponding RedactPII-[role] function in response to GET requests is incorrect because S3 event notifications are primarily used for notifying of new object creations and not for invoking a function in response to GET requests. The option that says: Set up S3 Replication for the bucket is incorrect. S3 Replication is used to automatically replicate objects across different buckets or across AWS regions. This would create additional copies of the data, contradicting the requirement to maintain only one copy of the records. The option that says: Use the GetObjectLegalHold API to retrieve the redacted data is incorrect. This API simply retrieves the current legal hold status for a specific S3 object. It doesn't play a role in the process of getting the redacted data. References: https://aws.amazon.com/s3/features/object-lambda/ https://aws.amazon.com/blogs/aws/introducing-amazon-s3-object-lambda-use-your-code-to-process-data-as-it-is-being-retrieved-from-s3/ https://aws.amazon.com/blogs/machine-learning/protect-pii-using-amazon-s3-object-lambda-to-process-and-modify-data-during-retrieval/ Check out this Amazon S3 Cheat Sheet: https://tutorialsdojo.com/amazon-s3/"},{question:"A development team has recently completed building their serverless application. They must zip their code artifacts, upload them to Amazon S3, produce the package template file for deployment, and deploy it to AWS. Which command is the MOST suitable to use to automate the deployment steps?",answers:[{text:"sam deploy",isCorrect:!0},{text:"sam publish",isCorrect:!1},{text:"sam package",isCorrect:!1},{text:"aws cloudformation deploy",isCorrect:!1}],explanation:"AWS SAM uses AWS CloudFormation as the underlying deployment mechanism. You can deploy your application by using AWS SAM command line interface (CLI) commands. You can also use other AWS services that integrate with AWS SAM to automate your deployments. After you develop and test your serverless application locally, you can package and deploy your application by using the sam deploy command. The sam deploy command zips your code artifacts, uploads them to Amazon S3, and produces a packaged AWS SAM template file that it uses to deploy your application. To deploy an application that contains one or more nested applications, you must include the CAPABILITY_AUTO_EXPAND capability in the sam deploy command. Hence, the correct answer is: sam deploy aws cloudformation deploy is incorrect. While this command can be used to deploy a CloudFormation stack, it expects that your artifacts are already packaged and uploaded to S3. It doesn't handle the packaging process implicitly. sam package is incorrect. This command simply prepares the serverless application for deployment by zipping artifacts, uploading them to S3, and generating a CloudFormation template with references to the uploaded artifacts in S3. It doesn't deploy the application. sam publish is incorrect because this command publishes an AWS SAM application to the AWS Serverless Application Repository and does not generate the template file. It takes a packaged AWS SAM template and publishes the application to the specified region. References: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html https://docs.aws.amazon.com/serverlessrepo/latest/devguide/what-is-serverlessrepo.html Check out this AWS SAM Cheat Sheet: https://tutorialsdojo.com/aws-serverless-application-model-sam/"},{question:"A developer is planning to add a global secondary index in a DynamoDB table. This will allow the application to query a specific index that can span all of the data in the base table, across all partitions. Which of the following should the developer consider when using this type of index? (Select TWO.)",answers:[{text:"When you query this index, you can choose either eventual consistency or strong consistency.",isCorrect:!1},{text:"Queries or scans on this index consume capacity units from the index, not from the base table.",isCorrect:!0},{text:"For each partition key value, the total size of all indexed items must be 10 GB or less.",isCorrect:!1},{text:"Queries or scans on this index consume read capacity units from the base table.",isCorrect:!1},{text:"Queries on this index support eventual consistency only.",isCorrect:!0}],explanation:'A global secondary index is an index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered "global" because queries on the index can span all of the data in the base table, across all partitions. To create a table with one or more global secondary indexes, use the CreateTable operation with the GlobalSecondaryIndexes parameter. For maximum query flexibility, you can create up to 20 global secondary indexes (default limit) per table. You must specify one attribute to act as the index partition key; you can optionally specify another attribute for the index sort key. It is not necessary for either of these key attributes to be the same as a key attribute in the table. Global secondary indexes inherit the read/write capacity mode from the base table. As shown in the above table, the following are the things that the developer should consider when using a global secondary index: - Queries or scans on this index consume capacity units from the index, not from the base table. - Queries on this index support eventual consistency only. The following options are incorrect because these are about local secondary indexes: - When you query this index, you can choose either eventual consistency or strong consistency. - Queries or scans on this index consume read capacity units from the base table - For each partition key value, the total size of all indexed items must be 10 GB or less. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ DynamoDB Scan vs Query: https://tutorialsdojo.com/dynamodb-scan-vs-query/ Comparison of AWS Services Cheat Sheets: https://tutorialsdojo.com/comparison-of-aws-services/'},{question:"The infrastructure of an application is designed such that a producer sends data to a consumer via HTTPS. The consumer may sometimes take a while to process the messages, which can result in unexpected timeouts and cause newer messages not to be acknowledged immediately. To resolve this issue, a developer decided to introduce an Amazon SQS standard queue into the system. However, duplicate messages are still not being handled properly. What should the developer do to ensure that messages are durably delivered and to prevent duplicate messages? (Select TWO.)",answers:[{text:"Use a delay queue.",isCorrect:!1},{text:"Create a FIFO queue as a replacement for the standard queue.",isCorrect:!0},{text:"Increase the timeout for the acknowledgement response.",isCorrect:!1},{text:"Configure the producer to set deduplication IDs for the messages.",isCorrect:!0},{text:"Increase the number of consumers polling from your standard queue.",isCorrect:!1}],explanation:"Amazon SQS offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components. Standard queues support a nearly unlimited number of transactions per second (TPS) per action. Standard queues support at-least-once message delivery. However, occasionally (because of the highly distributed architecture that allows nearly unlimited throughput), more than one copy of a message might be delivered out of order. FIFO queues have all the capabilities of the standard queue. The most important features of this queue type are FIFO (First-In-First-Out) delivery and exactly-once processing. Unlike standard queues, FIFO queues don't introduce duplicate messages. FIFO queues help you avoid sending duplicates to a queue. To configure deduplication, you must do one of the following: - Enable content-based deduplication. - Explicitly provide the message deduplication ID (or view the sequence number) for the message. The message deduplication ID is the token used for deduplication of sent messages. If a message with a particular message deduplication ID is sent successfully, any messages sent with the same message deduplication ID are accepted successfully but aren't delivered during the 5-minute (default) deduplication interval. Hence, the correct answers in this scenario are: - Configure the producer to set deduplication IDs for the messages - Create a FIFO queue as a replacement for the standard queue Increasing the timeout period for acknowledgment responses is incorrect. Although it can help reduce the chances of duplicate messages being sent, this option will still introduce duplicates since the system is using a Standard SQS queue. In addition, a longer timeout period may result in congestion in the system which can delay the delivery of messages that follows after an acknowledgment response. Increasing the number of consumers polling from your standard queue is incorrect. Although it can help consume messages more quickly, the use of a standard queue will still introduce duplicates since this type of queue does not support exactly-once processing, unlike FIFO queues. Using a delay queue is incorrect because this will just let you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period References: https://docs.amazonaws.cn/en_us/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagededuplicationid-property.html Check out this Amazon SQS Cheat Sheet: https://tutorialsdojo.com/amazon-sqs/"},{question:"A developer is planning to build a serverless Rust application in AWS using AWS Lambda and Amazon DynamoDB. Much to his disappointment, AWS Lambda does not natively support the Rust programming language. Can the developer still proceed with creating serverless Rust applications in AWS given the situation above?",answers:[{text:"No. The developer will have to wait for a new support release in AWS Lambda.",isCorrect:!1},{text:"Yes. The developer can submit a request ticket to AWS so that they can provide him a Lambda runtime environment that supports Rust.",isCorrect:!1},{text:"Yes. The developer will just have to use AWS Fargate instead of AWS Lambda.",isCorrect:!1},{text:"Yes. The developer can create a custom runtime for hist Rust applications and bootstrap it to an AWS Lambda function.",isCorrect:!0}],explanation:'AWS Lambda natively supports Java, Go, PowerShell, Node.js, C#, Python, and Ruby code. It also provides a Runtime API which allows you to use any additional programming languages to author your functions. A runtime is a program that runs a Lambda function\'s handler method when the function is invoked. You can include a runtime in your function\'s deployment package in the form of an executable file named bootstrap. Your custom runtime runs in the standard Lambda execution environment. It can be a shell script, a script in a language that\'s included in Amazon Linux, or a binary executable file that\'s compiled in Amazon Linux. Therefore, if the developer publishes a custom runtime for Rust, he can continue building his serverless application in AWS Lambda. Hence, the correct answer in this scenario is: Yes. The developer can create a custom runtime for hist Rust applications and bootstrap it to an AWS Lambda function. The option that says: "Yes. The developer can submit a request ticket to AWS so that they can provide him a Lambda runtime environment that supports Rust" is incorrect because you cannot request specific runtime environments for AWS Lambda from AWS. You would need to create this yourself using the Runtime API. The option that says: "Yes. The developer will just have to use AWS Fargate instead of AWS Lambda" is incorrect since this service is a serverless compute engine for containers. Unless the Rust application is running in Docker, which is not explicitly stated in the scenario, it\'ll be best to use AWS Lambda for serverless computing. The option that says: "No. The developer will have to wait for a new support release in AWS Lambda" is incorrect because there is no need to wait for a new feature release or for code translation since AWS Lambda allows you to create a runtime that appropriately handles your function code when invoked. References: https://docs.aws.amazon.com/lambda/latest/dg/runtimes-custom.html https://aws.amazon.com/blogs/aws/new-for-aws-lambda-use-any-programming-language-and-share-common-components/ Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/'},{question:"Your application is hosted on an Auto Scaling group of EC2 instances with a DynamoDB database. There were a lot of data discrepancy issues where the changes made by one user were always overwritten by another user. You noticed that this usually happens whenever there are a lot of people updating the same data. What should you do to solve this problem?",answers:[{text:"Use DynamoDB global tables and implement a pessimistic locking strategy.",isCorrect:!1},{text:"Implement an optimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table.",isCorrect:!0},{text:"Implement a pessimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table.",isCorrect:!1},{text:"Use DynamoDB global tables and implement an optimistic locking strategy.",isCorrect:!1}],explanation:'Optimistic locking is a strategy to ensure that the client-side item that you are updating (or deleting) is the same as the item in DynamoDB. If you use this strategy, then your database writes are protected from being overwritten by the writes of others — and vice-versa. Take note that: - DynamoDB global tables use a “last writer wins” reconciliation between concurrent updates. If you use Global Tables, last writer policy wins. So in this case, the locking strategy does not work as expected. - DynamoDBMapper transactional operations do not support optimistic locking. With optimistic locking, each item has an attribute that acts as a version number. If you retrieve an item from a table, the application records the version number of that item. You can update the item, but only if the version number on the server side has not changed. If there is a version mismatch, it means that someone else has modified the item before you did; the update attempt fails, because you have a stale version of the item. If this happens, you simply try again by retrieving the item and then attempting to update it. Optimistic locking prevents you from accidentally overwriting changes that were made by others; it also prevents others from accidentally overwriting your changes. Hence, implementing an optimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table is the correct answer in this scenario. Using DynamoDB global tables and implementing a pessimistic locking strategy is incorrect because you have to use optimistic locking here just as what was explained above. Implementing a pessimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table is incorrect because an optimistic locking strategy is a more suitable solution for this scenario. Although the provided steps here are correct, the name of the strategy is wrong. Using DynamoDB global tables and implementing an optimistic locking strategy is incorrect. Although it is correct to use the optimistic locking strategy, the use of DynamoDB global tables is wrong. This uses a "last writer wins" reconciliation between concurrent updates. If you use Global Tables, the last writer policy is in effect so in this case, the locking strategy will not work as expected. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.OptimisticLocking.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ Amazon DynamoDB Overview: https://www.youtube.com/watch?v=3ZOyUNIeorU'},{question:"You are working as a software developer for an online training company, which is currently developing a learning portal that will use a DynamoDB table. One of the acceptance criteria requires you to ensure that there will be no hot partitions in the table which will result in throttling and inefficient use of your provisioned I/O capacity. The portal contains hundreds of thousands of online courses including the ones from their 3rd-party educational partners, which may or may not have the same Course ID. The table is structured as shown below: Which of the following is the MOST suitable partition key to use in this scenario?",answers:[{text:"Item ID",isCorrect:!0},{text:"Course ID",isCorrect:!1},{text:"Course Price",isCorrect:!1},{text:"Course Name",isCorrect:!1}],explanation:"The primary key that uniquely identifies each item in a DynamoDB table can be simple (a partition key only) or composite (a partition key combined with a sort key). Generally speaking, you should design your application for uniform activity across all logical partition keys in the Table and its secondary indexes. You can determine the access patterns that your application requires, and estimate the total read capacity units and write capacity units that each table and secondary Index requires. The partition key portion of a table's primary key determines the logical partitions in which a table's data is stored. This, in turn, affects the underlying physical partitions. Provisioned I/O capacity for the table is divided evenly among these physical partitions. Therefore a partition key design that doesn't distribute I/O requests evenly can create \"hot\" partitions that result in throttling and use your provisioned I/O capacity inefficiently. The optimal usage of a table's provisioned throughput depends not only on the workload patterns of individual items but also on the partition-key design. This doesn't mean that you must access all partition key values to achieve an efficient throughput level, or even that the percentage of accessed partition key values must be high. It does mean that the more distinct partition key values that your workload accesses, the more those requests will be spread across the partitioned space. In general, you will use your provisioned throughput more efficiently as the ratio of partition key values accessed to the total number of partition key values increases. Hence, the correct answer in this scenario is Item ID since it uses automatically generated GUID of the online course items that provide more uniformity than the other attributes. Course ID is incorrect because although this can be used as a partition key, it is not as unique since the scenario says that the there are online courses from the company's 3rd-party educational partners which may or may not have the same Course ID. Hence, the most suitable attribute to be used as a partition key is Item ID. Course Name is incorrect because just as mentioned above, it is possible that there will be two or more online courses with the same name which means that this is not an appropriate attribute to be used. Course Price is incorrect because this is a non-unique value where two or more items can have the exact same price. Hence, it is not suitable to be used as a partition key. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-uniform-load.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"},{question:"A company is using a combination of CodeBuild, CodePipeline, and CodeDeploy services for its continuous integration and continuous delivery (CI/CD) pipeline on AWS. They want someone to perform a code review before a revision is allowed into the next stage of a pipeline. If the action is approved, the pipeline execution resumes, but if it is not, then the pipeline execution will not proceed. Which of the following is the MOST suitable solution to implement in this scenario?",answers:[{text:"Implement a manual approval actions configuration in CodePipeline. Send the approval request to an SNS Topic.",isCorrect:!0},{text:"Remodel the pipeline using AWS Serverless Application Model (AWS SAM)",isCorrect:!1},{text:"Implement a manual approval actions configuration in CodePipeline. Send the approval request to an SQS Queue.",isCorrect:!1},{text:"Split the processes into different Task states using Step Functions. Use a Wait state to set a timeout for approval.",isCorrect:!1}],explanation:"In AWS CodePipeline, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action. If the action is approved, the pipeline execution resumes. If the action is rejected - or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping - the result is the same as an action failing, and the pipeline execution does not continue. You might use manual approvals for these reasons: - You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline. - You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build artifact, before it is released. - You want someone to review new or updated text before it is published to a company website. You can configure an approval action to publish a message to an Amazon Simple Notification Service topic when the pipeline stops at the action. Amazon SNS delivers the message to every endpoint subscribed to the topic. You must use a topic created in the same AWS region as the pipeline that will include the approval action. When you create a topic, it is recommended that you give it a name that will identify its purpose, in formats such as tutorialsdojoManualApprovalPHL-us-east-2-approval. Hence, the correct answer is to Implement a manual approval actions configuration in CodePipeline. Send the approval request to an SNS Topic. The option that says: Remodel the pipeline using AWS Serverless Application Model (AWS SAM) is incorrect because this service is just a framework for building serverless applications, not a replacement for a CI/CD pipeline. The option that says: Implement a manual approval actions configuration in CodePipeline. Send the approval request to an SQS Queue is incorrect. Although setting up a manual approval is valid, the use of SQS is wrong because it doesn't have an integration with manual approval actions. Use SNS instead to send the approval action emails to the recipient who will either approve or deny the action. The option that says: Split the processes into different Task states using Step Functions. Use a Wait state to set a timeout for approval is incorrect as this would only add unnecessary complexity to the CI/CD pipeline. The requirement in the scenario can be achieved using the built-in manual approval actions in CodePipeline. References: https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html https://docs.aws.amazon.com/codepipeline/latest/userguide/actions.html Check out this AWS CodePipeline Cheat Sheet: https://tutorialsdojo.com/aws-codepipeline/ Comparison of AWS Services Cheat Sheets: https://tutorialsdojo.com/comparison-of-aws-services/"},{question:"A software development company uses AWS CodePipeline as its CI/CD platform to build, test, and push deployments to its production environment. Recently, a developer created a Lambda function that will push the build details to a separate DynamoDB table. The Lambda function should be triggered after a successful build on the Pipeline. Which of the following services will meet the specified requirement?",answers:[{text:"AWS CodeBuild",isCorrect:!1},{text:"AWS Systems Manager",isCorrect:!1},{text:"Amazon EventBridge (Amazon CloudWatch Events)",isCorrect:!0},{text:"AWS CloudTrail Events",isCorrect:!1}],explanation:"Monitoring is an important part of maintaining the reliability, availability, and performance of AWS CodePipeline. You should collect monitoring data from all parts of your AWS solution so that you can more easily debug a multi-point failure if one occurs. You can monitor CodePipeline events in EventBridge, which delivers a stream of real-time data from your own applications, software-as-a-service (SaaS) applications, and AWS services. EventBridge routes that data to targets such as Lambda functions and Simple Notification Service (SNS) topics. Depending on the type of state change, you might want to send notifications, capture state information, take corrective action, initiate events, or take other actions. Hence, the correct answer is Amazon EventBridge (Amazon CloudWatch Events). AWS CloudTrail Events is incorrect because this is just an event in CloudTrail that records activity in your AWS account, which can be an action taken by a user, role, or service that is monitorable by CloudTrail. A more suitable solution is to use Amazon EventBridge (Amazon CloudWatch Events) instead. AWS Systems Manager is incorrect because it is primarily used for managing and configuring AWS resources, including EC2 instances, databases, and more. It's not designed for event-driven automation based on state changes in AWS resources. AWS CodeBuild is incorrect because it cannot trigger a Lambda function directly. AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. You should use Amazon EventBridge (Amazon CloudWatch Events) for this scenario instead. References: https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html Check out this AWS CodePipeline Cheat Sheet: https://tutorialsdojo.com/aws-codepipeline/"},{question:'You are a newly hired developer in a leading investment bank which uses AWS as its cloud infrastructure. One of your tasks is to develop an application that will store financial data to an already existing S3 bucket, which has the following bucket policy: { "Version": "2012-10-17", "Id": "PutObjPolicy", "Statement": [ { "Sid": "AllowUploadCheck", "Effect": "Deny", "Principal": "*", "Action": "s3:PutObject", "Resource": "arn:aws:s3:::tutorialsdojo/*", "Condition": { "StringNotEquals": { "s3:x-amz-server-side-encryption": "AES256" } }}, { "Sid": "AllowNullCheck", "Effect": "Deny", "Principal": "*", "Action": "s3:PutObject", "Resource": "arn:aws:s3:::tutorialsdojo/*", "Condition": { "Null": { "s3:x-amz-server-side-encryption": "true" } } } ]} Which of the following statements is true about uploading data to this S3 bucket?',answers:[{text:"The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of Null.",isCorrect:!1},{text:"The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of true.",isCorrect:!1},{text:"The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of aws:kms.",isCorrect:!1},{text:"The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of AES256.",isCorrect:!0}],explanation:'Server-side encryption protects data at rest. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it rotates regularly. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data. If you need server-side encryption for all of the objects that are stored in a bucket, use a bucket policy. For example, the following bucket policy denies permissions to upload an object to the tutorialsdojo bucket unless the request includes the x-amz-server-side-encryption header to request server-side encryption: { "Version": "2012-10-17", "Id": "PutObjPolicy", "Statement": [ { "Sid": "DenyIncorrectEncryptionHeader", "Effect": "Deny", "Principal": "*", "Action": "s3:PutObject", "Resource": "arn:aws:s3:::tutorialsdojo/*", "Condition": { "StringNotEquals": { "s3:x-amz-server-side-encryption": "AES256" } } }, { "Sid": "DenyUnEncryptedObjectUploads", "Effect": "Deny", "Principal": "*", "Action": "s3:PutObject", "Resource": "arn:aws:s3:::tutorialsdojo/*", "Condition": { "Null": { "s3:x-amz-server-side-encryption": "true" } } } ]} Take note that the Sid (statement ID) is just an optional identifier that you provide for the policy statement. The Effect element is required and specifies whether the statement results in an allow or an explicit deny. The valid values for the Effect element are Allow and Deny. The value of the Sid element does not affect nor override the Effect element. Hence, the correct answer is: The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of AES256. The option that says: The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of Null is incorrect because the value of the header should be AES256 and not Null. Take note that the Null in the policy actually means that the request will be denied if there is no x-amz-server-side-encryption header included. The option that says: The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of true is incorrect because this header is not a boolean type. It can only accept two values: AES256 and aws:kms. The option that says: The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of aws:kms is incorrect. Based on the given bucket policy, the value of the header should be AES256, which means that the bucket is using Amazon S3-Managed Keys (SSE-S3). Conversely, if this header has a value of aws:kms, then it uses AWS KMS Keys (SSE-KMS). References: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html https://docs.aws.amazon.com/AmazonS3/latest/dev/DataDurability.html https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/ Check out this Amazon S3 Cheat Sheet: https://tutorialsdojo.com/amazon-s3/'},{question:"Your request to increase your account's concurrent execution limit to 2000 has been recently approved by AWS. There are 10 Lambda functions running in your account and you already specified a concurrency execution limit on one function at 400 and on another function at 200. Which of the following statements are TRUE in this scenario? (Select TWO.)",answers:[{text:"The remaining 1400 concurrent executions will be shared among the other 8 functions.",isCorrect:!0},{text:"You can still set a concurrency execution limit of 1400 to a third Lambda function.",isCorrect:!1},{text:"You can still set a concurrency execution limit of 1300 to a third Lambda function.",isCorrect:!0},{text:"The combined allocated 600 concurrent execution will be shared among the 2 functions.",isCorrect:!1},{text:"The unreserved concurrency pool is 600.",isCorrect:!1}],explanation:"The unit of scale for AWS Lambda is a concurrent execution. However, scaling indefinitely is not desirable in all scenarios. For example, you may want to control your concurrency for cost reasons or to regulate how long it takes you to process a batch of events, or to simply match it with a downstream resource. To assist with this, Lambda provides a concurrent execution limit control at both the account level and the function level. The concurrent executions refer to the number of executions of your function code that are happening at any given time. You can estimate the concurrent execution count, but the concurrent execution count will differ depending on whether or not your Lambda function is processing events from a poll-based event source. If you create a Lambda function to process events from event sources that aren't poll-based (for example, Lambda can process every event from other sources, like Amazon S3 or API Gateway), each published event is a unit of work, in parallel, up to your account limits. Therefore, the number of invocations these event sources make influences the concurrency. If you set the concurrent execution limit for a function, the value is deducted from the unreserved concurrency pool. For example, if your account's concurrent execution limit is 1000 and you have 10 functions, you can specify a limit on one function at 200 and another function at 100. The remaining 700 will be shared among the other 8 functions. AWS Lambda will keep the unreserved concurrency pool at a minimum of 100 concurrent executions so that functions that do not have specific limits set can still process requests. So, in practice, if your total account limit is 1000, you are limited to allocating 900 to individual functions. In this scenario, you still have 1400 concurrent executions remaining which will be shared by the other 8 Lambda functions in your AWS account. Take note that the unreserved account concurrency can't go below 100, which means that you only set a concurrency execution limit of 1300 to a single function or spread out to the remaining 8 functions. Hence, the correct answers in this scenario are: - The remaining 1400 concurrent executions will be shared among the other 8 functions. - You can still set a concurrency execution limit of 1300 to a third Lambda function. The option that says: the unreserved concurrency pool is 600 is incorrect because this is the value of the total reserved concurrency that you have allocated to the 2 Lambda functions. The option that says: you can still set a concurrency execution limit of 1400 to a third Lambda function is incorrect because the unreserved account concurrency cannot go below 100, which means that you only set a concurrency execution limit of 1300 to the third function or spread out to the remaining 8 functions. The option that says: the combined allocated 600 concurrent execution will be shared among the 2 functions is incorrect because the execution limit is per function only and will not be shared with other functions, which also have reserved concurrent executions. References: https://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html https://docs.aws.amazon.com/lambda/latest/dg/scaling.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/ Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"},{question:"A company uses AWS Systems Manager (SSM) Parameter Store to manage configuration details for multiple applications. The parameters are currently stored in the Standard tier. The company wants its operations team to be notified if there are sensitive parameters that haven’t been rotated within 90 days. Which must be done to meet the requirement?",answers:[{text:"Set up an Amazon EventBridge (Amazon CloudWatch Events) event pattern that captures SSM Parameter-related events. Use Amazon SNS to send notifications.",isCorrect:!1},{text:"Convert the sensitive parameters from Standard tier into Advanced tier. Set a NoChangeNotification policy with a value of 90 days. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification via Amazon SNS.",isCorrect:!0},{text:"Configure a NoChangeNotification policy with a value of 90 days. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification via Amazon SNS.",isCorrect:!1},{text:"Convert the sensitive parameters from Standard tier into Advanced tier. Set a ExpirationNotification policy with a value of 90 days. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification via Amazon SNS.",isCorrect:!1}],explanation:"Parameter policies help you manage a growing set of parameters by allowing you to assign specific criteria to a parameter, such as an expiration date or time to live. Parameter policies are especially helpful in forcing you to update or delete passwords and configuration data stored in Parameter Store, a capability of AWS Systems Manager. Take note that parameter policies are only available for parameters in the Advanced tier. Parameter Store offers the following types of policies: Expiration - deletes the parameter at a specific date ExpirationNotification - sends an event to Amazon EventBridge (Amazon CloudWatch Events) when the specified expiration time is reached. NoChangeNotification - sends an event to Amazon EventBridge (Amazon CloudWatch Events) when a parameter has not been modified for a specified period of time. The NoChangeNotification policy sends a notification based on the LastModifiedTime attribute of the parameter. If you change or edit a parameter, the system resets the notification time period based on the new value of LastModifiedTime. In the scenario's case, we want to be notified if specific parameters were not rotated in the last 90 days. In the scenario, the goal is to be notified if specific sensitive parameters have not been rotated within the past 90 days. Configuring the NoChangeNotification policy with a value of 90 days allows SSM to emit a notification to EventBridge whenever the LastModifiedTime of the sensitive parameters exceeds the specified time frame. However, setting the notification policy alone is not enough. You must configure Amazon EventBridge (Amazon CloudWatch Events) to capture the emitted events and route them to an Amazon SNS topic. Hence, the correct answer is: Convert the sensitive parameters from Standard tier into Advanced tier. Set a NoChangeNotification policy with a value of 90 days. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification via Amazon SNS. The option that says: Configure a NoChangeNotification policy with a value of 90 days. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification via Amazon SNS is incorrect because notification policies are not supported in the Standard tier. You must convert the parameters first into the Advanced tier. The option that says: Convert the sensitive parameters from Standard tier into Advanced tier. Set a ExpirationNotification policy with a value of 90 days. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification via Amazon SNS is incorrect because the ExpirationNotification policy is for notifying when a parameter is about to expire, not when it hasn't been rotated. In this case, the NoChangeNotification policy should be used instead. The option that says: Set up an Amazon EventBridge (Amazon CloudWatch Events) event pattern that captures SSM Parameter-related events. Use Amazon SNS to send notifications is incorrect. A notification policy must be enabled as well, otherwise, Amazon EventBridge (Amazon CloudWatch Events) won't be able to receive any notifications. References: https://docs.aws.amazon.com/systems-manager/latest/userguide/parameter-store-policies.html https://aws.amazon.com/about-aws/whats-new/2019/04/aws_systems_manager_parameter_store_introduces_advanced_parameters/ Check out this cheat sheet on AWS Secrets Manager vs Systems Manager Parameter Store: https://tutorialsdojo.com/aws-secrets-manager-vs-systems-manager-parameter-store/"},{question:"A web application is currently using an on-premises Microsoft SQL Server 2019 Enterprise Edition database. Your manager instructed you to migrate the application to Elastic Beanstalk and the database to RDS for SQL Server. For additional security, you must configure your database to automatically encrypt the actual data before it is written to storage, and automatically decrypt data when the data is read from storage. Which of the following services will you use to achieve this?",answers:[{text:"Enable RDS Encryption.",isCorrect:!1},{text:"Enable Transparent Data Encryption (TDE).",isCorrect:!0},{text:"Use Microsoft SQL Server Windows Authentication.",isCorrect:!1},{text:"Use IAM DB Authentication.",isCorrect:!1}],explanation:"Amazon RDS supports using Transparent Data Encryption (TDE) to encrypt stored data on your DB instances running Microsoft SQL Server. TDE automatically encrypts data before it is written to storage, and automatically decrypts data when the data is read from storage. Amazon RDS supports TDE for the following SQL Server versions and editions: - SQL Server 2019 Standard and Enterprise Editions - SQL Server 2017 Enterprise Edition - SQL Server 2016 Enterprise Edition - SQL Server 2014 Enterprise Edition - SQL Server 2012 Enterprise Edition Transparent Data Encryption is used in scenarios where you need to encrypt sensitive data. For example, you might want to provide data files and backups to a third party, or address security-related regulatory compliance issues. To enable transparent data encryption for an RDS SQL Server DB instance, specify the TDE option in an RDS option group that is associated with that DB instance. Transparent data encryption for SQL Server provides encryption key management by using a two-tier key architecture. A certificate, which is generated from the database master key, is used to protect the data encryption keys. The database encryption key performs the actual encryption and decryption of data on the user database. Amazon RDS backs up and manages the database master key and the TDE certificate. To comply with several security standards, Amazon RDS is working to implement automatic periodic master key rotation. TDE encrypts the actual data and log files at the database level. It automatically encrypts data before it is written to storage and automatically decrypts data when it is read from storage. Lastly, TDE provides encryption at the data level, protecting the data itself from unauthorized access. Hence, the correct answer is to Enable Transparent Data Encryption (TDE). The option that says: Use IAM DB Authentication is incorrect because this option just lets you authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. The more appropriate security feature to use here is TDE. The option that says: Enable RDS Encryption is incorrect. This option enables encryption at rest for the underlying storage volumes, but it does not automatically encrypt and decrypt the data itself. It relies on the database engine (in this case, SQL Server) to handle the encryption and decryption of the actual data. The option that says: Use Microsoft SQL Server Windows Authentication is incorrect because this option is primarily used if you want to integrate RDS with your AWS Directory Service for Microsoft Active Directory (also called AWS Managed Microsoft AD) to enable Windows Authentication to authenticate users. References: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.SQLServer.Options.TDE.html https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_SQLServer.html Check out this Amazon RDS Cheat Sheet: https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/"},{question:"A developer is managing an application hosted in EC2, which stores data in an S3 bucket. The application also uses HTTPS for secure communication. To comply with the new security policy, the developer must ensure that the data is encrypted at rest using an encryption key that is provided and managed by the company. The change should also provide AES-256 encryption to their data. Which of the following actions could the developer take to achieve this? (Select TWO.)",answers:[{text:"Implement Amazon S3 server-side encryption with Amazon S3-Managed Encryption Keys.",isCorrect:!1},{text:"Implement Amazon S3 server-side encryption with customer-provided keys (SSE-C).",isCorrect:!0},{text:"Implement Amazon S3 server-side encryption with AWS KMS Keys (SSE-KMS).",isCorrect:!1},{text:"Encrypt the data on the client-side before sending to Amazon S3 using their own master key.",isCorrect:!0},{text:"Use SSL to encrypt the data while in transit to Amazon S3.",isCorrect:!1}],explanation:"Data protection refers to protecting data while in transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3 data centers). You can protect data in transit by using SSL or by using client-side encryption. You have the following options for protecting data at rest in Amazon S3: Use Server-Side Encryption – You request Amazon S3 to encrypt your object before saving it on disks in its data centers and decrypt it when you download the objects. 1. Use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) 2. Use Server-Side Encryption with AWS KMS Keys (SSE-KMS) 3. Use Server-Side Encryption with Customer-Provided Keys (SSE-C) Use Client-Side Encryption – You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools. 1. Use Client-Side Encryption with AWS KMS Key 2. Use Client-Side Encryption Using a Client-Side Master Key Hence, the valid actions that the developer can implement in this scenario are: - Implement Amazon S3 server-side encryption with customer-provided keys (SSE-C) - Encrypt the data on the client-side before sending to Amazon S3 using their own master key. Using SSL to encrypt the data while in transit to Amazon S3 is incorrect because the requirement is to only secure the data at rest and not data in transit. Hence, you have to use server-side encryption instead. Moreover, the scenario explicitly states that the application already uses HTTPS for secure communication. Implementing Amazon S3 server-side encryption with AWS KMS Keys (SSE-KMS) is incorrect. Although you can upload the company's KMS keys (CMKs), the keys will be managed by KMS and not your company. This does not comply with the security policy mandated by the company. Implementing Amazon S3 server-side encryption with Amazon S3-Managed Encryption Keys is incorrect because the Amazon S3-Managed encryption does not comply with the policy mentioned in the given scenario since the keys are managed by AWS (through Amazon S3) and not by the company. The suitable server-side encryption that you should use here is SSE-C. References: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html https://docs.aws.amazon.com/AmazonS3/latest/dev/DataDurability.html Check out this Amazon S3 Cheat Sheet: https://tutorialsdojo.com/amazon-s3/"},{question:"You have two users concurrently accessing a DynamoDB table and submitting updates. If a user will modify a specific item in the table, she needs to make sure that the operation will not affect another user's attempt to modify the same item. You have to ensure that your update operations will only succeed if the item attributes meet one or more expected conditions. Which of the following DynamoDB features should you use in this scenario?",answers:[{text:"Conditional writes",isCorrect:!0},{text:"Batch Operations",isCorrect:!1},{text:"Update Expressions",isCorrect:!1},{text:"Projection Expressions",isCorrect:!1}],explanation:'By default, the DynamoDB write operations (PutItem, UpdateItem, DeleteItem) are unconditional: each of these operations will overwrite an existing item that has the specified primary key. DynamoDB optionally supports conditional writes for these operations. A conditional write will succeed only if the item attributes meet one or more expected conditions. Otherwise, it returns an error. Conditional writes are helpful in cases where multiple users attempt to modify the same item. For example, by adding a conditional expression that checks if the current value of the item is still the same, you can be sure that your update will not affect the operations of other users: aws dynamodb update-item \\ --table-name ProductCatalog \\ --key \'{"Id":{"N":"1"}}\' \\ --update-expression "SET Price = :newval" \\ --condition-expression "Price = :currval" \\ --expression-attribute-values file://expression-attribute-values.json Hence, the correct answer is conditional writes. Using projection expressions is incorrect because this is just a string that identifies the attributes you want to retrieve during a GetItem, Query, or Scan operation. Take note that the scenario calls for a feature that can be used during a write operation hence, this option is irrelevant. Using update expressions is incorrect because this simply specifies how UpdateItem will modify the attributes of an item such as for setting a scalar value or removing elements from a list or a map. This feature doesn\'t use any conditions which is what the scenario is looking for. Therefore, this option is incorrect. Using batch operations is incorrect because these are essentially wrappers for multiple read or write requests. Batch operations are primarily used when you want to retrieve or submit multiple items in DynamoDB through a single API call, which reduces the number of network round trips from your application to DynamoDB. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ReadingData https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/'},{question:"A company is currently in the process of integrating their on-premises data center to their cloud infrastructure in AWS. One of the requirements is to integrate the on-premises Lightweight Directory Access Protocol (LDAP) directory service to their AWS VPC using IAM. Which of the following provides the MOST suitable solution to implement if the identity store that they are using is not compatible with SAML?",answers:[{text:"Implement the AWS IAM Identity Center service to manage access between AWS and your LDAP.",isCorrect:!1},{text:"Create IAM roles to rotate the IAM credentials whenever LDAP credentials are updated.",isCorrect:!1},{text:"Create a custom identity broker application in your on-premises data center and use STS to issue short-lived AWS credentials.",isCorrect:!0},{text:"Set up an IAM policy that references the LDAP identifiers and AWS credentials.",isCorrect:!1}],explanation:"If your identity store is not compatible with SAML 2.0, then you can build a custom identity broker application to perform a similar function. The broker application authenticates users, requests temporary credentials for users from AWS, and then provides them to the user to access AWS resources. The application verifies that employees are signed into the existing corporate network's identity and authentication system, which might use LDAP, Active Directory, or another system. The identity broker application then obtains temporary security credentials for the employees. To get temporary security credentials, the identity broker application calls either AssumeRole or GetFederationToken to obtain temporary security credentials, depending on how you want to manage the policies for users and when the temporary credentials should expire. The call returns temporary security credentials consisting of an AWS access key ID, a secret access key, and a session token. The identity broker application makes these temporary security credentials available to the internal company application. The app can then use the temporary credentials to make calls to AWS directly. The app caches the credentials until they expire, and then requests a new set of temporary credentials. Hence, the correct answer is: Create a custom identity broker application in your on-premises data center and use STS to issue short-lived AWS credentials. The option that says: Setting up an IAM policy that references the LDAP identifiers and AWS credentials is incorrect because using an IAM policy is not enough to integrate your LDAP service into IAM. You need to use SAML, STS, or a custom identity broker instead. The option that says: Implementing the AWS IAM Identity Center service to manage access between AWS and your LDAP is incorrect because the identity store that you are using is not SAML-compatible. AWS IAM Identity Center does not support non-SAML authentication methods. The option that says: Creating IAM roles to rotate the IAM credentials whenever LDAP credentials are updated is incorrect because manually rotating the IAM credentials is not an optimal solution to integrate your on-premises and VPC network. You need to use SAML, STS, or a custom identity broker. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html https://aws.amazon.com/blogs/aws/aws-identity-and-access-management-now-with-identity-federation/ Check out this AWS Identity & Access Management (IAM) Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"},{question:"The company that you are working for recently decided to migrate and transform their monolithic application on-premises to a Lambda application. It is your responsibility to ensure that application works effectively in AWS. Which of the following are the best practices in developing Lambda functions? (Select TWO.)",answers:[{text:"Use recursive code.",isCorrect:!1},{text:"Include the core logic in the Lambda handler.",isCorrect:!1},{text:"Use AWS Lambda Environment Variables to pass operational parameters to your function.",isCorrect:!0},{text:"Use Amazon Inspector for troubleshooting.",isCorrect:!1},{text:"Take advantage of Execution Context reuse to improve the performance of your function.",isCorrect:!0}],explanation:"Below are some of the best practices in working with AWS Lambda Functions: - Separate the Lambda handler (entry point) from your core logic. - Take advantage of Execution Context reuse to improve the performance of your function - Use AWS Lambda Environment Variables to pass operational parameters to your function. - Control the dependencies in your function's deployment package. - Minimize your deployment package size to its runtime necessities. - Reduce the time it takes Lambda to unpack deployment packages - Minimize the complexity of your dependencies - Avoid using recursive code Hence, the correct answers in this scenario are: - Take advantage of Execution Context reuse to improve the performance of your function - Use AWS Lambda Environment Variables to pass operational parameters to your function Using recursive code is incorrect because this is a situation wherein the function automatically calls itself until some arbitrary criteria is met. This could lead to an unintended volume of function invocations and escalated costs. Including the core logic in the Lambda handler is incorrect because you have to separate the Lambda handler (entry point) from your core logic instead. Using Amazon Inspector for troubleshooting is incorrect because this service is primarily used for EC2 and not for Lambda. You have to use X-Ray instead of troubleshooting your functions. References: https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html https://docs.aws.amazon.com/lambda/latest/dg/lambda-x-ray.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A developer is building an AI-based traffic monitoring application using Lambda in AWS. Due to the complexity of the application, the developer must do certain modifications such as the way Lambda runs the function's setup code and how the invocation events are read from the Lambda runtime API. In this scenario, which feature of Lambda should you take advantage of to meet the above requirement?",answers:[{text:"Lambda@Edge",isCorrect:!1},{text:"DLQ",isCorrect:!1},{text:"Custom Runtime",isCorrect:!0},{text:"Layers",isCorrect:!1}],explanation:"You can implement an AWS Lambda runtime in any programming language. A runtime is a program that runs a Lambda function's handler method when the function is invoked. You can include a runtime in your function's deployment package in the form of an executable file named bootstrap. A runtime is responsible for running the function's setup code, reading the handler name from an environment variable, and reading invocation events from the Lambda runtime API. The runtime passes the event data to the function handler and posts the response from the handler back to Lambda. Your custom runtime runs in the standard Lambda execution environment. It can be a shell script, a script in a language that's included in Amazon Linux, or a binary executable file that's compiled in Amazon Linux. Take note that Lambda has a deployment package size limit of 50 MB for direct upload (zipped file) and 250 MB for layers (unzipped). Hence, the correct answer in this scenario is: Custom Runtime. Layers is incorrect because this just enables you to use libraries and other dependencies in your function without having to include them in your deployment package. Lambda@Edge is incorrect because this is actually a feature of Amazon CloudFront and not Lambda, which lets you run code closer to users of your application to improve performance and reduce latency. DLQ is incorrect because the Dead Letter Queue (DLQ) only directs unprocessed events to an Amazon SQS queue or an Amazon SNS topic to analyze the failure if the retries fail. This feature does not meet the required capabilities mentioned in the scenario. References: https://docs.aws.amazon.com/lambda/latest/dg/runtimes-custom.html https://docs.aws.amazon.com/lambda/latest/dg/runtimes-walkthrough.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A developer is creating a new global secondary index on a provisioned mode DynamoDB table. Since the application will store large quantities of data, the write capacity units must be specified for the expected workload on both the base table and its secondary index. Which of the following should the developer do to avoid any potential request throttling?",answers:[{text:"Ensure that the global secondary index's provisioned WCU is equal or greater than the WCU of the base table.",isCorrect:!0},{text:"Ensure that the global secondary index's provisioned RCU is equal or less than the RCU of the base table.",isCorrect:!1},{text:"Ensure that the global secondary index's provisioned RCU is equal or greater than the RCU of the base table.",isCorrect:!1},{text:"Ensure that the global secondary index's provisioned WCU is equal or less than the WCU of the base table.",isCorrect:!1}],explanation:"A global secondary index (GSI) is an index with a partition key and a sort key that can be different from those on the base table. It is considered \"global\" because queries on the index can span all of the data in the base table, across all partitions. Every global secondary index has its own provisioned throughput settings for read and write activity. Queries or scans on a global secondary index consume capacity units from the index, not from the base table. The same holds true for global secondary index updates due to table writes. When you create a global secondary index on a provisioned mode table, you must specify read and write capacity units for the expected workload on that index. The provisioned throughput settings of a global secondary index are separate from those of its base table. A Query operation on a global secondary index consumes read capacity units from the index, not the base table. When you put, update, or delete items in a table, the global secondary indexes on that table are also updated; these index updates consume write capacity units from the index, not from the base table. For example, if you Query a global secondary index and exceed its provisioned read capacity, your request will be throttled. If you perform heavy write activity on the table but a global secondary index on that table has insufficient write capacity, then the write activity on the table will be throttled. To avoid potential throttling, the provisioned write capacity for a global secondary index should be equal or greater than the write capacity of the base table since new updates will write to both the base table and global secondary index. Hence, the correct answer in this scenario is to ensure that the global secondary index's provisioned WCU is equal to or greater than the WCU of the base table. Ensuring that the global secondary index's provisioned WCU is equal or less than the WCU of the base table is incorrect because it should be the other way around, just as what is mentioned above. The provisioned write capacity for a global secondary index should be equal to or greater than the write capacity of the base table. Ensuring that the global secondary index's provisioned RCU is equal to or greater than the RCU of the base table is incorrect because you have to set the WCU and not the RCU. Ensuring that the global secondary index's provisioned RCU is equal or less than the RCU of the base table is incorrect because this should be WCU and in addition, the global secondary index's provisioned WCU should be set to a value that is equal or greater than the WCU of the base table to prevent request throttling. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html#GSI.ThroughputConsiderations https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.MessagesAndCodes Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"},{question:"A company is developing a serverless website that consists of images, videos, HTML pages and JavaScript files. There is also a requirement to serve the files with lowest possible latency to its global users. Which combination of services should be used in this scenario? (Select TWO.)",answers:[{text:"Amazon Glacier",isCorrect:!1},{text:"Amazon S3",isCorrect:!0},{text:"Amazon CloudFront",isCorrect:!0},{text:"Amazon Elastic File System",isCorrect:!1},{text:"Amazon EC2",isCorrect:!1}],explanation:"You can configure your application to deliver static content and decrease the end-user latency using Amazon S3 and Amazon CloudFront. High-resolution images, videos, and other static files can be stored in Amazon S3. CloudFront speeds up content delivery by leveraging its global network of data centers, known as edge locations, to reduce delivery time by caching your content close to your end-users. CloudFront fetches your content from an origin, such as an Amazon S3 bucket, an Amazon EC2 instance, an Amazon Elastic Load Balancing load balancer, or your own web server, when it's not already in an edge location. CloudFront can be used to deliver your entire website or application, including dynamic, static, streaming, and interactive content. You can set your Amazon S3 bucket as the origin of your CloudFront web distribution. Hence, the correct answers are: - Amazon S3 - Amazon CloudFront Amazon EC2 is incorrect because EC2 instances are more suited for dynamic, server-side processing and they are region-specific, which means the latency for users would vary depending on their geographical distance from the EC2 instance. They are not designed for low-latency global content delivery of static web assets. Amazon Elastic File System is incorrect because this is not a suitable service for storing static content, unlike S3. It is a regional service storing data within and across multiple Availability Zones (AZs) for high availability and durability. In addition, you can't directly connect it to CloudFront, unlike S3. Amazon Glacier is incorrect because this is primarily used for data archival with usually a long data retrieval time. Like EFS, you can't directly connect it to CloudFront, unlike Amazon S3. References: https://aws.amazon.com/getting-started/tutorials/deliver-content-faster/ https://aws.amazon.com/cloudfront/ https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/ Check out these Amazon S3 and CloudFront Cheat Sheets: https://tutorialsdojo.com/amazon-s3/ https://tutorialsdojo.com/amazon-cloudfront/"},{question:"A developer is managing a real-time fraud detection system that ingests a stream of data using Amazon Kinesis. The system works well with millisecond end-to-end latency, but the allocated shards are way underutilized based on the performance data in CloudWatch. Which of the following is the MOST suitable solution to reduce the cost and capacity of the stream?",answers:[{text:"Split cold shards",isCorrect:!1},{text:"Merge cold shards",isCorrect:!0},{text:"Split hot shards",isCorrect:!1},{text:"Merge hot shards",isCorrect:!1}],explanation:'The purpose of resharding in Amazon Kinesis Data Streams is to enable your stream to adapt to changes in the rate of data flow. You split shards to increase the capacity (and cost) of your stream. You merge shards to reduce the cost (and capacity) of your stream. One approach to resharding could be to split every shard in the stream—which would double the stream\'s capacity. However, this might provide more additional capacity than you actually need and therefore create unnecessary costs. You can also use metrics to determine which are your "hot" or "cold" shards, that is, shards that are receiving much more data, or much less data, than expected. You could then selectively split the hot shards to increase capacity for the hash keys that target those shards. Similarly, you could merge cold shards to make better use of their unused capacity. You can obtain some performance data for your stream from the Amazon CloudWatch metrics that Kinesis Data Streams publishes. However, you can also collect some of your own metrics for your streams. One approach would be to log the hash key values generated by the partition keys for your data records. Recall that you specify the partition key at the time that you add the record to the stream. Hence, the correct answer is to merge cold shards to reduce the capacity and the cost of running your Kinesis Data Stream. Splitting cold shards is incorrect because a cold shard is the one that receives fewer data which means that you have to merge them to reduce the capacity rather than split them. Merging hot shards is incorrect. Although merging shards is correct, the type of shard to be merged is wrong. A hot shard is the one that receives more data in the stream. Merging hot shards could potentially overload the newly merged shard with a high volume of data, causing a bottleneck in processing and degrading the overall performance of the stream. Splitting hot shards is incorrect because this will actually further increase both the cost and capacity of the stream rather than reduce it. Moreover, there are no hot shards in the stream since the scenario specifically mentioned that the shards are way underutilized. References: https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding.html Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/ Kinesis Scaling, Resharding and Parallel Processing: https://tutorialsdojo.com/kinesis-scaling-resharding-and-parallel-processing/'},{question:"A developer uses AWS X-Ray to create a trace on an instrumented web application to identify any performance bottlenecks. The segment documents being sent by the application contain annotations that the developer wants to utilize in order to identify and filter out specific data from the trace. Which of the following should the developer do in order to satisfy this requirement with minimal configuration? (Select TWO.)",answers:[{text:"Configure Sampling Rules in the AWS X-Ray Console.",isCorrect:!1},{text:"Fetch the data using the BatchGetTraces API.",isCorrect:!1},{text:"Send trace results to an S3 bucket then query the trace output using Amazon Athena.",isCorrect:!1},{text:"Use filter expressions via the X-Ray console.",isCorrect:!0},{text:"Fetch the trace IDs and annotations using the GetTraceSummaries API.",isCorrect:!0}],explanation:"The compute resources running your application logic send data about their work as segments. A segment provides the resource's name, details about the request, and details about the work done. A subset of segment fields are indexed by X-Ray for use with filter expressions. You can search for segments associated with specific information in the X-Ray console or by using the GetTraceSummaries API. Even with sampling, a complex application generates a lot of data. When you choose a time period of traces to view in the X-Ray console, you might get more results than the console can display. You can narrow the results to just the traces that you want to find by using a filter expression. Running the GetTraceSummaries operation retrieves IDs and annotations for traces available for a specified time frame using an optional filter. Hence, using filter expressions via the X-Ray console and fetching the trace IDs and annotations using the GetTraceSummaries API are the correct answers in this scenario. Fetching the data using the BatchGetTraces API is incorrect because this API simply retrieves a list of traces specified by ID. It does not support filter expressions nor returns the annotations. Sending trace results to an S3 bucket then querying the trace output using Amazon Athena is incorrect. Although this solution may work, this entails a lot of configuration which is contrary to what the scenario requires. There are other simpler methods of searching through traces in X-Ray such as using annotations and filter expressions. Configuring Sampling Rules in the AWS X-Ray Console is incorrect because sampling rules just tell the X-Ray SDK how many requests to record for a set of criteria. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html https://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html https://docs.aws.amazon.com/xray/latest/api/API_GetTraceSummaries.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/"},{question:"A startup recently launched a high-quality photo-sharing portal using Amazon Lightsail and Amazon S3. The team noticed that other external websites are linking and using the photos without permission. This situation has caused an increase in data transfer costs and potential revenue loss. Which of the following is the MOST effective method to solve this issue?",answers:[{text:"Configure the S3 bucket to remove public read access and use pre-signed URLs with expiry dates.",isCorrect:!1},{text:"Enable cross-origin resource sharing (CORS) which allows cross-origin GET requests from all origins.",isCorrect:!1},{text:"Use an Amazon CloudFront web distribution with signed URLs or signed cookies.",isCorrect:!0},{text:"Block the IP addresses of the offending websites using Network Access Control List.",isCorrect:!1}],explanation:"A signed URL contains extra information, such as an expiration date and time, providing greater control over access to your content. This additional information is presented in a policy statement, which is derived from either a predefined (canned) policy or a personalized (custom) policy. CloudFront signed cookies allow you to control who can access your content when you don't want to change your current URLs or when you want to provide access to multiple restricted files, such as all of the files in a website's subscribers' area. This topic explains the considerations when using signed cookies and describes how to set signed cookies using canned and custom policies. The most effective method to control unauthorized access to the photos and manage data transfer costs is to use an Amazon CloudFront web distribution with signed URLs or signed cookies. This approach allows the startup to enforce access controls at the CDN layer, ensuring that only authorized users can access the content. Additionally, CloudFront Functions can be used to validate referrer headers, adding another layer of protection by ensuring that only requests originating from the startup’s own domain are allowed to access the content. By using CloudFront with signed URLs or signed cookies, the startup can manage access effectively, providing a scalable solution that prevents unauthorized use while controlling data transfer costs. The use of CloudFront Functions to validate referrer headers adds an extra layer of security, ensuring that the photos are only accessible through authorized channels, thereby protecting the startup’s content and reducing unnecessary data transfer costs. Hence, the correct answer is: Use an Amazon CloudFront web distribution with signed URLs or signed cookies. The option that says: Enable cross-origin resource sharing (CORS) which allows cross-origin GET requests from all origins is incorrect as this will typically make the problem worse since you are allowing any website or origin to fetch the objects in the S3 bucket. Although using CORS is a valid solution, it should be properly configured to only enable access to your trusted domains. The option that says: Configure the S3 bucket to remove public read access and use pre-signed URLs with expiry dates is incorrect because it is not primarily scalable for large numbers of objects, as it would require generating pre-signed URLs for potentially thousands or millions of objects, making it impractical. Blocking the IP addresses of the offending websites using Network Access Control List is incorrect because a quick change in IP address would easily bypass this configuration; hence, this is not an efficient method to implement. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/SecurityAndPrivateContent.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html Check out this Amazon CloudFront Cheat Sheet: https://tutorialsdojo.com/amazon-cloudfront/"},{question:"An Elastic Beanstalk application becomes inaccessible for several minutes whenever a failed deployment is rolled back. A developer should recommend a strategy that will have the least impact on the application's availability if the deployment fails. Teams must be able to revert changes quickly as well. Which deployment method should the developer suggest?",answers:[{text:"All at Once",isCorrect:!1},{text:"Blue/Green",isCorrect:!0},{text:"Rolling with Additional Batches",isCorrect:!1},{text:"Rolling",isCorrect:!1}],explanation:"Because AWS Elastic Beanstalk performs an in-place update when you update your application versions, your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment and then swap CNAMEs of the two environments to redirect traffic to the new version instantly. Blue/green deployments require that your environment runs independently of your production database if your application uses one. If your environment has an Amazon RDS DB instance attached to it, the data will not transfer over to your second environment and will be lost if you terminate the original environment. In Elastic Beanstalk, you can choose from a variety of deployment methods: All at once – Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time while the deployment occurs. Rolling – Deploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch. Rolling with additional batch – Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during the deployment process. Immutable – Deploy the new version to a fresh group of instances by performing an immutable update. Traffic splitting - Percentage of client traffic routed to new version temporarily impacted Blue/Green - Deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version instantly. The scenario is asking for the least impact on the application’s availability if a deployment fails. Naturally, we’d want to roll back to the last working version if a deployment does not succeed. The rollback process for a Blue/green deployment is the fastest since all you have to do is switch back to the working environment’s URL. Hence, the correct answer is Blue/Green. All at once is incorrect because this method deploys the new version to all instances simultaneously, which causes your instances to be out of service for a short time while the deployment occurs. This is also the case when you revert a failed deployment. In short, All at once has the MOST impact on your application's availability in case the deployment fails. Rolling with additional batch is incorrect. Although this method ensures full capacity during deployment, its rollback process is quite slow because the deployment is done on fresh instances alongside the existing ones. Rolling is incorrect. With Rolling, your environment's capacity to serve traffic is reduced by the number of instances the new version is being rolled out to, which may impact the availability of your application. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/"},{question:"A leading insurance firm is hosting its customer portal in Elastic Beanstalk, which has an RDS database in AWS. The support team in your company discovered a lot of SQL injection attempts and cross-site scripting attacks on the portal, which is starting to affect the production environment. Which of the following services should you implement to mitigate this attack?",answers:[{text:"AWS Firewall Manager",isCorrect:!1},{text:"Amazon Guard​Duty",isCorrect:!1},{text:"AWS WAF",isCorrect:!0},{text:"Network Access Control List",isCorrect:!1}],explanation:"AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon API Gateway API, Amazon CloudFront or an Application Load Balancer. AWS WAF also lets you control access to your content. Based on conditions that you specify, such as the IP addresses that requests originate from or the values of query strings, API Gateway, CloudFront or an Application Load Balancer responds to requests either with the requested content or with an HTTP 403 status code (Forbidden). You also can configure CloudFront to return a custom error page when a request is blocked. At the simplest level, AWS WAF lets you choose one of the following behaviors: Allow all requests except the ones that you specify – This is useful when you want CloudFront or an Application Load Balancer to serve content for a public website, but you also want to block requests from attackers. Block all requests except the ones that you specify – This is useful when you want to serve content for a restricted website whose users are readily identifiable by properties in web requests, such as the IP addresses that they use to browse to the website. Count the requests that match the properties that you specify – When you want to allow or block requests based on new properties in web requests, you first can configure AWS WAF to count the requests that match those properties without allowing or blocking those requests. This lets you confirm that you didn't accidentally configure AWS WAF to block all the traffic to your website. When you're confident that you specified the correct properties, you can change the behavior to allow or block requests. Hence, the correct answer in this scenario is AWS WAF. Amazon Guard​Duty is incorrect because this is just a threat detection service that continuously monitors malicious activity and unauthorized behavior to protect your AWS accounts and workloads. AWS Firewall Manager is incorrect because this just simplifies your AWS WAF and AWS Shield Advanced administration and maintenance tasks across multiple accounts and resources. Network Access Control List is incorrect because this is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. References: https://aws.amazon.com/waf/ https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html https://aws.amazon.com/blogs/security/three-most-important-aws-waf-rate-based-rules/ Check out this AWS WAF Cheat Sheet: https://tutorialsdojo.com/aws-waf/"},{question:"A company has assigned a developer to automate its department's patch management, data synchronization, and other recurring tasks. The developer needs a service to coordinate multiple AWS services into serverless workflows. Which of the following is the MOST cost-effective service the developer should implement in this scenario?",answers:[{text:"AWS Lambda",isCorrect:!1},{text:"AWS Elastic Beanstalk",isCorrect:!1},{text:"AWS Batch",isCorrect:!1},{text:"AWS Step Functions",isCorrect:!0}],explanation:"AWS Step Functions provides serverless orchestration for modern applications. Orchestration centrally manages a workflow by breaking it into multiple steps, adding flow logic, and tracking the inputs and outputs between the steps. As your applications execute, Step Functions maintain the application state, tracking exactly which workflow step your application is in, and store an event log of data that is passed between application components. That means that if networks fail or components hang, your application can pick up right where it left off. Application development is faster and more intuitive with Step Functions because you can define and manage the workflow of your application independently from its business logic. Making changes to one does not affect the other. You can easily update and modify workflows in one place, without having to struggle with managing, monitoring, and maintaining multiple point-to-point integrations. Step Functions frees your functions and containers from excess code, so your applications are faster to write, more resilient, and easier to maintain. Hence, the correct answer is: AWS Step Functions. AWS Elastic Beanstalk is incorrect because this service is for deploying and scaling web applications and services. However, it’s not designed to coordinate multiple AWS services into serverless workflows. Lambda is incorrect. Although it is typically used for serverless computing, it does not provide a direct way to coordinate multiple AWS services into serverless workflows. AWS Batch is incorrect because it is primarily used to efficiently run hundreds of thousands of batch computing jobs in AWS. References: https://aws.amazon.com/step-functions/features/ https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html Check out this AWS Step Functions Cheat Sheet: https://tutorialsdojo.com/aws-step-functions/"},{question:"A developer has just finished writing a serverless application using AWS SAM (Serverless Application Model) on a local machine. There is a SAM template ready and the corresponding Lambda function code in a directory. The developer now wants to deploy this application to AWS. Which combination of steps should the developer follow to successfully deploy the SAM application? (Select THREE)",answers:[{text:"Package the SAM application for deployment.",isCorrect:!0},{text:"Build the SAM template using the AWS SDK for AWS CodeDeploy.",isCorrect:!1},{text:"Build the SAM template in the local environment",isCorrect:!0},{text:"Deploy the SAM template from AWS CodePipeline.",isCorrect:!1},{text:"Build the SAM template in an Amazon EC2 instance.",isCorrect:!1},{text:"Deploy the SAM template from an Amazon S3 bucket.",isCorrect:!0}],explanation:"AWS SAM uses AWS CloudFormation as the underlying deployment mechanism. You can deploy your application by using AWS SAM command line interface (CLI) commands. You can also use other AWS services that integrate with AWS SAM to automate your deployments. The typical AWS SAM deployment workflow starts with the sam build command, which compiles source code and readies deployment artifacts. Once built for deployment, the SAM template and the associated artifacts need to be stored in an S3 bucket. The sam deploy command takes care of this by first uploading the CloudFormation template to the S3 bucket. Though historically, the sam package command was used for this purpose, it's become somewhat legacy, as sam deploy , now implicitly handles the packaging. Once the template is in the S3 bucket, AWS CloudFormation references it to create or update the defined resources. Hence, the correct answers are: - Build the SAM template in the local environment - Package the SAM application for deployment. - Deploy the SAM template from an Amazon S3 bucket. The option that says: Deploy the SAM template from AWS CodePipeline is incorrect. AWS CodePipeline is primarily a continuous integration and continuous delivery (CI/CD) service that automates the build, test, and deploy phases of your release process. While CodePipeline can deploy SAM applications, it is not a required step for a local SAM deployment workflow. The option that says: Build the SAM template using the AWS SDK for AWS CodeDeploy is incorrect. The AWS SDK for CodeDeploy is typically used for management operations of the CodeDeploy service, not for building SAM templates. Building the SAM application is a separate process, typically done using the SAM CLI. The option that says: Build the SAM template in an Amazon EC2 instance is incorrect. This option is unnecessary. While you can technically build on an EC2 instance, it's not a requirement for SAM deployment. In the scenario, there's no condition that warrants the use of an EC2 instance. References: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html https://docs.aws.amazon.com/serverlessrepo/latest/devguide/what-is-serverlessrepo.html Check out this AWS SAM Cheat Sheet: https://tutorialsdojo.com/aws-serverless-application-model-sam/"},{question:"A startup has recently launched their new mobile game and is gaining a lot of new users everyday. The founders plan to add a new feature which will enable cross-device syncing of user profile data across mobile devices to improve the user experience. Which of the following services should they use to meet this requirement?",answers:[{text:"AWS Amplify",isCorrect:!1},{text:"Cognito User Pools",isCorrect:!1},{text:"Cognito Identity Pools",isCorrect:!1},{text:"Cognito Sync",isCorrect:!0}],explanation:"Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status. When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available. Amazon Cognito lets you save end user data in datasets containing key-value pairs. This data is associated with an Amazon Cognito identity, so that it can be accessed across logins and devices. To sync this data between the Amazon Cognito service and an end user’s devices, invoke the synchronize method. Each dataset can have a maximum size of 1 MB. You can associate up to 20 datasets with an identity. The Amazon Cognito Sync client creates a local cache for the identity data. Your app talks to this local cache when it reads and writes keys. This guarantees that all of your changes made on the device are immediately available on the device, even when you are offline. When the synchronize method is called, changes from the service are pulled to the device, and any local changes are pushed to the service. At this point the changes are available to other devices to synchronize. Amazon Cognito automatically tracks the association between identity and devices. Using the push synchronization, or push sync, feature, you can ensure that every instance of a given identity is notified when identity data changes. Push sync ensures that, whenever the sync store data changes for a particular identity, all devices associated with that identity receive a silent push notification informing them of the change. Hence, the correct answer is to use Cognito Sync. Cognito User Pools is incorrect because this is just a user directory that allows your users to sign in to your web or mobile app through Amazon Cognito. Cognito Identity Pools is incorrect because this simply enables you to create unique identities for your users and federate them with identity providers where you can obtain temporary, limited-privilege AWS credentials to access other AWS services. AWS Amplify is incorrect because this just makes it easy for you to create, configure, and implement scalable mobile and web apps powered by AWS. It does not have the ability to synchronize user profile data across mobile devices. References: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html https://docs.aws.amazon.com/cognito/latest/developerguide/push-sync.html"},{question:"You are configuring the task definitions of your ECS Cluster in AWS to make sure that the tasks are scheduled on instances with enough resources to run them. It should also follow the constraints that you specified both implicitly or explicitly. Which of the following options should you implement to satisfy the requirement which requires the LEAST amount of configuration?",answers:[{text:"Use a random task placement strategy.",isCorrect:!0},{text:"Use a binpack task placement strategy.",isCorrect:!1},{text:"Use a spread task placement strategy which uses the instanceId and host attributes.",isCorrect:!1},{text:"Use a spread task placement strategy with custom placement constraints.",isCorrect:!1}],explanation:"By default, tasks are randomly placed with RunTask or spread across Availability Zones with CreateService. Spread is typically used to achieve high availability by making sure that multiple copies of a task are scheduled across multiple instances based on attributes such as Availability Zones. A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service. Amazon ECS supports the following task placement strategies: binpack - Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use. random - Place tasks randomly. spread - Place tasks evenly based on the specified value. Accepted values are attribute key-value pairs, instanceId, or host. The Random task placement strategy is fairly straightforward as it doesn’t require further parameters. The two other strategies, such as binpack and spread, take opposite actions. Binpack places tasks on as few instances as possible, helping to optimize resource utilization, while spread places tasks evenly across your cluster to help maximize availability. By default, ECS uses spread with the ecs.availability-zone attribute to place tasks. Random places tasks on instances at random yet still honors the other constraints that you specified, implicitly or explicitly. Specifically, it still makes sure that tasks are scheduled on instances with enough resources to run them. Hence, the correct answer is to use a random task placement strategy for this scenario. Using a binpack task placement strategy is incorrect because this configuration will place the tasks based on the least available amount of CPU or memory. There are also additional configuration steps where you need to specify the type of field that ECS would be using such as CPU or memory. Using a spread task placement strategy which uses the instanceId and host attributes is incorrect because this entails a lot of configuration as compared to using the Random task placement strategy type. Using a spread task placement strategy with custom placement constraints is incorrect because a task placement constraint is just a rule that is considered during task placement. References: https://aws.amazon.com/blogs/compute/amazon-ecs-task-placement/ https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html Check out this Amazon ECS Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/"},{question:"A leading financial company has recently deployed its application to AWS using Lambda and API Gateway. However, they noticed that all metrics are being populated in their CloudWatch dashboard except for CacheHitCount and CacheMissCount. What could be the MOST likely cause of this issue?",answers:[{text:"They have not provided an IAM role to their API Gateway yet.",isCorrect:!1},{text:"API Gateway Private Integrations has not been configured yet.",isCorrect:!1},{text:"API Caching is not enabled in API Gateway.",isCorrect:!0},{text:"The provided IAM role to their API Gateway only has read access but no write privileges to CloudWatch.",isCorrect:!1}],explanation:"You can monitor API execution using CloudWatch, which collects and processes raw data from API Gateway into readable, near-real-time metrics. These statistics are recorded for a period of two weeks so that you can access historical information and gain a better perspective on how your web application or service is performing. By default, API Gateway metric data is automatically sent to CloudWatch in one-minute periods. The metrics reported by API Gateway provide information that you can analyze in different ways. The list below shows some common uses for the metrics. These are suggestions to get you started, not a comprehensive list. - Monitor the IntegrationLatency metrics to measure the responsiveness of the backend. - Monitor the Latency metrics to measure the overall responsiveness of your API calls. - Monitor the CacheHitCount and CacheMissCount metrics to optimize cache capacities to achieve a desired performance. CacheMissCount tracks the number of requests served from the backend in a given period, when API caching is enabled. On the other hand, CacheHitCount track the number of requests served from the API cache in a given period. Hence, the root cause of this issue is that the API Caching is not enabled in API Gateway which is why the CacheHitCount and CacheMissCount metrics are not populated. The option that says: they have not provided an IAM role to their API Gateway yet is incorrect because, in the first place, the scenario already mentioned that all metrics are being populated in their CloudWatch dashboard except for two metrics. This implies that some of the metrics are populated which means that the API Gateway already has an IAM Role associated with it. The option that says: the provided IAM role to their API Gateway only has read access but no write privileges to CloudWatch is incorrect because just as what is mentioned above, there is no issue with the IAM Role since all metrics are being populated except only for CacheHitCount and CacheMissCount. This means that the associated IAM Role already has write privileges to write logs to CloudWatch to begin with. The only reason why those two metrics are not being populated is that the API Caching is not enabled. The option that says: API Gateway Private Integrations has not been configured yet is incorrect because this feature only makes it easier to expose your HTTP/HTTPS resources behind an Amazon VPC for access by clients outside of the VPC. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-metrics-and-dimensions.html https://docs.aws.amazon.com/apigateway/latest/developerguide/monitoring-cloudwatch.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway"},{question:"An aerospace engineering company has recently migrated to AWS for their cloud architecture. They are using CloudFormation and AWS SAM as deployment services for both of their monolithic and serverless applications. There is a new requirement where you have to dynamically install packages, create files, and start services on your EC2 instances upon the deployment of the application stack using CloudFormation. Which of the following helper scripts should you use in this scenario?",answers:[{text:"cfn-signal",isCorrect:!1},{text:"cfn-init",isCorrect:!0},{text:"cfn-hup",isCorrect:!1},{text:"cfn-get-metadata",isCorrect:!1}],explanation:"AWS CloudFormation provides the following Python helper scripts that you can use to install software and start services on an Amazon EC2 instance that you create as part of your stack: cfn-init: Use to retrieve and interpret resource metadata, install packages, create files, and start services. cfn-signal: Use to signal with a CreationPolicy or WaitCondition, so you can synchronize other resources in the stack when the prerequisite resource or application is ready. cfn-get-metadata: Use to retrieve metadata for a resource or path to a specific key. cfn-hup: Use to check for updates to metadata and execute custom hooks when changes are detected. You call the scripts directly from your template. The scripts work in conjunction with resource metadata that's defined in the same template. The scripts run on the Amazon EC2 instance during the stack creation process. The scripts are not executed by default. You must include calls in your template to execute specific helper scripts. Hence, cfn-init helper script is the correct answer since it interprets the metadata that contains the sources, packages, files, and services. You run the script on the EC2 instance when it is launched. The script is installed by default on Amazon Linux and Windows AMIs. The cfn-get-metadata helper script is incorrect since it is only a wrapper script that retrieves either all metadata that is defined for a resource or path to a specific key or a subtree of the resource metadata, but does not interpret the resource metadata, install packages, create files, and start services. The cfn-signal helper script is incorrect since it does not perform any retrieval and interpretation of resource metadata, installation of packages, creation of files, and starting of services. Instead, it is a wrapper thats signals an AWS CloudFormation WaitCondition for synchronizing other resources in the stack when the application is ready. The cfn-hup helper script is incorrect because this is just a daemon that checks for updates to metadata and executes custom hooks when changes are detected. It does not retrieve and interpret the resource metadata, install packages, create files, and start services unlike cfn-init helper script. References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-init.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-helper-scripts-reference.html https://s3.amazonaws.com/cloudformation-examples/BoostrappingApplicationsWithAWSCloudFormation.pdf Check out this AWS CloudFormation Cheat Sheet: https://tutorialsdojo.com/aws-cloudformation/ AWS CloudFormation - Templates, Stacks, Change Sets: https://youtu.be/9Xpuprxg7aY"},{question:"A company has a suite of web applications that is heavily using RDS database in Multi-AZ Deployments configuration with several Read Replicas. For improved security, you were instructed to ensure that all of their database credentials, API keys, and other secrets are encrypted and rotated on a regular basis. You should also configure your applications to use the latest version of the encrypted credentials when connecting to the RDS database. Which of the following is the MOST appropriate solution to secure the credentials?",answers:[{text:"Store the credentials in AWS KMS.",isCorrect:!1},{text:"Store the credentials to AWS ACM.",isCorrect:!1},{text:"Use AWS Secrets Manager to store and encrypt the credentials and enable automatic rotation.",isCorrect:!0},{text:"Store the credentials to Systems Manager Parameter Store with a SecureString data type.",isCorrect:!1}],explanation:"AWS Secrets Manager is an AWS service that makes it easier for you to manage secrets. Secrets can be database credentials, passwords, third-party API keys, and even arbitrary text. You can store and control access to these secrets centrally by using the Secrets Manager console, the Secrets Manager command line interface (CLI), or the Secrets Manager API and SDKs. In the past, when you created a custom application that retrieves information from a database, you typically had to embed the credentials (the secret) for accessing the database directly in the application. When it came time to rotate the credentials, you had to do much more than just create new credentials. You had to invest time to update the application to use the new credentials. Then you had to distribute the updated application. If you had multiple applications that shared credentials and you missed updating one of them, the application would break. Because of this risk, many customers have chosen not to regularly rotate their credentials, which effectively substitutes one risk for another. Secrets Manager enables you to replace hardcoded credentials in your code (including passwords), with an API call to Secrets Manager to retrieve the secret programmatically. This helps ensure that the secret can't be compromised by someone examining your code, because the secret simply isn't there. Also, you can configure Secrets Manager to automatically rotate the secret for you according to a schedule that you specify. This enables you to replace long-term secrets with short-term ones, which helps to significantly reduce the risk of compromise. Hence, using AWS Secrets Manager to store and encrypt the credentials and enabling automatic rotation is the most appropriate solution for this scenario. Storing the credentials to Systems Manager Parameter Store with a SecureString data type is incorrect because, by default, Systems Manager Parameter Store doesn't rotate its parameters which is one of the requirements in the above scenario. Storing the credentials to AWS ACM is incorrect because it is just a managed private CA service that helps you easily and securely manage the lifecycle of your private certificates to allow SSL communication to your application. This is not a suitable service to store database or any other confidential credentials. Storing the credentials in AWS KMS is incorrect because this only makes it easy for you to create and manage encryption keys and control the use of encryption across a wide range of AWS services. This is primarily used for encryption and not for hosting your credentials. References: https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html https://aws.amazon.com/blogs/compute/sharing-secrets-with-aws-lambda-using-aws-systems-manager-parameter-store/ Check out these AWS Systems Manager and Secrets Manager Cheat Sheets: https://tutorialsdojo.com/aws-systems-manager/ https://tutorialsdojo.com/aws-secrets-manager/"},{question:"The operating cost of a serverless application is quite high and you are instructed to look for ways to lower the costs. As part of its processing, a Lambda function sends 320 strongly consistent read requests per second to a DynamoDB table which has a provisioned RCU of 5440. The average size of items stored in the database is 17 KB. Which of the following is the MOST suitable action that should you do to make the application more cost-effective while maintaining its performance?",answers:[{text:"Decrease the provisioned RCU down to 800.",isCorrect:!1},{text:"Switch the table from using provisioned mode to on-demand mode.",isCorrect:!1},{text:"Set the provisioned RCU to 1600.",isCorrect:!0},{text:"Implement exponential backoff.",isCorrect:!1}],explanation:"In this scenario, a Lambda function makes 320 strongly consistent read requests per second against a DynamoDB table which has a provisioned RCU of 5440. The average size of items stored in the database is 17 KB. It seems that the RCU is quite high and the calculations to come up with this value are incorrect. If you multiply 320 x 17, then you'll get 5,440 which is a correct calculation for WCU but not for the RCU. 1 RCU can do 1 strongly consistent read or 2 eventually consistent reads for an item up to 4KB. To get the RCU with strongly consistent reads, do the following steps: Step #1 Divide the average item size by 4 KB. Round up the result Average Item Size = 17 KB = 17KB/4KB = 4.25 ≈ 5 Step #2 Multiply the number of reads per second by the resulting value from Step 1. (Divide the product by 2 for eventually consistent reads) = 320 reads per second x 5 = 1,600 strongly consistent read requests Hence, the correct answer is to set the provisioned RCU to 1600 as this will lower the cost and still maintain the performance of your application. Implementing exponential backoff is incorrect because this is only applicable for error retries and error handling of the serverless application. Decreasing the provisioned RCU down to 800 is incorrect. Although this will lower the cost, it will not meet the strong consistency requirements of the application. Take note that the Lambda function makes read requests with a strong consistency type and not eventual consistency. Switching the table from using provisioned mode to on-demand mode is incorrect. Although this will lower the cost, the on-demand mode is more suitable for unpredictable application traffic. The scenario explicitly mentioned the exact application traffic, which is why the provisioned mode is more suitable to use. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ItemSizeCalculations.Reads Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ Calculating the Required Read and Write Capacity Unit for your DynamoDB Table: https://tutorialsdojo.com/calculating-the-required-read-and-write-capacity-unit-for-your-dynamodb-table/"},{question:"In the next financial year, a company has decided to develop a completely new version of its legacy application that will utilize Node.js and GraphQL. The new architecture aims to offer an end-to-end view of requests as they traverse the application and display a map of the underlying components. To achieve this, the application will be hosted in an Auto Scaling group (ASG) of Linux EC2 instances behind an Application Load Balancer (ALB) and must be instrumented to send trace data to the AWS X-Ray. Which of the following options is the MOST suitable way to satisfy this requirement?",answers:[{text:"Refactor your application to send segment documents directly to X-Ray by using the PutTraceSegments API.",isCorrect:!1},{text:"Use a user data script to install the X-Ray daemon.",isCorrect:!0},{text:"Enable AWS X-Ray tracing on the ASG’s launch template.",isCorrect:!1},{text:"Enable AWS Web Application Firewall (WAF) on the ALB to monitor web requests.",isCorrect:!1}],explanation:"The AWS X-Ray SDK does not send trace data directly to AWS X-Ray. To avoid calling the service every time your application serves a request, the SDK sends the trace data to a daemon, which collects segments for multiple requests and uploads them in batches. Use a script to run the daemon alongside your application. To properly instrument your application hosted in an EC2 instance, you have to install the X-Ray daemon by using a user data script. This will install and run the daemon automatically when you launch the instance. To use the daemon on Amazon EC2, create a new instance profile role or add the managed policy to an existing one. This will grant the daemon permission to upload trace data to X-Ray. The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon works in conjunction with the AWS X-Ray SDKs and must be running so that data sent by the SDKs can reach the X-Ray service. Hence, the correct answer is: Use a user data script to install the X-Ray daemon. The option that says: Enable AWS X-Ray tracing on the ASG’s launch template is incorrect. There's no option to enable X-Ray tracing in a launch template of an ASG. The option that says: Enable AWS Web Application Firewall (WAF) on the ALB to monitor web requests is incorrect. Although it can help monitor and protect the application from common web exploits, it's not capable of instrumenting the application. The option that says: Refactor your application to send segment documents directly to X-Ray by using the PutTraceSegments API is incorrect. Although this solution will work, it entails a lot of manual effort to perform. You don't need to do this because you can just install the X-Ray daemon on the instance to automate this process. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ec2.html https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html#xray-daemon-permissions Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/"},{question:"A media company seeks to protect its copyrighted images from unauthorized distribution. They want images uploaded to their Amazon S3 bucket to be automatically watermarked. A developer has already prepared the Lambda function for this image-processing job. Which option must the developer configure to automatically invoke the function at each upload?",answers:[{text:"Enable S3 Storage Lens to monitor the bucket and configure the Lambda function to be invoked whenever the metrics indicate a new object creation.",isCorrect:!1},{text:"Set up an Amazon S3 Event Notification to trigger the Lambda function when an ObjectCreated:Put event is detected in the bucket.",isCorrect:!0},{text:"Use S3 Object Lambda to process images on retrieval and apply watermarks dynamically before the images are served to users.",isCorrect:!1},{text:"Configure an S3 Lifecycle policy to transition images to the INTELLIGENT_TIERING storage class. Use S3 Inventory to generate a report of images that weren't watermarked and set up the Lambda function to process the report.",isCorrect:!1}],explanation:"You can use the Amazon S3 Event Notifications feature to receive notifications when certain events happen in your S3 bucket. To enable notifications, add a notification configuration that identifies the events that you want Amazon S3 to publish. Make sure that it also identifies the destinations where you want Amazon S3 to send the notifications. Amazon S3 can send event notification messages to the following destinations: - Amazon SQS queue - AWS Lambda function - Amazon SNS topic - Amazon EventBridge In the given scenario, you can set up a notification for the ObjectCreated:Put event to immediately trigger a Lambda function when an object is uploaded to the S3 bucket. Hence, the correct answer is: Set up an Amazon S3 Event Notification to trigger the Lambda function when an ObjectCreated:Put event is detected in the bucket. The option that says: Enable S3 Storage Lens to monitor the bucket and configure the Lambda function to be invoked whenever the metrics indicate a new object creation is incorrect. S3 Storage Lens just provide visibility into storage usage and activity trends. It does not trigger actions or Lambda functions based on object operations. The option that says: Configure an S3 Lifecycle policy to transition images to the INTELLIGENT_TIERING storage class. Use S3 Inventory to generate a report of images that weren't watermarked and set up the Lambda function to process the report is incorrect. S3 Lifecycle policies simply manage storage transitions and object expirations, not event-driven actions like invoking Lambda functions upon uploads. Moreover, S3 Inventory just provides object lists and their metadata, but it doesn't automatically invoke Lambda functions upon image uploads. The option that says: Use S3 Object Lambda to process images on retrieval and apply watermarks dynamically before the images are served to users is incorrect because S3 Object Lambda is primarily designed to transform objects at retrieval, not at upload. While it can dynamically apply watermarks, it does so when the object is accessed, not as part of the upload process, which would lead to watermarking every time the image is retrieved rather than just once upon upload. References: https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-event-notifications.html Check out this Amazon S3 Cheat Sheet: https://tutorialsdojo.com/amazon-s3/https://tutorialsdojo.com/amazon-s3/ Check out this blog about S3 Event Notification: https://tutorialsdojo.com/amazon-s3-event-notifications/"},{question:"You were recently hired by a media company that is planning to build a news portal using Elastic Beanstalk and DynamoDB database, which already contains a few data. There is already an existing DynamoDB Table that has an attribute of ArticleName which acts as the partition key and a Category attribute as its sort key. You are instructed to develop a feature that will query the ArticleName attribute but will use a different sort key other than the existing one. The feature also requires strong read consistency to fetch the most up-to-date data. Which of the following solutions should you implement?",answers:[{text:"Create a Global Secondary Index that uses the ArticleName attribute and a different sort key.",isCorrect:!1},{text:"Create a Local Secondary Index that uses the ArticleName attribute and a different sort key.",isCorrect:!1},{text:"Create a new DynamoDB table with a Local Secondary Index that uses the ArticleName attribute with a different sort key. Migrate the data from the existing table to the new table.",isCorrect:!0},{text:"Create a Global Secondary Index which uses the ArticleName attribute and your alternative sort key as projected attributes.",isCorrect:!1}],explanation:"A local secondary index maintains an alternate sort key for a given partition key value. A local secondary index also contains a copy of some or all of the attributes from its base table; you specify which attributes are projected into the local secondary index when you create the table. The data in a local secondary index is organized by the same partition key as the base table, but with a different sort key. This lets you access data items efficiently across this different dimension. For greater query or scan flexibility, you can create up to five local secondary indexes per table. Suppose that an application needs to find all of the threads that have been posted within the last three months. Without a local secondary index, the application would have to Scan the entire Thread table and discard any posts that were not within the specified time frame. With a local secondary index, a Query operation could use LastPostDateTime as a sort key and find the data quickly. To create a Local Secondary Index, make sure that the primary key of the index is the same as the primary key/partition key of the table, just as shown below. Then you must select an alternative sort key which is different from the sort key of the table. When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write operations that were successful. A strongly consistent read might not be available if there is a network delay or outage. Strongly consistent reads are not supported on global secondary indexes. The primary key of a local secondary index must be composite (partition key and sort key). A local secondary index lets you query over a single partition, as specified by the partition key value in the query. Local secondary indexes are created at the same time that you create a table. You cannot add a local secondary index to an existing table, nor can you delete any local secondary indexes that currently exist. Hence, the correct answer in this scenario is to create a new DynamoDB table with a Local Secondary Index that uses the ArticleName attribute with a different sort key then migrate the data from the existing table to the new table. Creating a Global Secondary Index that uses the ArticleName attribute and a different sort key is incorrect because it is stated in the scenario that you are still using the same partition key, but with an alternate sort key that warrants the use of a local secondary index instead of a global secondary index. Creating a Global Secondary Index which uses the ArticleName attribute and your alternative sort key as projected attributes is incorrect because using a local secondary index is a more appropriate solution to be used in this scenario just as explained above. Moreover, projected attributes are just attributes stored in the index that can be returned by queries and scans performed on the index hence, these are not useful in satisfying the provided requirement. Creating a Local Secondary Index that uses the ArticleName attribute and a different sort key is incorrect. Although it uses the correct type of index, you cannot add a local secondary index to an already existing table. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html Global Secondary Index vs. Local Secondary Index: https://tutorialsdojo.com/global-secondary-index-vs-local-secondary-index/ Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"},{question:"You currently have an IAM user for working in the development environment using shell scripts that call the AWS CLI. The EC2 instance that you are using already contains the access key credential set and an IAM role, which are used to run the CLI and access the development environment. You were given a new set of access key credentials with another IAM role that allows you to access and manage the production environment. Which of the following is the EASIEST way to switch from one role to another?",answers:[{text:"Create a new instance profile in the AWS CLI configuration file then append the --profile parameter, along with the new profile name, whenever you run the CLI command.",isCorrect:!1},{text:"Store the production access key credentials set in the instance metadata and call this whenever you need to access the production environment.",isCorrect:!1},{text:"Create a new profile for the role in the AWS CLI configuration file then append the --profile parameter, along with the new profile name, whenever you run the CLI command.",isCorrect:!0},{text:"Store the production access key credentials set in the user data of the instance and call this whenever you need to access the production environment.",isCorrect:!1}],explanation:"Using roles to grant permissions to applications that run on EC2 instances requires a bit of extra configuration. An application running on an EC2 instance is abstracted from AWS by the virtualized operating system. Because of this extra separation, an additional step is needed to assign an AWS role and its associated permissions to an EC2 instance and make them available to its applications. This extra step is the creation of an instance profile that is attached to the instance. The instance profile contains the role and can provide the role's temporary credentials to an application that runs on the instance. Those temporary credentials can then be used in the application's API calls to access resources and to limit access to only those resources that the role specifies. Note that only one role can be assigned to an EC2 instance at a time, and all applications on the instance share the same role and permissions. Using roles in this way has several benefits. Because role credentials are temporary and rotated automatically, you don't have to manage credentials, and you don't have to worry about long-term security risks. In addition, if you use a single role for multiple instances, you can make a change to that one role and the change is propagated automatically to all the instances. Imagine that you have an IAM user for working in the development environment and you occasionally need to work with the production environment at the command line with the AWS CLI. You already have an access key credential set available to you. This can be the access key pair that is assigned to your standard IAM user. Or, if you signed in as a federated user, it can be the access key pair for the role that was initially assigned to you. If your current permissions grant you the ability to assume a specific IAM role, then you can identify that role in a \"profile\" in the AWS CLI configuration files. That command is then run with the permissions of the specified IAM role, not the original identity. Note that when you specify that profile in an AWS CLI command, you are using the new role. In this situation, you cannot make use of your original permissions in the development account at the same time. The reason is that only one set of permissions can be in effect at a time. Hence, the correct answer is to create a new profile for the role in the AWS CLI configuration file then append the --profile parameter, along with the new profile name, whenever you run the CLI command. Storing the production access key credentials set in the instance metadata and calling this whenever you need to access the production environment is incorrect because instance metadata is primarily used to fetch the data about your instance that you can use to configure or manage the running instance. This is not suitable for use in storing the access keys of your AWS CLI. Creating a new instance profile in the AWS CLI configuration file then appending the --profile parameter along with the new profile name whenever you run the CLI command is incorrect because an instance profile is just a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. This is different from an AWS CLI profile, which you can use for switching to various profiles. In addition, an instance profile is associated with the instance and not configured in the AWS CLI. Storing the production access key credentials set in the user data of the instance and calling this whenever you need to access the production environment is incorrect because user data is primarily used to configure an instance during launch, or to run a configuration script. Just like instance metadata, this is not suitable for use in storing the access keys of your AWS CLI. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html Check out this AWS Identity & Access Management (IAM) Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"},{question:"A developer must set up a caching layer in front of the tutorialsdojo database. The developer should come up with a function that ensures cached data is always up-to-date. Stale records in the cache must be automatically deleted as well to prevent the build-up of extra data. Which pseudocode best represents this caching strategy?",answers:[{text:"save_item(item_id, item_value): ttl = 500 cache.set(item_id, item_value, 500) return 'ok'",isCorrect:!1},{text:"save_item(item_id, item_value): tutorialsdojo.query(\"UPDATE Customers SET value = %s WHERE id = %s\", item_value, item_id) cache.delete(item_id) return 'ok'",isCorrect:!1},{text:"save_item(item_id, item_value): ttl = 500 tutorialsdojo.query(\"UPDATE Customers SET value = %s WHERE id = %s\", item_value, item_id) cache.set(item_id, item_value, ttl) return 'ok'",isCorrect:!0},{text:"save_item(item_id, item_value): ttl = 500 tutorialsdojo.query(\"SELECT Customers WHERE id = %s\", item_id) cache.set(item_id, item_value, 500) return 'ok'",isCorrect:!1}],explanation:"The write-through strategy adds or updates data in the cache whenever data is written to the database. With this strategy, the data in the cache is never stale since the data in the cache is updated every time it is written to the database. This is why the data in the cache is always current. One of its disadvantages is that, since most data are never read in the cluster, you are actually wasting resources and disk space. This issue can be rectified by simply adding TTL to minimize wasted space. In the scenario, to implement write-through caching, you must first update the item in the tutorialsdojo database by running an \"UPDATE\" query, then adds the updated item to the cache with a time-to-live (TTL) value of 500 seconds. This ensures that the item is always up to date in both the cache and the database, and it will expire from the cache after 500 seconds. This way, the cache is not cluttered with stale data, and the performance will not degrade over time. Hence, the correct answer is the option that says: save_item(item_id, item_value): ttl = 500 tutorialsdojo.query(\"UPDATE Customers SET value = %s WHERE id = %s\", item_value, item_id) cache.set(item_id, item_value, ttl) return 'ok' The option that says: save_item(item_id, item_value): ttl = 500 tutorialsdojo.query(\"SELECT Customers WHERE id = %s\", item_id) cache.set(item_id, item_value, 500) return 'ok' is an incorrect implementation of write-through caching because the code is not updating the database when a new item is added, or an existing item is updated. Instead, it is making a SELECT query to the database. This means that the primary storage is not being updated with the latest data, and the cache will hold stale data. The option that says: save_item(item_id, item_value): ttl = 500 cache.set(item_id, item_value, 500) return 'ok' does not implement write-through caching because the function only stores the item in the cache and doesn't update the database. The option that says: save_item(item_id, item_value): tutorialsdojo.query(\"UPDATE Customers SET value = %s WHERE id = %s\", item_value, item_id) cache.delete(item_id) return 'ok' is an incorrect implementation of write-through caching because it updates the item in the database, but it also deletes the item from the cache. This means that the cache will not have the updated item, and subsequent requests for the same item will have to be retrieved from the primary database, which is slower than retrieving it from the cache. References: https://aws.amazon.com/caching/best-practices/ https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html Check out this Amazon Elasticache Cheat Sheet: https://tutorialsdojo.com/amazon-elasticache/"},{question:"You are developing an online game where the app preferences and game state of the player must be synchronized across devices. It should also allow multiple users to synchronize and collaborate shared data in real time. Which of the following is the MOST appropriate solution that you should implement in this scenario?",answers:[{text:"Integrate Amazon Pinpoint to your mobile app.",isCorrect:!1},{text:"Integrate AWS Amplify to your mobile app.",isCorrect:!1},{text:"Integrate AWS AppSync to your mobile app.",isCorrect:!0},{text:"Integrate Amazon Cognito Sync to your mobile app.",isCorrect:!1}],explanation:"AWS AppSync simplifies application development by letting you create a flexible API to securely access, manipulate, and combine data from one or more data sources. AppSync is a managed service that uses GraphQL to make it easy for applications to get exactly the data they need. With AppSync, you can build scalable applications, including those requiring real-time updates, on a range of data sources such as NoSQL data stores, relational databases, HTTP APIs, and your custom data sources with AWS Lambda. For mobile and web apps, AppSync additionally provides local data access when devices go offline, and data synchronization with customizable conflict resolution, when they are back online. AWS AppSync is quite similar with Amazon Cognito Sync which is also a service for synchronizing application data across devices. It enables user data like app preferences or game state to be synchronized as well however, the key difference is that, it also extends these capabilities by allowing multiple users to synchronize and collaborate in real time on shared data. Hence, the correct answer is to integrate AWS AppSync to your mobile app. Integrating AWS Amplify to your mobile app is incorrect because this service just makes it easy to create, configure, and implement scalable mobile and web apps powered by AWS. This is primarily used to automate the application release process of both your frontend and backend allowing you to deliver features faster, and not for synchronizing application data across devices. Integrating Amazon Cognito Sync to your mobile app is incorrect. Although this service can also be used in synchronizing application data across devices, it does not allow multiple users to synchronize and collaborate in real-time on shared data, unlike AWS AppSync. Integrating Amazon Pinpoint to your mobile app is incorrect because this service simply allows you to engage with your customers across multiple messaging channels. This is primarily used to send push notifications, emails, SMS text messages, and voice messages. References: https://aws.amazon.com/appsync/ https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html"},{question:"A company is transitioning their systems to AWS due to the limitations of their on-premises data center. As part of this project, a developer was assigned to build a brand new serverless architecture in AWS, which will be composed of AWS Lambda, API Gateway, and DynamoDB in a single stack. She needs a simple and reliable framework that will allow her to share configuration such as memory and timeouts between resources and deploy all related resources together as a single, versioned entity. Which of the following is the MOST appropriate service that the developer should use in this scenario?",answers:[{text:"AWS Systems Manager",isCorrect:!1},{text:"AWS CloudFormation",isCorrect:!1},{text:"Serverless Application Framework",isCorrect:!1},{text:"AWS SAM",isCorrect:!0}],explanation:"The AWS Serverless Application Model (AWS SAM) is an open source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML. AWS SAM is natively supported by AWS CloudFormation and provides a simplified way of defining the Amazon API Gateway APIs, AWS Lambda functions, and Amazon DynamoDB tables needed by your serverless application. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax. Then, CloudFormation provisions your resources with reliable deployment capabilities. Hence, the correct answer is AWS SAM. AWS CloudFormation is incorrect. Although this service can deploy the serverless application to AWS, it is still more appropriate to use AWS SAM instead. AWS SAM can simplify the deployment of the serverless application by deploying all related resources together as a single, versioned entity. AWS Systems Manager is incorrect because it is more focused on management and operations of AWS resources, such as automation, patching, and configuration, but it is not a deployment or application modeling tool. Serverless Application Framework is incorrect. Although it is a well-known framework for building and deploying serverless applications into the AWS cloud, this is not an AWS native solution. It also does not allow configuration of DynamoDB databases or API Gateway APIs, unlike AWS SAM. References:https://aws.amazon.com/serverless/sam/ https://aws.amazon.com/serverless/developer-tools/ https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html Check out this AWS Serverless Application Model Cheat Sheet: https://tutorialsdojo.com/aws-serverless-application-model-sam/"},{question:"A financial mobile application has a serverless backend API which consists of DynamoDB, Lambda, and Cognito. Due to the confidential financial transactions handled by the mobile application, there is a new requirement provided by the company to add a second authentication method that doesn't rely solely on user name and password. Which of the following is the MOST suitable solution that the developer should implement?",answers:[{text:"Create a custom application that integrates with Amazon Cognito which implements the second layer of authentication.",isCorrect:!1},{text:"Use a new IAM policy to a user pool in Cognito.",isCorrect:!1},{text:"Use Cognito with SNS to allow additional authentication via SMS.",isCorrect:!1},{text:"Integrate multi-factor authentication (MFA) to a user pool in Cognito to protect the identity of your users.",isCorrect:!0}],explanation:"You can add multi-factor authentication (MFA) to a user pool to protect the identity of your users. MFA adds a second authentication method that doesn't rely solely on usernames and passwords. You can choose to use SMS text messages, or time-based one-time (TOTP) passwords as second factors in signing in your users. You can also use adaptive authentication with its risk-based model to predict when you might need another authentication factor. It's part of the user pool's advanced security features, which also include protections against compromised credentials. Multi-factor authentication (MFA) increases security for your app by adding another authentication method, and not relying solely on user name and password. You can choose to use SMS text messages, or time-based one-time (TOTP) passwords as second factors in signing in your users. With adaptive authentication, you can configure your user pool to require second-factor authentication in response to an increased risk level. Hence, the correct answer in this scenario is to integrate multi-factor authentication (MFA) to a user pool in Cognito to protect the identity of your users. Creating a custom application that integrates with Amazon Cognito which implements the second layer of authentication is incorrect. Although this option is viable, it is not the most suitable solution in this scenario since you can simply use MFA as a second-factor authentication for the mobile app. Using a new IAM policy to a user pool in Cognito is incorrect because an IAM Policy alone cannot implement a second-factor authentication. You have to configure Cognito to use MFA instead. Using Cognito with SNS to allow additional authentication via SMS is incorrect. Although this is part of the MFA setup, using this solution alone is not enough if you didn't enable MFA in the first place. References: https://docs.aws.amazon.com/cognito/latest/developerguide/managing-security.html https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html Tutorials Dojo's AWS Certified Developer Associate Exam Study Guide: https://tutorialsdojo.com/aws-certified-developer-associate/"},{question:"An application running on an EC2 instance regularly fetches large amounts of data from multiple S3 buckets. A data analysis team will perform ad-hoc queries on the data. To reduce costs and optimize the process, the application requires a solution that can perform serverless queries directly on the data stored in S3 without the need to load it into a database first. Which is the MOST suitable service that will help accomplish this requirement?",answers:[{text:"Amazon EMR",isCorrect:!1},{text:"AWS Step Functions",isCorrect:!1},{text:"Amazon Athena",isCorrect:!0},{text:"Amazon Redshift Spectrum",isCorrect:!1}],explanation:"Amazon Athena is a serverless query service which enables fast analysis of data stored in Amazon S3 using standard SQL. With minimal configuration through the AWS Management Console, Athena allows you to run ad-hoc SQL queries on S3 data and obtain results in seconds. It supports various file formats, including JSON, CSV, ORC, and Parquet. Athena is ideal for ad-hoc queries and operates on a pay-per-query pricing model, making it highly cost-effective for analyzing large datasets without the need to manage any infrastructure. Athena directly addresses the need for querying data in S3 without moving it to a database, which significantly lowers costs and optimizes data analysis processes. Hence, the correct answer is: Amazon Athena. The option that says: Amazon EMR is incorrect because this service is a managed Hadoop framework that helps process large datasets using tools like Apache Spark and Hive. While it can analyze data in S3, it requires setting up clusters, adding infrastructure management, and is not as serverless or cost-efficient as Athena for ad-hoc SQL queries. The option that says: Amazon Redshift Spectrum is incorrect. Although it allows querying S3 data, it requires a Redshift cluster and involves additional setup and cost, making it less ideal for serverless ad-hoc querying. The option that says: AWS Step Functions is incorrect because this service only lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. It doesn't provide a function to do an in-place query to an S3 bucket. References: https://docs.aws.amazon.com/athena/latest/ug/what-is.html https://docs.aws.amazon.com/athena/latest/ug/when-should-i-use-ate.html Check out this Amazon Athena Cheat Sheet: https://tutorialsdojo.com/amazon-athena/"},{question:"A serverless application, which uses a DynamoDB database, is experiencing throttling issues during peak times. To troubleshoot the problem, you were instructed to get the total number of write capacity units consumed for the table and any secondary indexes whenever the UpdateItem operation is sent. In this scenario, what is the MOST appropriate value for the ReturnConsumedCapacity parameter that you should set in the update request?",answers:[{text:"INDEXES",isCorrect:!0},{text:"TRUE",isCorrect:!1},{text:"TOTAL",isCorrect:!1},{text:"NONE",isCorrect:!1}],explanation:"To create, update, or delete an item in a DynamoDB table, use one of the following operations: - PutItem - UpdateItem - DeleteItem For each of these operations, you need to specify the entire primary key, not just part of it. For example, if a table has a composite primary key (partition key and sort key), you must supply a value for the partition key and a value for the sort key. To return the number of write capacity units consumed by any of these operations, set the ReturnConsumedCapacity parameter to one of the following: TOTAL — returns the total number of write capacity units consumed. INDEXES — returns the total number of write capacity units consumed, with subtotals for the table and any secondary indexes that were affected by the operation. NONE — no write capacity details are returned. (This is the default.) Hence, the correct answer is to add the ReturnConsumedCapacity parameter with a value of INDEXES in every update request. Setting the parameter to TRUE is incorrect because the ReturnConsumedCapacity parameter is not a boolean type. The valid values that you can use are TOTAL, INDEXES and NONE only. Setting the parameter to TOTAL is incorrect because this just returns the total number of write capacity units consumed but not the subtotals for the table and any secondary indexes that were affected by the operation. Setting the parameter to NONE is incorrect because this is the default value where no write capacity details are returned. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.WritingData https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_PutItem.html#API_PutItem_RequestParameters Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"},{question:"A developer is planning to use the AWS Elastic Beanstalk console to run the AWS X-Ray daemon on the EC2 instances in her application environment. She will use X-Ray to construct a service map to help identify issues with her application and to provide insight on which application component to optimize. The environment is using a default Elastic Beanstalk instance profile. Which IAM managed policy does Elastic Beanstalk use for the X-Ray daemon to upload data to X-Ray?",answers:[{text:"AWSXRayDaemonWriteAccess",isCorrect:!0},{text:"AWSXRayElasticBeanstalkWriteAccess",isCorrect:!1},{text:"AWSXrayReadOnlyAccess",isCorrect:!1},{text:"AWSXrayFullAccess",isCorrect:!1}],explanation:"You can use AWS Identity and Access Management (IAM) to grant X-Ray permissions to users and compute resources in your account. IAM controls access to the X-Ray service at the API level to enforce permissions uniformly, regardless of which client (console, AWS SDK, AWS CLI) your users employ. To use the X-Ray console to view service maps and segments, you only need read permissions. To enable console access, add the AWSXrayReadOnlyAccess managed policy to your IAM user. For local development and testing, create an IAM user with read and write permissions. Generate access keys for the user and store them in the standard AWS SDK location. You can use these credentials with the X-Ray daemon, the AWS CLI, and the AWS SDK. To deploy your instrumented app to AWS, create an IAM role with write permissions and assign it to the resources running your application. AWSXRayDaemonWriteAccess includes permission to upload traces, and some read permissions as well to support the use of sampling rules. The read and write policies do not include permission to configure encryption key settings and sampling rules. Use AWSXrayFullAccess to access these settings, or add configuration APIs in a custom policy. For encryption and decryption with a customer-managed key that you create, you also need permission to use the key. On supported platforms, you can use a configuration option to run the X-Ray daemon on the instances in your environment. You can enable the daemon in the Elastic Beanstalk console or by using a configuration file. To upload data to X-Ray, the X-Ray daemon requires IAM permissions in the AWSXRayDaemonWriteAccess managed policy. These permissions are included in the Elastic Beanstalk instance profile. Hence, the correct answer is the AWSXRayDaemonWriteAccess managed policy. AWSXrayReadOnlyAccess is incorrect because this policy is primarily used if you just want a read-only access to X-Ray. AWSXrayFullAccess is incorrect. Although this can provide the required access to the daemon, this is not being used in Elastic Beanstalk as it does not abide by the standard security advice of granting the least privilege. AWSXRayElasticBeanstalkWriteAccess is incorrect because this is not an available managed policy. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-debugging.html https://docs.aws.amazon.com/xray/latest/devguide/xray-permissions.html https://docs.aws.amazon.com/xray/latest/devguide/security.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/ Instrumenting your Application with AWS X-Ray: https://tutorialsdojo.com/instrumenting-your-application-with-aws-x-ray/"},{question:"Your Lambda function initializes a lot of external dependencies such as database connections and HTTP endpoints, which are required for data processing. It also fetches static data with a size of 20 MB from a third-party provider over the Internet every time the function is invoked. This adds significant time in the total processing, which greatly affects the performance of their serverless application. Which of the following should you do to improve the performance of your function?",answers:[{text:"Increase the CPU allocation of the function by submitting a service limit increase ticket to AWS.",isCorrect:!1},{text:"Place the database and HTTP initialization logic outside the Lambda function handler and store the external files in the /tmp directory.",isCorrect:!0},{text:"Allocate more memory to your function.",isCorrect:!1},{text:"Use unreserved concurrency for your function.",isCorrect:!1}],explanation:'When AWS Lambda executes your Lambda function, it provisions and manages the resources needed to run your Lambda function. When you create a Lambda function, you specify configuration information, such as the amount of memory and maximum execution time that you want to allow for your Lambda function. When a Lambda function is invoked, AWS Lambda launches an execution context based on the configuration settings you provide. The execution context is a temporary runtime environment that initializes any external dependencies of your Lambda function code, such as database connections or HTTP endpoints. This affords subsequent invocations better performance because there is no need to "cold-start" or initialize those external dependencies, as explained below. It takes time to set up an execution context and do the necessary "bootstrapping", which adds some latency each time the Lambda function is invoked. You typically see this latency when a Lambda function is invoked for the first time or after it has been updated because AWS Lambda tries to reuse the execution context for subsequent invocations of the Lambda function. After a Lambda function is executed, AWS Lambda maintains the execution context for some time in anticipation of another Lambda function invocation. In effect, the service freezes the execution context after a Lambda function completes and thaws the context for reuse if AWS Lambda chooses to reuse the context when the Lambda function is invoked again. Each execution context provides 512 MB - 10,240 MB of additional disk space in the /tmp directory. The directory content remains when the execution context is frozen, providing a transient cache that can be used for multiple invocations. You can add extra code to check if the cache has the data that you stored. Hence, the correct answer in this scenario is: Place the database and HTTP initialization logic outside the Lambda function handler and store the external files in the /tmp directory. The option that says: Increase the CPU allocation of the function by submitting a service limit increase ticket to AWS is incorrect because, in the first place, you cannot do that in AWS. In the AWS Lambda resource model, you choose the amount of memory you want for your function, which will then automatically allocate proportional CPU power to your function. An increase in memory size triggers an equivalent increase in CPU available to your function. This is the proper way to increase the CPU allocation and not by submitting a support ticket. In addition, the root cause of this issue is not the CPU nor the memory, but the 20 MB file that is always downloaded by your function. The option that says: Allocate more memory to your function is incorrect because this will just increase the amount of memory available to the function during execution and not solve the underlying issue. The actual processing time may be reduced by having more memory but there is still a lot of time wasted in downloading the 20 MB file every time the function is invoked. The option that says: Use unreserved concurrency for your function is incorrect because the issue does not relate to concurrency. Just as mentioned above, the root cause is that the function downloads a large file every time it is invoked, which causes significant delays and time-outs. References: https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html#function-configuration Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/'},{question:"A developer has recently deployed an application, which is hosted in an Auto Scaling group of EC2 instances and processes data from an Amazon Kinesis Data Stream. Each of the EC2 instances has exactly one KCL worker processing one Kinesis data stream which has 10 shards. Due to performance issues, the systems operations team has resharded the data stream to increase the number of open shards to 20. What is the maximum number of running EC2 instances that should ideally be kept to maintain application performance?",answers:[{text:"40",isCorrect:!1},{text:"10",isCorrect:!1},{text:"20",isCorrect:!0},{text:"30",isCorrect:!1}],explanation:"Resharding enables you to increase or decrease the number of shards in a stream in order to adapt to changes in the rate of data flowing through the stream. The Kinesis Client Library (KCL) ensures that for every shard, there is a record processor running and processing that shard. It also tracks the shards in the stream using an Amazon DynamoDB table. Typically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances. To scale up processing in your application, you should test a combination of these approaches: - Increasing the instance size (because all record processors run in parallel within a process) - Increasing the number of instances up to the maximum number of open shards (because shards can be processed independently) - Increasing the number of shards (which increases the level of parallelism) Thus, the maximum number of instances you can launch is 20, to match the number of open shards with a ratio of 1:1. Although you can launch 10 instances in which each instance handles 2 shards, this is not the maximum number of instances you can deploy for your application. Hence, this option is incorrect. Take note that the maximum number of your instances is not half the number of open shards. Launching 30 or 40 instances is incorrect because you should ensure that the number of instances does not exceed the number of open shards. The maximum number of instances that you should deploy is 20. References: https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-scaling.html Check out this Amazon Kinesis Cheat Sheet: https://tutorialsdojo.com/amazon-kinesis/"},{question:"A developer has recently launched a new API Gateway service which is integrated with AWS Lambda. He enabled API caching and per-key cache invalidation features in the API Gateway to comply with the requirement of the front-end development team which will use the API. The front-end team will have to invalidate an existing cache entry in some scenarios and fetch the latest data from the integration endpoint. Which of the following should the consumers of the API do to invalidate the cache in API Gateway?",answers:[{text:"Send a request with the Cache-Control: no-cache header.",isCorrect:!1},{text:"Send a request with the Cache-Control: INVALIDATE_CACHE header.",isCorrect:!1},{text:"Configure the front-end application to clear the browser cache before fetching data from API Gateway.",isCorrect:!1},{text:"Send a request with the Cache-Control: max-age=0 header.",isCorrect:!0}],explanation:"A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the Cache-Control: max-age=0 header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint. Ticking the Require authorization checkbox ensures that not every client can invalidate the API cache. If most or all of the clients invalidate the API cache, this could significantly increase the latency of your API. Hence, to only allow authorized clients to invalidate an API Gateway cache entry when submitting API requests, you can just send a request with the Cache-Control: max-age=0 header. Sending a request with the Cache-Control: no-cache header is incorrect because you have to use value of the max-age directive in API Gateway instead of the no-cache directive. This just forces the cache to submit the request to the origin server for validation before releasing a cached copy. Configuring the frontend application to clear the browser cache before fetching data from API Gateway is incorrect because the browser cache and the API Gateway cache are not connected with each other. The correct method of invalidating the cache is to add the Cache-Control: max-age=0 header. Sending a request with the Cache-Control: INVALIDATE_CACHE header is incorrect because there is no directive called INVALIDATE_CACHE in the Cache-Control header. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching https://aws.amazon.com/api-gateway/faqs/#Throttling_and_Caching Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"You are a developer for a global technology company, which heavily uses AWS with regional offices in San Francisco, Manila, and Bangalore. Most of the clients of your company are using serverless computing in which you are responsible for ensuring that their applications are working efficiently. Which of the following options are valid considerations in improving the performance of your Lambda function? (Select TWO.)",answers:[{text:"The concurrent execution limit is enforced against the sum of the concurrent executions of all function.",isCorrect:!0},{text:"An increase in memory size triggers an equivalent increase in CPU available to your function.",isCorrect:!0},{text:"Lambda automatically creates Elastic IP's that enable your function to connect securely to other resources within your private VPC.",isCorrect:!1},{text:"You have to install the X-Ray daemon in Lambda to enable active tracing.",isCorrect:!1},{text:"You can throttle all incoming executions and stop processing any invocations to your function by setting concurrency to false.",isCorrect:!1}],explanation:"You can use the AWS Lambda API or console to configure settings on your Lambda functions. Basic function settings include the description, role, and runtime that you specify when you create a function in the Lambda console. You can configure more settings after you create a function, or use the API to set things like the handler name, memory allocation, and security groups during creation. Lambda counts a request each time it starts executing in response to an event notification or invocation call, including test invokes from the console. You are charged based on the total number of requests processed across all of your Lambda functions. Duration is calculated from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 1ms. The price depends on the amount of memory you allocate to your function. In the AWS Lambda resource model, you choose the amount of memory you want for your function and are allocated proportional CPU power and other resources. An increase in memory size triggers an equivalent increase in CPU available to your function. The unit of scale for AWS Lambda is a concurrent execution. However, scaling indefinitely is not desirable in all scenarios. For example, you may want to control your concurrency for cost reasons or to regulate how long it takes you to process a batch of events, or to simply match it with a downstream resource. To assist with this, Lambda provides a concurrent execution limit control at both the account level and the function level. Hence, the valid considerations in improving the performance of Lambda functions are: - An increase in memory size triggers an equivalent increase in CPU available to your function. - The concurrent execution limit is enforced against the sum of the concurrent executions of all functions. The option that says: You have to install the X-Ray daemon in Lambda to enable active tracing is incorrect because you only have to install the X-Ray daemon if you are using Elastic Beanstalk, ECS, or EC2 instances. You simply need to tick the Enable AWS X-Ray checkbox in the Lambda function to enable active tracing. The option that says: Lambda automatically creates Elastic IPs that enable your function to connect securely to other resources within your private VPC is incorrect because Lambda actually creates ENI (Elastic Network Interface) and not Elastic IPs if the function is connected to your VPC. The option that says: You can throttle all incoming executions and stop processing any invocations to your function by setting concurrency to false is incorrect because the concurrency setting is not a boolean type which is why setting it as false is invalid. To throttle all incoming executions, you can manually set the concurrency to 0 or just click the 'Throttle' button in the Lambda console. References: https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html https://aws.amazon.com/lambda/pricing/ https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A product design firm has adopted a remote work policy and wants to provide employees with access to a suite of CAD software through EC2 Spot instances. These instances will be deployed using a CloudFormation template. The development team must be able to securely obtain software license keys in the template each time it is needed. Which solution meets this requirement while offering the most secure and cost-effective approach?",answers:[{text:"Store the license key as a SecureString in AWS Systems Manager (SSM) Parameter Store. Use the ssm-secure dynamic reference to retrieve the secret in the CloudFormation template.",isCorrect:!0},{text:"Embed the license keys in the Mapping section of the CloudFormation template. Let users choose the correct license key using the Parameter section. Enable the NoEcho attribute on the parameter.",isCorrect:!1},{text:"Pass the license key in the Parameter section of the CloudFormation template during stack creation. Enable the NoEcho attribute on the parameter.",isCorrect:!1},{text:"Store the license key as a secret in AWS Secrets Manager. Use the secretsmanager dynamic reference to retrieve the secret in the CloudFormation template.",isCorrect:!1}],explanation:"Dynamic references provide a compact, powerful way for you to reference external values that are stored and managed in other services, such as the AWS Systems Manager Parameter Store or AWS Secrets Manager. When you use a dynamic reference, CloudFormation retrieves the value of the specified reference when necessary during stack and change set operations and passes the value to the appropriate resource. CloudFormation does not store the actual reference value. The following snippet shows how you can use the ssm-secure dynamic reference to retrieve an IAM user's password from the Parameter Store for console login. IAMUserPassword pertains to the parameter name followed by the version number. MyIAMUser: Type: AWS::IAM::User Properties: UserName: 'MyUserName' LoginProfile: Password: '{{resolve:ssm-secure:IAMUserPassword:10}}' In the scenario, storing the license key as SecureString means encrypting it using a KMS key, making it more secure than storing it in plaintext. It's also more cost-effective than Secrets Manager since you don't pay for the number of parameters you store in the Parameter Store (Standard tier). Hence, the correct answer is: Store the license key as a SecureString in AWS Systems Manager (SSM) Parameter Store. Use the ssm-secure dynamic reference to retrieve the secret in the CloudFormation template. The option that says: Pass the license key in the Parameter section of the CloudFormation template during stack creation. Enable the NoEcho attribute on the parameter is incorrect. Although using the NoEcho attribute can help prevent the license key from being displayed in plaintext in the CloudFormation logs and console outputs, The option that says: Store the license key as a secret in AWS Secrets Manager. Use the secretsmanager dynamic reference to retrieve the secret in the CloudFormation template is incorrect. Although using Secrets Manager is a valid approach, it's less cost-effective compared to using SSM Parameter Store. With Secrets Manager, there is a monthly cost associated with storing secrets, whereas SSM Parameter Store (Standard tier) is free of charge. The option that says: Embed the license keys in the Mapping section of the CloudFormation template. Let users choose the correct license key using the Parameter section. Enable the NoEcho attribute on the parameter is incorrect. Embedding sensitive data, such as license keys, within CloudFormation templates poses a security risk, as the data can be viewed by anyone with access to the template. References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/security-best-practices.html#creds https://catalog.workshops.aws/cfn101/en-US/intermediate/templates/dynamic-references Check out this AWS CloudFormation cheat sheet: https://tutorialsdojo.com/aws-cloudformation/"},{question:"You are planning to launch a Lambda function integrated with API Gateway. It is required to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. Which of the following options is the MOST appropriate method use to meet this requirement?",answers:[{text:"HTTP Proxy integration",isCorrect:!1},{text:"HTTP custom integration",isCorrect:!1},{text:"Lambda custom integration",isCorrect:!0},{text:"Lambda proxy integration",isCorrect:!1}],explanation:"You choose an API integration type according to the types of integration endpoint you work with and how you want data to pass to and from the integration endpoint. For a Lambda function, you can have two types of integration: - Lambda proxy integration - Lambda custom integration In Lambda proxy integration, the setup is simple. If your API does not require content encoding or caching, you only need to set the integration's HTTP method to POST, the integration endpoint URI to the ARN of the Lambda function invocation action of a specific Lambda function, and the credential to an IAM role with permissions to allow API Gateway to call the Lambda function on your behalf. In Lambda non-proxy (or custom) integration, in addition to the proxy integration setup steps, you also specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. For an AWS service action, you have the AWS integration of the non-proxy type only. API Gateway also supports the mock integration, where API Gateway serves as an integration endpoint to respond to a method request. The Lambda custom integration is a special case of the AWS integration, where the integration endpoint corresponds to the function-invoking action of the Lambda service. The Lambda custom integration is a type of integration that lets an API expose AWS service actions. In AWS integration, you must configure both the integration request and integration response and set up necessary data mappings from the method request to the integration request, and from the integration response to the method response. To configure your API Gateway with this type of configuration, you have to set the resource with an AWS integration type. Hence, Lambda custom integration is correct as it matches the description depicted in the scenario. Lambda proxy integration is incorrect as this type of integration is the one where you do not have to configure both the integration request and integration response. HTTP custom integration is incorrect because this type is only used where you need to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. Take note that the scenario uses an application hosted in Lambda which is why you have to use Lambda integration instead. HTTP proxy integration is incorrect because the scenario uses an application hosted in Lambda which is why you have to use Lambda integration instead. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-simple-proxy-for-lambda-input-format Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A software engineer is building a serverless application in AWS consisting of Lambda, API Gateway, and DynamoDB. She needs to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML to determine the caller's identity. Which of the features of API Gateway is the MOST suitable one that she should use to build this feature?",answers:[{text:"Cross-Origin Resource Sharing (CORS)",isCorrect:!1},{text:"Resource Policy",isCorrect:!1},{text:"Cross-Account Lambda Authorizer",isCorrect:!1},{text:"Lambda Authorizers",isCorrect:!0}],explanation:"A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. When a client makes a request to one of your API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output. There are two types of Lambda authorizers: - A token-based Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token. - A request parameter-based Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of headers, query string parameters, stageVariables, and $context variables. It is possible to use an AWS Lambda function from an AWS account that is different from the one in which you created your Lambda authorizer function by using a Cross-Account Lambda Authorizer. Therefore, the correct answer in this scenario is to use Lambda Authorizers since this feature is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML. Resource Policy is incorrect because this is simply a JSON policy document that you attach to an API to control whether a specified principal (typically an IAM user or role) can invoke the API. This can't be used to implement a custom authorization scheme. Cross-Origin Resource Sharing (CORS) is incorrect because this just defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. Cross-Account Lambda Authorizer is incorrect because this just enables you to use an AWS Lambda function from a different AWS account as your API authorizer function. Moreover, this is not a valid Lambda authorizer type. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-lambda-authorizer-input.html https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-lambda-authorizer-cross-account-lambda-authorizer.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A company has an application hosted in an On-Demand EC2 instance in your VPC. The developer has been instructed to create a shell script that fetches the instance's associated public and private IP addresses. What should the developer do to complete this task?",answers:[{text:"Get the public and private IP addresses from the instance user data service using the http://169.254.169.254/latest/userdata/ endpoint.",isCorrect:!1},{text:"Get the public and private IP addresses from the instance metadata service using the http://169.254.169.254/latest/meta-data/ endpoint.",isCorrect:!0},{text:"Get the public and private IP addresses from Amazon CloudWatch.",isCorrect:!1},{text:"Get the public and private IP addresses from AWS CloudTrail.",isCorrect:!1}],explanation:"Instance metadata is data about your EC2 instance that you can use to configure or manage the running instance. Because your instance metadata is available from your running instance, you do not need to use the Amazon EC2 console or the AWS CLI. This can be helpful when you're writing scripts to run from your instance. For example, you can access the local IP address of your instance from instance metadata to manage a connection to an external application. To view the private IPv4 address, public IPv4 address, and all other categories of instance metadata from within a running instance, use the following URL: http://169.254.169.254/latest/meta-data/. Hence, the correct answer is: Get the public and private IP addresses from the instance metadata service using the http://169.254.169.254/latest/meta-data/ endpoint. The option that says: Get the public and private IP addresses from Amazon CloudWatch is incorrect because there is no direct way to fetch the public and private IP addresses of the EC2 instance using CloudWatch. The option that says: Get the public and private IP addresses from AWS CloudTrail is incorrect because CloudTrail is primarily used to track the API activity of each AWS service. Just like CloudWatch, there is no easy way to get the associated IP addresses of the EC2 instance using CloudTrail. The option that says: Get the public and private IP addresses from the instance user data service using the http://169.254.169.254/latest/userdata/ endpoint is incorrect because a user data is mainly used to perform common automated configuration tasks and run scripts after the instance starts. You will not find the associated IP addresses of the EC2 instance from its user data. You have to use the metadata service instead. References: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-identity-documents.html Check out this Amazon EC2 Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/"},{question:"A developer wants to perform additional processing on newly inserted items in Amazon DynamoDB using AWS Lambda. In order to implement this requirement, the developer will have to use DynamoDB Streams to automatically send the new items in the table to a Lambda function for processing. Given the scenario, what steps should be performed by the developer to integrate his/her DynamoDB to his/her Lambda functions? (Select TWO.)",answers:[{text:"Create an event source mapping in Lambda to send records from your stream to a Lambda function.",isCorrect:!0},{text:"Select AWSLambdaBasicExecutionRole managed policy as the function's execution role.",isCorrect:!1},{text:"Select AWSLambdaDynamoDBExecutionRole managed policy as the function's execution role.",isCorrect:!0},{text:"Create an SNS topic to capture new records from DynamoDB.",isCorrect:!1},{text:"Create a trigger for a Firehose stream that uses a Lambda function for data processing.",isCorrect:!1}],explanation:"You can use an AWS Lambda function to process records in an Amazon DynamoDB Streams stream. With DynamoDB Streams, you can trigger a Lambda function to perform additional work each time a DynamoDB table is updated. You need to create an event source mapping to tell Lambda to send records from your stream to a Lambda function. You can create multiple event source mappings to process the same data with multiple Lambda functions, or process items from multiple streams with a single function. To configure your function to read from DynamoDB Streams in the Lambda console, create a DynamoDB trigger. You also need to assign the following permissions to Lambda: dynamodb:DescribeStream dynamodb:GetRecords dynamodb:GetShardIterator dynamodb:ListStreams The AWSLambdaDynamoDBExecutionRole managed policy already includes these permissions. Hence, the correct answers are: - Create an event source mapping in Lambda to send records from your stream to a Lambda function. - Select AWSLambdaDynamoDBExecutionRole managed policy as the function's execution role. The option that says: Create an SNS topic to capture new records from DynamoDB is incorrect as there is no need to do this. Your Lambda triggers should be able to catch the events from DynamoDB Streams. The option that says: Select AWSLambdaBasicExecutionRole managed policy with full access to DynamoDB as the function's execution role is incorrect because it lacks the necessary permissions. This role only provides Lambda permissions to upload logs to CloudWatch. The option that says: Create a trigger for a Firehose stream that uses a Lambda function for data processing is incorrect. You can only use Lambda functions as triggers for DynamoDB Streams. References: https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/ AWS Lambda Integration with Amazon DynamoDB Streams: https://tutorialsdojo.com/aws-lambda-integration-with-amazon-dynamodb-streams/"},{question:"You are working as an IT Consultant for a top investment bank in Europe which uses several serverless applications in their AWS account. They just launched a new API Gateway service with a Lambda proxy integration and you were instructed to test out the new API. However, you are getting a Connection refused error whenever you use this Invoke URL http://779protaw8.execute-api.us-east-1.amazonaws.com/tutorialsdojo/ of the API Gateway. Which of the following is the MOST likely cause of this issue?",answers:[{text:"You are not using HTTP/2 in invoking the API.",isCorrect:!1},{text:"You are not using FTP in invoking the API.",isCorrect:!1},{text:"You are not using HTTPS in invoking the API.",isCorrect:!0},{text:"You are not using WebSocket in invoking the API.",isCorrect:!1}],explanation:"All of the APIs created with Amazon API Gateway expose HTTPS endpoints only. Amazon API Gateway does not support unencrypted (HTTP) endpoints. By default, Amazon API Gateway assigns an internal domain to the API that automatically uses the Amazon API Gateway certificate. When configuring your APIs to run under a custom domain name, you can provide your own certificate for the domain. Calling a deployed API involves submitting requests to the URL for the API Gateway component service for API execution, known as execute-api. The base URL for REST APIs is in the following format: https://{restapi_id}.execute-api.{region}.amazonaws.com/{stage_name}/ where {restapi_id} is the API identifier, {region} is the region, and {stage_name} is the stage name of the API deployment. Hence, the most likely cause of the issue in the scenario is that you are not using HTTPS in invoking the API. The option that says: you are not using HTTP/2 in invoking the API is incorrect because API Gateway only supports HTTPS. The option that says: you are not using FTP in invoking the API is incorrect because API Gateway is using HTTPS to expose the APIs. FTP is primarily used for accessing file servers and not Web APIs. The option that says: you are not using WebSocket in invoking the API is incorrect because all of the APIs created with Amazon API Gateway expose HTTPS endpoints only. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-call-api.html https://aws.amazon.com/api-gateway/faqs/ Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A company has an application hosted in an ECS Cluster that heavily uses an RDS database. A developer needs to closely monitor how the different processes on a DB instance use the CPU, such as the percentage of the CPU bandwidth or the total memory consumed by each process to ensure application performance. Which of the following is the MOST suitable solution that the developer should implement?",answers:[{text:"Use CloudWatch to track the CPU Utilization of your database.",isCorrect:!1},{text:"Use Enhanced Monitoring in RDS.",isCorrect:!0},{text:"Develop a shell script that collects and publishes custom metrics to CloudWatch which tracks the real-time CPU Utilization of the RDS instance.",isCorrect:!1},{text:"Track the CPU% and MEM% metrics which are readily available in the Amazon RDS console.",isCorrect:!1}],explanation:"Amazon RDS provides metrics in real time for the operating system (OS) that your DB instance runs on. You can view the metrics for your DB instance using the console or consume the Enhanced Monitoring JSON output from CloudWatch Logs in a monitoring system of your choice. By default, Enhanced Monitoring metrics are stored in the CloudWatch Logs for 30 days. To modify the amount of time the metrics are stored in the CloudWatch Logs, change the retention for the RDSOSMetrics log group in the CloudWatch console. Take note that there are certain differences between CloudWatch and Enhanced Monitoring Metrics. CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance, and Enhanced Monitoring gathers its metrics from an agent on the instance. As a result, you might find differences between the measurements, because the hypervisor layer performs a small amount of work. The differences can be greater if your DB instances use smaller instance classes because then there are likely more virtual machines (VMs) that are managed by the hypervisor layer on a single physical instance. Enhanced Monitoring metrics are useful when you want to see how different processes or threads on a DB instance use the CPU. Hence, the correct answer is to use Enhanced Monitoring in RDS. Developing a shell script that collects and publishes custom metrics to CloudWatch which tracks the real-time CPU Utilization of the RDS instance is incorrect. Although you can use Amazon CloudWatch Logs and CloudWatch dashboard to monitor the CPU Utilization of the database instance, using CloudWatch alone is still not enough to get the specific percentage of the CPU bandwidth and total memory consumed by each database process. The data provided by CloudWatch is not as detailed as compared with the Enhanced Monitoring feature in RDS. Take note as well that you do not have direct access to the instances/servers of your RDS database instance, unlike with your EC2 instances where you can install a CloudWatch agent or a custom script to get CPU and memory utilization of your instance. Using CloudWatch to track the CPU Utilization of your database is incorrect. Although you can use Amazon CloudWatch to monitor the CPU Utilization of your database instance, it does not provide the percentage of the CPU bandwidth and total memory consumed by each database process in your RDS instance. Take note that CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance, while RDS Enhanced Monitoring gathers its metrics from an agent on the instance. Tracking the CPU% and MEM% metrics which are readily available in the Amazon RDS console is incorrect because these metrics are not readily available in the Amazon RDS console, which is contrary to what is being stated in this option. References: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#USER_Monitoring.OS.CloudWatchLogs https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html#monitoring-cloudwatch Check out these Amazon CloudWatch and RDS Cheat Sheets: https://tutorialsdojo.com/amazon-cloudwatch/ https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/"},{question:"A developer recently deployed a serverless application, which consists of a Lambda function, API Gateway, and DynamoDB using the sam deploy CLI command. The Lambda function is invoked through the API Gateway and then processes and stores the data in a DynamoDB table with an average time of 20 minutes. However, the IT Support team noticed that there are several terminated Lambda invocations that happen every day, which is causing data discrepancies. Which of the following options is the MOST likely root cause of this problem?",answers:[{text:"The concurrent execution limit has been reached.",isCorrect:!1},{text:"The Lambda function contains a recursive code and has been running for over 15 minutes.",isCorrect:!1},{text:"The serverless application should be deployed using the sam publish CLI command instead.",isCorrect:!1},{text:"The failed Lambda invocations have been running for over 15 minutes and reached the maximum execution time.",isCorrect:!0}],explanation:"A Lambda function consists of code and any associated dependencies. In addition, a Lambda function also has configuration information associated with it. Initially, you specify the configuration information when you create a Lambda function. Lambda provides an API for you to update some of the configuration data. You pay for the AWS resources that are used to run your Lambda function. To prevent your Lambda function from running indefinitely, you specify a timeout. When the specified timeout is reached, AWS Lambda terminates execution of your Lambda function. It is recommended that you set this value based on your expected execution time. Take note that you can invoke a Lambda function synchronously either by calling the Invoke operation or by using an AWS SDK in your preferred runtime. If you anticipate a long-running Lambda function, your client may time out before function execution completes. To avoid this, update the client timeout or your SDK configuration. The default timeout is 3 seconds and the maximum execution duration per request in AWS Lambda is 900 seconds, which is equivalent to 15 minutes. Hence, the most likely root cause in this scenario is that the failed Lambda invocations have been running for over 15 minutes and reached the maximum execution time. The option that says: The serverless application should be deployed using the sam publish CLI command instead is incorrect as this CLI command just publishes an AWS SAM application to the AWS Serverless Application Repository. The fact that some invocations of the Lambda function work fine means that the deployment is successful. Hence, there is no issue on the deployment process of the serverless application but only on its maximum execution time. The option that says: The concurrent execution limit has been reached is incorrect because, by default, the AWS Lambda limits the total concurrent executions across all functions within a given region to 1000. By setting a concurrency limit on a function, Lambda guarantees that allocation will be applied specifically to that function, regardless of the amount of traffic processing the remaining functions. If that limit is exceeded, the function will be throttled but not terminated, which is in contrast with what is happening in the scenario. The option that says: The Lambda function contains a recursive code and has been running for over 15 minutes is incorrect because having a recursive code in your Lambda function does not directly result to an abrupt termination of the function execution. This is a scenario wherein the function automatically calls itself until some arbitrary criteria is met. This could lead to an unintended volume of function invocations and escalated costs, but not an abrupt termination because Lambda will throttle all invocations to the function. Reference: https://docs.aws.amazon.com/lambda/latest/dg/limits.html https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A web application hosted in Elastic Beanstalk has a configuration file named .ebextensions/debugging.config which has the following content: option_settings: aws:elasticbeanstalk:xray: XRayEnabled: true For its database tier, it uses RDS with Multi-AZ deployments configuration and Read Replicas. There is a new requirement to record calls that your application makes to RDS and other internal or external HTTP web APIs. The tracing information should also include the actual SQL database queries sent by the application, which can be searched using the filter expressions in the X-Ray Console. Which of the following should you do to satisfy the above task?",answers:[{text:"Add annotations in the subsegment section of the segment document.",isCorrect:!0},{text:"Add metadata in the subsegment section of the segment document.",isCorrect:!1},{text:"Add annotations in the segment document.",isCorrect:!1},{text:"Add metadata in the segment document.",isCorrect:!1}],explanation:"Even with sampling, a complex application generates a lot of data. The AWS X-Ray console provides an easy-to-navigate view of the service graph. It shows health and performance information that helps you identify issues and opportunities for optimization in your application. For advanced tracing, you can drill down to traces for individual requests, or use filter expressions to find traces related to specific paths or users. When you instrument your application, the X-Ray SDK records information about incoming and outgoing requests, the AWS resources used, and the application itself. You can add other information to the segment document as annotations and metadata. Annotations are simple key-value pairs that are indexed for use with filter expressions. Use annotations to record data that you want to use to group traces in the console or when calling the GetTraceSummaries API. X-Ray indexes up to 50 annotations per trace. Metadata are key-value pairs with values of any type, including objects and lists, but that is not indexed. Use metadata to record data you want to store in the trace but don't need to use for searching traces. You can view annotations and metadata in the segment or subsegment details in the X-Ray console. A trace segment is a JSON representation of a request that your application serves. A trace segment records information about the original request, information about the work that your application does locally, and subsegments with information about downstream calls that your application makes to AWS resources, HTTP APIs, and SQL databases. Hence, adding annotations in the subsegment section of the segment document is the correct answer. Adding annotations in the segment document is incorrect. Although the use of annotations is correct, you have to add this in the subsegment section of the segment document since you want to trace the downstream call to RDS and not the actual request to your application. Adding metadata in the segment document is incorrect because metadata is primarily used to record custom data that you want to store in the trace but not for searching traces since this can't be picked up by filter expressions in the X-Ray Console. You have to use annotations instead. In addition, you have to add this in the subsegment section of the segment document since you want to trace the downstream call to RDS and not the actual request to your application. Adding metadata in the subsegment section of the segment document is incorrect because, just as mentioned above, metadata is just used to record custom data that you want to store in the trace but not for searching traces. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-annotations https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/"},{question:"You are designing the DynamoDB table that will be used by your Node.js application. It will have to handle 10 writes per second and then 20 eventually consistent reads per second where all the items have a size of 2 KB for both operations. Which of the following are the most optimal WCU and RCU that you should provision to the table?",answers:[{text:"20 RCU and 20 WCU",isCorrect:!1},{text:"10 RCU and 20 WCU",isCorrect:!0},{text:"40 RCU and 40 WCU",isCorrect:!1},{text:"40 RCU and 20 WCU",isCorrect:!1}],explanation:"When you create a new provisioned table in DynamoDB, you must specify its provisioned throughput capacity—the amount of read and write activity that the table will be able to support. DynamoDB uses this information to reserve sufficient system resources to meet your throughput requirements. You can optionally allow DynamoDB auto-scaling to manage your table's throughput capacity. However, you still must provide initial settings for read and write capacity when you create the table. DynamoDB auto scaling uses these initial settings as a starting point and then adjusts them dynamically in response to your application's requirements. You specify throughput requirements in terms of capacity units—the amount of data your application needs to read or write per second. You can modify these settings later, if needed, or enable DynamoDB auto-scaling to modify them automatically. 1 WCU can do 1 write per second for an item up to 1KB. To get the required WCU, simply multiply the given average item size by the required writes per second. In the scenario, the DynamoDB table is expected to perform 10 writes per second of a 2KB item. Multiplying 10 by 2 gives 20 WCU. 1 RCU can do 1 strongly consistent read or 2 eventually consistent reads for an item up to 4KB. To get the RCU with eventually consistent reads, do the following steps: Step #1 Divide the average item size by 4 KB. Round up the result Average Item Size = 2 KB = 2KB/4KB = 0.5 ≈ 1 Step #2 Multiply the number of reads per second by the resulting value from Step 1. Divide the product by 2 for eventually consistent reads. = 20 reads per second x 1 = 20 RCU Since the type of read being asked is eventually consistent, we get half of 20, which is 10. = 20/2 = 10 RCU Hence, the correct answer is to provision 10 RCU and 20 WCU to your DynamoDB table. The 20 RCU and 20 WCU setting is incorrect because this would be the result if you use strong consistency reads. Remember that the scenario explicitly said that eventual consistency reads would be used. The 40 RCU and 20 WCU is incorrect because 40 RCU is overkill for the required eventual consistency reads. If the scenario was asking for transactional read requests, then this option could have been correct. The 40 RCU and 40 WCU setting is incorrect because this would be the result if you chose transactional requests both on your reads and writes. Take note that the scenario didn't say that the database is using DynamoDB Transactions. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html#ItemSizeCalculations.Writes https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ Calculating the Required Read and Write Capacity Unit for your DynamoDB Table: https://tutorialsdojo.com/calculating-the-required-read-and-write-capacity-unit-for-your-dynamodb-table/"},{question:"You are a software developer for a multinational investment bank which has a hybrid cloud architecture with AWS. To improve the security of their applications, they decided to use AWS Key Management Service (KMS) to create and manage their encryption keys across a wide range of AWS services. You were given the responsibility to integrate AWS KMS with the financial applications of the company. Which of the following are the recommended steps to locally encrypt data using AWS KMS that you should follow? (Select TWO.)",answers:[{text:"Erase the encrypted data key from memory and store the plaintext data key alongside the locally encrypted data.",isCorrect:!1},{text:"Use the GenerateDataKey operation to get a data encryption key then use the plaintext data key in the response to encrypt data locally.",isCorrect:!0},{text:"Encrypt data locally using the Encrypt operation.",isCorrect:!1},{text:"Erase the plaintext data key from memory and store the encrypted data key alongside the locally encrypted data.",isCorrect:!0},{text:"Use the GenerateDataKeyWithoutPlaintext operation to get a data encryption key then use the plaintext data key in the response to encrypt data locally.",isCorrect:!1}],explanation:"When you encrypt your data, your data is protected, but you have to protect your encryption key. One strategy is to encrypt it. Envelope encryption is the practice of encrypting plaintext data with a data key, and then encrypting the data key under another key. You can even encrypt the data encryption key under another encryption key, and encrypt that encryption key under another encryption key. But, eventually, one key must remain in plaintext so you can decrypt the keys and your data. This top-level plaintext encryption key is known as the root key. AWS KMS helps you to protect your encryption keys by storing and managing them securely. Root keys stored in AWS KMS, known as AWS KMS keys, never leave the AWS KMS FIPS validated hardware security modules unencrypted. To use an AWS KMS key, you must call AWS KMS. It is recommended that you use the following pattern to encrypt data locally in your application: 1. Use the GenerateDataKey operation to get a data encryption key. 2. Use the plaintext data key (returned in the Plaintext field of the response) to encrypt data locally, then erase the plaintext data key from memory. 3. Store the encrypted data key (returned in the CiphertextBlob field of the response) alongside the locally encrypted data. Hence, the valid steps in this scenario are the following: - Use the GenerateDataKey operation to get a data encryption key then use the plaintext data key in the response to encrypt data locally. - Erase the plaintext data key from memory and store the encrypted data key alongside the locally encrypted data. The option that says: Use the GenerateDataKeyWithoutPlaintext operation to get a data encryption key then using the plaintext data key in the response to encrypt data locally is incorrect because you have to use the GenerateDataKey operation instead. This is because the GenerateDataKeyWithoutPlaintext operation will not return the plaintext data key just as its name implies. The option that says: Erase the encrypted data key from memory and storing the plaintext data key alongside the locally encrypted data is incorrect because it should be the other way around. You have to erase the plaintext data key from memory and store the encrypted data key alongside the locally encrypted data. The option that says: Encrypt data locally using the Encrypt operation is incorrect because the Encrypt operation is primarily used to encrypt RSA keys, database passwords, or other sensitive information. This operation can also be used to move encrypted data from one AWS region to another; however, this is not recommended if you want to encrypt your data locally. You have to use the GenerateDataKey operation instead. References: https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping Check out these Amazon S3 and AWS KMS Cheat Sheets: https://tutorialsdojo.com/amazon-s3/ https://tutorialsdojo.com/aws-key-management-service-aws-kms/"}]},{id:"aws-developer-12",title:"AWS Certified Developer Associate Practice Exams 6",description:"Additional practice questions covering AWS development topics.",questions:[{question:"A developer has enabled the lifecycle policy of an application deployed in Elastic Beanstalk. The lifecycle is set to limit the application version to 15 versions. The developer wants to keep the source code in an S3 bucket, yet, it gets deleted. What change should the developer do?",answers:[{text:"Modify the value of the Set the application versions limit by age option to zero.",isCorrect:!1},{text:"Configure the Retention setting to retain the source bundle in the S3 bucket.",isCorrect:!0},{text:"Modify the value of the Set application versions limit by the total count option to zero.",isCorrect:!1},{text:"Trigger a Lambda function to copy the source code to another S3 bucket.",isCorrect:!1}],explanation:"Each time you upload a new version of your application with the Elastic Beanstalk console or the EB CLI, Elastic Beanstalk creates an application version. If you don't delete versions that you no longer use, you will eventually reach the application version quota and be unable to create new versions of that application. You can avoid hitting the quota by applying an application version lifecycle policy to your applications. A lifecycle policy tells Elastic Beanstalk to delete application versions that are old or to delete application versions when the total number of versions for an application exceeds a specified number. Elastic Beanstalk applies an application's lifecycle policy each time you create a new application version and deletes up to 100 versions each time the lifecycle policy is applied. Elastic Beanstalk deletes old versions after creating the new version and does not count the new version towards the maximum number of versions defined in the policy. In the Retention section of the application version lifecycle settings, you can either choose to delete the source bundle in S3 or retain it. To solve the problem in the scenario, the Retention option must be configured to the Retain source bundle in S3. Hence, the correct answer is: Configure the retention setting to retain the source bundle in S3. The option that says: Modify the value of the Set application versions limit by the total count option to zero is incorrect because the minimum number that you can set for the application versions limit by total count is 1. The option that says: Trigger a Lambda function to copy the source code to another S3 bucket is incorrect. Although this is possible, this solution entails unnecessary configurations as you can just change the Retention of the application version lifecycle settings. The option that says: Modify the value of the Set the application versions limit by age option to zero is incorrect. You can't set an application version limit by age that is less than 1. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html https://aws.amazon.com/about-aws/whats-new/2017/05/aws-elastic-beanstalk-supports-version-lifecycle-management/ Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/"},{question:"A developer is writing an application that will download hundreds of media files. Each file must be encrypted with a unique encryption key within the application before storing it in an S3 bucket. The developer needs a cost-effective solution with low management overhead. Which of the following is the most suitable solution?",answers:[{text:"Enable the SSE-S3 for the S3 bucket. Directly store the files in the bucket.",isCorrect:!1},{text:"Use the CreateKey API command to generate a CMK for each file to encrypt them.",isCorrect:!1},{text:"Use an open-source key generator to produce a unique key. Use the key to encrypt the files.",isCorrect:!1},{text:"Use the GenerateDataKey API command to generate a data key for each file to encrypt them. Store the encrypted data key and the file.",isCorrect:!0}],explanation:"The GenerateDataKey generates a unique symmetric data key for client-side encryption. This operation returns a plaintext copy of the data key and a copy that is encrypted under a customer master key (CMK) that you specify. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data. GenerateDataKey returns a unique data key for each request. The bytes in the plaintext key are not related to the caller or the CMK. Hence, the correct answer is: Use the GenerateDataKey API command to generate a data key for each file to encrypt them. Store the encrypted data key and the file. The option that says: Use the CreateKey API command to generate a CMK for each file to encrypt them is incorrect. A CMK costs $1/month, which means you'd have to spend over $1,000 just to generate unique encryption keys. In addition, a CMK can only encrypt 4 KB of data, which does not make it suitable for encrypting media files. The option that says: Enable the SSE-S3 for the S3 bucket. Directly store the files in the bucket is incorrect. When you use Server-Side Encryption with SSE-S3, each object is encrypted with a unique key. However, the scenario requires you to generate a unique key for each file prior to uploading. This means that the encryption must be done at the client-side. The option that says: Use an open-source key generator to produce a unique key. Use the key to encrypt the files is incorrect. Although this will save costs, it'll be difficult to manage the keys and you'd have to write your own logic for encrypting and decrypting the files. AWS KMS makes it a lot easier to manage the data keys. Also, you don't have to write your own code as every KMS function is wrapped in an API. References: https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html https://docs.aws.amazon.com/cli/latest/reference/kms/generate-data-key.html Check out this AWS KMS Cheat Sheet: https://tutorialsdojo.com/aws-key-management-service-aws-kms/"},{question:"A developer is building a serverless workflow using Step Functions. The developer has to implement a solution that will help the State Machine handle and recover from State exception errors. Which of the following actions should the developer do?",answers:[{text:"Use a try and catch logic inside the application code to capture the exception error. Then, use a Retry field in the state definition.",isCorrect:!1},{text:"Use Catch and Retry fields in the state machine definition.",isCorrect:!0},{text:"Use a Catch field inside the application code to capture the exception error. Then, use a Retry field in the state definition.",isCorrect:!1},{text:"Use Catch and Retry fields inside the application code.",isCorrect:!1}],explanation:"AWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that maintain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by your business logic. Task and Parallel states can have a field named Retry, whose value must be an array of objects known as retriers. An individual retrier represents a certain number of retries, usually at increasing time intervals. A retrier contains the following fields: ErrorEquals A non-empty array of strings that match error names. When a state reports an error, Step Functions scans through the retriers. When the error name appears in this array, it implements the retry policy described in this retrier. IntervalSeconds An integer that represents the number of seconds before the first retry attempt (1 by default). MaxAttempts A positive integer that represents the maximum number of retry attempts (3 by default). If the error recurs more times than specified, retries cease and normal error handling resumes. A value of 0 specifies that the error or errors are never retried. BackoffRate The multiplier by which the retry interval increases during each attempt (2.0 by default) Task and Parallel states can have a field named Catch. This field's value must be an array of objects, known as catchers. A catcher contains the following fields. ErrorEquals A non-empty array of strings that match error names, specified exactly as they are with the retrier field of the same name. Next A string that must exactly match one of the state machine's state names. ResultPath A path that determines what input is sent to the state specified in the Next field. When a state reports an error and either there is no Retry field, or if retries fail to resolve the error, Step Functions scans through the catchers in the order listed in the array. When the error name appears in the value of a catcher's ErrorEquals field, the state machine transitions to the state named in the Next field. In the given scenario, you can use the Catch and Retry fields inside the state machine definition to capture an exception error and attempt to recover from it by automatically retrying the state. Hence, the correct answer is: Use Catch and Retry fields in the state machine definition. The option that says: Use a try and catch logic inside the application code to capture the exception error. Then, use a Retry field in the state definition is incorrect because you don't have to implement a try and catch logic yourself in the application code as this can already be easily done by declaring a Catch field in your state machine definition. The option that says: Use Catch and Retry fields inside the application code is incorrect because these fields are only applicable to the Amazon States Language. The option that says: Use a Catch field inside the application code to capture the exception error. Then, use a Retry field in the state definition is incorrect because the Catch field is explicitly a function of the Amazon States Language and will not work if implemented in the application code. References: https://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html https://aws.amazon.com/step-functions/ Check out this AWS Step Functions Cheat Sheet: https://tutorialsdojo.com/aws-step-functions/"},{question:"An update was made on an AWS Lambda-based application. It is invoked by an API Gateway endpoint with caching enabled to improve latency requests. The developer expected to get the latest data as a response when he tested the application. However, he kept getting stale data upon trying many times. What should the developer do that will require the LEAST amount of effort to resolve the issue? (Select TWO.)",answers:[{text:"Include Cache-Control: max-age=0 HTTP header on the API request.",isCorrect:!0},{text:"Create a new REST API endpoint and disable caching.",isCorrect:!1},{text:"Set the new endpoint as a trigger for the lambda function.",isCorrect:!1},{text:"Grant permission to the client to invalidate caching when there’s a request using the IAM execution role.",isCorrect:!0},{text:"Include Cache-Control: no-cache HTTP header on the API request.",isCorrect:!1}],explanation:"A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the Cache-Control: max-age=0 header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint. To grant permission for a client, attach a policy of the following to an IAM execution role for the user. This policy allows the API Gateway execution service to invalidate the cache for requests on the specified resource (or resources). Hence, the correct answers are: - Include Cache-Control: max-age=0 HTTP header on the API request. - Grant permission to the client to invalidate caching when there’s a request using the IAM execution role. The option that says: Create a new REST API endpoint and disable caching is incorrect. You don’t have to create a new REST API endpoint so you can disable caching. Invalidate cache is already a built-in feature on API Gateway. The option that says: Include Cache-Control: no-cache HTTP header on the API request is incorrect. Although “no-cache” is a valid value for the Cache-Control HTTP header, it is not the right value when invalidating API Gateway cache. The Cache-Control: max-age=0 header must be used. The option that says: Set the new endpoint as a trigger for the lambda function is incorrect. There is no sense in setting a new endpoint as a trigger for the lambda function as you can use the existing endpoint. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html Check out this Amazon API Gateway Cheat Sheet: https://aws.amazon.com/api-gateway/"},{question:"A developer is building an application that uses Amazon CloudFront to distribute thousands of images stored in an S3 bucket. The developer needs a fast and cost-efficient solution that will allow him to update the images immediately without waiting for the object’s expiration date. Which solution meets the requirements?",answers:[{text:"Update the images by invalidating them from the edge caches.",isCorrect:!1},{text:"Upload the new images in the S3 bucket and wait for the objects in the edge locations to expire to reflect the changes.",isCorrect:!1},{text:"Update the images by using versioned file names.",isCorrect:!0},{text:"Disable the CloudFront distribution and re-enable it to update the images in all edge locations.",isCorrect:!1}],explanation:"When you update existing files in a CloudFront distribution, AWS recommends that you include some sort of version identifier either in your file names or in your directory names to give yourself better control over your content. This identifier might be a date-time stamp, a sequential number, or some other method of distinguishing two versions of the same object. For example, instead of naming a graphic file image.jpg, you might call it image_1.jpg. When you want to start serving a new version of the file, you'd name the new file image_2.jpg, and you'd update the links in your web application or website to point to image_2.jpg. Alternatively, you might put all graphics in an images_v1 directory and, when you want to start serving new versions of one or more graphics, you'd create a new images_v2 directory, and you'd update your links to point to that directory. With versioning, you don't have to wait for an object to expire before CloudFront begins to serve a new version of it, and you don't have to pay for object invalidation. Hence, the correct answer is: Update the images by using versioned file names. The option that says: Update the images by invalidating them from the edge caches is incorrect. While this will update the images, it is not a cost-efficient solution as you have to pay for the additional invalidation requests. The option that says: Disable the CloudFront distribution and re-enable it to update the images in all edge locations is incorrect. CloudFront distributes files to edge locations only when the files are requested, not when you put new or updated files in your origin. Therefore, this is not the proper approach to update the images. The option that says: Upload the new images in the S3 bucket and wait for the objects in the edge locations to expire to reflect the changes is incorrect. This a time-consuming solution as you have to wait for the objects to expire just to have them updated. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/UpdatingExistingObjects.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html https://aws.amazon.com/blogs/aws/simplified-multiple-object-invalidation-for-amazon-cloudfront/ Check out this Amazon CloudFront Cheat Sheet: https://tutorialsdojo.com/amazon-cloudfront/"},{question:"A developer is building a ReactJS application that will be hosted on Amazon S3. Amazon Cognito handles the registration and signing of users using the AWS Software Development Kit (SDK) for JavaScript. The JSON Web Token (JWT) received upon authentication will be stored on the browser's local storage. After signing in, the application will use the JWT as an authorizer to access an API Gateway endpoint. What are the steps needed to implement the scenario above? (Select THREE.)",answers:[{text:"Create an Amazon Cognito User Pool.",isCorrect:!0},{text:"Set the name of the header that will be used from the request to the Cognito User Pool as a token source for authorization.",isCorrect:!0},{text:"Set the name of the header that will be used from the request to the Cognito Identity Pool as a token source for authorization.",isCorrect:!1},{text:"Create an Amazon Cognito Identity Pool.",isCorrect:!1},{text:"On the API Gateway Console, create an authorizer using the Cognito User Pool ID.",isCorrect:!0},{text:"Choose and set the authentication provider for your website.",isCorrect:!1}],explanation:"As an alternative to using IAM roles and policies or Lambda Authorizers (formerly known as custom authorizers), you can use an Amazon Cognito User Pool to control who can access your API in Amazon API Gateway. To use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn't authorized to make the call because the client did not have credentials that could be authorized. Hence, the correct steps to implement the solution are as follows: - Create an Amazon Cognito User Pool. - On the API Gateway Console, create an authorizer using the Cognito User Pool ID. - Set the name of the header that will be used from the request to the Cognito User Pool as a token source for authorization. The option that says: Create an Amazon Cognito Identity Pool is incorrect because you can not use Cognito Identity Pool as an authorizer for API Gateway. The only valid authorizers for API Gateway are AWS Lambda and Amazon Cognito User Pool. The option that says: Choose and set the authentication provider for your website is incorrect because this step is done during the creation of a Cognito Identity Pool. The option that says: Set the name of the header that will be used from the request to the Cognito Identity Pool as a token source for authorization is incorrect because Cognito Identity Pool cannot be used as an authorizer for API Gateway. You should use the Cognito User Pool. References: https://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/ https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html Check out this Amazon Cognito Cheat Sheet: https://tutorialsdojo.com/amazon-cognito/ https://tutorialsdojo.com/amazon-cognito-user-pools-and-identity-pools-explained/"},{question:"A serverless application is created to process numerous files. Each invocation takes 5 minutes to complete. The Lambda function's execution time is too slow for the application. Considering that the Lambda function does not return any important data, which method will accelerate data processing the most?",answers:[{text:"Use asynchronous Event Lambda invocations. Configure the function to process the files in parallel.",isCorrect:!0},{text:"Use synchronous RequestResponse Lambda invocations. Process the files one by one.",isCorrect:!1},{text:"Merge all of the files into a single file then process them all at once with an asynchronous Event Lambda invocation.",isCorrect:!1},{text:"Compress the files to reduce their size, then process them with synchronous RequestResponse Lambda invocations.",isCorrect:!1}],explanation:"Several AWS services, such as Amazon Simple Storage Service (Amazon S3) and Amazon Simple Notification Service (Amazon SNS), invoke functions asynchronously to process events. When you invoke a function asynchronously, you don't wait for a response from the function code. You hand off the event to Lambda and Lambda handles the rest. You can configure how Lambda handles errors and can send invocation records to a downstream resource to chain together components of your application. For asynchronous invocation, Lambda places the event in a queue and returns a success response without additional information. A separate process reads events from the queue and sends them to your function. To invoke a function asynchronously, set the invocation type parameter to Event. Since we are not concerned about waiting for any important data, we can just invoke the Lambda function asynchronously. Because of its non-blocking design, data processing in parallel would be a lot faster than running them in sequence in this kind of scenario. For example, a Lambda function that processes three files for four minutes each would take a total of 12 minutes processing time. If run asynchronously, ideally, the processing time would only take about four minutes. Hence, the correct answer is: Use asynchronous Event Lambda invocations. Configure the function to process the files in parallel. The option that says: Compress the files to reduce their size, then process them with synchronous RequestResponse Lambda invocations is incorrect. Compression may reduce the amount of time it takes to process a file, but the effect won't be as significant as when running them in parallel. The option that says: Use synchronous RequestResponse Lambda invocations. Process the files one by one is incorrect because this will not make any improvement in the current processing speed of the Lambda function. Additionally, there is no sense in running the Lambda function synchronously because it is not expected to return any important data. The option that says: Merge all of the files into a single file then process them all at once with an asynchronous Event Lambda invocation is incorrect because doing this negates the advantage of using an asynchronous invocation, which is parallelizing workloads. Additionally, there'll be a higher chance of the Lambda function hitting its maximum execution time. References: https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html https://aws.amazon.com/blogs/architecture/understanding-the-different-ways-to-invoke-lambda-functions/ Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A developer is building a serverless URL shortener using Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. The application code as well as the stack that defines the cloud resources should be written in Python. The code should also be reusable in case an update must be done to the stack. Which of the following actions must be done by the developer to meet the requirements above?",answers:[{text:"Use AWS CloudFormation to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.",isCorrect:!1},{text:"Use AWS SDK for Python (boto3) to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.",isCorrect:!1},{text:"Use AWS CDK to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.",isCorrect:!0},{text:"Use AWS CloudShell to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.",isCorrect:!1}],explanation:'The AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to model and provision your cloud application resources using familiar programming languages. The AWS CDK has first-class support for TypeScript, JavaScript, Python, Java, and C#. The AWS CDK can also update your deployed resources after you modify your app using the appropriate CDK commands. You can think of the CDK as a cloud infrastructure "compiler". It provides a set of high-level class libraries, called Constructs, that abstract AWS cloud resources and encapsulate AWS best practices. Constructs can be snapped together into object-oriented CDK applications that precisely define your application infrastructure and take care of all the complex boilerplate logic. When you run your CDK application, it is compiled into a CloudFormation Template, the "assembly language" for AWS cloud infrastructure. The template is then ready for processing by the CloudFormation provisioning engine for deployment into your AWS account. The CDK tools make it easy to define your application infrastructure stack, while the CloudFormation service takes care of the safe and dependable provisioning of your stack. Since the scenario requires the provisioning of cloud resources using a programming language (Python), the correct answer is: Use AWS CDK to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda. The option that says: Use CloudFormation to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda is incorrect because CloudFormation only allows JSON and YAML in defining cloud resources in a stack. The option that says: Use AWS SDK for Python (boto3) to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda is incorrect because boto3 is just a library for Python that lets you use AWS resources programmatically, allowing easy integration with your application. The option that says: Use AWS CloudShell to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda is incorrect because AWS CloudShell simply provides a browser-based terminal for managing AWS resources. It does not offer the ability to define and manage infrastructure as code in a reusable way, as required by the scenario. References: https://aws.amazon.com/blogs/developer/aws-cdk-developer-preview/ https://aws.amazon.com/cdk/ Check out this AWS Cloud Development Kit Cheat Sheet: https://tutorialsdojo.com/aws-cloud-development-kit-cdk/'},{question:"An application has a feature that displays GIFs based on keyword inputs. The code streams random GIF links from an external API to your local machine. When run, the application's process takes longer than expected. You are suspecting that the new function sendRequest() you added is the culprit. Which of the following actions should you do to determine the latency of the function?",answers:[{text:"Use CloudTrail to record and store event logs for actions made by your function.",isCorrect:!1},{text:"Using AWS X-Ray, disable sampling to efficiently trace all requests for calls.",isCorrect:!1},{text:"Using CloudWatch, troubleshoot the issue by checking the logs.",isCorrect:!1},{text:"Using AWS X-Ray, define an arbitrary subsegment inside the code to instrument the function.",isCorrect:!0}],explanation:"AWS X-ray analyzes and debugs production, distributed applications, such as those built using a microservices architecture. With, X-Ray, you can identify performance bottlenecks, edge case errors, and other hard to detect issues. A segment can break down the data about the work done into subsegments. Subsegments provide more granular timing information and details about downstream calls that your application made to fulfill the original request. A subsegment can contain additional details about a call to an AWS service, an external HTTP API, or an SQL database. You can define arbitrary subsegments to instrument specific functions or lines of code in your application. Subsegments extend a trace's segment with details about work done in order to serve a request. Each time you make a call with an instrumented client, the X-Ray SDK records the information generated in a subsegment. You can create additional subsegments to group other subsegments, to measure the performance of a section of code, or to record annotations and metadata. Hence, the correct answer is: Using AWS X-Ray, define an arbitrary subsegment inside the code to instrument the function. The option that says: Using AWS X-Ray, disable sampling to efficiently trace all requests for calls is incorrect because sampling is simply used to ensure efficient tracing and to provide a representative sample of the requests that your application serves. Additionally, sampling will help you save money by reducing the amount of traces for high-volume and unimportant requests. The option that says: Using CloudWatch, troubleshoot the issue by checking the logs is incorrect because CloudWatch is not suited for debugging applications. CloudWatch is just used to capture performance metrics and log data. But, it can not help you debug the applications' internal logic flow or determine where the potential bottlenecks are. The option that says: Use CloudTrail to record and store event logs for actions made by your function is incorrect because CloudTrail just tracks user activity and API usage to enable governance, compliance, operational auditing, and risk auditing of your AWS account. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-annotations https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-nodejs-subsegments.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/"},{question:"A developer is building an AWS Lambda-based Java application that optimizes pictures uploaded to an S3 bucket. Upon running several tests, the Lambda function shows a cold start of about 5 seconds. Which of the following could the developer do to reduce the cold start time? (Select TWO.)",answers:[{text:"Add the Spring Framework to the project and enable dependency injection.",isCorrect:!1},{text:"Run the Lambda function in a VPC to gain access to Amazon’s high-end infrastructure.",isCorrect:!1},{text:"Reduce the deployment package’s size by including only the needed modules from the AWS SDK for Java.",isCorrect:!0},{text:"Increase the memory allocation setting for the Lambda function.",isCorrect:!0},{text:"Increase the timeout setting for the Lambda function.",isCorrect:!1}],explanation:"A cold start happens when a system needs to create a new resource in response to an event/request. Cold starts are not unique to Lambda. There are also cold starts in container orchestration, high-performance computing, or any places where IT resources need to be spun up. In AWS Lambda, whenever you execute a helper function/pre-handler code where you need to do things like pulling data from an S3 bucket, connecting to a database, pulling configuration information and dependencies, or anything of the sorts, it gets executed on the INIT code where the partial cold start occurs. It's important to note that basically everything that you're doing outside of the handler function will block its execution. When it comes to thinking about pre handler code dependencies that you want to use, remember that less is more. The more targeted you are at the resource that you include, the better the overall performance your function will have during its execution. You also have the option to tweak the power of the resources that run your function by increasing the memory allocated to your function to optimize its overall performance. Hence, the correct answers are: - Reduce the deployment package’s size by including only the needed modules from the AWS SDK for Java. - Increase the memory allocation setting for the Lambda function. The option that says: Increase the timeout setting for the Lambda function is incorrect as this will not reduce the cold start time. This is usually done in solving the problem of execution timeout. The option that says: Run the Lambda function in a VPC to gain access to Amazon's high-end infrastructure is incorrect because this solution will give no performance gain that will reduce the cold start of the Lambda function. This is usually done to a Lambda function to access private resources in a VPC. The option that says: Add the Spring Framework to the project and enable dependency injection is incorrect as doing this can have a significant increase in your function's cold start time. References: https://aws.amazon.com/blogs/compute/new-for-aws-lambda-predictable-start-up-times-with-provisioned-concurrency/ https://github.com/awslabs/aws-serverless-java-container/wiki/Quick-start---Spring-Boot Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/ End Cold Starts in Your Serverless Apps with AWS Lambda Provisioned Concurrency: https://youtu.be/EML6FKBdsNU"},{question:"A developer is writing a web application that will allow users to save and retrieve images in an Amazon S3 bucket. The users are required to register and log in to access the application. Which combination of AWS Services should the Developer utilize for implementing the user authentication module of the application?",answers:[{text:"Amazon Cognito Identity Pools and IAM Role.",isCorrect:!1},{text:"Amazon Cognito Identity Pools and User Pools.",isCorrect:!0},{text:"Amazon Cognito User Pools and AWS Key Management Service (KMS)",isCorrect:!1},{text:"Amazon User Pools and AWS Security Token Service (STS)",isCorrect:!1}],explanation:"A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Google, Facebook, Amazon, or Apple, and through SAML identity providers. Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limited-privilege AWS credentials to access other AWS services. Hence, the correct answer is: Amazon Cognito Identity Pools and User Pools. Amazon Cognito Identity Pools and IAM Role are incorrect. The solution is not enough to meet the requirements as you have to use Cognito User pools to allow users to sign in to your application. Amazon Cognito User Pools and AWS Key Management Service (KMS) are incorrect because AWS KMS is just a service that is used to encrypt data. Amazon User Pools and AWS Security Token Service (STS) are incorrect. While it is true that you need AWS STS to allow users to access Amazon S3, it is already abstracted by the Amazon Cognito Identity Pools. That being said, you have to configure an Identity Pool to accept users federated with your Cognito User Pool. References: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html Check out this Amazon Cognito Cheat Sheet: https://tutorialsdojo.com/amazon-cognito/"},{question:"A university is gradually migrating some of its physical documents to the AWS cloud. They will start by moving their alumnus' historical records to Amazon S3. The storage solution should provide a secure and durable object storage with the lowest cost. Which of the following types of S3 storage should you recommend?",answers:[{text:"Amazon S3 One-Zone",isCorrect:!1},{text:"Amazon S3 Infrequent Access",isCorrect:!1},{text:"Amazon S3 Glacier Deep Archive",isCorrect:!0},{text:"Amazon S3 Glacier",isCorrect:!1}],explanation:"Amazon S3 Glacier Deep Archive is an S3 storage class that provides secure and durable object storage for long-term retention of data that is accessed once or twice in a year. From just $0.00099 per GB-month (less than one-tenth of one cent, or about $1 per TB-month), S3 Glacier Deep Archive offers the lowest cost storage in the cloud, at prices significantly lower than storing and maintaining data in on-premises magnetic tape libraries or archiving data off-site. Amazon S3 Glacier and S3 Glacier Deep Archive are designed to be the lowest-cost Amazon S3 storage classes, allowing you to archive large amounts of data at a very low cost. This makes it feasible to retain all the data you want for use cases like data lakes, analytics, IoT, machine learning, compliance, and media asset archiving. You pay only for what you need, with no minimum commitments or up-front fees. Hence, the correct answer is: Amazon S3 Glacier Deep Archive. Amazon S3 Glacier is incorrect. Although it is a valid solution for archiving data records, it is more expensive than the Amazon S3 Glacier Deep Archive. With Amazon S3 Glacier, storage is priced from $0.004 per gigabyte per month, while Amazon S3 Glacier Deep Archive is priced from $0.00099 per GB-month. Amazon S3 Infrequent Access and Amazon S3 One-Zone are both incorrect because these services are not suitable for data archiving. References: https://aws.amazon.com/about-aws/whats-new/2019/03/S3-glacier-deep-archive/ https://aws.amazon.com/s3/pricing/ Check out this Amazon S3 Cheat Sheet: https://tutorialsdojo.com/amazon-s3/"},{question:"A developer is tasked with enhancing the performance of an online learning platform that uses a serverless architecture. The platform relies on Amazon API Gateway to handle user requests, AWS Lambda to process quiz submissions, Amazon DynamoDB to store course progress data, and Amazon S3 to host video lectures. During peak hours, the platform experiences high latency caused by increased read operations on DynamoDB, leading to a poor user experience. Which AWS service or feature should the developer implement to optimize DynamoDB read performance and reduce latency?",answers:[{text:"Amazon CloudFront",isCorrect:!1},{text:"Amazon DynamoDB Streams",isCorrect:!1},{text:"Amazon Elastic Load Balancing",isCorrect:!1},{text:"Amazon DynamoDB Accelerator (DAX)",isCorrect:!0}],explanation:"Amazon DynamoDB Accelerator (DAX) is a fully managed, in-memory caching service designed to enhance the performance of DynamoDB by delivering microsecond response times for read-intensive applications. By offloading read operations to DAX, applications can achieve significantly reduced latency. It is particularly beneficial for use cases such as real-time bidding, social gaming, and trading platforms that demand rapid data access. DAX seamlessly integrates with existing DynamoDB applications, requiring minimal code changes, and supports API compatibility, simplifying the process of adding caching to DynamoDB tables. In addition to performance improvements, DynamoDB Accelerator reduces the operational complexity of managing cache invalidation, data consistency, and cluster maintenance. It operates as a write-through cache, ensuring that data written to DynamoDB is automatically synchronized with the cache, maintaining consistency between the two. DynamoDB Accelerator also supports encryption at rest and transit, enhancing security for sensitive data. With its ability to handle millions of requests per second and its managed infrastructure, DynamoDB Accelerator (DAX) provides a scalable and reliable solution for high-performance data retrieval applications. Hence, the correct answer is: Amazon DynamoDB Accelerator (DAX). The option that says: Amazon CloudFront is incorrect because it is primarily designed as a Content Delivery Network (CDN) to cache and deliver static content like videos, images, or web pages with low latency. While it can simply enhance the performance of S3-hosted video lectures, it does not directly address DynamoDB read operations or latency issues. The option that says:Amazon Elastic Load Balancing is incorrect. This option only distributes incoming traffic across multiple targets, such as EC2 instances or containers to ensure high availability and fault tolerance. It is unrelated to DynamoDB performance optimization in a serverless architecture and cannot reduce read latency on DynamoDB. The option that says: Amazon DynamoDB Streams is incorrect because it is typically used to capture table data changes for event-driven workflows. It simply provides a mechanism to process data updates but does not optimize or improve the performance of read operations, which is the issue described in this scenario. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"},{question:"A developer is using the AWS CLI to interact with different AWS services. An UnauthorizedOperation error, as shown below, is received after running the stop-instance command: Along with the response is an additional failure message displayed in ciphertext format. How can the developer decode the message?",answers:[{text:"Decode the message by calling the AWS IAM decode-authorization-message command.",isCorrect:!1},{text:"Decode the message by calling the AWS KMS decrypt command.",isCorrect:!1},{text:"Decode the message by calling the AWS STS decode-authorization-message command.",isCorrect:!0},{text:"Decode the message using an external cryptography library.",isCorrect:!1}],explanation:"The AWS STS DecodeAuthorizationMessage API decodes additional information about the authorization status of a request from an encoded message returned in response to an AWS request. For example, if a user is not authorized to perform an operation that he or she has requested, the request returns a Client.UnauthorizedOperation response (an HTTP 403 response). Some AWS operations additionally return an encoded message that can provide details about this authorization failure. The message is encoded because the details of the authorization status can constitute privileged information that the user who requested the operation should not see. To decode an authorization status message, a user must be granted permissions via an IAM policy to request the DecodeAuthorizationMessage (sts:DecodeAuthorizationMessage) action. Hence, the correct answer is: Decode the message by calling the AWS STS decode-authorization-message command. The option that says: Decode the message by calling the AWS KMS decrypt command is incorrect. This will fail because the encoded message isn't encrypted by an AWS KMS key; therefore, you won't be able to decrypt it via KMS. The option that says: Decode the message by calling the AWS IAM decode-authorization-message command is incorrect because this command is not available to AWS IAM. The option that says: Decode the message using an external cryptography library is incorrect because the encoded message is generated from the AWS STS. Therefore, only the AWS STS service can decode it. You won't be able to decrypt it with a KMS key or any external key. References: https://docs.aws.amazon.com/STS/latest/APIReference/API_DecodeAuthorizationMessage.html https://docs.aws.amazon.com/cli/latest/reference/sts/decode-authorization-message.html Check out this AWS IAM Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"},{question:"An application executes GET operations to various AWS services. The development team is using AWS X-Ray to trace all the calls made to AWS. As one of the developers, you are responsible for maintaining a particular block of code on the application. To save time, you only want to record data associated with the code to group the traces in the AWS console. Which of the following X-Ray features should you use?",answers:[{text:"Subsegment",isCorrect:!1},{text:"Metadata",isCorrect:!1},{text:"Annotations",isCorrect:!0},{text:"Sampling",isCorrect:!1}],explanation:"AWS X-Ray analyzes and debugs production, distributed applications, such as those built using a microservices architecture. With, X-Ray, you can identify performance bottlenecks, edge case errors, and other hard to detect issues. When you instrument your application, the X-Ray SDK records information about incoming and outgoing requests, the AWS resources used, and the application itself. You can add other information to the segmented document as annotations and metadata. Annotations and metadata are aggregated at the trace level and can be added to any segment or subsegment. Annotations are simple key-value pairs that are indexed for use with filter expressions. Use annotations to record data that you want to use to group traces in the console, or when calling the GetTraceSummaries API. Hence, the correct answer is: Annotations. Metadata is incorrect because you can not group traces with it. Metadata are key-value pairs with values of any type, including objects and lists, but that is not indexed. You commonly use metadata to record data that you want to store in the trace but don't need to search for traces. Sampling is incorrect because it is just used to ensure efficient tracing and to provide a representative sample of the requests that your application serves. Additionally, sampling will help you save money by reducing the amount of traces for high-volume and unimportant requests. Subsegment is incorrect because it is only used to provide more granular timing information and details about downstream calls that your application made to fulfill the original request. It cannot group traces from recorded data. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-nodejs-segment.html Check out this AWS X-Ray Cheat Sheet: https://tutorialsdojo.com/aws-x-ray/"},{question:"A company is running an Artificial Intelligence (AI) software for its automotive clients using the AWS Cloud. The software is used for identifying road obstructions for autonomous driving and predicting failure on vehicle components. The company wants to extend its usage and access based on different levels (students, professionals, and hobbyist developers) by exposing an API through API Gateway. The company should regulate access to the API and monetize it by charging based on usage. What should the company do?",answers:[{text:"Create three stages and enable CloudWatch metrics. Set up an alarm for each stage according to the ApiName, Method, Resource, and Stage dimensions.",isCorrect:!1},{text:"Create three stages. Specify a quota and throttle requests according to the level of access.",isCorrect:!1},{text:"Create three Authorizers to control API access.",isCorrect:!1},{text:"Create three Usage Plans. Specify a quota and throttle requests according to the level of access.",isCorrect:!0}],explanation:"A usage plan specifies who can access one or more deployed API stages and methods—and also how much and how fast they can access them. The plan uses API keys to identify API clients and meters access to the associated API stages for each key. It also lets you configure throttling limits and quota limits that are enforced on individual client API keys. This feature allows developers to build and monetize APIs and to create ecosystems around them. You can create usage plans for different levels of access (Bronze, Silver, and Gold), different categories of users (Student, Individual, Professional, or Enterprise), and so forth. Hence, the correct answer is: Create three Usage Plans. Specify a quota and throttle requests according to the level of access. The option that says: Create three stages. Specify a quota and throttle requests according to the level of access is incorrect because you have to create a usage plan to monetize APIs. Moreover, you can't specify a quota in a stage. The option that says: Create three Authorizers to control API access is incorrect as this will just deny unauthorized users. The solution must regulate API access from authorized users. The option that says: Create three stages and enable CloudWatch metrics. Set up an alarm for each stage according to the ApiName, Method, Resource, and Stage dimensions is incorrect. You can't regulate and monetize API access through CloudWatch. References: https://aws.amazon.com/blogs/aws/new-usage-plans-for-amazon-api-gateway/ https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A developer needs to build a queueing mechanism for an application that will run on AWS. The application is expected to consume SQS messages that are larger than 256 KB and up to 1 GB in size. How should the developer manage the SQS messages?",answers:[{text:"Use Amazon S3 and the Amazon SQS Extended Client Library for Java",isCorrect:!0},{text:"Use Amazon S3 and the Amazon SQS CLI",isCorrect:!1},{text:"Use Amazon S3 and the Amazon SQS HTTP API",isCorrect:!1},{text:"Use Amazon S3 and the Amazon SQS Console",isCorrect:!1}],explanation:"To manage large Amazon Simple Queue Service (Amazon SQS) messages, you can use Amazon Simple Storage Service (Amazon S3) and the Amazon SQS Extended Client Library for Java. This is especially useful for storing and consuming messages up to 2 GB. Unless your application requires repeatedly creating queues and leaving them inactive or storing large amounts of data in your queues, consider using Amazon S3 for storing your data. You can use the Amazon SQS Extended Client Library for Java to do the following: - Specify whether messages are always stored in Amazon S3 or only when the size of a message exceeds 256 KB - Send a message that references a single message object stored in an S3 bucket - Retrieve the message object from an S3 bucket - Delete the message object from an S3 bucket You can use the Amazon SQS Extended Client Library for Java to manage Amazon SQS messages using Amazon S3 only with the AWS SDK for Java. You can't do this with the AWS CLI, the Amazon SQS console, the Amazon SQS HTTP API, or any of the other AWS SDKs. Hence, the correct answer is: Use Amazon S3 and the Amazon SQS Extended Client Library for Java. Using Amazon S3 and the Amazon SQS Console is incorrect because the SQS Console does not support the storing of SQS messages that exceed 256 KB. Using Amazon S3 and the Amazon SQS HTTP API is incorrect because you can't store large SQS messages in an S3 bucket using the SQS HTTP API. Using Amazon S3 and the Amazon SQS CLI is incorrect because the SQS CLI does not support storing of SQS messages to Amazon S3 via the Amazon SQS Extended Client Library for Java. References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/quotas-messages.html Check out this Amazon SQS Cheat Sheet: https://tutorialsdojo.com/amazon-sqs/"},{question:"A microservices application's Customer and Payment service components have two separate DynamoDB tables. New items inserted into the Customer service table must be dynamically updated in the Payment service table. How can the Payment service get near real-time update",answers:[{text:"Create a Firehose stream to stream all the changes from the Customer service table and trigger a Lambda function to update the Payment service table.",isCorrect:!1},{text:"Use a Kinesis data stream to stream all the changes from the Customer service database directly into the Payment service table.",isCorrect:!1},{text:"Create a scheduled Amazon EventBridge (Amazon CloudWatch Events) rule that invokes a Lambda function every minute to update the changes from the Customer service table into the Payment service table.",isCorrect:!1},{text:"Enable DynamoDB Streams to stream all the changes from the Customer service table and trigger a Lambda function to update the Payment service table.",isCorrect:!0}],explanation:"DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real-time. Amazon DynamoDB is integrated with AWS Lambda so that you can create triggers—pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables. If you enable DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with an AWS Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records. The Lambda function can perform any actions you specify, such as sending a notification or initiating a workflow. For example, you can write a Lambda function to persist changes from one DynamoDB database to another. Hence, the correct answer is: Enable DynamoDB Streams to stream all the changes from the Customer service table and trigger a Lambda function to update the Payment service table. The option that says: Create a Firehose stream to stream all the changes from the Customer service table and trigger a Lambda function to update the Payment service table is incorrect because DynamoDB does not support Data Firehose, hence, this solution is not possible. The option that says: Use a Kinesis data stream to stream all the changes from the Customer service database directly into the Payment service table is incorrect. Kinesis Data Stream is a valid service for streaming changes from a DynamoDB table. However, it cannot directly write stream records to a DynamoDB table. You need a processing layer that will poll records from the stream and update the other DynamoDB table. The option that says: Create a scheduled Amazon EventBridge (Amazon CloudWatch Events) rule that invokes a Lambda function every minute to update the changes from the Customer service table into the Payment service table is incorrect. With this method, you would have to update new items in batches. Take note that the requirement is to stream changes to the Payment service table in near real-time. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/ Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"},{question:"A company wants to know how its monolithic application will perform on a microservice architecture. The Lead Developer has deployed the application on Amazon ECS using the EC2 launch type. He terminated the container instance after testing; however, the container instance still appears as a resource in the ECS cluster. What is the possible cause of this?",answers:[{text:"When a container instance is terminated in the running state, the container instance is not automatically deregistered from the cluster.",isCorrect:!1},{text:"After terminating the container instance in the running state, the container instance must be manually deregistered in the Amazon ECS Console.",isCorrect:!1},{text:"When a container instance is terminated in the stopped state, the container instance is not automatically deregistered from the cluster.",isCorrect:!0},{text:"After terminating the container instance in the stopped state, the container instance must be manually deregistered in the Amazon EC2 Console since it was launched using the EC2 launch type.",isCorrect:!1}],explanation:"If you terminate a container instance in the RUNNING state, that container instance is automatically removed or deregistered from the cluster. However, if you terminate a container instance in the STOPPED state, that container instance isn't automatically removed from the cluster. To deregister your container instance from the cluster, you should deregister it after terminating it in the STOPPED state by using the Amazon ECS Console or AWS Command Line Interface. The deregistered container instance will no longer appear as a resource in your Amazon ECS cluster. Hence, the correct answer is: When a container instance is terminated in the stopped state, the container instance is not automatically deregistered from the cluster. The option that says: When a container instance is terminated in the running state, the container instance is not automatically deregistered from the cluster is incorrect because terminating a container instance in the RUNNING state will automatically deregister it from the cluster. The option that says: After terminating the container instance in the running state, the container instance must be manually deregistered in the Amazon ECS Console is incorrect because you do not have to manually deregister a container instance terminated in the RUNNING state. It will automatically be deregistered from the cluster. The option that says: After terminating the container instance in the stopped state, the container instance must be manually deregistered in the Amazon EC2 Console since it was launched using the the EC2 launch type is incorrect because while it is true that you should manually deregister the container instance terminated in the STOPPED state, this should be done in the Amazon ECS Console and not on the Amazon EC2 Console. References: https://aws.amazon.com/premiumsupport/knowledge-center/deregister-ecs-instance/ https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html Check out this Amazon ECS Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/"},{question:"An application that runs on a local Linux server is migrated to a Lambda function to save costs. The Lambda function shows an Unable to import module error when invoked. As the developer, how can you fix the error?",answers:[{text:"Download the missing modules in the lib directory. Package all files under the lib directory into a ZIP file and upload it to AWS Lambda.",isCorrect:!1},{text:"Import the missing modules in the Lambda code. The Lambda function will automatically install them when invoked.",isCorrect:!1},{text:"Install the missing modules locally to your application’s folder. Package the folder into a ZIP file and upload it to AWS Lambda.",isCorrect:!0},{text:"Run a Linux command inside the Lambda function to install the missing modules.",isCorrect:!1}],explanation:"ModuleNotFoundError and Module cannot be loaded are common errors for Lambda functions. These errors are usually due to an incorrect folder structure or file permissions with the deployment package .zip file. To fix this error: 1. Install all dependency modules local to the function project. 2. Build the deployment package by zipping up the project folder for deployment to Lambda. 3. Upload the deployment package. Hence, the correct answer is: Install the missing modules locally to your application’s folder. Package the folder into a ZIP file and upload it to AWS Lambda. The option that says: Import the missing modules in the Lambda code. The Lambda function will automatically install them when invoked is incorrect. This will still result in the same error because the Lambda function won't be able to recognize the modules that are defined in your code. The option that says: Download the missing modules in the lib directory. Package all files under the lib directory into a ZIP file and upload it to AWS Lambda is incorrect. This won't work. The modules and your Lambda code must be under the same directory level before packaging them into a ZIP file. The option that says: Run a Linux command inside the Lambda function to install the missing modules is incorrect. Although you can run Linux commands via custom runtimes in AWS Lambda, you can't install dependencies directly on a Lambda function. References: https://aws.amazon.com/premiumsupport/knowledge-center/build-python-lambda-deployment-package/ https://docs.aws.amazon.com/lambda/latest/dg/python-package.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A developer is building an application with Amazon DynamoDB as its database. The application needs to group the PUT, UPDATE, and DELETE actions into a single all-or-nothing operation to make changes against multiple items in the DynamoDB table. Which DynamoDB operation should the developer use?",answers:[{text:"Query",isCorrect:!1},{text:"Scan",isCorrect:!1},{text:"TransactWriteItems",isCorrect:!0},{text:"BatchWriteItem",isCorrect:!1}],explanation:"With Amazon DynamoDB transactions, you can group multiple actions together and submit them as a single all-or-nothing TransactWriteItems or TransactGetItems operation. TransactWriteItems is a synchronous and idempotent write operation that groups up to 25 write actions in a single all-or-nothing operation. These actions can target up to 25 distinct items in one or more DynamoDB tables within the same AWS account and in the same Region. The aggregate size of the items in the transaction cannot exceed 4 MB. The actions are completed atomically so that either all of them succeed or none of them succeeds. You can add the following types of actions to a transaction: Put — Initiates a PutItem operation to create a new item or replace an old item with a new item, conditionally or without specifying any condition. Update — Initiates an UpdateItem operation to edit an existing item's attributes or add a new item to the table if it does not already exist. Use this action to add, delete, or update attributes on an existing item conditionally or without a condition. Delete — Initiates a DeleteItem operation to delete a single item in a table identified by its primary key. ConditionCheck — Checks that an item exists or checks the condition of specific attributes of the item. Hence, the correct answer is: TransactWriteItems BatchWriteItem is incorrect because this is not a single all-or-nothing operation, which means some requests in a BatchWriteItem operation can succeed or fail. Also, this DynamoDB operation does not support UpdateItem , which is one of the requirements in the scenario. Scan is incorrect because this just reads all of the items in a table. Query is incorrect because this simply enables you to get items in a table based on primary key values. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html https://aws.amazon.com/blogs/aws/new-amazon-dynamodb-transactions/ Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/ Quick DynamoDB Overview: https://youtu.be/3ZOyUNIeorU"},{question:"A media analytics company provides APIs as a service that enables users to retrieve aggregated, daily-updated data on video performance metrics, such as views, likes, and watch times. These APIs are built using Amazon API Gateway and AWS Lambda, with data sourced from a pre-computed file in Amazon S3 that updates every 24 hours. Due to a surge in traffic, users have reported increased latency when accessing the API. The company aims to enhance the API's responsiveness without changing the backend architecture. Which approach would improve the responsiveness of the APIs?",answers:[{text:"Enable cross-origin resource sharing (CORS) using the API Gateway.",isCorrect:!1},{text:"Configure Amazon CloudFront as a caching layer in front of API Gateway.",isCorrect:!1},{text:"Enable Amazon API Gateway caching.",isCorrect:!0},{text:"Use Amazon ElastiCache to store frequently requested data in memory.",isCorrect:!1}],explanation:"Amazon API Gateway Caching is a powerful feature that enables users to improve the performance and scalability of its APIs by storing responses in memory. By enabling caching, API Gateway can store responses to API requests for a specified period. With subsequent requests with the same parameters, API Gateway can quickly serve the cached response, reducing the need to invoke the backend Lambda function. This leads to faster response times, less load on backend systems, and a more cost-effective solution by minimizing the executions required for frequently accessed data. API Gateway caching is especially beneficial when the data served is relatively static or changes infrequently, such as aggregated metrics that update once daily. API Gateway caching is highly configurable, allowing users to specify the time-to-live (TTL) for cached data, meaning how long the data should be stored before being refreshed. This TTL can be adjusted based on the data's nature and the API's specific needs. For instance, if the video performance metrics are updated once every 24 hours, a TTL of 24 hours ensures that cached responses remain valid for each update cycle. Additionally, caching can be applied at both the stage and method levels, providing flexibility in managing which API endpoints should be cached and for how long. By using API Gateway's built-in caching feature, users can significantly reduce the latency experienced by end-users, enhance overall API performance, and improve user experience. This feature also integrates seamlessly with AWS's broader environment, ensuring reliable and scalable API management without additional infrastructure changes. Hence, the correct answer is: Enable Amazon API Gateway caching. The option that says: Enable cross-origin resource sharing (CORS) using the API Gateway is incorrect. Cross-origin resource sharing (CORS) is primarily used to enable secure resource sharing between different domains, typically for web applications running in the browser. It does not directly impact the performance or responsiveness of the API itself. While CORS is crucial for handling cross-origin requests, it doesn’t address the latency issues caused by frequent backend invocations and thus does not serve as a solution to improve the responsiveness of the API. The option that says: Use Amazon ElastiCache to store frequently requested data in memory is incorrect. While ElastiCache is an effective tool for caching, it typically requires changes to the backend architecture to integrate the cache layer with the Lambda functions. This approach would involve additional configuration and management overhead, which goes against the goal of improving responsiveness without altering the existing backend architecture. The option that says: Configure Amazon CloudFront as a caching layer in front of API Gateway is incorrect. Amazon CloudFront is primarily a content delivery network (CDN) that caches content at edge locations to speed up delivery for users across the globe. While it can help with reducing latency by caching API responses, it typically requires more complex configuration compared to API Gateway’s built-in caching. CloudFront is more suited for static content or large-scale applications where global distribution is essential. Still, for API performance improvements, API Gateway’s own caching is a more direct and simpler solution. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A developer has been instructed to automate the creation of the snapshot of an existing Amazon EC2 instance. The engineer created a script that uses the AWS Command Line Interface (CLI) to run the necessary API call. He is getting an InvalidInstanceID.NotFound error whenever the script is run. What is the most likely cause of the error?",answers:[{text:"The AWS Access Key Id used to configure the AWS CLI is invalid.",isCorrect:!1},{text:"The Image Id used in running the command for creating a snapshot is incorrect.",isCorrect:!1},{text:"The AWS Region, where the programmatic access for the AWS CLI is created, does not match the region where the instance lives.",isCorrect:!1},{text:"The AWS Region name used to configure the AWS CLI does not match the region where the instance lives.",isCorrect:!0}],explanation:"For general use, the aws configure command is the fastest way to set up your AWS CLI installation. When you enter this command, the AWS CLI prompts you for four pieces of information: - Access Key ID - Secret Access Key - AWS Region - Output format Access keys consist of an access key ID and secret access keys are used to sign programmatic requests that you make to AWS. AWS Region identifies the AWS Region whose servers you want to send your requests to by default. This is typically the Region closest to you, but it can be any Region. For example, you can type us-west-2 to use US West (Oregon). This is the Region that all later requests are sent to unless you specify otherwise in an individual command. Output format specifies how the results are formatted. The value can be any of the values in the following list. If you don't specify an output format, JSON is used as the default. The InvalidInstanceID.NotFound error suggests that an instance does not exist. Ensure that you have indicated the AWS Region in which the instance is located if it's not in the default Region. This error may occur because the ID of a recently created instance has not propagated through the system.Since it was mentioned in the scenario that the EC2 instance already exists, we can conclude that there is a mismatch in the AWS Region configured in the CLI. It means that the EC2 instance is located in another Region which is why the developer got the error message. Hence, the correct answer is: The AWS Region name used to configure the AWS CLI does not match the region where the instance lives. The option that says: The AWS Region, where the programmatic access for the AWS CLI is created, does not match with the region where the instance lives is incorrect because the programmatic access is just another way of presenting yourself as an IAM User. IAM users are global entities which means it can not be associated with a particular region. The option that says: The AWS Access Key Id used to configure the AWS CLI is invalid is incorrect because you will get an InvalidAccessKeyId error as a response if you do not have the correct AWS Access Key Id. The scenario's issue is about the InvalidInstanceID.NotFound error. The option that says: The Image Id used in running the command for creating a snapshot is incorrect because the error is about the instance Id and not the Image Id. References: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html https://aws.amazon.com/getting-started/hands-on/backup-to-s3-cli/ Check out this AWS IAM Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"},{question:"A developer uses AWS Serverless Application Model (SAM) in a local machine to create a serverless Python application. After defining the required dependencies in the requirements.txt file, the developer is now ready to test and deploy. What are the steps to successfully deploy the application?",answers:[{text:"Upload and build the SAM template in an EC2 instance. Run the sam deploy command to package and deploy the SAM template.",isCorrect:!1},{text:"Build the SAM template in the local machine. Run the sam deploy command to package and deploy the SAM template from AWS CodePipeline.",isCorrect:!1},{text:"Run the sam init command. Build the SAM template in the local machine and call the sam deploy command to package and deploy the SAM template from an S3 bucket.",isCorrect:!1},{text:"Build the SAM template in the local machine and call the sam deploy command to package and deploy the SAM template from an S3 bucket.",isCorrect:!0}],explanation:"Here are the SAM CLI commands needed to deploy serverless applications: sam init - Initializes a serverless application with an AWS SAM template. The template provides a folder structure for your Lambda functions and is connected to an event source such as APIs, S3 buckets, or DynamoDB tables. This application includes everything you need to get started and to eventually extend it into a production-scale application. sam build - The sam build command builds any dependencies that your application has, and copies your application source code to folders under .aws-sam/build to be zipped and uploaded to Lambda. sam deploy - performs the functionality of sam package. You can use the sam deploy command to directly package and deploy your application. Since the application's runtime and dependencies are already defined, the next step is to call the sam build command to install and build the dependencies of the application. After running a series of local tests, you can now package and deploy the SAM template into an S3 bucket via the sam deploy command. Hence, the correct answer is: Build the SAM template in the local machine and call the sam deploy command to package and deploy the SAM template from an S3 bucket. The option that says: Build the SAM template in the local machine. Run the sam deploy command to package and deploy the SAM template from AWS CodePipeline is incorrect because it suggests using AWS CodePipeline directly with the sam deploy command. While CodePipeline can primarily be used for CI/CD, the sam deploy command itself does not interact directly with CodePipeline. Instead, it packages and deploys the application using an S3 bucket and CloudFormation. The option that says: Run the sam init command. Build the SAM template in the local machine and call the sam deploy command to package and deploy the SAM template from an S3 bucket is incorrect. You don't have to run the sam init command because from the conditions given, it is assumed that the runtime and the folder structure of the application have already been established. The option that says: Upload and build the SAM template in an EC2 instance. Run the sam deploy command to package and deploy the SAM template is incorrect because you can build the SAM template from your local computer by using the SAM CLI. Creating an EC2 instance just adds unnecessary costs. Also, SAM can only deploy from Amazon S3. References: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-getting-started-hello-world.html https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-command-reference.html Check out this AWS Serverless Application Model Cheat Sheet: https://tutorialsdojo.com/aws-serverless-application-model-sam/"},{question:"A team of developers needs permission to launch EC2 instances with an instance role that will allow them to update items in a DynamoDB table. Each developer has access to IAM users that belongs in the same IAM group. Which of the following steps must be done to implement the solution?",answers:[{text:"Create an IAM role with an IAM policy that will allow access to the EC2 instances. Add the DynamoDB service to the trust policy of the role. Create a custom policy with the iam:GetRole permission. Attach the policy to the IAM group.",isCorrect:!1},{text:"Create an IAM role with an IAM policy that will allow access to the DynamoDB table. Add the EC2 service to the trust policy of the role. Create a custom policy with the iam:PassRole permission. Attach the policy to the IAM group.",isCorrect:!0},{text:"Create an IAM role with an IAM policy that will allow access to the DynamoDB table. Add the EC2 service to the trust policy of the role. Create a custom policy with iam:GetRolePolicy and iam:PutRolePolicy permissions. Attach the policy to the IAM group.",isCorrect:!1},{text:"Create an IAM role with an IAM policy that will allow access to the EC2 instances. Add the DynamoDB service to the trust policy of the role. Create a custom policy with the iam:PassRole permission. Attach the policy to the IAM group.",isCorrect:!1}],explanation:"Before you can access any AWS services via CLI/API from an EC2 instance, you must first configure and specify your access credentials. A more secure approach is by allowing the EC2 to assume an IAM role so it can access AWS services on your behalf. This way, your credentials are never stored or exposed. According to the scenario, the EC2 instances (that will be launched by the developers) need access to a DynamoDB table. First, we need to create an IAM role with permission that will allow access to the DynamoDB table. After creating the role, you must add the EC2 service as a trusted entity in the role's trust policy. You need to do this so EC2 instances can assume the IAM role. Afterwhich, you have to attach the following policy to the IAM Group: If the developers don’t have iam:PassRole permission, he or she can’t associate a role with the instance during launch. The PassRole permission helps you make sure that a user doesn’t pass a role to an EC2 instance where the role has more permissions than you want the user to have. For example, Alice might be allowed to perform only EC2 and S3 actions. If Alice could pass a role to the EC2 instance that allows additional actions, she could log into the instance, get temporary security credentials via the role she passed, and make calls to AWS that you don’t intend. Hence, the correct answer is: Create an IAM role with an IAM policy that will allow access to the DynamoDB table. Add the EC2 service to the trust policy of the role. Create a custom policy with iam:PassRole permission. Attach the policy to the IAM group. The option that says: Create an IAM role with an IAM policy that will allow access to the DynamoDB table. Add the EC2 service to the trust policy of the role. Create a custom policy with iam:GetRolePolicy and iam:PutRolePolicy permissions. Attach the policy to the IAM group is incorrect as you have to add the iam:PassRole permission instead. The iam:GetRolePolicy and iam:PutRolePolicy permissions are just used to grant permissions to retrieve and create an inline policy document that is embedded with a specified IAM role. The option that says: Create an IAM role with an IAM policy that will allow access to the EC2 instances. Add the DynamoDB service to the trust policy of the role. Create a custom policy with iam:GetRole permission. Attach the policy to the IAM group is incorrect because the scenario requires you to allow access to DynamoDB from the EC2 instance and not the other way around. Also, you need to have iam:PassRole permission. The option that says: Create an IAM role with an IAM policy that will allow access to the EC2 instances. Add the DynamoDB service to the trust policy of the role. Create a custom policy with iam:PassRole permission. Attach the policy to the IAM group is incorrect because the IAM policy of the role should allow access to DynamoDB and not EC2. The reason for this is to allow EC2 instances to call DynamoDB requests on the user's behalf. References: https://aws.amazon.com/blogs/security/granting-permission-to-launch-ec2-instances-with-iam-roles-passrole-permission/ https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html Check out this AWS IAM Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"},{question:"A company is running an e-commerce application on an Amazon EC2 instance. A newly hired developer has been tasked to monitor and handle the necessary updates on the EC2 instance every Saturday. The developer is working from home and needs remote access to the webserver. As the system administrator, you’re looking to use the AWS STS API to give the developer temporary credentials and enforce Multi-factor Authentication (MFA) to protect specific programmatic calls against the instance that could adversely affect the server. Which of the following STS API should you use?",answers:[{text:"AssumeRoleWithSAML",isCorrect:!1},{text:"GetSessionToken",isCorrect:!0},{text:"AssumeRoleWithWebIdentity",isCorrect:!1},{text:"GetFederationToken",isCorrect:!1}],explanation:"AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). Below is the summary of the available STS API: AssumeRole - is useful for allowing existing IAM users to access AWS resources that they don't already have access to. For example, the user might need access to resources in another AWS account. It is also useful as a means to temporarily gain privileged access—for example, to provide multi-factor authentication (MFA). You must call this API using existing IAM user credentials. AssumeRoleWithWebIdentity - returns a set of temporary security credentials for federated users who are authenticated through a public identity provider. Examples of public identity providers include Login with Amazon, Facebook, Google, or any OpenID Connect (OIDC)-compatible identity provider. AssumeRoleWithSAML - returns a set of temporary security credentials for federated users who are authenticated by your organization's existing identity system. The users must also use SAML 2.0 (Security Assertion Markup Language) to pass authentication and authorization information to AWS. This API operation is useful in organizations that have integrated their identity systems (such as Windows Active Directory or OpenLDAP) with software that can produce SAML assertions. GetFederationToken - returns a set of temporary security credentials (consisting of an access key ID, a secret access key, and a security token) for a federated user. A typical use is in a proxy application that gets temporary security credentials on behalf of distributed applications inside a corporate network. You must call the GetFederationToken operation using the long-term security credentials of an IAM user. GetSessionToken - returns a set of temporary security credentials to an existing IAM user. This is useful for providing enhanced security, such as allowing AWS requests only when MFA is enabled for the IAM user. Because the credentials are temporary, they provide enhanced security when you have an IAM user who accesses your resources through a less secure environment. All of the options given provide temporary credentials to make API calls against AWS resources, but GetSessionToken is the only API that supports MFA. Hence, the correct answer is GetSessionToken. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html#stsapi_comparison https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html Check out this AWS IAM Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"},{question:"A Lamba function has multiple sub-functions that are chained together to process large data synchronously. When invoked, the function tends to exceed its maximum timeout limit. This has prompted the developer to break the Lambda function into manageable coordinated states using Step Functions, enabling each sub-function to run in separate processes. Which of the following type of states should the developer use to run processes?",answers:[{text:"Pass State",isCorrect:!1},{text:"Parallel State",isCorrect:!1},{text:"Task State",isCorrect:!0},{text:"Wait State",isCorrect:!1}],explanation:"AWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that maintain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by your business logic. Step Functions can help solve the problem of timeout errors of Lambda functions. Imagine a Lambda function that has four utility functions that are run sequentially. Each of those functions takes 5 minutes to finish which translates to a total execution time of 20 minutes. This is a problem since Lambda can only run for a maximum of 15 minutes. To solve this, we can refactor the functions inside the Lambda function into individual Step Functions states. This way, each function is contained in a separate Lambda function, which has its own execution timeout. Individual states can make decisions based on their input, perform actions, and pass output to other states. In AWS Step Functions, you define your workflows in the Amazon States Language. The Step Functions console provides a graphical representation of that state machine to help visualize your application logic. States are elements in your state machine. A state is referred to by its name, which can be any string, but must be unique within the scope of the entire state machine. States can perform a variety of functions in your state machine: Task State - Do some work in your state machine Choice State - Make a choice between branches of execution Fail or Succeed State - Stop execution with failure or success Pass State - Simply pass its input to its output or inject some fixed data, without performing work. Wait State - Provide a delay for a certain amount of time or until a specified time/date. Parallel State - Begin parallel branches of execution. Map State - Dynamically iterate steps. Out of all the types of State, only the Task State and the Parallel State can be used to run processes in the state machine. In the given scenario, the application logic inside the Lambda function process data synchronously. In this case, Task State should be used. Pass State is incorrect because this type of state cannot perform work as it simply passes its input data to its output. Pass State is mainly used for constructing and debugging state machines. Parallel State is incorrect. Although it can be used to run processes in a state machine, this type of state should only be used when you want to run processes asynchronously. Parallel state executes each branch concurrently and independently. In the given scenario, the Lambda function processes data synchronously. This means that each output of a function is piped as an input to the next function. The Task State is much more applicable in this scenario. Wait State is incorrect because this type of state just provides a delay mechanism to your state machine. References: https://aws.amazon.com/step-functions/ https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-task-state.html Check out this AWS Step Functions Cheat Sheet: https://tutorialsdojo.com/aws-step-functions/"},{question:"A San Francisco-based tech startup is building a cross-platform mobile app that can notify the user of upcoming astronomical events. Your mobile app authenticates with the Identity Provider (IdP) using the provider's SDK and Amazon Cognito. Once the end-user is authenticated with the IdP, the OAuth or OpenID Connect token returned from the IdP is passed by your app to Amazon Cognito. Which of the following is returned for the user to provide a set of temporary, limited-privilege AWS credentials?",answers:[{text:"Cognito ID",isCorrect:!0},{text:"Cognito Key Pair",isCorrect:!1},{text:"Cognito API",isCorrect:!1},{text:"Cognito SDK",isCorrect:!1}],explanation:"You can use Amazon Cognito to deliver temporary, limited-privilege credentials to your application so that your users can access AWS resources. Amazon Cognito identity pools support both authenticated and unauthenticated identities. You can retrieve a unique Amazon Cognito identifier (identity ID) for your end-user immediately if you're allowing unauthenticated users or after you've set the login tokens in the credentials provider if you're authenticating users. When your mobile app authenticates with the Identity Provider (IdP) using Amazon Cognito, the token returned from the IdP is passed to Amazon Cognito, which then returns a Cognito ID for the user. This Cognito ID is used to provide a set of temporary, limited-privilege AWS credentials through the Cognito Identity Pool. Hence, the correct answer is: Cognito ID. Cognito SDK is incorrect because this is not a unique Amazon Cognito identifier but a software development kit that is available in various programming languages. Cognito Key Pair is incorrect because this is not a unique Amazon Cognito identifier but a cryptography key. Cognito API is incorrect because this is not a unique Amazon Cognito identifier and is primarily used as an Application Programming Interface. Reference: http://docs.aws.amazon.com/cognito/latest/developerguide/getting-credentials.html Check out this Amazon Cognito Cheat Sheet: https://tutorialsdojo.com/amazon-cognito/"},{question:"A developer needs to view the percentage of used memory and the number of TCP connections of instances inside an Auto Scaling Group. To achieve this, the developer must send the metrics to Amazon CloudWatch. Which approach provides the MOST secure way of authenticating a CloudWatch PUT request?",answers:[{text:"Create an IAM user with programmatic access. Attach a cloudwatch:PutMetricDatapermission and store the access key and secret key in the instance’s configuration file.",isCorrect:!1},{text:"Create an IAM role with cloudwatch:PutMetricData permission for the new Auto Scaling launch template from which you launch instances.",isCorrect:!0},{text:"Create an IAM user with programmatic access. Attach a cloudwatch:PutMetricData permission and update the Auto Scaling launch template to insert the access key and secret key into the instance user data.",isCorrect:!1},{text:"Modify the existing Auto Scaling launch template to use an IAM role with the cloudwatch:PutMetricData permission for the instances.",isCorrect:!1}],explanation:"A launch template is an instance configuration template that an Auto Scaling group uses to launch EC2 instances. When you create a launch template, you specify information for the instances. Include the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping. If you've launched an EC2 instance before, you specified the same information in order to launch the instance. You can specify your launch template with multiple Auto Scaling groups. However, you can only specify one launch template for an Auto Scaling group at a time, and you can't modify a launch template after you've created it. To change the launch template for an Auto Scaling group, you must create a launch template and then update your Auto Scaling group with it. Access to AWS resources requires permissions. You can create IAM roles and users that include the permissions that you need for the CloudWatch agent to write metrics to CloudWatch and for the CloudWatch agent to communicate with Amazon EC2 and AWS Systems Manager. You use IAM roles on Amazon EC2 instances, and you use IAM users with on-premises servers. In the given scenario, we can create a launch template and specify an IAM role with cloudwatch:PutMetricData permission. Then, use that launch template to create an auto-scaling group. Hence, the correct answer is: Create an IAM role with cloudwatch:PutMetricData permission for the new Auto Scaling launch template from which you launch instances. The option that says: Create an IAM user with programmatic access. Attach a cloudwatch:PutMetricData permission and store the access key and secret key in the instance's configuration file is incorrect because using IAM roles for applications that run on Amazon EC2 instances to provide credentials to the application is more secure. The option that says: Create an IAM user with programmatic access. Attach a cloudwatch:PutMetricData permission and update the Auto Scaling launch template to insert the access key and secret key into the instance via user data is incorrect because you can't update a launch template after you've created it. The option that says: Modify the existing Auto Scaling launch template to use an IAM role with the cloudwatch:PutMetricData permission for the instances is incorrect because modifying an existing launch template is not possible. References: https://docs.aws.amazon.com/autoscaling/ec2/userguide/change-launch-config.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/launch-templates.html Check out this Amazon CloudWatch Cheat Sheet: https://tutorialsdojo.com/amazon-cloudwatch/"},{question:"A mobile game developer is using DynamoDB as a data store and a Web Identity Federation for authorization and authentication. Each item in the DynamoDB table contains the attributes for individual user's game data such as user ID, game scores, and top score where the user ID is the partition key. The developer must control user access to specific data items based on their IDs. In doing so, users will only be able to obtain items that they own. Which of the following solutions must be implemented by the developer?",answers:[{text:"Modify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:Select condition key to the user IDs.",isCorrect:!1},{text:"Modify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:Attributes condition key to the user IDs.",isCorrect:!1},{text:"Modify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:LeadingKeys condition key to the user IDs.",isCorrect:!0},{text:"Modify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:ReturnValues condition key to the user IDs.",isCorrect:!1}],explanation:"In DynamoDB, you can control access to individual data items and attributes in a table. For example, you can do the following: - Grant permissions on a table, but restrict access to specific items in that table based on certain primary key values. An example might be a social networking app for games, where all users' saved game data is stored in a single table, but no users can access data items that they do not own, as shown in the following illustration: - Hide information so that only a subset of attributes is visible to the user. An example might be an app that displays flight data for nearby airports, based on the user's location. Airline names, arrival and departure times, and flight numbers are all displayed. However, attributes such as pilot names or the number of passengers are hidden, as shown in the following illustration: To implement this kind of fine-grained access control, you write an IAM permissions policy that specifies conditions for accessing security credentials and the associated permissions. You then apply the policy to IAM users, groups, or roles that you create using the IAM console. Your IAM policy can restrict access to individual items in a table, access to the attributes in those items, or both at the same time. You can optionally use web identity federation to control access by users who are authenticated by Login with Amazon, Facebook, or Google. You use the IAM Condition element to implement a fine-grained access control policy. By adding a Condition element to a permissions policy, you can allow or deny access to items and attributes in DynamoDB tables and indexes, based upon your particular business requirements. In the given scenario, we are only required to restrict access to specific items in the table based on User Id which is the partition key. We can achieve this by inserting a dynamodb:LeadingKeys condition key to the IAM policy associated with the Identity provider's role. Hence, the correct answer is: Modify the IAM Policy associated with the Identity provider’s role by adding a dynamodb:LeadingKeys condition key. The option that says: Modify the IAM Policy associated with the Identity provider's role by adding a dynamodb:Attributes condition key to the user IDs is incorrect because this type of condition key is used for granting permissions that will limit access to specific attributes in the table. Note that the question is requiring access control to the items. The option that says: Modify the IAM Policy associated with the Identity provider's role by adding a dynamodb:ReturnValues condition key to the user IDs is incorrect because this type of condition key is just used for getting the item attributes as they appear before or after they are updated. The option that says: Modify the IAM Policy associated with the Identity provider's role by adding a dynamodb:Select condition key to the user IDs is incorrect because this type of condition key is mainly used for specifying attributes to be returned in the result of a Query or Scan request. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html#FGAC_DDB.ConditionKeys https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WIF.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"},{question:"A developer has an application that stores sensitive data to an Amazon DynamoDB table. AWS KMS must be used to encrypt the data before sending it to the table and to manage the encryption keys. Which of the following features are supported when using KMS? (Select TWO.)",answers:[{text:"Creation of symmetric encryption and asymmetric KMS keys",isCorrect:!0},{text:"Automatic key rotation for KMS keys in custom key stores",isCorrect:!1},{text:"Importing a custom key material to an asymmetric KMS key",isCorrect:!1},{text:"Using AWS Certificate Manager as a custom key store",isCorrect:!1},{text:"Re-enabling disabled keys",isCorrect:!0}],explanation:"AWS Key Management Service (AWS KMS) is a managed service that makes it easy for you to create and control KMS keys, the encryption keys used to encrypt your data. KMS keys are protected by hardware security modules (HSMs) that are validated by the FIPS 140-2 Cryptographic Module Validation Program except in the China (Beijing) and China (Ningxia) Regions. AWS KMS is integrated with most other AWS services that encrypt your data. AWS KMS is also integrated with AWS CloudTrail to log the use of your KMS keys for auditing, regulatory, and compliance needs. You can perform the following key management functions in AWS KMS: - Create symmetric and asymmetric keys where the key material is only ever used within the service - Create symmetric keys where the key material is generated and used within a custom key store under your control. - Import your own symmetric key for use within the service. - Create both symmetric and asymmetric data key pairs for local use within your applications. - Define which IAM users and roles can manage keys. - Define which IAM users and roles can use keys to encrypt and decrypt data. - Choose to have keys that were generated by the service to be automatically rotated on an annual basis. - Temporarily disable keys so they cannot be used by anyone. - Re-enable disabled keys. - Schedule the deletion of keys that you no longer use. - Audit the use of keys by inspecting logs in AWS CloudTrail. By default, AWS KMS creates the key material for a KMS key. You cannot extract, export, view, or manage this key material. Also, you cannot delete this key material; you must delete the KMS key. However, you can import your own key material into a KMS key or create the key material for it in the AWS CloudHSM cluster associated with an AWS KMS custom key store. There are also types of KMS keys that are not eligible for automatic key rotation such as asymmetric keys, keys in custom key stores, and keys with imported key material. Hence, the correct answers are: - Re-enabling disabled keys. - Creation of symmetric encryption and asymmetric KMS keys. The option that says: Using AWS Certificate Manager as a custom key store is incorrect because you can simply use AWS CloudHSM as a custom key store for AWS KMS. The option that says: Importing a custom key material to an asymmetric KMS key is incorrect because you can only import your own key material into symmetric keys, not asymmetric keys. The option that says: Automatic key rotation for KMS keys in custom key stores is incorrect because automatic key rotation is only supported in symmetric encryption KMS keys. Automatic key rotation is not available for asymmetric keys, keys in custom key stores, and keys with imported key material. References: https://docs.aws.amazon.com/kms/latest/developerguide/overview.html https://aws.amazon.com/kms/faqs/ Check out this AWS KMS Cheat Sheet: https://tutorialsdojo.com/aws-key-management-service-aws-kms/"},{question:"A startup has recently opened an AWS account to develop a cloud-native web application. The CEO wants to improve the security of the account by implementing the best practices in managing access keys in AWS. Which actions follow the security best practices in IAM? (Select TWO.)",answers:[{text:"Use IAM roles for applications that need access to AWS services.",isCorrect:!0},{text:"Delete any access keys to your AWS account root user.",isCorrect:!0},{text:"Save the access key in your application code for convenience.",isCorrect:!1},{text:"Maintain at least one access key for your AWS account root user",isCorrect:!1},{text:"Regularly rotate the credentials for all the account users except for the administrator user for tracking purposes.",isCorrect:!1}],explanation:"AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources. When you first create an AWS account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user and is accessed by signing in with the email address and password that you used to create the account. AWS strongly recommends that you do not use the root user for your everyday tasks, even the administrative ones. Access keys provide programmatic access to AWS. Do not embed access keys within unencrypted code or share these security credentials between users in your AWS account. For applications that need access to AWS, configure the program to retrieve temporary security credentials using an IAM role. Hence, the correct answers are: - Delete any access keys to your AWS account root user. - Use IAM roles for applications that need access to AWS services. The option that says: Save the access key in your application code for convenience is incorrect. Since access keys are long-term credentials, anyone who might get hold of your application code could easily use the access key inside it to use AWS services on behalf of your account as long as the credentials are valid. The option that says: Regularly rotate the credentials for all the account users except for the administrator user for tracking purposes is incorrect. AWS recommends changing your own passwords and access keys regularly. Make sure that all IAM users in your account do as well. That way, if a password or access key is compromised without your knowledge, you limit how long the credentials can be used to access your resources. The option that says: Maintain at least one access key for your AWS account root user is incorrect because AWS recommends deleting all access keys to root users. You can create a separate IAM user instead if you want to have full admin access over your AWS resources. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#delegate-using-roles https://docs.aws.amazon.com/IAM/latest/UserGuide/intro-structure.html Check out this AWS IAM Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"},{question:"A company is planning to launch an online cross-platform game that expects millions of users. The developer wants to use an in-house authentication system for user identification. Each user identifier must be kept consistent across devices and platforms. How can the developer achieve this?",answers:[{text:"Generate a unique IAM access key for each user and use the access key ID as the unique identifier.",isCorrect:!1},{text:"Generate a universally unique identifier (UUID) for each device. Store the UUID with the user in a DynamoDB table.",isCorrect:!1},{text:"Use developer-authenticated identities in Amazon Cognito to generate unique identifiers for the users.",isCorrect:!0},{text:"Create an IAM Role for each user and use its Amazon Resource Name (ARN) as unique identifiers.",isCorrect:!1}],explanation:"Amazon Cognito supports developer authenticated identities, in addition to web identity federation through Facebook (Identity Pools), Google (Identity Pools), Login with Amazon (Identity Pools), and Sign in with Apple (Identity Pools). With developer authenticated identities, you can register and authenticate users via your own existing authentication process, while still using Amazon Cognito to synchronize user data and access AWS resources. Using developer authenticated identities involves interaction between the end-user device, your backend for authentication, and Amazon Cognito. Developers can use their own authentication system with Cognito. What this means is that your app can benefit from all of the features of Amazon Cognito while utilizing your own authentication system. This works by your app requesting a unique identity ID for your end-users based on the identifier you use in your own authentication system. You can use the Cognito identity ID to save and synchronize user data across devices with the Cognito sync service or retrieve temporary, limited-privilege AWS credentials to securely access your AWS resources. The process is simple, you first request a token for your users by using the server-side Cognito API for developer authenticated identities. Cognito then creates a valid token for your users. You can then exchange this token with Amazon Secure Token Service for AWS credentials. With developer authenticated identities, a new API, GetOpenIdTokenForDeveloperIdentity, was introduced. This API call replaces the use of GetId and GetOpenIdToken (APIs needed in the basic authflow) from the device and should be called from your backend as part of your own authentication API. Because this API call is signed by your AWS credentials, Cognito can trust that the user identifier supplied in the API call is valid. This replaces the token validation Cognito performs with public providers. Hence, the correct answer is: Use developer-authenticated identities in Amazon Cognito to generate unique identifiers for the users. The option that says: Generate a universally unique identifier (UUID) for each device. Store the UUID with the user in a DynamoDB table is incorrect because this would produce multiple identifiers for a single user. The scenario requires you to identify users via a unique identifier regardless of the device they use. The option that says: Generate a unique IAM access key for each user and use the access key ID as the unique identifier is incorrect because this means that you'll have to create a unique IAM user (with programmatic access) for each user just for the sake of identification, which is impractical. IAM user is mainly used for accessing services in an AWS account and not for web application authentication. The option that says: Create an IAM Role for each user and use its Amazon Resource Name (ARN) as unique identifiers is incorrect because this requires creating an IAM Role for each user. An IAM Role is not suited for web application authentication, not to mention, that it has a limit per account. Using it to identify game users is infeasible and will not scale well, especially for an application with millions of users. References: https://docs.aws.amazon.com/cognito/latest/developerguide/developer-authenticated-identities.html https://aws.amazon.com/blogs/mobile/understanding-amazon-cognito-authentication-part-2-developer-authenticated-identities/ https://aws.amazon.com/blogs/mobile/amazon-cognito-announcing-developer-authenticated-identities/ Check out this Amazon Cognito Cheat Sheet: https://tutorialsdojo.com/amazon-cognito/"},{question:"A Software Engineer is developing a Node.js application that will be deployed using Elastic Beanstalk. The application source code is currently inside a folder called MyApp. He wants to add a configuration file named tutorialsdojo.config to the application. Where should the file be placed?",answers:[{text:"Inside the MyApp folder at the root level",isCorrect:!1},{text:"Inside the package.json",isCorrect:!1},{text:"Inside the .ebextensions folder",isCorrect:!0},{text:"Inside the .elasticbeanstalk folder",isCorrect:!1}],explanation:"You can add AWS Elastic Beanstalk configuration files (.ebextensions) to your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle. Hence, the correct answer is: Inside the .ebextensions folder. The option that says: Inside the package.json is incorrect. This is just the file that contains the libraries needed by the Node.js application to run. The option that says: Inside the .elasticbeanstalk folder is incorrect because this is simply the folder that contains the environment configuration settings for the current running environment. The option that says: Inside the MyApp folder at the root level is incorrect. Documents with a .config file extension should be placed in the .ebextensions folder. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/"},{question:"A company has a serverless application on AWS. They are using AWS Lambda for business logic and Amazon API Gateway for handling client requests. They have published a version of the AccountService:Prod function with the alias AccountService:Beta. The internal team wants to test these updates before promoting them to production without impacting live users. Which configuration should the company take?",answers:[{text:"Create a new stage named 'Beta' in Amazon API Gateway and use stage variables to reference the Lambda functions in Prod and Beta.",isCorrect:!0},{text:"Modify the existing 'Prod' stage in Amazon API Gateway to create a new stage variable that references the Lambda functions in Prod and Beta.",isCorrect:!1},{text:"Create a 'Beta' stage in Amazon API Gateway. In the Lambda function configuration, add a condition to the AccountService:Prod function that checks for an environment variable and, if set to 'Beta', invokes the AccountService:Beta alias instead",isCorrect:!1},{text:"Update the integration request settings in the 'Prod' stage of the Amazon API Gateway to select the Lambda function alias based on a query parameter.",isCorrect:!1}],explanation:"With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints. For example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, http://example.com). In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage, and calls a different web host (for example, beta.example.com). Similarly, stage variables can be used to specify a different AWS Lambda function name for each stage in your API. In the scenario, by creating a new stage named 'Beta', the company can safely test updates by routing internal traffic to this stage, which will reference the AccountService:Beta version of the Lambda function. This will allow testers to invoke the new version of the function while end users continue to access the stable, production-ready AccountService:Prod version via the 'Prod' stage. Hence, the correct answer is: Create a new stage named 'Beta' in Amazon API Gateway and use stage variables to reference the Lambda functions in Prod and Beta. The option that says: Modify the existing 'Prod' stage in Amazon API Gateway to create a new stage variable that references the Lambda functions in Prod and Beta is incorrect. The 'Prod' stage contains settings critical for the production environment, such as API keys and rate limits. Testing on this stage could disrupt live traffic and affect real users even though you're hitting a non-prod Lamdba function. The option that says: Update the integration request settings in the 'Prod' stage of the Amazon API Gateway to select the Lambda function alias based on a query parameter is incorrect. This is risky since it can expose the beta environment to end users as well. The option that says: Create a 'Beta' stage in Amazon API Gateway. In the Lambda function configuration, add a condition to the AccountService:Prod function that checks for an environment variable and, if set to 'Beta', invokes the AccountService:Beta alias instead is incorrect. This approach would require deploying code changes to the production function, which adds complexity and inadvertently affects the production environment. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html https://aws.amazon.com/blogs/compute/using-api-gateway-stage-variables-to-manage-lambda-functions/ Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A startup is integrating an event-driven alerting tool with a third-party platform. The platform requires a publicly accessible HTTPS endpoint to receive webhook requests, which will be processed by a Lambda function. Given that the platform signs each request with a secret key and includes it in the headers, the developer must ensure that the Lambda function executes the domain logic only when a webhook request comes from a valid user. Which action would satisfy the requirement with the least amount of development effort?",answers:[{text:'Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the "lambda:FunctionUrlAuthType": "NONE" condition is present. Write a custom authorization logic based on a signature provided in the HTTP headers.',isCorrect:!0},{text:'Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the "lambda:CodeSigningConfigArn": "arn:aws:lambda:<AWS_REGION>:<ACCOUNT_NUMBER>:code-signing-config:csc-<SIGNING_SECRET>" condition is present.',isCorrect:!1},{text:'Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the "lambda:FunctionUrlAuthType": "AWS_IAM" condition is present.',isCorrect:!1},{text:"Configure API Gateway to connect with the Lambda function using a Lambda proxy integration. Create a Lambda function authorizer to validate incoming requests based on a signature provided in the HTTP headers.",isCorrect:!1}],explanation:'If you need a simple way to configure an HTTPS endpoint in front of your Lambda function without having to learn and configure additional services besides Lambda, you can use Lambda function URLs. This can be useful in cases where you need to implement a simple webhook handler or form validator that runs within an individual Lambda function and does not require additional functionality beyond processing incoming requests. By using Lambda function URLs, you can directly invoke your Lambda function using a simple HTTPS request without needing to set up and configure additional services like API Gateway. This approach can be a simple and efficient way to handle incoming requests and integrate with other services or third-party platforms that require a publicly accessible HTTPS endpoint. There are two types of authorization available for Lambda function URLs: AWS_IAM - the function URL can only be invoked by an IAM user or role with the necessary permissions. This can be useful in cases where you need to restrict access to the Lambda function to a specific set of users or roles within your organization. NONE - anyone can invoke the Lambda function using the URL. This approach can be useful in cases where you want to make the Lambda function publicly accessible and do not require any additional authentication or authorization beyond the URL. However, you may still need to validate the incoming requests in the Lambda function to ensure that the request comes from a trusted source. By setting the "lambda:FunctionUrlAuthType" condition to "NONE," the function will be publicly accessible without requiring any additional authentication. However, you still need to write custom authorization logic to verify the signature provided in the HTTP headers and ensure that the request is coming from a valid user. Hence the correct answer is: Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the "lambda:FunctionUrlAuthType": "NONE" condition is present. Write a custom authorization logic based on a signature provided in the HTTP headers. The option that says: Configure API Gateway to connect with the Lambda function using a Lambda proxy integration. Create a Lambda function authorizer to validate incoming requests based on a signature provided in the HTTP headers is incorrect. While it\'s a valid solution, this is not the best choice because it involves additional setup and configuration of API Gateway to only invoke a single Lambda function. The option that says: Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the "lambda:FunctionUrlAuthType": "AWS_IAM" condition is present is incorrect. This authentication type means that the Lambda function can only be invoked by an authorized IAM user or role. The scenario specifically mentions that each request are signed before they are received by the Lambda function The option that says: Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the "lambda:CodeSigningConfigArn": "arn:aws:lambda:<AWS_REGION>:<ACCOUNT_NUMBER>:code-signing-config:csc-<SIGNING_SECRET>" condition is present is incorrect, as code signing is a security feature that verifies the integrity of code running in your Lambda functions and helps ensure that only trusted code is deployed. References: https://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html https://aws.amazon.com/blogs/aws/announcing-aws-lambda-function-urls-built-in-https-endpoints-for-single-function-microservices/ Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/'},{question:"A developer plans to use AWS Elastic Beanstalk to deploy a microservice application. The application will be implemented in a multi-container Docker environment. How should the developer configure the container definitions in the environment?",answers:[{text:"Use the eb config command to configure the container definitions.",isCorrect:!1},{text:"Configure the container definitions in the Dockerrun.aws.json.config and put it inside the .ebextensions folder.",isCorrect:!1},{text:"Configure the container definitions in the Amazon ECS Console when building the Docker environment.",isCorrect:!1},{text:"Configure the container definitions in the Dockerrun.aws.json file.",isCorrect:!0}],explanation:"Standard generic and preconfigured Docker platforms on Elastic Beanstalk support only a single Docker container per Elastic Beanstalk environment. In order to get the most out of Docker, Elastic Beanstalk lets you create an environment where your Amazon EC2 instances run multiple Docker containers side by side. The following diagram shows an example Elastic Beanstalk environment configured with three Docker containers running on each Amazon EC2 instance in an Auto Scaling group: Container instances—Amazon EC2 instances running Multicontainer Docker in an Elastic Beanstalk environment—require a configuration file named Dockerrun.aws.json. This file is specific to Elastic Beanstalk and can be used alone or combined with source code and content in a source bundle to create an environment on a Docker platform. Hence, the correct answer is: Configure the container definitions in the Dockerrun.aws.json file. The option that says: Configure the container definitions in the Amazon ECS Console when building the Docker environment is incorrect because the application must be deployed using Elastic Beanstalk. Therefore, you must configure the container definitions in the Dockerrun.aws.json file. The option that says: Configure the container definitions in the Dockerrun.aws.json.config and put it inside the .ebextensions folder is incorrect because the Dockerrun.aws.json file should be placed on the same level where your application file resides. The option that says: Use the eb config command to configure the container definitions is incorrect. This is just a command that you can use to change the configuration settings of your environment. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecstutorial.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/single-container-docker-configuration.html Check out this AWS Elastic Beanstalk Cheat Sheet: https://tutorialsdojo.com/aws-elastic-beanstalk/"},{question:"A developer is testing a Lambda function that was created from a CloudFormation template. While the function executes without errors, it isn't generating logs in Amazon CloudWatch Logs, and the developer cannot find associated log streams or log groups. Upon inspection, the following observations were made: The function's code contains appropriate logging statements. The Lambda function is associated with an execution role that establishes a trusted relationship with the Lambda service; however, this role has no permissions assigned. The Lambda function does not have any resource-based policies. Which configuration must be done to resolve the issue?",answers:[{text:"Update the execution role by adding the AWSLambdaBasicExecutionRole managed policy.",isCorrect:!0},{text:"Update the execution role by adding the CloudWatchLambdaInsightsExecutionRolePolicy managed policy.",isCorrect:!1},{text:"Associate the function with a resource-based policy that contains the logs:CreateLogGroup, logs:CreateLogStream, and logs:PutLogEvents permissions.",isCorrect:!1},{text:"Associate the function with a resource-based policy that contains the logs:PutLogEvents permissions.",isCorrect:!1}],explanation:"A Lambda function's execution role is an AWS Identity and Access Management (IAM) role that grants the function permission to access AWS services and resources. For example, you might create an execution role that has permission to send logs to Amazon CloudWatch and upload trace data to AWS X-Ray. The AWSLambdaBasicExecutionRole is a managed policy provided by AWS that includes permissions essential for a Lambda function to create and write logs to Amazon CloudWatch Logs. These permissions include permissions to Log actions such as logs:CreateLogGroup, logs:CreateLogStream, and logs:PutLogEvents. When you create a Lambda function using the AWS Management Console, the execution role that AWS automatically creates for you often includes this managed policy. However, when defining a Lambda function via a CloudFormation template or other Infrastructure as Code (IAC) methods, you might need to explicitly attach this policy to the function's execution role to ensure that it has the appropriate logging permissions. Hence, the correct answer is: Update the execution role by adding the AWSLambdaBasicExecutionRole managed policy. The option that says: Associate the function with a resource-based policy that contains the logs:CreateLogGroup, logs:CreateLogStream, and logs:PutLogEvents permissions is incorrect. While this option contains the required permissions, they are defined as resource-based policies rather than as part of the execution role's policy. Resource-based policies just define what AWS services or users are allowed to do with the Lambda function (like invoking the function). The permissions required to write to CloudWatch Logs should be associated with the execution role. The option that says: Update the execution role by adding the CloudWatchLambdaInsightsExecutionRolePolicy managed policy is incorrect. Although this policy might have permissions similar to AWSLambdaBasicExecutionRole, its target resource is specific to Lambda Insights and not for Lambda functions. The option that says: Associate the function with a resource-based policy that contains the logs:PutLogEvents permissions is incorrect. This option suggests associating the Lambda function with a resource-based policy containing only the logs:PutLogEvents permission, which is insufficient. This permission alone allows the function to send log data to existing streams but does not permit the creation of new log groups or streams in Amazon CloudWatch Logs. References: https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html https://repost.aws/knowledge-center/lambda-cloudwatch-log-streams-error Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A developer is debugging an issue in an AWS Lambda-based application. To save time searching through logs, the developer wants the function to return the corresponding log location of an invocation request. Which approach should the developer take with the least amount of effort?",answers:[{text:"Extract the log stream name from the Event object of the handler function.",isCorrect:!1},{text:"Extract the log stream name from the Context object of the handler function.",isCorrect:!0},{text:"Extract the invocation request id from the Context object of the handler function. Then, call the FilterLogEvents API and pass the request id to filter results.",isCorrect:!1},{text:"Extract the invocation request id from the Event object of the handler. Call the FilterLogEvents API and use the request id to filter results.",isCorrect:!1}],explanation:"When Lambda runs your function, it passes a context object to the handler. This object provides methods and properties that provide information about the invocation, function, and execution environment. One of the properties that you can get from the context object is the log_stream_name which gives the log location of a function instance. As shown in the screenshot above, we can easily retrieve the corresponding log stream of a request by returning the value of context.log_stream_name. For the full list of context methods and properties, see this link. Hence, the correct answer is: Extract the log stream name from the Context object of the handler function. The option that says: Extract the invocation request id from the Context object of the handler function. Then, call the FilterLogEvents API and pass the request id to filter results is incorrect because this adds unnecessary steps to meet the requirement. The log stream name is directly available in the Context object. The following options can be eliminated because the log stream name and request-id are not properties of the event object: - Extract the log stream name from the Event object of the handler function. - Extract the invocation request id from the Event object of the handler function. Call the FilterLogEvents API and use the request id to filter results. References: https://docs.aws.amazon.com/lambda/latest/dg/nodejs-context.html https://docs.aws.amazon.com/lambda/latest/dg/python-handler.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A company uses AWS CodeDeploy in their CI/CD pipeline to handle in-place deployments of their web application on EC2 instances. Recently, a new version of the application was pushed, which contained a code regression. The deployment group is configured with automatic rollback. What happens if the deployment of the new version fails?",answers:[{text:"CodeDeploy pauses the current deployment, restores the last stable version from S3, and uses the deployment ID of the most recent deployment with a SUCCEEDED status.",isCorrect:!1},{text:"CodeDeploy redeploys the last known good version of an application with a new deployment ID.",isCorrect:!0},{text:"CodeDeploy reroutes traffic back to the blue environment and terminates the green environment.",isCorrect:!1},{text:"CodeDeploy reverts the EC2 instance to a previous AMI snapshot taken during the last successful deployment.",isCorrect:!1}],explanation:'In AWS CodeDeploy, during an in-place deployment, the previous version of the application on each compute resource is stopped, and then the latest application revision is installed. After installation, the new version of the application is started and validated. You can configure a deployment group or deployment to automatically roll back when a deployment fails or when a monitoring threshold you specify is met. In this case, CodeDeploy redeploys the last known good version of an application revision. These rolled-back deployments are technically new deployments, with new deployment IDs, rather than restored versions of a previous deployment. Hence, the correct answer is: CodeDeploy redeploys the last known good version of an application with a new deployment ID. The option that says: CodeDeploy pauses the current deployment, restores the last stable version from S3, and uses the deployment ID of the most recent deployment with a SUCCEEDED status is incorrect. CodeDeploy doesn\'t "pause" deployments during a rollback. Instead, it initiates a new deployment with a new unique deployment ID for the last known good revision. It does not reuse the deployment ID of the previous successful deployment. The option that says: CodeDeploy reroutes traffic back to the blue environment and terminates the green environment is incorrect because this describes a "blue/green" deployment strategy. While AWS CodeDeploy does support blue/green deployments, the question explicitly mentioned "in-place deployments.", where the application is directly updated on the existing instances. The option that says: CodeDeploy reverts the EC2 instance to a previous AMI snapshot taken during the last successful deployment is incorrect. AWS CodeDeploy does not take or revert to AMI snapshots as part of its deployment process. Instead, it focuses on deploying application revisions and keeps a record of these revisions. The process of managing AMIs and snapshots is outside the scope of CodeDeploy. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html https://docs.aws.amazon.com/whitepapers/latest/introduction-devops-aws/in-place-deployments.html Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/'},{question:"A company uses a Linux, Apache, MySQL, and PHP (LAMP) web service stack to host an on-premises application for its car rental business. The manager wants to move its operation into the Cloud using Amazon Web Services. Which combination of services could be used to run the application that will require the least amount of configuration?",answers:[{text:"Amazon API Gateway and Amazon RDS",isCorrect:!1},{text:"Amazon EC2 and Amazon Aurora",isCorrect:!0},{text:"Amazon ECS and Amazon EFS",isCorrect:!1},{text:"Amazon S3 and Amazon CloudFront",isCorrect:!1}],explanation:"You can install an Apache web server with PHP and MySQL support on your Amazon Linux instance (sometimes called a LAMP web server or LAMP stack). You can use this server to host a static website or deploy a dynamic PHP application that reads and writes information to a database. To decouple the database from the application, you can choose from the AWS Database services that support MySQL (e.g., Amazon RDS, Amazon Aurora) From the options given, we can deploy a LAMP web server by using an EC2 instance and an Amazon Aurora database for MySQL. Hence, the correct answer is Amazon EC2 and Amazon Aurora. Amazon S3 and Amazon CloudFront are incorrect because Amazon S3 is only capable of serving static websites. You cannot use an S3 bucket to host a LAMP web server. Amazon ECS and Amazon EFS are incorrect. Although it is possible to containerize a LAMP web server and host it on Amazon ECS, it's not suitable for the scenario because the web server is just a monolithic application rather than a microservice. Configuring ECS entails more effort than EC2 and Amazon EFS is only used for POSIX-compliant storage. Amazon API Gateway and Amazon RDS are incorrect. While you can host a MySQL database using Amazon RDS, you can't use API Gateway to host the Apache web server. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/install-LAMP.html https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Tutorials.WebServerDB.CreateWebServer.html Check out these Amazon EC2 and Amazon Aurora Cheat Sheets: https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/ https://tutorialsdojo.com/amazon-aurora/"},{question:"A developer needs to use IAM roles to list all EC2 instances that belong to the development environment in an AWS account. Which methods could be done to verify IAM access to describe instances? (Select TWO.)",answers:[{text:"Use the IAM Policy Simulator to validate the permission for the IAM role.",isCorrect:!0},{text:"Run the describe-instances command with the --dry-run parameter.",isCorrect:!0},{text:"Run the get-group-policy command.",isCorrect:!1},{text:"Validate the IAM role’s permission by querying the in-line policies within the EC2 instance metadata.",isCorrect:!1},{text:"Run the get-role command.",isCorrect:!1}],explanation:"The --dry-run parameter checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is DryRun-Operation. Otherwise, it is UnauthorizedOperation. With the IAM policy simulator, you can test and troubleshoot identity-based policies, IAM permissions boundaries, Organizations service control policies (SCPs), and resource-based policies. You can test policies that are attached to IAM users, groups, or roles in your AWS account. If more than one policy is attached to the user, group, or role, you can test all the policies or select individual policies to test. You can test which actions are allowed or denied by the selected policies for specific resources. Hence, the correct answers are: - Run the describe-instances command with the --dry-run parameter. - Use the IAM Policy Simulator to validate the permission for the IAM role. The option that says: Validate the IAM role’s permission by querying the in-line policies within the EC2 instance metadata is incorrect because in-line policies are not available in the EC2 instance metadata. The option that says: Run the get-group-policy command is incorrect because this command just retrieves the specified inline policy document that is embedded in the specified IAM group. Since the developer is using IAM roles, this command won't work because IAM roles can't be associated with an IAM group. The option that says: Run the get-role command is incorrect. While this command retrieves information about an IAM role, it does not provide information about the permissions attached to that IAM role. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-help.html Check out this AWS IAM Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"},{question:"An application is used to upload images to an Amazon S3 bucket. Once an event occurs, a Lambda function is triggered to compress the photos. However, it has been discovered that the processing time of the function is longer than expected. Which change will improve the processing time of the function most effectively?",answers:[{text:"Configure the S3 bucket to send notifications to an SQS queue. Use the SQS queue with the Lambda function to process the image.",isCorrect:!1},{text:"Increase the timeout value of the function.",isCorrect:!1},{text:"Increase the memory allocation of the function.",isCorrect:!0},{text:"Run the function with Lambda@Edge which will run the code closer to the users of your application, reducing your application’s latency.",isCorrect:!1}],explanation:"Allocating more memory to a Lambda function also increases the amount of CPU, network, and other resources allocated to it. By provisioning more memory, you can improve the performance and speed of your function while potentially reducing your costs. You should benchmark your use case to determine where the breakeven point is for running faster and using more memory vs running slower and using less memory. In the scenario, by increasing the memory allocation of the function, the CPU power and network throughput available to the function are also increased, which can speed up the execution of the function and result in faster processing times for the images. Hence, the correct answer is: Increase the memory allocation of the function. The option that says: Increase the timeout value of the function is incorrect. This can provide more time for the function to execute before it times out, which may be useful if the function is being terminated prematurely. However, simply increasing the timeout value may not necessarily improve the processing time of the function. The option that says: Configure the S3 bucket to send notifications to an SQS queue. Use the SQS queue with the Lambda function to process the image is incorrect. Adding an SQS queue to the solution won't necessarily make the Lambda function run faster but can improve the overall fault tolerance of the system. The option that says: Run the function with Lambda@Edge which will run the code closer to the users of your application, reducing your application’s latency is incorrect. This may improve latency, but it may not necessarily improve the processing time of the function. Additionally, this solution will require you to set up a CloudFront distribution, which introduces additional costs. References: https://aws.amazon.com/blogs/architecture/best-practices-for-developing-on-aws-lambda/ https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"An application uses the PutObject operation in parallel to upload hundreds of thousands of objects per second to an S3 bucket. To meet security compliance, the developer uses the server-side encryption in AWS KMS (SSE-KMS) to encrypt objects as they get stored in the S3 bucket. There is a noticeable performance degradation after making the change. Which of the following is the most likely cause of the problem?",answers:[{text:"The KMS key does not use an alias required for the server-side encryption with AWS KMS (SSE-KMS).",isCorrect:!1},{text:"The Amazon S3 throttles the PutObject operation for objects encrypted with SSE-KMS.",isCorrect:!1},{text:"The AWK KMS key uses an AES 256 algorithm, which makes the encryption process slower. AES 128 should be used instead.",isCorrect:!1},{text:"The API request rate has exceeded the quota for AWS KMS API operations.",isCorrect:!0}],explanation:"AWS KMS establishes quotas for the number of API operations requested in each second. You can make API requests directly or by using an integrated AWS service that makes API requests to AWS KMS on your behalf. The quota applies to both kinds of requests. For example, you might store data in Amazon S3 using server-side encryption with AWS KMS (SSE-KMS). Each time you upload or download an S3 object that's encrypted with SSE-KMS, Amazon S3 makes a GenerateDataKey (for uploads) or Decrypt (for downloads) request to AWS KMS on your behalf. These requests count toward your quota, so AWS KMS throttles the requests if you exceed a combined total of 5,500 (or 10,000 or 30,000 depending upon your AWS Region) uploads or downloads per second of S3 objects encrypted with SSE-KMS. Hence, the correct answer is: The API request rate has exceeded the quota for AWS KMS API operations. The option that says: The Amazon S3 throttles the PutObject operation for objects encrypted with SSE-KMS is incorrect because Amazon S3 can automatically scale to high request rates with or without server-side encryption through parallelization. The option that says: The AWK KMS key uses an AES 256 algorithm, which makes the encryption process slower. AES 128 should be used instead is incorrect. While it is true that AES 256 is technically slower than AES 128 (because it has a larger key size), the performance difference is hardly noticeable. That being said, this is unlikely to be the cause of the problem. The option that says: The KMS key does not use an alias required for the server-side encryption with AWS KMS (SSE-KMS) is incorrect because an alias is simply a reference name that points to a key. It is not required for using a KMS key in SSE-KMS. The key can be referenced directly by its ARN or an alias, but the absence of an alias wouldn't cause performance degradation. References: https://docs.aws.amazon.com/kms/latest/developerguide/requests-per-second.html#rps-from-service https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html Check out this AWS KMS Cheat Sheet: https://tutorialsdojo.com/aws-key-management-service-aws-kms/"},{question:"A development team is building a website that displays an analytics dashboard. The team uses AWS CodeBuild to compile the website from a source code residing on Github. A member was instructed to configure CodeBuild to run with a proxy server for privacy and security reasons. A RequestError timeout error appears on CloudWatch whenever CodeBuild is accessed. Which is a possible solution to resolve the issue?",answers:[{text:"Modify the proxy element of the buildspec.yml file on the source code root directory.",isCorrect:!0},{text:"Modify the artifacts element of the buildspec.yml file on the source code root directory.",isCorrect:!1},{text:"Modify the proxy element of the AppSpec.yml file on the source code root directory.",isCorrect:!1},{text:"Modify the phases element of the AppSpec.yml file on the source code root directory.",isCorrect:!1}],explanation:"You can use AWS CodeBuild with a proxy server to regulate HTTP and HTTPS traffic to and from the Internet. To run CodeBuild with a proxy server, you install a proxy server in a public subnet and CodeBuild in a private subnet in a VPC. Below are possible causes of error when running CodeBuild with a proxy server: ssl-bump is not configured properly. Your organization's security policy does not allow you to use ssl-bump. Your buildspec.yml file does not have proxy settings specified using a proxy element. If you do not use ssl-bump for an explicit proxy server, add a proxy configuration to your buildspec.yml using a proxy element. version: 0.2proxy:upload-artifacts: yeslogs: yes Hence, the correct answer is: Modify the proxy element of the buildspec.yml file on the source code root directory. The following options are both incorrect because the AppSpec file is used in AWS CodeDeploy and not in AWS CodeBuild: - Modify the phases element of the AppSpec.yml file on the source code root directory. - Modify the proxy element of the AppSpec.yml file on the source code root directory. The option that says: Modify the artifacts element of the buildspec.yml file on the source code root directory is incorrect because the artifacts element represents information about where CodeBuild can find the build output and how CodeBuild prepares it for uploading to the S3 output bucket. You can not configure proxy settings here. Use the proxy element if you want to run CodeBuild in an explicit proxy server. References: https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html#build-spec.proxy Check out this AWS CodeBuild Cheat Sheet: https://tutorialsdojo.com/aws-codebuild/ AWS CodeBuild Overview: https://youtu.be/1zA6mK9BdA4"},{question:"A developer wants to cut down the execution time of the scan operation on a DynamoDB table during periods of low demand without interfering with typical workloads. The operation consumes half of the strongly consistent read capacity units within regular operating hours. How can the developer improve this scan operation?",answers:[{text:"Perform a rate-limited parallel scan operation.",isCorrect:!0},{text:"Use a parallel scan operation.",isCorrect:!1},{text:"Use eventually consistent reads for the scan operation instead of strongly consistent reads.",isCorrect:!1},{text:"Perform a rate-limited sequential scan operation.",isCorrect:!1}],explanation:"By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data. The larger the table or index being scanned, the more time the Scan takes to complete. In addition, a sequential Scan might not always be able to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table's data across multiple physical partitions, a Scan operation can only read one partition at a time. For this reason, the throughput of a Scan is constrained by the maximum throughput of a single partition. To address these issues, the Scan operation can logically divide a table or secondary index into multiple segments, with multiple application workers scanning the segments in parallel. Each worker can be a thread (in programming languages that support multithreading) or an operating system process. To perform a parallel scan, each worker issues its own Scan request with the following parameters: To make the most of your table’s provisioned throughput, you’ll want to use the Parallel Scan API operation so that your scan is distributed across your table’s partitions. But be careful that your scan doesn’t consume your table’s provisioned throughput and cause the critical parts of your application to be throttled. To avoid throttling, you need to rate-limit your client application. Hence, the correct answer is: Perform a rate-limited parallel scan operation. The option that says: Perform a rate-limited sequential scan operation is incorrect because a DynamoDB scan operation is sequential by default, therefore, there will be no improvement in the execution time of the current scan operation. The option that says: Use a parallel scan operation is incorrect because running a parallel scan alone might consume all of your table's provisioned throughput which may affect your application's normal workload. You must use a rate-limiter along with it. The option that says: Use eventually consistent reads for the scan operation instead of strongly consistent reads is incorrect. You might reduce the cost of your provisioned throughput but the scan operation will still run sequentially, making no improvements at all. References: https://aws.amazon.com/blogs/developer/rate-limited-scans-in-amazon-dynamodb/ https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Scan.html https://amazon-dynamodb-labs.com/design-patterns/ex2scan/step2.html Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"},{question:"A developer uses Amazon ECS to orchestrate two Docker containers. He needs to configure ECS to allow the two containers to share log data. Which configuration should the developer do?",answers:[{text:"Use two task definitions for each container and mount an EFS volume between the tasks.",isCorrect:!1},{text:"Specify the containers in a single pod specification and configure EFS as its volume type.",isCorrect:!1},{text:"Specify the containers in a single task definition and configure EFS as its volume type.",isCorrect:!0},{text:"Use two pod specifications for each container and mount an EFS volume between the pods.",isCorrect:!1}],explanation:"A task definition is required to run Docker containers in Amazon ECS. The following are some of the parameters you can specify in a task definition: - The Docker image to use with each container in your task - How much CPU and memory to use with each task or each container within a task - The launch type to use, which determines the infrastructure on which your tasks are hosted - The Docker networking mode to use for the containers in your task - The logging configuration to use for your tasks - Whether the task should continue to run if the container finishes or fails - The command the container should run when it is started - Any data volumes that should be used with the containers in the task - The IAM role that your tasks should use Before you can run Docker containers on Amazon ECS, you must create a task definition. You can define multiple containers and data volumes in a single task definition. Hence, the correct answer is: Specify the containers in a single task definition and configure EFS as its volume type. The option that says: Use two task definitions for each container and mount an EFS volume between the tasks is incorrect because you can define two containers in a task definition. Creating two task definitions is unnecessary. A pod is an execution unit specifically used in Kubernetes. Since the scenario mentioned that the containers are to be orchestrated using Amazon ECS, The following options are both incorrect: - Use two pod specifications for each container and mount an EFS volume between the pods. - Specify the containers in a single pod specification and configure EFS as its volume type. References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html https://docs.aws.amazon.com/AmazonECS/latest/developerguide/example_task_definitions.html Check out this Amazon ECS Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/"},{question:"Several development teams worldwide will be collaboratively working on a project hosted on an AWS Elastic Beanstalk environment. The developers need to be able to deploy incremental code updates without re-uploading the entire project. Which of the following actions will reduce the upload and deployment time with the LEAST amount of effort?",answers:[{text:"Configure event notifications on a central S3 bucket and allow access to all developers. Invoke a Lambda Function that will deploy the code to Elastic Beanstalk when a PUT event occurs.",isCorrect:!1},{text:"Host the code repository on an EC2 instance and allow access to all the developers. Write a script that will automate the deployment process to Elastic Beanstalk.",isCorrect:!1},{text:"Create an AWS CodeCommit repository and allow access to all developers. Deploy the code to Elastic Beanstalk.",isCorrect:!0},{text:"Upload the code to an Amazon EFS mounted on an EC2 instance. Write a script that will automate the deployment process to Elastic Beanstalk.",isCorrect:!1}],explanation:"CodeCommit is a secure, highly scalable, managed source control service that hosts private Git repositories. CodeCommit eliminates the need for you to manage your own source control system or worry about scaling its infrastructure. AWS CodeCommit is designed for collaborative software development. You can easily commit, branch, and merge your code allowing you to easily maintain control of your team's projects. CodeCommit also supports pull requests, which provide a mechanism to request code reviews and discuss code with collaborators. You can create a repository from the AWS Management Console, AWS CLI, or AWS SDKs and start working with the repository using Git. You can use the EB CLI to deploy your application directly from your AWS CodeCommit repository. With CodeCommit, you can upload only your changes to the repository when you deploy, instead of uploading your entire project. Hence, the correct answer is: Create an AWS CodeCommit repository and allow access to all developers. Deploy the code to Elastic Beanstalk. The option that says: Host the code repository on an EC2 instance and allow access to all the developers. Write a script that will automate the deployment process to Elastic Beanstalk is incorrect because this solution entails a lot of effort — from selecting the AMI to managing the instance, setting up a code repository, and configuring scalability. AWS CodeCommit is already a managed and highly scalable source control service that has native integration with Elastic Beanstalk. The option that says: Configure event notifications on a central S3 bucket and allow access to all developers. Invoke a Lambda Function that will deploy the code to Elastic Beanstalk when a PUT event occurs is incorrect. Amazon S3 does not have the functionality to modify stored objects. Incremental code updates are not possible with S3, which defies the requirement for the scenario. The option that says: Upload the code to an Amazon EFS mounted on an EC2 instance. Write a script that will automate the deployment process to Elastic Beanstalk is incorrect. Although EFS is a good use case for concurrent access, it is not a suitable solution for a source control service. EFS is less performant to workloads that require random access over large files. Use EFS if you want to distribute highly parallelized workloads like analytical workloads and media processing across several machines. References: https://aws.amazon.com/premiumsupport/knowledge-center/deploy-codecommit-elastic-beanstalk/ https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli-codecommit.html Check out these AWS CodeCommit and AWS Elastic Beanstalk Cheat Sheets: https://tutorialsdojo.com/aws-elastic-beanstalk/ https://tutorialsdojo.com/aws-codecommit/"},{question:"A development team uses AWS Step Functions to orchestrate a serverless workflow that processes incoming data streams with AWS Lambda. The team requires a solution to extract custom metrics, such as processing times, directly from the logs generated by a Lambda function. These metrics must be analyzed for operational insights, with alarms set up to detect anomalies in real-time. Which approach should be used to meet this requirement?",answers:[{text:"Configure Amazon Data Firehose to stream logs from the Lambda function to an Amazon Redshift cluster. Use SQL queries to analyze metrics and set alerts for anomalies.",isCorrect:!1},{text:"Use Amazon's open-source libraries to format logs in the Amazon CloudWatch embedded metric format. Then, use CloudWatch to monitor, view, and set alarms for the custom metrics.",isCorrect:!0},{text:"Use Amazon CloudWatch Lambda Insights to extract and monitor custom metrics for processing times. Set up alarms and dashboards in Lambda Insights for real-time analysis.",isCorrect:!1},{text:"Send custom metric data directly to Amazon EventBridge using the PutMetricData API operation. Create EventBridge rules to trigger actions based on the metrics.",isCorrect:!1}],explanation:"The Amazon CloudWatch embedded metric format allows you to generate custom metrics asynchronously in the form of logs written to CloudWatch Logs. You can embed custom metrics alongside detailed log event data, and CloudWatch automatically extracts the custom metrics so that you can visualize and alarm on them, for real-time incident detection. Additionally, the detailed log events associated with the extracted metrics can be queried using CloudWatch Logs Insights to provide deep insights into the root causes of operational events. Embedded metric format helps you generate actionable custom metrics from ephemeral resources such as Lambda functions and containers. By using the embedded metric format to send logs from these ephemeral resources, you can now easily create custom metrics without having to instrument or maintain separate code, while gaining powerful analytical capabilities on your log data. The Amazon CloudWatch embedded metric format (EMF) is a JSON specification used to instruct CloudWatch Logs to automatically extract metric values embedded in structured log events. You can use CloudWatch to graph and create alarms on the extracted metric values. The following is an example of embedded metric format. By using EMF, you can extract and monitor these custom metrics directly from your logs, enabling you to gain operational insights and set alarms based on these metrics. Additionally, Amazon's open-source libraries provide a convenient way to format logs in the EMF. These libraries can be integrated into your application to structure the log events with the necessary metric data, ensuring that CloudWatch can accurately extract and process these custom metrics. Hence, the correct answer is: Use Amazon's open-source libraries to format logs in the Amazon CloudWatch embedded metric format. Then, use CloudWatch to monitor, view, and set alarms for the custom metrics. The option that says: Send custom metric data directly to Amazon EventBridge using the PutMetricData API operation. Create EventBridge rules to trigger actions based on the metrics is incorrect because Amazon EventBridge is typically used for event-driven architectures, not for monitoring or custom metrics. EventBridge rules are used to respond to events, not to monitor metrics. The option that says: Use Amazon CloudWatch Lambda Insights to extract and monitor custom metrics for processing times. Set up alarms and dashboards in Lambda Insights for real-time analysis is incorrect because Lambda Insights is primarily designed for monitoring and troubleshooting Lambda performance but does not directly extract custom metrics from logs. The option that says: Configure Amazon Data Firehose to stream logs from the Lambda function to an Amazon Redshift cluster. Use SQL queries to analyze metrics and set alerts for anomalies is incorrect. While this approach can be used for log analysis, it is overly complex for the requirement of extracting custom metrics from logs in real-time. Data Firehose and Amazon Redshift are more suited for large-scale data analysis rather than real-time monitoring and alerting. References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Embedded_Metric_Format_Specification.html https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Embedded_Metric_Format.html Check out this Amazon CloudWatch Cheat Sheet: https://tutorialsdojo.com/amazon-cloudwatch/"},{question:"A developer is managing several microservices built using API Gateway and AWS Lambda. The Developer wants to deploy new updates to one of the APIs. He wants to ensure a smooth transition between the versions by giving users enough time to migrate to the new version before retiring the previous one. Which solution should the developer implement?",answers:[{text:"Implement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target resource then redeploy it to the same stage.",isCorrect:!1},{text:"Implement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target resource then redeploy it to a new stage.",isCorrect:!0},{text:"Implement the updates and publish a new version of the Lambda function. Then, issue the new Lambda invocation URL.",isCorrect:!1},{text:"Implement the updates on the Lambda function. Create a CloudFront distribution and use the function as the origin.",isCorrect:!1}],explanation:"To deploy an API, you create an API deployment and associate it with a stage. A stage is a logical reference to a lifecycle state of your API (for example, dev, prod, beta, v2). API stages are identified by the API ID and stage name. They're included in the URL that you use to invoke the API. Each stage is a named reference to an API deployment and is made available for client applications to call. In the scenario, you can apply the new updates to the backend Lambda function and publish it as a new version. Then, update the integration request of the target API resource by replacing the old Lambda function ARN with the new version's ARN. Finally, deploy the resource to a new stage and use the new Invoke URL in your application. This way, existing users will be able to access both versions. You can retire the old version eventually after all users have migrated to the new one. Hence, the correct answer is: Implement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target resource then redeploy it to a new stage. The option that says: Implement the updates and publish a new version of the Lambda function. Then, issue the new Lambda invocation URL is incorrect. Although you can invoke a Lambda function by its ARN, you can't directly invoke a Lambda by its URL because AWS does not provide one. However, you can invoke it through a URL when integrated with API Gateway. The option that says: Implement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target resource then redeploy it to the same stage is incorrect because the objective is to publish two live endpoints (one for version 1 and another for version 2) that the users can access. Therefore, we need to create two stages. Deploying the new version to the same stage would overwrite the old version. The option that says: Implement the updates on the Lambda function. Create a CloudFront distribution and use the function as the origin is incorrect because a Lambda function is not a valid origin for a CloudFront distribution. Moreover, CloudFront is for content delivery and not used for exposing backend services as API endpoints. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A developer wants to expose a legacy web service that uses an XML-based Simple Object Access Protocol (SOAP) interface through API Gateway. However, there is a compatibility issue since most modern applications communicate data in JSON format. Which is the most cost-effective method that will overcome this issue?",answers:[{text:"Use API Gateway to create a WebSocket API. Transform the incoming JSON into XML using mapping templates. Forward the request into the SOAP interface by using a Lambda function and parse the response (XML) into JSON before sending back to API Gateway.",isCorrect:!1},{text:"Use API Gateway to create a RESTful API. Send the incoming JSON to an HTTP server hosted on an EC2 instance and have it transform the data into XML and vice versa before sending it to the legacy application.",isCorrect:!1},{text:"Use API Gateway to create a RESTful API. Transform the incoming JSON into XML for the SOAP interface through an Application Load Balancer and vice versa. Put the legacy web service behind the ALB.",isCorrect:!1},{text:"Use API Gateway to create a RESTful API. Transform the incoming JSON into XML using mapping templates. Forward the request into the SOAP interface by using a Lambda function and parse the response (XML) into JSON before sending back to API Gateway.",isCorrect:!0}],explanation:"You (or your organization) probably has some existing web services that respond to older protocols such as XML-RPC or SOAP. You can use the API Gateway to modernize these services. In API Gateway, an API's method request can take a payload in a different format from the corresponding integration request payload, as required in the backend. Similarly, the backend may return an integration response payload different from the method response payload, as expected by the frontend. API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response. Hence, the correct answer is: Use API Gateway to create a RESTful API. Transform the incoming JSON into XML using mapping templates. Forward the request into the SOAP interface by using a Lambda function and parse the response (XML) into JSON before sending back to API Gateway. The option that says: Use API Gateway to create a WebSocket API. Transform the incoming JSON into XML using mapping templates. Forward the request into the SOAP interface by using a Lambda function and parse the response (XML) into JSON before sending back to API Gateway is incorrect. The WebSocket protocol is mainly used for applications that require bidirectional persistent connection such as push notifications and chat messaging applications. For this scenario, using a REST-based approach is the more appropriate solution. The option that says: Use API Gateway to create a RESTful API. Transform the incoming JSON into XML for the SOAP interface through an Application Load Balancer and vice versa. Put the legacy web service behind the ALB is incorrect because ALB is not capable of transforming data. The option that says: Use API Gateway to create a RESTful API. Send the incoming JSON to an HTTP server hosted on an EC2 instance and have it transform the data into XML and vice versa before sending it to the legacy application is incorrect. Although this could work, this means that you'll have to provision and run an EC2 instance 24/7, which is more expensive than just using a Lambda Function. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-data-transformations.html https://github.com/mwittenbols/How-to-use-Lambda-and-API-Gateway-to-consume-XML-instead-of-JSON Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A developer has a Python script that relies on the low-level BatchGetItem API to fetch large amounts of data from a DynamoDB table. The script often encounters responses with partial results. A significant portion of the data appears under UnprocessedKeys. Which approaches can the developer implement to handle data retrieval MOST reliably? (Select TWO.)",answers:[{text:"Implement an exponential backoff algorithm with a randomized delay between retries of the batch request.",isCorrect:!0},{text:"Use the AWS software development kit (AWS SDK) to send batch requests.",isCorrect:!0},{text:"Implement a logic that immediately retries the batch request.",isCorrect:!1},{text:"Increase the read capacity units (RCUs) for the DynamoDB table and enable Auto Scaling.",isCorrect:!1},{text:"Create a Global Secondary Index (GSI) with its own read capacity settings.",isCorrect:!1}],explanation:"A single BatchGetItem operation can retrieve up to 16 MB of data, which can contain as many as 100 items. BatchGetItem returns a partial result if: - The response size limit is exceeded - The table's provisioned throughput is exceeded - More than 1MB per partition is requested - An internal processing failure occurs. For example, if you ask to retrieve 100 items, but each individual item is 300 KB in size, the system returns 52 items (so as not to exceed the 16 MB limit). It also returns an appropriate UnprocessedKeys value so you can get the next page of results. If desired, your application can include its own logic to assemble the pages of results into one dataset. If none of the items can be processed due to insufficient provisioned throughput on all of the tables in the request, then BatchGetItem returns a ProvisionedThroughputExceededException. If at least one of the items is successfully processed, then BatchGetItem completes successfully while returning the keys of the unread items in UnprocessedKeys. If DynamoDB returns any unprocessed items, you should retry the batch operation on those items. It's recommended that you use an exponential backoff algorithm. Exponential backoff is a technique where, if a request to a server fails, you wait a bit before retrying. If it keeps failing, you wait longer each time. The main idea is to reduce the frequency of calls over time, which helps avoid overloading the server, giving it a better chance to recover and respond successfully. If you retry the batch operation immediately, the underlying read or write requests can still fail due to throttling on the individual tables. Adding progressively longer waits between retries using exponential backoff can make the individual requests in the batch much more likely to succeed. You can implement an exponential backoff yourself in your code or simply use the AWS SDK, which comes with automatic retry logic and exponential backoff. Hence, the correct answers are: - Implement an exponential backoff algorithm with a randomized delay between retries of the batch request. - Use the AWS software development kit (AWS SDK) to send batch requests. The option that says: Increase the read capacity units (RCUs) for the DynamoDB table and enable Auto Scaling is incorrect. Although this solution can help improve the throughput of the table and reduce the likelihood of throttling, it's not the most reliable method for handling partial results. The batch operation can still return partial results even if the table has sufficient read capacity due to other factors such as large response sizes. The option that says: Create a Global Secondary Index (GSI) with its own read capacity settings is incorrect. The presence of a GSI does not change how BatchGetItem behaves, and it won't help with managing UnprocessedKeys. The option that says: Implement a logic that immediately retries the batch request is incorrect. Retrying the batch operation immediately has more chance of failing than succeeding due to throttling. References: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#BatchOperations https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/ Check out this Amazon DynamoDB Cheat Sheet: https://tutorialsdojo.com/amazon-dynamodb/"},{question:"Some static assets stored in an S3 bucket need to be accessed by a user on the development account. The S3 bucket is in the production account. According to the company policy, the sharing of full credentials between accounts is prohibited. What steps should be done to delegate access across the two accounts? (Select THREE.)",answers:[{text:"Log in to the production account and create a policy that will use STS to assume the IAM role in the development account. Attach the policy to the IAM users.",isCorrect:!1},{text:"On the development account, create an IAM role and specify the production account as a trusted entity.",isCorrect:!1},{text:"Set the policy that will grant access to S3 for the IAM role created in the production account.",isCorrect:!0},{text:"Set the policy that will grant access to S3 for the IAM role created in the development account.",isCorrect:!1},{text:"Log in to the development account and create a policy that will use STS to assume the IAM role in the production account. Attach the policy to the IAM users.",isCorrect:!0},{text:"On the production account, create an IAM role and specify the development account as a trusted entity.",isCorrect:!0}],explanation:"The problem is about delegating access to the development account to use the S3 Bucket on the production account. The steps to execute this are as follows: You use the AWS Management Console to establish trust between the Production account (ID number XXXXXXXXXXXX) and the Development account (ID number YYYYYYYYYYYY) by creating an IAM role. When you create the role, you define the Development account as a trusted entity and specify a permissions policy that allows trusted users to access the S3 bucket. On the development account, create an STS policy to assume the role created on the production account. This can be done by referencing the ARN of the role that was created to establish trust between the Production account and the Development account. Hence, the correct answers are: - On the production account, create an IAM role and specify the development account as a trusted entity. - Set the policy that will grant access to S3 for the IAM role created in the production account - Log in to the development account and create a policy that will use STS to assume the IAM role in the production account. Attach the policy to corresponding IAM users. The option that says: On the development account, create an IAM role and specify the production account as a trusted entity is incorrect. Since the S3 bucket is in the production account, the role should also be created in the production account. The option that says: Set the policy that will grant access to S3 for the IAM role created in the development account is incorrect because the policy associated with the role that will grant access to S3 should be created on the production account. The option that says: Log in to the production account and create a policy that will use STS to assume the IAM role in the development account. Attach the policy to the IAM users is incorrect. The policy that will use STS should be created on the account you are delegating access to, which is the development account. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html https://aws.amazon.com/blogs/security/how-to-enable-cross-account-access-to-the-aws-management-console/ Check out this AWS IAM Cheat Sheet: https://tutorialsdojo.com/aws-identity-and-access-management-iam/"},{question:"A developer plans to launch an EC2 instance, with Amazon Linux 2 as its AMI, using the AWS Console. A security group with port 80 that is open to public access will be associated with the instance. He wants to quickly build and test an Apache webserver with an index.html displaying a hello world message. Which of the following should the developer do?",answers:[{text:"Configure the metadata at the creation of the EC2 instance to run a script that will install the Apache webserver after the instance starts.",isCorrect:!1},{text:"Configure the user data at the creation of the EC2 instance to run a script that will install and create the Apache webserver after the instance starts.",isCorrect:!0},{text:"Connect to the instance via port 80. Run the commands that will install and create the Apache webserver.",isCorrect:!1},{text:"Connect to the instance via port 22. Run the commands that will install and create the Apache webserver.",isCorrect:!1}],explanation:"When you launch an instance in Amazon EC2, you have the option of passing user data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. You can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives. You can also pass this data into the launch wizard as plain text, as a file (this is useful for launching instances using the command line tools), or as base64-encoded text (for API calls). Hence, the correct answer is: Configure the user data at the creation of the EC2 instance to run a script that will install and create the Apache webserver after the instance starts. The option that says: Connect to the instance via port 22. Run the commands that will install and create the Apache webserver is incorrect because the scenario mentioned that the setting of the instance security group is only open to port 80. The developer will get a timeout error if he tries to connect to the instance via port 22. The option that says: Connect to the instance via port 80. Run the commands that will install and create the Apache webserver is incorrect. Although the security group's port 80 is open to the public, you can not establish a remote connection to the EC2 instance thru port 80. Port number 80 is used for sending and receiving web pages from an HTTP server. The option that says: Configure the metadata at the creation of the EC2 instance to run a script that will install the Apache webserver after the instance starts is incorrect because you can not run scripts on metadata. Metadata is just a list of details about your instance. You should add a User Data instead. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/launching-instance.html Check out this Amazon EC2 Cheat Sheet: https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/"},{question:"A Ruby developer is looking to offload some of the processing on his application to the AWS cloud without managing any servers. The submodules must be written in Ruby, which mainly invokes API calls to an external web service. The response from the API call is parsed and stored in a MongoDB database. What should he do to develop the Lambda function in his preferred programming language?",answers:[{text:"Create a Lambda function on Ruby with a custom runtime and use the AWS SDK for Ruby.",isCorrect:!1},{text:"Create a Lambda function using the AWS SDK for Ruby.",isCorrect:!1},{text:"Create a Lambda function with a supported runtime version for Ruby.",isCorrect:!0},{text:"Create a Lambda function with a custom runtime to use Ruby. Then include the runtime in the function's deployment package. Migrate it to a layer that you manage independently from the function.",isCorrect:!1}],explanation:"AWS Lambda natively supports Java, Go, PowerShell, Node.js, C#, Python, and Ruby code, and provides a Runtime API, which allows you to use any additional programming languages to author your functions. You can use the custom runtime to create a Lambda function if your preferred language is not available. You can implement an AWS Lambda runtime in any programming language. A runtime is a program that runs a Lambda function's handler method when the function is invoked. You can include a runtime in your function's deployment package in the form of an executable file named bootstrap. Because AWS Lambda supports Ruby by default, there's no additional configuration needed. Hence, the correct answer is: Create a Lambda function with a supported runtime version for Ruby. The option that says: Create a Lambda function with a custom runtime to use Ruby. Then include the runtime in the function's deployment package. Migrate it to a layer that you manage independently from the function is incorrect because a custom runtime is used to run programming languages that are not readily available to AWS Lambda. The option that says: Create a Lambda function on Ruby with a custom runtime and use the AWS SDK for Ruby is incorrect because the application will make calls to an external API and not to AWS Resources. AWS SDK is used for making API calls to different AWS services. Additionally, you don't have to make use of the custom runtime because AWS Lambda natively supports Ruby. The option that says: Create a Lambda function using the AWS SDK for Ruby is incorrect because the AWS SDK for Ruby just provides Ruby classes to access AWS services. It is not a valid runtime environment. References: https://aws.amazon.com/lambda/faqs/ https://docs.aws.amazon.com/lambda/latest/dg/runtimes-custom.html?icmpid=docs_lambda_help Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"A developer is hosting a static website from an S3 bucket. The website makes requests to an API Gateway endpoint integrated with a Lambda function (non-proxy). The developer noticed that the requests were failing. Upon debugging, he found a: \"No 'Access-Control-Allow-Origin' header is present on the requested resource\" error message. What should the developer do to resolve this issue?",answers:[{text:"In the API Gateway Console, enable cross-origin resource sharing (CORS) for the method in the specified resource.",isCorrect:!0},{text:"Set the value of the Access-Control-Max-Age header to 0.",isCorrect:!1},{text:"Set the value of the Access-Control-Allow-Credentials header to true.",isCorrect:!1},{text:"In the Amazon S3 Console, enable cross-origin resource sharing (CORS) on the S3 bucket where the website is hosted.",isCorrect:!1}],explanation:"Cross-origin resource sharing (CORS) is a browser security feature that restricts cross-origin HTTP requests that are initiated from scripts running in the browser. If your REST API's resources receive non-simple cross-origin HTTP requests, you need to enable CORS support. For a Lambda custom (non-proxy) integration, HTTP custom (non-proxy) integration, or AWS service integration, you can set up the required headers by using API Gateway method response and integration response settings. When you enable CORS by using the AWS Management Console, API Gateway creates an OPTIONS method and attempts to add the Access-Control-Allow-Origin header to your existing method integration responses. Hence, the correct answer is: In the API Gateway Console, enable cross-origin resource sharing (CORS) for the method in the specified resource. The option that says: In the Amazon S3 Console, enable cross-origin resource sharing (CORS) on the S3 bucket where the website is hosted is incorrect. This could be a possible solution if the website is interacting with resources from another website-enabled S3 bucket. The option that says: Set the value of the Access-Control-Max-Age header to 0 is incorrect because this header is simply used to indicate how long the results of a preflight request can be cached. The option that says: Set the value of the Access-Control-Allow-Credentials header to true is incorrect because this is just a response header that tells browsers whether to expose the response to frontend JavaScript code when the request's credentials mode (Request.credentials) is include. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers Check out this Amazon API Gateway Cheat Sheet: https://tutorialsdojo.com/amazon-api-gateway/"},{question:"A Lambda function is being developed to process a 50MB gzip-compressed file that will be uploaded to an S3 bucket on a daily basis. The function must have access to a storage location where it can load and unzip the file. After processing, the file will be delivered to another S3 bucket. Which solution can the developer implement that requires the LEAST effort and cost?",answers:[{text:"Download the file to the /tmp directory. From there, consume and process the data before sending it to the S3 bucket.",isCorrect:!0},{text:"Download the file to an Amazon Elastic Block Store (EBS) volume. From there, consume and process the data before sending it to the S3 bucket.",isCorrect:!1},{text:"Download the file to a separate S3 bucket. From there, consume and process the data before sending it to the S3 bucket that contains the logs.",isCorrect:!1},{text:"Download the file to Amazon Elastic File System. From there, consume and process the data before sending it to the S3 bucket.",isCorrect:!1}],explanation:"The Lambda execution environment provides ephemeral storage for your code to use at /tmp. This space has a size that can be set between 512 MB (free) and 10,240 MB. For this feature, you are charged for the storage you configure over the 512 MB free limit for the duration of your function invokes. In the scenario, the /tmp directory can be used as a staging area for unzipping the file. Also, since the file size is relatively small (50MB), even when unzipped, the default 512 MB should be enough for the job, making the solution the best option in terms of effort and cost. Hence, the correct answer is: Download the file to the /tmp directory. From there, consume and process the data before sending it to the S3 bucket. The option that says: Download the file to Amazon Elastic File System. From there, consume and process the data before sending it to the S3 bucket is incorrect. Although this is a valid solution, using EFS entails additional effort for this type of problem. For example, the Lambda function must be configured first to connect to the VPC where the EFS is located. The option that says: Download the file to a separate S3 bucket. From there, consume and process the data before sending it to the S3 bucket that contains the logs is incorrect. This solves nothing as the file will still remain uncompressed. Optionally, you can load the file into the Lambda function's memory and decompress it from there, but this method eats up a lot of memory and is subjected to the memory limits of the Lambda function. The option that says: Download the file to an Amazon Elastic Block Store (EBS) volume. From there, consume and process the data before sending it to the S3 bucket is incorrect. Using Amazon EBS as storage for unzipping a file is a costly and overkill solution. References: https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html https://aws.amazon.com/blogs/aws/aws-lambda-now-supports-up-to-10-gb-ephemeral-storage/ Check out this AWS Lambda Cheat Sheet: https://tutorialsdojo.com/aws-lambda/"},{question:"An application is hosted in the us-east-1 region. The app needs to be recreated on the us-east-2, ap-northeast-1, and ap-southeast-1 region using the same Amazon Machine Image (AMI). As the developer, you have to use AWS CloudFormation to rebuild the application using a template. Which of the following actions is the most suitable way to configure the CloudFormation template for the scenario?",answers:[{text:"Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Parameters section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Ref function to retrieve the desired Image Id from the region key.",isCorrect:!1},{text:"Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::FindInMap function to retrieve the desired Image Id from the region key.",isCorrect:!0},{text:"Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::ImportValue function to retrieve the desired Image Id from the region key.",isCorrect:!1},{text:"Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::GetAtt function to retrieve the desired Image Id from the region key.",isCorrect:!1}],explanation:"The optional Mappings section matches a key to a corresponding set of named values. For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. You use the Fn::FindInMap intrinsic function to retrieve values in a map. Hence, the correct answer is: Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::FindInMap function to retrieve the desired Image Id from the region key. The option that says: Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::ImportValue function to retrieve the desired Image Id from the region key is incorrect because the Fn::ImportValue function is just used to return the value of an output exported by another stack. It can’t be used to retrieve values from a Mappings section. The option that says: Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::GetAtt function to retrieve the desired Image Id from the region key is incorrect because the Fn::GetAtt function is simply used to return the value of an attribute from a resource in the template and not in a Mappings section. The option that says: Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Parameters section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Ref function to retrieve the desired Image Id from the region key is incorrect because the Parameters section is mainly used to declare values within a specified parameter. For example, you can specify the allowed Amazon EC2 instance type for the stack to use when you create or update the stack. Although you can specify the values for the Image Id in the Parameters section, it does not give you the flexibility to map the Image Ids according to its correct region. The Mappings section is more suited for this type of use case. References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-findinmap.html Check out this AWS CloudFormation Cheat Sheet: https://tutorialsdojo.com/aws-cloudformation/"},{question:"A developer is managing an Application Load Balancer that targets a Lambda function. The developer needs to obtain all values of identical query parameters key that is supplied in a request. How can the developer implement this?",answers:[{text:"Replace the Application Load Balancer with a Network Load Balancer and enable multi-value headers.",isCorrect:!1},{text:"Enable the multi-value headers on the Application Load Balancer.",isCorrect:!0},{text:"Decode the URL encoded query string values in the Lambda function.",isCorrect:!1},{text:"Set a custom HTTP response header in the Lambda function.",isCorrect:!1}],explanation:"Application Load Balancers provide two advanced options that you may want to configure when you use ALBs with AWS Lambda: support for multi-value headers and health check configurations. You can set up these options in Target Groups section on the Amazon EC2 console. If requests from a client or responses from a Lambda function contain headers with multiple values or contains the same header multiple times or query parameters with multiple values for the same key, you can enable support for multi-value header syntax. After you enable multi-value headers, the headers and query parameters exchanged between the load balancer and the Lambda function use arrays instead of strings. For example, suppose the client supplies a query string like: ?name=foo&name=bar If you’ve enabled multi-value headers, ALB supplies these duplicate parameters in the event object as: ‘name’: [‘foo’, ‘bar’] ALB applies the same processing to duplicate HTTP headers. If you do not enable multi-value header syntax and a header or query parameter has multiple values, the load balancer uses the last value that it receives. Hence, the correct answer is: Enable the multi-value headers on the Application Load Balancer. The option that says: Set a custom HTTP response header in the Lambda function is incorrect because this can only be done when integrated with API Gateway. The option that says: Replace the Application Load Balancer with a Network Load Balancer and enable multi-value headers is incorrect. The Network Load Balancer does not support Lambda functions as a target type. The option that says: Decode the URL encoded query string values in the Lambda function is incorrect. This won't change anything. The load balancer will still use the last value of the query parameter it receives regardless if its URL is encoded or not. References: https://aws.amazon.com/blogs/networking-and-content-delivery/lambda-functions-as-targets-for-application-load-balancers/ https://docs.aws.amazon.com/elasticloadbalancing/latest/application/lambda-functions.html Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet: https://tutorialsdojo.com/aws-elastic-load-balancing-elb/"},{question:"A startup plans to use Amazon Cognito User Pools to easily manage their users' sign-up and sign-in workflows to an application. To save time from designing the User Interface (UI) for the login page, the development team has decided to use Cognito's built-in UI. However, the product manager finds the UI bland and instructed the developer to include the product logo on the web page. How should the developer meet the above requirements?",answers:[{text:"Upload the logo to an S3 bucket and point the S3 endpoint on the custom login page.",isCorrect:!1},{text:"Create a login page with the product logo and upload it to an S3 bucket. Point the S3 endpoint in the Cognito app settings.",isCorrect:!1},{text:"Create a login page with the product logo and upload it to Amazon Cognito.",isCorrect:!1},{text:"Upload the logo to the Amazon Cognito app settings and use that logo on the custom login page.",isCorrect:!0}],explanation:"You can use the AWS Management Console, or the AWS CLI or API, to specify customization settings for the built-in app UI experience. You can upload a custom logo image to be displayed in the app. You can also choose many CSS customizations. You can specify app UI customization settings for a single client (with a specific clientId) or for all clients (by setting the clientId to ALL). If you specify ALL, the default configuration will be used for every client that has no UI customization set previously. If you specify UI customization settings for a particular client, it will no longer fall back to the ALL configuration. In the given scenario, you can include the product logo on the webpage by uploading the logo in the Cognito app settings under UI customization. Hence, the correct answer is: Upload the logo to the Amazon Cognito app settings and use that logo on the custom login page. The option that says: Upload the logo to an S3 bucket and point the S3 endpoint on the custom login page is incorrect as this procedure simply won't work. The logo should be uploaded on the Cognito app settings and not on the S3 bucket. The option that says: Create a login page with the product logo and upload it to Amazon Cognito is incorrect. You don't have to create a login page as this is already hosted by Cognito. The option that says: Create a login page with the product logo and upload it to an S3 bucket. Point the S3 endpoint in the Cognito app settings is incorrect because there is no such option in the Cognito app settings. References: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-app-ui-customization.html https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html Check out this Amazon Cognito Cheat Sheet: https://tutorialsdojo.com/amazon-cognito/"},{question:"A development team needs to deploy an application revision into three environments: Test, Staging, and Production. The application should be deployed into the Test environment first, then Staging, and then Production. Which approach will conveniently allow the team to deploy the application into different environments?",answers:[{text:"Create multiple deployment groups for each environment using AWS CodeDeploy.",isCorrect:!0},{text:"Create separate S3 buckets for each environment to deploy the application.",isCorrect:!1},{text:"Create separate CloudFormation templates for each environment to deploy the application.",isCorrect:!1},{text:"Create, configure, and deploy multiple application projects for each environment using CodeBuild.",isCorrect:!1}],explanation:"In an EC2/On-Premises deployment, a deployment group is a set of individual instances targeted for deployment. A deployment group contains individually tagged instances, Amazon EC2 instances in Amazon EC2 Auto Scaling groups, or both. You can associate more than one deployment group with an application in CodeDeploy. This makes it possible to deploy an application revision to different sets of instances at different times. For example, you might use one deployment group to deploy an application revision to a set of instances tagged Test where you ensure the code's quality. Next, you deploy the same application revision to a deployment group with instances tagged Staging for additional verification. Finally, when you are ready to release the latest application to customers, you deploy to a deployment group that includes instances tagged Production. Hence, the correct answer is: Create multiple deployment groups for each environment using AWS CodeDeploy. The option that says: Create separate S3 buckets for each environment to deploy the application is incorrect. While S3 buckets can be used to store application artifacts, they are not designed for managing deployments. This approach would require additional scripting and manual steps to deploy the application from S3 to the respective environments, making it less convenient and more error-prone compared to using a dedicated deployment service like CodeDeploy. The option that says: Create, configure, and deploy multiple application projects for each environment using CodeBuild is incorrect because you can't use AWS CodeBuild to perform code deployments. CodeBuild is simply a service that allows you to compile and run tests on your code before deployment. The option that says: Create separate CloudFormation templates for each environment to deploy the application is incorrect because CloudFormation is primarily used for infrastructure as code, but it's not designed specifically for managing code deployments in a sequential manner across environments. This approach would be inefficient and overly complex compared to using CodeDeploy. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html https://aws.amazon.com/codedeploy/faqs/ Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/"},{question:"A developer is building a serverless application that will send out a newsletter to customers using AWS Lambda. The Lambda function will be invoked at a 7-day interval. Which method will provide an automated and serverless approach to trigger the function?",answers:[{text:"Configure a scheduled Amazon EventBridge (Amazon CloudWatch Events) that triggers every week to invoke the Lambda function.",isCorrect:!0},{text:"Run a cron job in an Amazon EC2 instance that will trigger the Lambda function every week.",isCorrect:!1},{text:"Implement a task timer using Step Functions that will send a newsletter every week.",isCorrect:!1},{text:"Add an environment variable named DAYS for the Lambda function and set its value to 7.",isCorrect:!1}],explanation:"Amazon EventBridge (Amazon CloudWatch Events) help you to respond to state changes in your AWS resources. When your resources change state, they automatically send events into an event stream. You can create rules that match selected events in the stream and route them to your AWS Lambda function to take action. To solve the given problem, we can set up a Schedule event source that will invoke the Lambda function responsible for sending a newsletter every 7 days. Hence, the correct answer is: Configure a scheduled Amazon EventBridge (Amazon CloudWatch Events) that triggers every week to invoke the Lambda function. The option that says: Run a cron job in an Amazon EC2 instance that will trigger the Lambda function every week is incorrect because this approach does not comply with the required solution. Amazon EC2 is not a serverless compute service. The option that says: Add an environment variable named DAYS for the Lambda function and set its value to 7 is incorrect because an environment variable is just an optional key-value pair that is stored in the Lambda function's version-specific configuration. The option that says: Implement a task timer using Step Functions that will send a newsletter every week is incorrect. Although serverless, using Step Functions for a basic application that pushes data is unnecessarily complex and may incur additional costs. References: https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents.html https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html Check out this Amazon CloudWatch Cheat Sheet: https://tutorialsdojo.com/amazon-cloudwatch/"},{question:"A company plans to conduct an online survey to distinguish the users who bought its product from those who didn't. The survey will be processed by Step Functions which comprises four states that will manage the application logic and error handling of the state machine. It is required to aggregate all the data that passes through the nodes if the process fails. What should the company do to meet the requirements?",answers:[{text:"Include a Catch field in the state machine definition to capture the errors. Then, use ItemsPath to include each node's input data with its output.",isCorrect:!1},{text:"Include a Catch field in the state machine definition to capture the error. Then, use ResultPath to include each node’s input data with its output.",isCorrect:!0},{text:"Include a Parameters field in the state machine definition to capture the errors. Then, use ResultPath to include each node's input data with its output.",isCorrect:!1},{text:"Include a Parameters field in the state machine definition to capture the errors. Then, use ItemsPath to include each node's input data with its output.",isCorrect:!1}],explanation:'The output of a state can be a copy of its input, the result it produces (for example, the output from a Task state’s Lambda function), or a combination of its input and result. Use ResultPath to control which combination of these is passed to the state output. You can use a Catch field to capture the error in a Task and Parallel State. This field\'s value must be an array of objects, known as catchers. A catcher contains the following fields: ErrorEquals - A non-empty array of strings that match error names. Next - A string that must exactly match one of the state machine\'s state names. ResultPath - A path that determines what input is sent to the state specified in the Next field. When a state reports an error and either there is no Retry field, or if retries fail to resolve the error, Step Functions scans through the catchers in the order listed in the array. When the error name appears in the value of a catcher\'s ErrorEquals field, the state machine transitions to the state named in the Next field. Below is the visual representation of the workflow that is given in the scenario: The four states that handle the application logic and error handling are as follows: Choice State - "Yes or No" Task State - "YesMessage" and "NoMessage" Pass State - "Cause Of Error" The workflow is initiated by passing an input of values "yes" or "no". On the left side, we can see that an error has occurred during the "NoMessage" task state (as labeled by its orange color) when a "no" value was passed as an input. On the right side, we can see that all data that passes through the nodes (input, error, output) are aggregated in a single step output. This can be done by including a Catch field in the state machine definition to capture the error in a state and the ResultPath to include each node\'s input with its output. Hence, the correct answer is: Include a Catch field in the state machine definition to capture the errors. Then, use ResultPath to include each node’s input data with its output. The option that says: Include a Parameters field in the state machine definition to capture the errors. Then, use ResultPath to include each node’s input data with its output is incorrect because the Parameters field is mainly used for passing a collection of key-value pairs, where the values are either static values that you define in your state machine definition, or that are selected from the input using a path. It can\'t be used to capture errors in a state. The option that says: Include a Catch field in the state machine definition to capture the errors. Then, use ItemsPath to include each node’s input data with its output is incorrect because the ItemsPath is only applicable in a Map state. The option that says: Include a Parameters field in the state machine definition to capture the errors. Then, use ItemsPath to include each node’s input data with its output is incorrect because the Parameters field cannot be used to capture errors in a state. ItemsPath is also incorrect because this is only applicable in a Map state. References: https://docs.aws.amazon.com/step-functions/latest/dg/input-output-resultpath.html https://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html Check out this AWS Step Functions Cheat Sheet: https://tutorialsdojo.com/aws-step-functions/'},{question:"A developer is building a new feature for an application deployed on an EC2 instance in the N. Virginia region. A co-developer suggests to upload the code on Amazon S3 and use CodeDeploy to deploy the new version of the application. The deployment fails during the DownloadBundle deployment lifecycle event with the UnknownError: not opened for reading error. What is the possible cause of this?",answers:[{text:"The EC2 instance’s IAM profile does not have the permissions to access the application code in Amazon S3.",isCorrect:!0},{text:"Versioning is not enabled on the Amazon S3 Bucket where the application code resides.",isCorrect:!1},{text:"The DownloadBundle deployment lifecycle event is not supported in the N. Virginia region.",isCorrect:!1},{text:"Wrong configuration of the DownloadBundle lifecycle event in the AppSec file.",isCorrect:!1}],explanation:"An EC2/On-Premises deployment hook is executed once per deployment to an instance. You can specify one or more scripts to run in a hook. Each hook for a lifecycle event is specified with a string on a separate line. Here are descriptions of the hooks available for use in your AppSpec file. DownloadBundle deployment lifecycle event will throw an error whenever: - The EC2 instance’s IAM profile does not have permission to access the application code in the Amazon S3. - An Amazon S3 internal error occurs. - The instances you deploy to are associated with one AWS Region (for example, US West Oregon), but the Amazon S3 bucket that contains the application revision is related to another AWS Region (for example, US East N. Virginia). Hence, the correct answer is: The EC2 instance’s IAM profile does not have the permissions to access the application code in Amazon S3. The option that says: Wrong configuration of the DownloadBundle lifecycle event in the AppSec file is incorrect because you can not manually configure DownloadBundle in the Appsec file. The CodeDeploy Agent installed on the EC2 instance manages the DownloadBundle lifecycle event. The option that says: The DownloadBundle deployment lifecycle event is not supported in the N. Virginia region is incorrect because CodeDeploy is supported in N. Virginia. The option that says: Versioning is not enabled on the Amazon S3 Bucket where the application code resides is incorrect because versioning on Amazon S3 Bucket is just used to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-downloadbundle https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html Check out this AWS CodeDeploy Cheat Sheet: https://tutorialsdojo.com/aws-codedeploy/"},{question:"A developer is building a serverless API composed of an API Gateway and several Lambda functions. All resources are defined using Cloud Development Kit (CDK) L2 constructs. The developer wants to test some of the Lambda functions in their local environment. Given that AWS SAM and AWS CDK are already configured locally, what combination of actions must the developer do? (Select TWO.)",answers:[{text:"Execute the sam local invoke command and specify the location of the synthesized CloudFormation template and identifier of each function.",isCorrect:!0},{text:"Run the cdk bootstrap command to prepare the staging of CDK assets.",isCorrect:!1},{text:"Run the cdk synth command and indicate the stack name of Lambda functions to be tested.",isCorrect:!0},{text:"Execute the sam package and specify the S3 bucket where the Lambda code will be uploaded for local testing.",isCorrect:!1},{text:"Execute the sam local start-lambda and specify the location of the synthesized CloudFormation template along with function identifiers.",isCorrect:!1}],explanation:'The sam local invoke command allows you to test AWS Lambda functions locally by emulating the Lambda execution environment. However, to test resources defined in AWS CDK, you must first convert the CDK constructs into a format that SAM can understand. This is where the cdk synth command comes into play. It synthesizes or "compiles" your CDK application into an AWS CloudFormation template. By running cdk synth, you generate the necessary CloudFormation template that sam local invoke can be used to locally execute and test your Lambda functions. Hence, the correct answers are: - Run the cdk synth command and indicate the stack name of Lambda functions to be tested. - Execute the sam local invoke command and specify the location of the synthesized CloudFormation template and identifier of each function. The option that says: Run the cdk bootstrap command to prepare the staging of CDK assets is incorrect. This command simply sets up the necessary resources in the AWS account to manage the deployments of CDK applications. It\'s a prerequisite step for deploying resources, not for local testing. The option that says: Execute the sam package and specify the S3 bucket where the Lambda code will be uploaded for local testing is incorrect. This just packages a CloudFormation template for deployment to AWS. The option that says: Execute the sam local start-lambda and specify the location of the synthesized CloudFormation template along with function identifiers is incorrect. While this command is indeed used for local testing, it starts a local endpoint that emulates the AWS Lambda service. This is more suitable for scenarios where you want to test the Lambda function as it would be invoked by other AWS services. Since the developer wants to test only some of the Lambda functions, the sam local invoke command is a more direct and appropriate choice for invoking specific Lambda functions one at a time. References: https://docs.aws.amazon.com/cdk/v2/guide/troubleshooting.html https://repost.aws/knowledge-center/cdk-customize-bootstrap-cfntoolkit https://docs.aws.amazon.com/cdk/v2/guide/cli.html Check out this Amazon Cloud Development Kit (CDK) Cheat Sheet: https://tutorialsdojo.com/aws-cloud-development-kit-cdk/'}]},{id:"aws-developer-4540356",title:"Practice Test #1 (AWS Certified Developer Associate - DVA-C02)",description:"Practice questions for AWS Developer Associate (DVA-C02) exam — covering core AWS services like SQS, Lambda, DynamoDB, and more.",questions:[{question:"Your SQS consumer has failed to process a message and you realise it is taking a long time for the message to be read back by other applications. How can you decrease that time?",answers:[{text:"decrease VisibilityTimeout",isCorrect:!0},{text:"disable LongPolling",isCorrect:!1},{text:"Use a FIFO queue",isCorrect:!1},{text:"Use a lower DelaySeconds",isCorrect:!1}],explanation:"Once a message is received by one of the distributed consumers, the message cannot be read again until the visibility timeout of that particular message expires. By setting an appropriate visibility timeout for the specific message, we can control how sooner or later the message is avaialble for processing by other applications. Read more: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html"},{question:"Your SQS consumer takes once in a while an unpredictable long time to process messages, which in turn result in duplicate processing of these messages as they go longer than their default VisibilityTimeout. How can you remedy the situation for the few problematic messages?",answers:[{text:"Use ChangeMessageVisibility",isCorrect:!0},{text:"Use DeleteMessage",isCorrect:!1},{text:"Change the Visibilty Timeout",isCorrect:!1},{text:"Use Long Polling",isCorrect:!1}],explanation:"Once a message is received by one of the distributed consumers, the message cannot be read again until the visibility timeout of that particular message expires. If the message is not processed within the visibility timeout, there are chances that the message will be received again by the distributed consumers leading to duplication in processing of message. By setting a sufficient visibility timeout for the specific message, we can ensure that the same message is not received again by the distributed consumers as the original consumer will delete the message once it has processed. The visibility timeout of the few problematic messages can be altered using ChangeMessageVisibility. Read more: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html"},{question:"You have a CloudFormation template and it starts with:Transform: 'AWS::Serverless-2016-10-31' What does the Transform represent?",answers:[{text:"It's a SAM template",isCorrect:!0},{text:"It's an intrinsic function",isCorrect:!1},{text:"It's a Lambda function definition",isCorrect:!1}],explanation:"The AWS::Serverless transform is specifically used for transforming an entire template written in the AWS Serverless Application Model (AWS SAM) syntax into a compliant AWS CloudFormation template. Read more: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-aws-serverless.html"},{question:'You are currently storing 500 GB of data on your S3 bucket "my-corp". What\'s the maximum file size an S3 bucket can contain?',answers:[{text:"5 TB",isCorrect:!0},{text:"1 TB",isCorrect:!1},{text:"infinite",isCorrect:!1},{text:"50 TB",isCorrect:!1}],explanation:"Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 terabytes. The largest object that can be uploaded in a single PUT is 5 gigabytes. Read more: https://aws.amazon.com/s3/faqs/"},{question:"You are creating multiple users and assigning the same IAM policy to them as they perform the same role. What should you have done differently?",answers:[{text:"Create a group, assign the policy to the group and the users to the group",isCorrect:!0},{text:"Never assign the same policy to two user",isCorrect:!1},{text:"Use only one account shared by your users",isCorrect:!1},{text:"Let your users have admin rights",isCorrect:!1}],explanation:"IAM Groups is an abstraction making the management of user groups easier. For instance we want to add a new policy to all the users in a group. Now instead of assigining the policy to each individual user, it can just be assigned once to the IAM Group. Read more: https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html"},{question:"You want a deployment tool which will also provision instances if they are needed. Which service should you use?",answers:[{text:"Elastic Beanstalk",isCorrect:!0},{text:"CodeDeploy",isCorrect:!1},{text:"CodePipeline",isCorrect:!1},{text:"ECS Classic",isCorrect:!1}],explanation:"AWS Elastic Beanstalk lets you manage all of the resources that run your application as environments where each environment runs only a single application version at a time.When an environment is being created, Elastic Beanstalk provisions all the required resources needed to run the application version. Read more: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html"},{question:"You would like to store user session in an in-memory store that has extremely low latency. What service should you use?",answers:[{text:"ElastiCache",isCorrect:!0},{text:"RDS",isCorrect:!1},{text:"DynamoDB",isCorrect:!1},{text:"Parameter Store",isCorrect:!1}],explanation:"ElastiCache is a managed service offering from AWS for in-memory data storages. RDS and DynamoDB offering store data on the disk which obviously has a much higher read latency than that of memory. Read more: https://aws.amazon.com/elasticache/"},{question:"You have stored all your applications secrets in SSM parameter store and your audit team needs to get a report to understand who has been issuing API calls against SSM parameter store. What do you use?",answers:[{text:"CloudTrail",isCorrect:!0},{text:"SSM Parameter Store List feature",isCorrect:!1},{text:"SSM Parameter Store Access Logs in CloudWatch Logs",isCorrect:!1},{text:"SSM Parameter Store Access Logs in S3",isCorrect:!1}],explanation:"CloudTrail captures all API calls for Systems Manager as events, including calls from the Systems Manager console and from code calls to the Systems Manager APIs. Read more: https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-cloudtrail-logs.html"},{question:"Which pseudo parameter can you use to make your CloudFormation independent of the accounts they're running under?",answers:[{text:"AWS::AccountId",isCorrect:!0},{text:"AWS::NoValue",isCorrect:!1},{text:"AWS::Region",isCorrect:!1},{text:"AWS::StackName",isCorrect:!1}],explanation:"AWS::AccountId returns the AWS account ID of the account in which the stack is being created Read more: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html"},{question:"You want to deploy an application that relies on RDS and ElastiCache for storing data and user session data respectively. You have chosen Elastic Beanstalk as the platform to deploy the platform. The requirements are that the RDS database must be able to be used across different environments and never lost, while ElastiCache can be safely deployed for each environment and lost if an environment is destroyed. Which configuration will allow you to achieve that? (select two)",answers:[{text:"RDS database defined externally and referenced through environment variables",isCorrect:!0},{text:"ElastiCache defined in .ebextensions/",isCorrect:!0},{text:"RDS database defined in .ebextensions/",isCorrect:!1},{text:"ElastiCache database defined externally and referenced through environment variables",isCorrect:!1},{text:"ElastiCache bundled with the application source code",isCorrect:!1}],explanation:"Any resources created as part of your .ebextensions is part of your CloudFormation template and will get deleted if the environment is terminated. Resources that need to persist environments deletions must be created externally"},{question:"You are looking to deploy applications a fleet of EC2 instances, with precise blue/green strategy. You want full control over the deployment steps and the possibility to add hooks. Which service should you use?",answers:[{text:"CodeDeploy",isCorrect:!0},{text:"CodeBuild",isCorrect:!1},{text:"Elastic Beanstalk",isCorrect:!1},{text:"CodePipeline",isCorrect:!1}],explanation:"AWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, or serverless Lambda functions.AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change.AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services. Read more: https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html"},{question:"One of your SQS message is consistently failing to be processed by consumers. It is breaking your applications and you would like to automatically store them somewhere for further debugging. What should you do?",answers:[{text:"Implement a DLQ",isCorrect:!0},{text:"Use DeleteMessage",isCorrect:!1},{text:"Reduce the VisibiltyTimeout",isCorrect:!1},{text:"Increase the VisibilityTimeout",isCorrect:!1}],explanation:"Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.  Read more: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html"},{question:"You want to mask some fields returned by your Lambda functions before going back to the clients, through the API Gateway. How can you achieve that?",answers:[{text:"Use Mapping Templates",isCorrect:!0},{text:"Deploy an interceptor shell script",isCorrect:!1},{text:"Use a stage variable",isCorrect:!1},{text:"Use a Lambda custom interceptor",isCorrect:!1}],explanation:"API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response. Read more: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-mapping-template-reference.html"},{question:"What is the naming convention for files allowing you to control your Elastic Beanstalk settings?",answers:[{text:".ebextensions/<mysettings>.config",isCorrect:!0},{text:".ebextensions_<mysettings></mysettings>.config",isCorrect:!1},{text:".config/<mysettings>.ebextensions",isCorrect:!1},{text:".config_<mysettings>.ebextensions",isCorrect:!1}],explanation:"You can add AWS Elastic Beanstalk configuration files (.ebextensions) to your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle. Read more: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html"},{question:"In order to retrieve the region ID of an EC2 instance created with CloudFormation, which function should you use?",answers:[{text:"!GetAtt",isCorrect:!0},{text:"!Ref",isCorrect:!1},{text:"!Sub",isCorrect:!1},{text:"!FindInMap",isCorrect:!1}],explanation:"The Fn::GetAtt intrinsic function returns the value of an attribute from a resource in the template. We can use the above function to get the regions ID attribute of the required EC2 instance by passing region ID as the attributeName and EC2 instance ID as logicalNameOfResource. Read more: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html"},{question:"Your application needs to have an authentication mechanism that returns a JWT (JSON Web Token) after the user has identified. Which service validates the requirements?",answers:[{text:"Cognito User Pools",isCorrect:!0},{text:"API Gateway",isCorrect:!1},{text:"Cognito Identity Pools",isCorrect:!1},{text:"Cognito Sync",isCorrect:!1}],explanation:"After successfully authenticating a user, Amazon Cognito issues JSON web tokens (JWT) that you can use to secure and authorize access to your own APIs, or exchange for AWS credentials. Read more: https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-with-identity-providers.html"},{question:"How many arguments does the CloudFormation !FindInMap function take?",answers:[{text:"3",isCorrect:!0},{text:"2",isCorrect:!1},{text:"4",isCorrect:!1},{text:"1",isCorrect:!1}],explanation:"Following are the argumentsMapNameTopLevelKeySecondLevelKey Read more: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-findinmap.html"},{question:"You know that your build time should not exceed 5 minutes for a particular application. Recently, some HTTP network was hung and CodeBuild was running for over an hour. How can you prevent that issue in the future?",answers:[{text:"Enable CodeBuild timeouts",isCorrect:!0},{text:"Use AWS Lambda",isCorrect:!1},{text:"Use AWS CloudWatch Events",isCorrect:!1},{text:"Use VPC Flow Logs",isCorrect:!1}],explanation:"By setting the timeout configuration, the build process will automatically terminate post the expiry of configured timeout.  Read more: https://docs.aws.amazon.com/codebuild/latest/userguide/change-project.html"},{question:"What is the most simple way to run a CLI command against a region that hasn't been configured as the default?",answers:[{text:"Use the --region parameter",isCorrect:!0},{text:"You need the override the default region use aws configure",isCorrect:!1},{text:"You should create a new profile just for that other region",isCorrect:!1},{text:"Use boto3 dependency injection",isCorrect:!1}],explanation:"If the region parameter is not set, then the CLI command is executed against the default AWS region. Read more: https://docs.aws.amazon.com/cli/latest/topic/config-vars.html#general-options"},{question:"You would like to distribute your API to various customers and ensure their consumption is limited by the plans they have subscribed to. Which feature allows you to do that?",answers:[{text:"Usage Plans & API Keys",isCorrect:!0},{text:"AWS Billing",isCorrect:!1},{text:"CloudTrail",isCorrect:!1},{text:"AWS Lambda Custom Authorizers",isCorrect:!1}],explanation:"You can configure usage plans and API keys to allow customers to access selected APIs at agreed-upon request rates and quotas that meet their business requirements and budget constraints. Read more: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html"},{question:"The company has a requirement to delete the data from CloudWatch Logs after 1 week for security compliance. Which feature will help you in achieving that?",answers:[{text:"CloudWatch log expiration policies",isCorrect:!0},{text:"AWS Lambda with a CloudWatch Event",isCorrect:!1},{text:"CloudWatch S3 exporter",isCorrect:!1},{text:"CloudWatch Log Streams",isCorrect:!1}],explanation:"By default, logs are kept indefinitely and never expire. You can adjust the retention policy for each log group, keeping the indefinite retention, or choosing a retention periods between 10 years and one day. Read more: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html"},{question:"You are looking to push Docker images into ECR with your AWS CodePipeline and CodeBuild. The last step fails with an authorization issue. What is the issue?",answers:[{text:"Double check your IAM permissions for CodeBuild service",isCorrect:!0},{text:"Delete and re-create ECR repositories",isCorrect:!1},{text:"Open an AWS support ticket",isCorrect:!1},{text:"You first need to run an ECS instance",isCorrect:!1}],explanation:"By default, IAM users don't have permission to create or modify Amazon ECR resources, or perform tasks using the Amazon ECR API. Read more: https://docs.aws.amazon.com/AmazonECR/latest/userguide/ECR_IAM_policies.html"},{question:"You are building a movie database. Which attribute is the best partition key to maximize data distribution in DynamoDB?",answers:[{text:"movie_id",isCorrect:!0},{text:"producer_name",isCorrect:!1},{text:"lead_actor_name",isCorrect:!1},{text:"movie_language",isCorrect:!1}],explanation:"As the movie_id is unique in nature across the entire collection of movies database, hence it is the suitable candidate for the partition key in this usecase. Read more: https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/"},{question:"What service does Elastic Beanstalk rely on in order to deploy applications to AWS Cloud?",answers:[{text:"CloudFormation",isCorrect:!0},{text:"CodeCommit",isCorrect:!1},{text:"AWS Lambda",isCorrect:!1},{text:"Systems Manager",isCorrect:!1}],explanation:"Beanstalk behind the scenes creates a cloudformation template. Have a try creating a new Beanstalk environment!"},{question:"You would like to deploy a new application version without spending any incremental money while making sure the application does not experience any downtime. Which deployment mode fits your needs?",answers:[{text:"rolling",isCorrect:!0},{text:"immutable",isCorrect:!1},{text:"all at once",isCorrect:!1},{text:"rolling with additional batches",isCorrect:!1},{text:"blue/green",isCorrect:!1}],explanation:"All at once deployment policy causes downtime.All other deployment policies except rolling policy don't cause downtime but incur additional expenses. Read more: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.htmlhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html"},{question:"You need to dynamically obtain the instance-id of your EC2 instances from within your applications running on the EC2 instances. What's the easiest way to achieve it?",answers:[{text:"Use EC2 meta data service",isCorrect:!0},{text:"Attach an IAM role and allow the ec2 describes-instances permission",isCorrect:!1},{text:"Use EC2 User Data",isCorrect:!1},{text:"Inject that information using AWS Lambda",isCorrect:!1}],explanation:"As your instance metadata is available from your running instance, you do not need to use the Amazon EC2 console or the AWS CLI. This can be helpful when you're writing scripts to run from your instance. For example, you can access the local IP address of your instance from instance metadata to manage a connection to an external application. Read more: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html#instancedata-data-retrieval"},{question:"You are looking to automatically deploy AWS Lambda functions, with precise deployment rollout strategies. Which AWS service should you use?",answers:[{text:"CodeDeploy",isCorrect:!0},{text:"CodeBuild",isCorrect:!1},{text:"CodeCommit",isCorrect:!1},{text:"CodePipeline",isCorrect:!1}],explanation:"You can deploy a nearly unlimited variety of application content, such as code, serverless AWS Lambda functions, web and configuration files, executables, packages, scripts, multimedia files, and so on. Read more: https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html"},{question:"Which of the following is not a valid CloudFormation section?",answers:[{text:"Dependencies",isCorrect:!0},{text:"Resources",isCorrect:!1},{text:"Parameters",isCorrect:!1},{text:"Conditions",isCorrect:!1}],explanation:"Refer to the link under  Read more:  column for more detailed explaination. Read more: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html"},{question:"You are trying to track API calls made in your AWS account for compliance purposes. Which service should you use?",answers:[{text:"CloudTrail",isCorrect:!0},{text:"CloudWatch Metrics",isCorrect:!1},{text:"X-Ray",isCorrect:!1},{text:"CloudWatch Alarms",isCorrect:!1}],explanation:"AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. Read more: https://aws.amazon.com/cloudtrail/"},{question:"Which of the following is NOT an added resource type by Serverless Application Model?",answers:[{text:"AWS::Serverless::UserPool",isCorrect:!0},{text:"AWS::Serverless::Function",isCorrect:!1},{text:"AWS::Serverless::Api",isCorrect:!1},{text:"AWS::Serverless::SimpleTable",isCorrect:!1}],explanation:"AWS::Serverless::Function creates a Lambda function, IAM execution role, and event source mappings which trigger the function.AWS::Serverless::Api creates a collection of Amazon API Gateway resources and methods that can be invoked through HTTPS endpoints.The AWS::Serverless::SimpleTable resource creates a DynamoDB table with a single attribute primary key. Read more: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#format"},{question:"You would like to implement a 3rd party authentication for your AWS API Gateway. Which authorizer do you choose?",answers:[{text:"Lambda Authorizer",isCorrect:!0},{text:"IAM permissions with sigv4",isCorrect:!1},{text:"Cognito User Pools",isCorrect:!1}],explanation:"An Amazon API Gateway Lambda authorizer is a Lambda function that you provide to control access to your API methods. A Lambda authorizer uses bearer token authentication strategies, such as OAuth or SAML. It can also use information described by headers, paths, query strings, stage variables, or context variables request parameters. Read more: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html"},{question:"You have a requirement to leverage a NoSQL hosted database in the AWS Cloud, that can auto scale. Which database would you choose?",answers:[{text:"DynamoDB",isCorrect:!0},{text:"RDS",isCorrect:!1},{text:"Redshift",isCorrect:!1},{text:"ElastiCache",isCorrect:!1}],explanation:"Amazon DynamoDB is a nonrelational database that delivers reliable performance at any scale. It's a fully managed, multi-region, multi-master database that provides consistent single-digit millisecond latency.RDS is managed service for RDBMS, Redshift is a petabyte scale warehouse service and ElastCache is an in-memory storage service. Read more: https://aws.amazon.com/dynamodb/"},{question:"Your Elastic Beanstalk application must encrypt payloads of up to 10MB. Which method will help you achieve that?",answers:[{text:"Use the Encryption SDK",isCorrect:!0},{text:"KMS Encrypt API Call",isCorrect:!1},{text:"S3 Encrypt API Call",isCorrect:!1},{text:"IAM Encrypt API call",isCorrect:!1}],explanation:"The AWS Encryption SDK is an encryption library that helps make it easier for you to implement encryption best practices in your application.  Read more: https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/introduction.html"},{question:"In order to pass artifacts between various stages, which AWS Service is leveraged by CodePipeline?",answers:[{text:"S3",isCorrect:!0},{text:"SQS",isCorrect:!1},{text:"AWS Lambda",isCorrect:!1},{text:"EFS",isCorrect:!1}],explanation:"The Artifact Store is an Amazon S3 bucket that CodePipeline uses to store artifacts used by pipelines. When you first use the CodePipeline console in a region to create a pipeline, CodePipeline automatically generates this S3 bucket in the AWS region. It stores artifacts for all pipelines in that region in this bucket. Read more: https://docs.aws.amazon.com/codepipeline/latest/userguide/S3-artifact-encryption.html"},{question:"Your API Gateway is enabled to send access logs to CloudWatch but it seems that CloudWatch isn't receiving any data. What is the likely root cause of this issue?",answers:[{text:"Check the Service IAM permissions",isCorrect:!0},{text:"Check the CloudWatch Logs Policy",isCorrect:!1},{text:"Also enable X-Ray integration",isCorrect:!1},{text:"Ensure in-flight encryption is disabled",isCorrect:!1}],explanation:"To enable CloudWatch Logs, you must grant API Gateway permission to read and write logs to CloudWatch for your account. The AmazonAPIGatewayPushToCloudWatchLogs managed policy (with an ARN of arn:aws:iam::aws:policy/service-role/AmazonAPIGatewayPushToCloudWatchLogs) has all the required permissions Read more: https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-logging.html"},{question:"You have a fleet of 100 000 trucks that continuously send data to your cloud. This big data stream must be delivered to multiple consuming applications. This solution must be able to scale as the load increases. Which technology is the most suitable?",answers:[{text:"Kinesis Stream",isCorrect:!0},{text:"SNS",isCorrect:!1},{text:"SQS",isCorrect:!1},{text:"Lambda",isCorrect:!1}],explanation:"Kinesis data streams can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.  Read more: https://aws.amazon.com/kinesis/data-streams/"},{question:"CodePipeline fails with an authorisation issue. What is the best way to determine the root cause of the problem?",answers:[{text:"Check the Service IAM permissions",isCorrect:!0},{text:"Check changes in CloudTrail",isCorrect:!1},{text:"Check the recent changes in CloudWatch",isCorrect:!1},{text:"Audit the S3 bucket CORS policy where the artifacts are stored",isCorrect:!1}],explanation:"Any authorization related failure directly indicates to us that our first point of debugging should be validating the IAM permissions for the service. Read more: https://aws.amazon.com/cloudtrail/https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html"},{question:"Which of the following integration technology pushes data to consumers?",answers:[{text:"SNS",isCorrect:!0},{text:"SQS",isCorrect:!1},{text:"Kinesis",isCorrect:!1},{text:"SES",isCorrect:!1}],explanation:"SNS pushes while others pull"},{question:"Your client is asking how KMS Encryption works. You explain that  ",answers:[{text:"KMS stores the CMK, and receives data from the clients, which it encrypts and sends back.",isCorrect:!0},{text:"KMS receives CMK from the client at every Encrypt call, and encrypts the data with that",isCorrect:!1},{text:"KMS sends the CMK to the client, which performs the encryption and then deletes the CMK",isCorrect:!1},{text:"KMS generates a new CMK for each Encrypt call and encrypts the data with it",isCorrect:!1}],explanation:"Refer to the link under  Read more:  column for more detailed explaination. Read more: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html"},{question:"You have requested credentials using STS when your applications boots. After an hour, your application stops working and needs to be manually restarted to work again. What is happening?",answers:[{text:"Your application needs to renew the credentials after 1 hour when they expire",isCorrect:!0},{text:"Your IAM policy is wrong",isCorrect:!1},{text:"A lambda function revokes your access every hour",isCorrect:!1},{text:"The IAM service is experiencing downtime once an hour",isCorrect:!1}],explanation:"Credentials that are created by using account credentials can range from 900 seconds (15 minutes) up to a maximum of 3,600 seconds (1 hour), with a default of 1 hour. Hence you need to renew the credentials post expiry. Read more: https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html"},{question:"Which protocol is not supported by CloudFront?",answers:[{text:"UDP",isCorrect:!0},{text:"HTTP",isCorrect:!1},{text:"HTTPS",isCorrect:!1},{text:"RTMP",isCorrect:!1}],explanation:" Read more: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-overview.html"},{question:"Which following two terms belong to the SNS product? (select two)",answers:[{text:"topic",isCorrect:!0},{text:"subscription",isCorrect:!0},{text:"queue",isCorrect:!1},{text:"consumer",isCorrect:!1},{text:"retention",isCorrect:!1}],explanation:"Using Amazon SNS topics, your publisher systems can fan out messages to a large number of subscriber endpoints for parallel processing Read more: https://aws.amazon.com/sns/"},{question:"You have deployed a Java application on EC2 and made sure to enable the X-Ray SDK. The application was sending data to X-Ray when you tested it from your personal computer. The deployed application fails to send data to X-Ray. Which of the following component debugging won't help resolve the problem?",answers:[{text:"X-Ray sampling",isCorrect:!0},{text:"EC2 X-Ray agent",isCorrect:!1},{text:"EC2 Instance Role",isCorrect:!1},{text:"Security Groups Rules",isCorrect:!1}],explanation:"The X-Ray SDK applies a sampling algorithm to efficiently trace and provide a representative sample of the requests that your application serves. This can be utilised post the data is being successfully being sent to X-Ray and in no way helps us in determining the cause of failure to send the data to X-Ray. Read more: https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-sampling"},{question:"You want to provision your own Docker images that can be used as an input for CodeBuild. These images will contain cached dependencies as well as special tooling for your builds that are proprietary to your company. Where should you push these images?",answers:[{text:"ECR",isCorrect:!0},{text:"S3",isCorrect:!1},{text:"EFS",isCorrect:!1},{text:"EBS",isCorrect:!1}],explanation:"Amazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. Read more: https://docs.aws.amazon.com/codebuild/latest/userguide/sample-ecr.html"},{question:"Your CodeBuild has failed and you would like to perform some hands-on debugging. How should you proceed?",answers:[{text:"Run AWS CodeBuild locally",isCorrect:!0},{text:"SSH into the CodeBuild Docker container",isCorrect:!1},{text:"Freeze the CodeBuild during its next execution",isCorrect:!1},{text:"Enable detailed monitoring",isCorrect:!1}],explanation:"With the Local Build support for AWS CodeBuild, now you just specify the location of your source code, choose your build settings, and CodeBuild runs build scripts for compiling, testing, and packaging your code. Read more: https://aws.amazon.com/blogs/devops/announcing-local-build-support-for-aws-codebuild/"},{question:"You are looking to run the same application multiple times on your EC2 instance and expose it with a load balancer. You should use",answers:[{text:"Application Load Balancer + ECS",isCorrect:!0},{text:"Classic Load Balancer + ECS",isCorrect:!1},{text:"Application Load Balancer + Beanstalk",isCorrect:!1},{text:"Classic Load Balancer + Beanstalk",isCorrect:!1}],explanation:"Application Load Balancers allow containers to use dynamic host port mapping (so that multiple tasks from the same service are allowed per container instance). Read more: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html"},{question:"Someone just deleted an EC2 instance. Which AWS Service will allow you to find out who did it?",answers:[{text:"CloudTrail",isCorrect:!0},{text:"EC2 Logs",isCorrect:!1},{text:"CloudWatch",isCorrect:!1},{text:"Config",isCorrect:!1}],explanation:"AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. Read more: https://aws.amazon.com/cloudtrail/"},{question:"Which section in mandatory in CloudFormation templates?",answers:[{text:"Resources",isCorrect:!0},{text:"Parameters",isCorrect:!1},{text:"Outputs",isCorrect:!1},{text:"Mappings",isCorrect:!1}],explanation:"Resources is the only mandatory section of the CloudFormation template and rest all sections are optional. Read more: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html"},{question:"You want to give a temporary read access to your S3 files. How can you do this ?",answers:[{text:"Using pre-signed URL",isCorrect:!0},{text:"Using bucket policy",isCorrect:!1},{text:"Using Routing Policy",isCorrect:!1},{text:"Not possible",isCorrect:!1}],explanation:"All objects by default are private. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a presigned URL, using their own security credentials, to grant time-limited permission to download the objects. Read more: https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html"},{question:"In DynamoDB, what's the maximum number of attributes that can be combined to form a primary key?",answers:[{text:"2",isCorrect:!0},{text:"1",isCorrect:!1},{text:"3",isCorrect:!1},{text:"4",isCorrect:!1}],explanation:"The primary key can just consist of a simple partition key as a single attribute or a max of two attributes as a composite key consisting of partition key and sort key. Read more: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html#HowItWorks.CoreComponents.PrimaryKey"},{question:"An application is hosted by a 3rd party and is exposed as yourapp.3rdparty.com. You would like to have your users access your application using mydomain.com , which you own and manage under Route53. What record should you create?",answers:[{text:"Create a CNAME record",isCorrect:!0},{text:"Create an A record",isCorrect:!1},{text:"Create a PTR record",isCorrect:!1},{text:"Create an Alias Record",isCorrect:!1}],explanation:"CNAME records can be used to alias one name to another. Read more: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/ResourceRecordTypes.html#CNAMEFormat"},{question:"Your business model requires that your website will be visited daily from 8am to 11pm except weekends.What is the best EC2 payment plan while reducing the cost of hosting ?",answers:[{text:"Scheduled Instances",isCorrect:!0},{text:"On-demand Instances",isCorrect:!1},{text:"Classic Instances",isCorrect:!1},{text:"Spot Instances",isCorrect:!1}],explanation:"Scheduled instances allow you to reserve capacity on a recurring basis with a daily, weekly, or monthly schedule over the course of a one-year term. Read more: https://aws.amazon.com/blogs/aws/new-scheduled-reserved-instances/"},{question:"You would like to have a secured and versioned way of storing your database credentials. Which service do you pick?",answers:[{text:"SSM Parameter Store",isCorrect:!0},{text:"EFS",isCorrect:!1},{text:"EBS",isCorrect:!1},{text:"KMS",isCorrect:!1}],explanation:"AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. Read more: https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html"},{question:"Your client is migrating an application to the cloud and needs a technology to ensure your application can be stateless and store data in a low latency in memory store. What do you recommend?",answers:[{text:"ElastiCache",isCorrect:!0},{text:"RDS",isCorrect:!1},{text:"DynamoDB",isCorrect:!1},{text:"S3",isCorrect:!1}],explanation:"ElastiCache is a managed service offering from AWS for in-memory data storages. Read more: https://aws.amazon.com/elasticache/"},{question:"You want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another. Which routing policy is suitable for your case ?",answers:[{text:"Geoproximity",isCorrect:!0},{text:"Geolocation",isCorrect:!1},{text:"Failover",isCorrect:!1},{text:"Latency",isCorrect:!1},{text:"Weighted",isCorrect:!1}],explanation:"Simple routing policy   : Use for a single resource that performs a given function for your domain.Failover routing policy   : Use when you want to configure active-passive failover.Geolocation routing policy   : Use when you want to route traffic based on the location of your users.Geoproximity routing policy   : Use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.Latency routing policy   : Use when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the best latency.Multivalue answer routing policy   : Use when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random.Weighted routing policy   : Use to route traffic to multiple resources in proportions that you specify. Read more: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html"},{question:"CodeBuild fails as it cannot pull a Docker image stored in ECR. What is the most likely reason?",answers:[{text:"Missing IAM permissions for the CodeBuild Service",isCorrect:!0},{text:"The Docker image is missing some tags",isCorrect:!1},{text:"CodeBuild cannot work with custom Docker images",isCorrect:!1},{text:"The Docker image is too big",isCorrect:!1}],explanation:"By default, IAM users don't have permission to create or modify Amazon ECR resources, or perform tasks using the Amazon ECR API. Read more: https://docs.aws.amazon.com/AmazonECR/latest/userguide/ECR_IAM_policies.html"},{question:"What can't CloudFormation Conditions be applied to?",answers:[{text:"Parameters",isCorrect:!0},{text:"Resources",isCorrect:!1},{text:"Conditions",isCorrect:!1},{text:"Outputs",isCorrect:!1}],explanation:"You can create a condition and then associate it with a resource or output so that AWS CloudFormation only creates the resource or output if the condition is true. Optionally within each condition, you can reference another condition as well. Read more: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html"},{question:"You would like to query items based on an attribute that is not part of the DynamoDB partition key. What should you do?",answers:[{text:"Create a GSI",isCorrect:!0},{text:"Call Scan",isCorrect:!1},{text:"Create a LSI",isCorrect:!1},{text:"Migrate away from DynamoDB",isCorrect:!1}],explanation:"Some applications might need to perform many kinds of queries, using a variety of different attributes other than the specified partition key as query criteria. To support these requirements, you can create one or more global secondary indexes and issue Query requests against these indexes. Read more: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html"},{question:"What are SNS messages sent to?",answers:[{text:"topic",isCorrect:!0},{text:"queue",isCorrect:!1},{text:"stream",isCorrect:!1},{text:"forum",isCorrect:!1}],explanation:"Using Amazon SNS topics, your publisher systems can fan out messages to a large number of subscriber endpoints for parallel processing Read more: https://aws.amazon.com/sns/"},{question:"Which of the following isn't a supported mechanism of authentication for API Gateway?",answers:[{text:"STS",isCorrect:!0},{text:"IAM permissions with sigv4",isCorrect:!1},{text:"Lambda Authorizer",isCorrect:!1},{text:"Cognito User Pools",isCorrect:!1}],explanation:"Following mechanisms are supported by the API Gateways for authentication Resource policiesStandard AWS IAM roles and policiesCross-origin resource sharing (CORS)Lambda authorizersAmazon CognitoClient-side SSL certificatesUsage plans Read more: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html"},{question:"Should non exported CloudFormation outputs must a region-level unique name?",answers:[{text:"no",isCorrect:!0},{text:"yes",isCorrect:!1}],explanation:"Only the exported CloudFormation, Export names must be unique within a region for each AWS account. Read more: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputs-section-structure.html"},{question:"You have created a target group which has marked all your EC2 instances as unhealthy. Surprisingly, when you directly enter the IP of the EC2 instances in your web browser, you are able to access your website. So you know it is running, but what could be the reason of your instances being marked as unhealthy? (select two)",answers:[{text:"The security group of the EC2 instance does not allow for traffic from the security group of the Application Load Balancer",isCorrect:!0},{text:"The route for the health check is misconfigured",isCorrect:!0},{text:"The EBS volumes have been unproperly mounted",isCorrect:!1},{text:"Your web-app has a runtime that is not supported by the Application Load Balancer",isCorrect:!1},{text:"You need to attach Elastic IP to the EC2 instances",isCorrect:!1}],explanation:"You must ensure that your load balancer can communicate with registered targets on both the listener port and the health check port. Whenever you add a listener to your load balancer or update the health check port for a target group used by the load balancer to route requests, you must verify that the security groups associated with the load balancer allow traffic on the new port in both directions. Read more: https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-health-checks.htmlhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html"},{question:"Which language is not supported by CloudFormation?",answers:[{text:"Python",isCorrect:!0},{text:"YAML",isCorrect:!1},{text:"JSON",isCorrect:!1}],explanation:"A CloudFormation template is stored as a text file whose format complies with the JavaScript Object Notation (JSON) or YAML standard. Read more: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/gettingstarted.templatebasics.html"},{question:"You would like to deploy a new application version using Beanstalk but don't want to bring your capacity down. You are also very cost conscious and want to ensure any added cost will be minimal. Which deployment fits your needs?",answers:[{text:"rolling with additional batches",isCorrect:!0},{text:"immutable",isCorrect:!1},{text:"all at once",isCorrect:!1},{text:"rolling",isCorrect:!1},{text:"blue/green",isCorrect:!1}],explanation:"Rolling with additonal batches is the most optimum deployment policy when a user doesn't want to compromise on the output capacity of the application but also keep the additional expense incurs to the minimum.Rolling with additional batch   : Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during the deployment process. Read more: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.htmlhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html"},{question:"You have a Pipeline in AWS CodePipeline that contains a lot of steps. Some steps need to be ordered while other can run simultaneously. You would like to speed up your CodePipeline. How do you achieve it?",answers:[{text:"Re-design the pipeline to include steps to occur in parallel where possible",isCorrect:!0},{text:"Upgrade the AWS CodePipeline instance type",isCorrect:!1},{text:'Ask for a "speed boost" from the AWS support',isCorrect:!1},{text:"Enable pipeline caching",isCorrect:!1}],explanation:"Actions in parallel in CodePipeline can help with speed requirements Read more: https://docs.aws.amazon.com/codepipeline/latest/userguide/reference-pipeline-structure.html#action-requirements"}]},{id:"aws-developer-4540360",title:"Practice Test #2 (AWS Certified Developer Associate - DVA-C02)",description:"Practice questions for AWS Developer Associate (DVA-C02) exam — covering core AWS services like SQS, Lambda, DynamoDB, and more.",questions:[{question:"You are looking to deploy applications a fleet of on-premise instances, with precise blue/green strategy. You want full control over the deployment steps and the possibility to add hooks. Which service should you use?",answers:[{text:"CodeDeploy",isCorrect:!0},{text:"CodeBuild",isCorrect:!1},{text:"Elastic Beanstalk",isCorrect:!1},{text:"CodePipeline",isCorrect:!1}],explanation:"AWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, or serverless Lambda functions.During a blue/green deployment, the latest application revision is installed on replacement instances, and traffic is rerouted to these instances when you choose, either immediately or as soon as you are done testing the new environment.You can launch and track the status of your deployments through the AWS CodeDeploy console or the AWS CLI. You receive a report that lists when each application revision was deployed and to which Amazon EC2 instances. Read more: https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html"},{question:"You have a deployed a Lambda functions written in Golang that has loaded the X-Ray SDK. The IAM permissions of the Lambda function do allow you to send data to X-Ray. But things still don't work, what's a likely source of the problem?",answers:[{text:"Lambda X-Ray active tracing must be enabled",isCorrect:!0},{text:"The IAM permissions",isCorrect:!1},{text:"Authorise Lambda source in the X-Ray console",isCorrect:!1},{text:"The X-Ray agent must be loaded with the code",isCorrect:!1}],explanation:"If your Lambda function runs on a schedule, or is invoked by a service that is not instrumented, you can configure Lambda to sample and record invocations with active tracing.Readf morehttps://docs.aws.amazon.com/xray/latest/devguide/xray-services-lambda.html"},{question:"We need to perform 10 strongly consistent reads per second of 6KB each. How many RCU do we need?",answers:[{text:"20",isCorrect:!0},{text:"60",isCorrect:!1},{text:"30",isCorrect:!1},{text:"10",isCorrect:!1}],explanation:"One read capacity unit represents one strongly consistent read per second for an item up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units. Read more: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html"},{question:"You are trying to upload an object of 500GB of size, but the upload is failing with an error message indicating the file is too big. What's the problem?",answers:[{text:"You need to use multi-part upload for large files",isCorrect:!0},{text:"The maximum file size is 5 GB",isCorrect:!1},{text:"Your IAM permissions are incorrect",isCorrect:!1},{text:"You need to place a service limit request increase with AWS",isCorrect:!1}],explanation:"In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Read more: https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html"},{question:"Which of the following can be used to implement a REST API? (select two)",answers:[{text:"ALB + ECS",isCorrect:!0},{text:"API Gateway + Lambda",isCorrect:!0},{text:"SES + S3",isCorrect:!1},{text:"CloudWatch + CloudFront",isCorrect:!1},{text:"EBS + RDS",isCorrect:!1}],explanation:" Read more: https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-4/ https://aws.amazon.com/blogs/compute/microservice-delivery-with-amazon-ecs-and-application-load-balancers/"},{question:"Which of the following statements isn't true for SNS?",answers:[{text:"SNS Messages can be retrieved at a later date",isCorrect:!0},{text:"Messages are sent to all subscribers",isCorrect:!1},{text:"Lambda can be a target of SNS",isCorrect:!1},{text:"SQS can be a target of SNS",isCorrect:!1}],explanation:"Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. Using Amazon SNS topics, your publisher systems can fan out messages to a large number of subscriber endpoints for parallel processing, including Amazon SQS queues, AWS Lambda functions, and HTTP/S webhooks. Additionally, SNS can be used to fan out notifications to end users using mobile push, SMS, and email.  Read more: https://aws.amazon.com/sns/"},{question:"Each ECS Task must have an IAM policy attached to it. How do you achieve that?",answers:[{text:"Create an IAM Role for ECS and assign it to the tasks",isCorrect:!0},{text:"Assign an IAM role to the EC2 instance",isCorrect:!1},{text:"Load the credentials within the docker container",isCorrect:!1},{text:"Use the parameter store to pass in AWS credentials",isCorrect:!1}],explanation:"You must also create a role for your tasks to use before you can specify it in your task definitions. You can create the role using the Amazon Elastic Container Service Task Role service role in the IAM console. Then you can attach your specific IAM policy to the role that gives the containers in your task the permissions you desire. Read more: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html"},{question:"You would like each of your API Gateway stages (dev, test, prod) to point to a specific Lambda function version. Which mechanism would you need to use? (select two)",answers:[{text:"Stage Variables",isCorrect:!0},{text:"Lambda Aliases",isCorrect:!0},{text:"Lambda Versions",isCorrect:!1},{text:"Lambda X-Ray integration",isCorrect:!1},{text:"Mapping Templates",isCorrect:!1}],explanation:"Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of an API. They act like environment variables and can be used in your API setup and mapping templates.With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints. Read more: https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html"},{question:"While you send messages one by one to Kinesis Stream, you are getting a ProvisionedThroughputException. How can you improve the situation while keeping cost constant? (select two)",answers:[{text:"Batch messages",isCorrect:!0},{text:"Use Exponential Backoff",isCorrect:!0},{text:"Increase the number of shards",isCorrect:!1},{text:"Decrease the Stream retention duration",isCorrect:!1}],explanation:"To reduce overhead and increase throughput, the application produce records in batches.  Read more: https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/"},{question:'You are currently storing 500 GB of data on your S3 bucket "my-corp". What\'s the maximum total volume of data that your S3 bucket can contain?',answers:[{text:"unlimited",isCorrect:!0},{text:"10 TB",isCorrect:!1},{text:"25 TB",isCorrect:!1},{text:"5 TB",isCorrect:!1}],explanation:"The total volume of data and number of objects you can store are unlimited. Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 terabytes. The largest object that can be uploaded in a single PUT is 5 gigabytes. Read more: https://aws.amazon.com/s3/faqs/"},{question:"Which of the following is not a valid CloudFormation Parameter Type?",answers:[{text:"DependentParameter",isCorrect:!0},{text:"String",isCorrect:!1},{text:"CommaDelimitedList",isCorrect:!1},{text:"AWS::EC2::KeyPair::KeyName",isCorrect:!1}],explanation:" Read more: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html"},{question:"What's the best practice regarding accessing the AWS Root account?",answers:[{text:"It should be accessible by one admin only after enabling Multi-factor authentication",isCorrect:!0},{text:"It should be accessible by 3 to 6 members of the IT team",isCorrect:!1},{text:"It should be accessible using the access_key_id and secret_access_key_id",isCorrect:!1},{text:"It should be accessible by no one, throw away the passwords after creating the account",isCorrect:!1}],explanation:"If you continue to use the root user credentials, we recommend that you follow the security best practice to enable multi-factor authentication (MFA) for your account. Because your root user can perform sensitive operations in your account, adding an additional layer of authentication helps you to better secure your account. Multiple types of MFA are available. Read more: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html#id_root-user_manage_mfa"},{question:"Which key does allow you to expose outputs to other CloudFormation stacks?",answers:[{text:"Export",isCorrect:!0},{text:"Expose",isCorrect:!1},{text:"Output",isCorrect:!1},{text:"Import",isCorrect:!1}],explanation:"To export a stack's output value, use the Export field in the Output section of the stack's template. Read more: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html"},{question:"Your application deployed to Elastic Beanstalk from CodePipeline (CodeCommit => CodeBuild) takes a very long time to be deployed due to the fact it has to resolve many dependencies on each of your 100 target EC2 instances. What do you suggest to improve the performance while having the minimal amount of code and developer impact?",answers:[{text:"Bundle the dependencies in the source code during the last stage of CodeBuild",isCorrect:!0},{text:"Bundle the dependencies in the source code in CodeCommit",isCorrect:!1},{text:"Store the dependencies in S3",isCorrect:!1},{text:"Create a custom platform for Elastic Beanstalk",isCorrect:!1}],explanation:"Bundling the dependencies with the code during the build output phase is a great way to speed up Beanstalk deployments"},{question:"You would like to audit the actual usage of your KMS key. Which service can help you achieve that?",answers:[{text:"CloudTrail",isCorrect:!0},{text:"VPC Flow Logs",isCorrect:!1},{text:"KMS policies",isCorrect:!1},{text:"IAM",isCorrect:!1}],explanation:"CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This event history simplifies security analysis, resource change tracking, and troubleshooting. Read more: https://aws.amazon.com/cloudtrail/"},{question:"Which of the following statement is correct for CloudFormation?",answers:[{text:"The order in which resources are created must not be specified",isCorrect:!0},{text:"CloudFormation templates are static and do not allow parameters",isCorrect:!1},{text:"CloudFormation templates can only be valid for one region and account",isCorrect:!1},{text:"CloudFormation templates lock the configuration of the created resources",isCorrect:!1}],explanation:"CloudFormation is declarative, not imperative Read more:  https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-concepts.html"},{question:"You would like to add hooks to your AWS CodeDeploy deployment. How can you achieve that?",answers:[{text:"define a .appspec file in the root directory",isCorrect:!0},{text:"define a .buildspec file in the root directory",isCorrect:!1},{text:"define a .buildspec file in the codebuild/ directory",isCorrect:!1},{text:"define a .appspec file in the codebuild/ directory",isCorrect:!1}],explanation:"An AppSpec file must be a YAML-formatted file named appspec.yml and it must be placed in the root of the directory structure of an application's source code. Otherwise, deployments fail. Read more: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html"},{question:"You would like to improve the CPU and network performance of your Lambda functions without changing your code. How should you do it?",answers:[{text:"Increase the RAM assigned to your function",isCorrect:!0},{text:"Increase the instance type for your functions",isCorrect:!1},{text:"Change your function runtime to use Golang",isCorrect:!1},{text:"Increase the timeout",isCorrect:!1}],explanation:"Specify the amount of memory you want to allocate for your Lambda function. AWS Lambda allocates CPU power proportional to the memory by using the same ratio as a general purpose Amazon EC2 instance type, such as an M3 type. Read more: https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html"},{question:"You are hosting a public AWS website on S3 bucket but every user is currently getting a 403 (Forbidden) error. How can you remedy this error?",answers:[{text:"Create a bucket policy",isCorrect:!0},{text:"Create an IAM role",isCorrect:!1},{text:"Enable CORS",isCorrect:!1},{text:"Enable Encrytion",isCorrect:!1}],explanation:"Bucket policy is an access policy option available for you to grant permission to your Amazon S3 resources. It uses JSON-based access policy language.  Read more: https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html"},{question:"Which ECS config must you enable to allow your ECS tasks to endorse IAM roles?",answers:[{text:"ECS_ENABLE_TASK_IAM_ROLE",isCorrect:!0},{text:"ECS_AVAILABLE_LOGGING_DRIVERS",isCorrect:!1},{text:"ECS_ENGINE_AUTH_DATA",isCorrect:!1},{text:"ECS_CLUSTER",isCorrect:!1}],explanation:"ECS_ENABLE_TASK_IAM_ROLE=trueEnables IAM roles for tasks for containers with the bridge and default network modes. Read more: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html#enable_task_iam_roles"},{question:"How can you retrieve a nested JSON attribute in your DynamoDB query?",answers:[{text:"Specify a ProjectionExpression",isCorrect:!0},{text:"Use a FilterExpression",isCorrect:!1},{text:"Use the --query parameter",isCorrect:!1}],explanation:"A projection expression is a string that identifies the attributes you want. To retrieve a single attribute, specify its name. For multiple attributes, the names must be comma-separated. Read more: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html"},{question:"What isn't a target sink for Kinesis Firehose?",answers:[{text:"ElastiCache",isCorrect:!0},{text:"S3",isCorrect:!1},{text:"Redshift",isCorrect:!1},{text:"ElasticSearch",isCorrect:!1}],explanation:"Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk. Read more: https://aws.amazon.com/kinesis/data-firehose/faqs/"},{question:"Which of the following is not a valid CloudFormation section?",answers:[{text:"Groups",isCorrect:!0},{text:"Parameters",isCorrect:!1},{text:"Mappings",isCorrect:!1},{text:"Metadata",isCorrect:!1}],explanation:" Read more: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html"},{question:"You want to perform a slow rollout of a new API version using API Gateway. Which feature should you use?",answers:[{text:"Canary Deployment",isCorrect:!0},{text:"Stage Variables",isCorrect:!1},{text:"Mapping Templates",isCorrect:!1},{text:"Custom Authorizers",isCorrect:!1}],explanation:"In a canary release deployment, total API traffic is separated at random into a production release and a canary release with a pre-configured ratio. Typically, the canary release receives a small percentage of API traffic and the production release takes up the rest. The updated API features are only visible to API traffic through the canary. You can adjust the canary traffic percentage to optimize test coverage or performance. Read more: https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html"},{question:"We need to write 6 objects per second of 4.5 KB each in DynamoDB. What's the target WCU?",answers:[{text:"30",isCorrect:!0},{text:"24",isCorrect:!1},{text:"15",isCorrect:!1},{text:"46",isCorrect:!1}],explanation:"One write capacity unit represents one write per second for an item up to 1 KB in size. If you need to write an item that is larger than 1 KB, DynamoDB will need to consume additional write capacity units. The total number of write capacity units required depends on the item size. Read more: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html"},{question:"You would like to build a serverless REST API. Which technology combo will you choose?",answers:[{text:"API Gateway + Lambda",isCorrect:!0},{text:"ELB + EC2",isCorrect:!1},{text:"ALB + ECS on EC2",isCorrect:!1},{text:"Route 53 + EC2",isCorrect:!1}],explanation:" Read more: https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-4/"},{question:"When you rollback a CodeDeploy, where is the application deployed first?",answers:[{text:"To the failed instances",isCorrect:!0},{text:"To the non-failed instances",isCorrect:!1},{text:"To new instances",isCorrect:!1},{text:"You cannot rollback CodeDeploy",isCorrect:!1}],explanation:"AWS CodeDeploy rolls back deployments by redeploying a previously deployed revision of an application as a new deployment on the failed instances. Read more: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html"},{question:"What do you need to do to ensure your Kinesis Stream can scale? (select two)",answers:[{text:"The partition key must take a great number of different values",isCorrect:!0},{text:"You need to add shards",isCorrect:!0},{text:"You need to enable auto-scale",isCorrect:!1},{text:"The partition key must only take few values",isCorrect:!1}],explanation:"Kinesis Data Streams segregates the data records belonging to a stream into multiple shards. It uses the partition key that is associated with each data record to determine which shard a given data record belongs to. Partition keys are Unicode strings with a maximum length limit of 256 bytes.A stream is composed of one or more shards, each of which provides a fixed unit of capacity. Each shard can support up to 5 transactions per second for reads, up to a maximum total data read rate of 2 MB per second and up to 1,000 records per second for writes, up to a maximum total data write rate of 1 MB per second (including partition keys). The data capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the capacities of its shards. Read more: https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html"},{question:"Your company does want S3 server side encryption but want to be able to provide the encryption key. Which encryption mechanism suits best?",answers:[{text:"SSE-C",isCorrect:!0},{text:"SSE-KMS",isCorrect:!1},{text:"Client Side Encryption",isCorrect:!1},{text:"SSE-S3",isCorrect:!1}],explanation:"Use Server-Side Encryption with Customer-Provided Keys (SSE-C)   : You manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects. Read more: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html"},{question:"Your application load balancer cannot connect to your EC2 instance, it experiences a timeout. What's the problem?",answers:[{text:"Security Groups",isCorrect:!0},{text:"IAM Roles",isCorrect:!1},{text:"The application is down",isCorrect:!1},{text:"The ALB is warming up",isCorrect:!1}],explanation:"Check your security group rules of your EC2 instance. You need a security group rule that allows inbound traffic from your public IPv4 address on the proper port. Read more: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html#TroubleshootingInstancesConnectionTimeout"},{question:"You would like to see the logs of your failed CodeBuilds. How should you do it?",answers:[{text:"Enable S3 and CloudWatch integration",isCorrect:!0},{text:"Use AWS Lambda integration",isCorrect:!1},{text:"Use CloudWatch Events",isCorrect:!1},{text:"Use Kinesis",isCorrect:!1}],explanation:"If there is any build output, the build environment uploads its output to an Amazon S3 bucket.While the build is running, you can use the AWS CodeBuild console, AWS CLI, or AWS SDKs, to get summarized build information from AWS CodeBuild and detailed build information from Amazon CloudWatch Logs. Read more: https://docs.aws.amazon.com/codebuild/latest/userguide/concepts.html"},{question:"Do you have to provision EC2 instances in order for CodePipeline to be able to run?",answers:[{text:"no",isCorrect:!0},{text:"yes",isCorrect:!1}],explanation:"CodePipeline is a serverless managed service"},{question:"You would like to deploy a database that has the capability to perform joins and fast per-row OLTP processing. Which database technology is best?",answers:[{text:"RDS",isCorrect:!0},{text:"DynamoDB",isCorrect:!1},{text:"Redshift",isCorrect:!1},{text:"ElastiCache",isCorrect:!1}],explanation:"Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. Amazon RDS supports the most demanding database applications. You can choose between two SSD-backed storage options: one optimized for high-performance OLTP applications, and the other for cost-effective general-purpose use. Read more: https://aws.amazon.com/rds/"},{question:"You want to have full control over the deployment of your Elastic Beanstalk application and control the portion of the traffic that will go to your new application version, so that you can perform extended testing. When the time is right, you want a simple way to switch the environment for the end users. Which deployment mechanism fits best?",answers:[{text:"blue/green",isCorrect:!0},{text:"rolling",isCorrect:!1},{text:"immutable",isCorrect:!1},{text:"all at once",isCorrect:!1},{text:"rolling with additional batches",isCorrect:!1}],explanation:"As AWS Elastic Beanstalk performs an in-place update when you update your application versions, your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs of the two environments to redirect traffic to the new version at what ever point you want the traffic to be comepeletly redirected to the new environment. Read more: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html"},{question:"You would like to see a list of the API calls performed by your CodePipeline pipeline. Which service will help you with that?",answers:[{text:"CloudTrail",isCorrect:!0},{text:"CodePipeline Logs",isCorrect:!1},{text:"IAM",isCorrect:!1},{text:"CloudWatch Logs",isCorrect:!1}],explanation:"CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This event history simplifies security analysis, resource change tracking, and troubleshooting. Read more: https://aws.amazon.com/cloudtrail/"},{question:"You would like to pass 1 MB of encrypted data to your AWS Lambda function, that it needs to load before properly functioning. Which method will work best?",answers:[{text:"Envelope Encrption and store as file within the code",isCorrect:!0},{text:"KMS direct encryption and store as file",isCorrect:!1},{text:"Envelope Encryption and store as environment variable",isCorrect:!1},{text:"KMS Encryption and store as environment variable",isCorrect:!1}],explanation:'AWS Lambda environment variables have a maximum size of a few KB. Additionally, the direct "Encrypt" API of KMS also has a few KB limit. To encrypt 1 MB, you need to use the Encryption SDK and pack the encrypted file with the lambda function.  Read more: https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/introduction.html'},{question:"What is the maximum number of messages than can be stored in a SQS queue?",answers:[{text:"no limit",isCorrect:!0},{text:"10000",isCorrect:!1},{text:"100000",isCorrect:!1},{text:"10000000",isCorrect:!1}],explanation:'There are no message limits for storing in SQS, but "in flight messages" do have limits. Make sure to delete messages after you have processed them. Read more:  https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-limits.html '},{question:"You need to store data that needs to be available on a tablet, a mobile device and a web application for the same user. Which product allows you to achieve this?",answers:[{text:"Cognito Sync",isCorrect:!0},{text:"Cognito Identity Pools",isCorrect:!1},{text:"S3",isCorrect:!1},{text:"EFS",isCorrect:!1}],explanation:"Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. Read more: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html"},{question:"You have SSH into your EC2 instance. How can you check if it can terminate other EC2 instances without actually terminating them?",answers:[{text:"Use the AWS CLI --dry-run option",isCorrect:!0},{text:"Use the AWS CLI --test option",isCorrect:!1},{text:"Retrieve the policy using the EC2 meta data service and use the IAM policy simulator",isCorrect:!1},{text:"Using the CLI, create a dummy EC2 and delete it using another CLI call",isCorrect:!1}],explanation:"The --dry-run option checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is DryRunOperation . Otherwise, it is UnauthorizedOperation . Read more: https://docs.aws.amazon.com/cli/latest/reference/ec2/terminate-instances.html"},{question:"You application is deployed on EC2 and needs to access an internal API through API Gateway. Which authentication method will provide the easiest security?",answers:[{text:"IAM permissions with sigv4",isCorrect:!0},{text:"Lambda Authorizer",isCorrect:!1},{text:"Cognito User Pools",isCorrect:!1}],explanation:"Although you can grant user access to the API created with API Gateway at the individual IAM user level, it is recommended that you grant access to Amazon API Gateway APIs at the IAM group level. Read more: https://aws.amazon.com/premiumsupport/knowledge-center/iam-authentication-api-gateway/"},{question:"Which of these RDS features will force your developers to change their application code to fully take advantage of it?",answers:[{text:"RDS Read Replicas",isCorrect:!0},{text:"RDS Multi AZ",isCorrect:!1},{text:"RDS backups",isCorrect:!1},{text:"RDS Maintenance Upgrades",isCorrect:!1}],explanation:"RDS read replicas create new databases that have to be referenced by your application code Read more:  https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html"},{question:"You have enabled X-Ray integration with AWS lambda but can't see any data go through X-Ray. What's a likely reason of this?",answers:[{text:"Fix the IAM Role",isCorrect:!0},{text:"Enable X-Ray sampling",isCorrect:!1},{text:"X-Ray only works with AWS Lambda aliases",isCorrect:!1},{text:"Change the security groups rules",isCorrect:!1}],explanation:"Create an IAM role with write permissions and assign it to the resources running your application. AWSXRayDaemonWriteAccess includes permission to upload traces. Read more: https://docs.aws.amazon.com/xray/latest/devguide/xray-permissions.html"},{question:"A CloudFormation stack name must be unique across  ?",answers:[{text:"the same region",isCorrect:!0},{text:"the same account",isCorrect:!1},{text:"all accounts",isCorrect:!1},{text:"doesn't need to be unique",isCorrect:!1}],explanation:"The stack name is an identifier that helps you find a particular stack from a list of stacks. A stack name can contain only alphanumeric characters (case-sensitive) and hyphens. It must start with an alphabetic character and can't be longer than 128 characters.Must be unique within a region"},{question:"Your instance in us-east-1a just got terminated, and the attached EBS volume is now available. Your colleague tells you he can't seem to attach it to your instance in us-east-1b.",answers:[{text:"EBS volumes are AZ locked",isCorrect:!0},{text:"EBS volumes are region locked",isCorrect:!1},{text:"He's missing IAM permissions",isCorrect:!1}],explanation:"You can attach an available EBS volume to one of your instances that is in the same Availability Zone as the volume. Read more: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html"},{question:"All of a sudden, your CodePipeline breaks because it says it cannot find the target Elastic Beanstalk environment to deploy your application to. What should you do to find the root cause of this problem? (select two)",answers:[{text:'Look in CloudTrail for a "delete" event in Elastic Beanstalk',isCorrect:!0},{text:"Look in IAM for Policy changes",isCorrect:!0},{text:"Look in CloudFormation for deletions",isCorrect:!1},{text:"Look in CodePipeline for changes",isCorrect:!1},{text:"Look in S3 for access logs",isCorrect:!1}],explanation:"CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This event history simplifies security analysis, resource change tracking, and troubleshooting.Filter the CloudTrail data for delete events should help you in debugging the situation.Verify that appropriate policies have been attached to AWS Identity and Access Management (IAM) users or groups for accessing the target Elastic Beanstalk environements. Read more: https://aws.amazon.com/cloudtrail/https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html"},{question:"Which hook step should be used in appspec.yml file to ensure the application is properly running after being deployed?",answers:[{text:"ValidateService",isCorrect:!0},{text:"AfterInstall",isCorrect:!1},{text:"ApplicationStart",isCorrect:!1},{text:"AllowTraffic",isCorrect:!1}],explanation:"ValidateService is the last deployment lifecycle event. It is used to verify the deployment was completed successfully. Read more: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order"},{question:"In CloudFormation, Stack-A exports some outputs that are referenced by Stack-B and Stack-C. In which order should you delete the stacks? ",answers:[{text:"Stack B, then Stack C, then Stack A",isCorrect:!0},{text:"Stack A, then Stack B, then Stack C",isCorrect:!1},{text:"Stack A, Stack C then Stack B",isCorrect:!1},{text:"Stack C then Stack A then Stack B",isCorrect:!1}],explanation:"You could delete C, then B then A. A must come after B and C because the exported outputs are referenced there Read more: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html"},{question:"You would like to control Elastic Beanstalk configuration dynamically as code. How can you achieve that?",answers:[{text:"Include config files in .ebextensions/ at the root of your source code",isCorrect:!0},{text:"Deploy a CloudFormation wrapper",isCorrect:!1},{text:"Use SSM parameter store as an input to your Elastic Beanstalk Configurations",isCorrect:!1},{text:"Use an AWS Lambda hook",isCorrect:!1}],explanation:"The option_settings section of a configuration file defines values for configuration options. Configuration options let you configure your Elastic Beanstalk environment, the AWS resources in it, and the software that runs your application. Configuration files are only one of several ways to set configuration options. Read more: see option_settings in https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html"},{question:"What security mechanisms are available out of the box and require no code change for Kinesis Streams? (select two)",answers:[{text:"KMS encryption for data at rest",isCorrect:!0},{text:"Encryption in flight with HTTPS endpoint",isCorrect:!0},{text:"SSE-C encryption",isCorrect:!1},{text:"Client Side Encryption",isCorrect:!1},{text:"Envelope Encryption",isCorrect:!1}],explanation:"Server-side encryption is a feature in Amazon Kinesis Data Streams that automatically encrypts data before it's at rest by using an AWS KMS customer master key (CMK) you specify. Data is encrypted before it's written to the Kinesis stream storage layer, and decrypted after it's retrieved from storage. As a result, your data is encrypted at rest within the Kinesis Data Streams service. Also the HTTPS protocol ensures that data inflight is encrypted as well. Read more: https://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html"},{question:"Where must CloudFormation templates be stored in order to be loaded by CloudFormation?",answers:[{text:"S3",isCorrect:!0},{text:"EBS",isCorrect:!1},{text:"EFS",isCorrect:!1},{text:"ECR",isCorrect:!1}],explanation:"If you upload a local template file, AWS CloudFormation uploads it to an Amazon Simple Storage Service (Amazon S3) bucket in your AWS account. If you don't already have an S3 bucket that was created by AWS CloudFormation, it creates a unique bucket for each Region in which you upload a template file. If you already have an S3 bucket that was created by AWS CloudFormation in your AWS account, AWS CloudFormation adds the template to that bucket. Read more: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html"},{question:"Your application is using an Application Load Balancer. It turns out your application only sees traffic coming from private IP which are in fact your load balancer's. What should you do to find the true IP of the clients connected to your website?",answers:[{text:"Look into the X-Forwarded-For header in the backend",isCorrect:!0},{text:"Modify the front-end of the website so that the users send their IP in the requests",isCorrect:!1},{text:"Look into the X-Forwarded-Proto header in the backend",isCorrect:!1}],explanation:"The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header. Elastic Load Balancing stores the IP address of the client in the X-Forwarded-For request header and passes the header to your server. Read more: https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html"},{question:"You would like to deploy an application and define all the database authentication and authorization using IAM policies. Which database will be the best fit?",answers:[{text:"DynamoDB",isCorrect:!0},{text:"RDS MySQL",isCorrect:!1},{text:"ElastiCache",isCorrect:!1},{text:"MongoDB",isCorrect:!1}],explanation:"Access to Amazon DynamoDB requires credentials. Those credentials must have permissions to access AWS resources, such as an Amazon DynamoDB table. You can use AWS Identity and Access Management (IAM) to help secure access to your DynamoDB service. Read more: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/authentication-and-access-control.html"},{question:"You would like to persist data of your EC2 instances so that the data can be detached and mounted to other instances if need be. Which technology should you choose?",answers:[{text:"EBS",isCorrect:!0},{text:"S3",isCorrect:!1},{text:"FTP",isCorrect:!1},{text:"RDS",isCorrect:!1}],explanation:"Amazon Elastic Block Store (Amazon EBS) provides persistent block storage volumes for use with Amazon EC2 instances in the AWS Cloud. Each Amazon EBS volume is automatically replicated within its Availability Zone to protect you from component failure, offering high availability and durability. Read more: rhttps://aws.amazon.com/ebs/"},{question:"Your company does not trust Amazon web services for their encryption services, but would like to store data encrypted in Amazon S3. Which method suits best these requirements?",answers:[{text:"Client Side Encryption",isCorrect:!0},{text:"SSE-C",isCorrect:!1},{text:"SSE-S3",isCorrect:!1},{text:"SSE-KMS",isCorrect:!1}],explanation:"You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools. Read more: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html"},{question:"I have an ASG and an ALB, and I setup my ASG to get health status of instances thanks to my ALB. One instance has just been reported unhealthy. What will happen?",answers:[{text:"The ASG will terminate the EC2 Instance",isCorrect:!0},{text:"The ASG will detach the EC2 instance from the group, and leave it running",isCorrect:!1},{text:"The ASG will keep the instance running and re-start the application",isCorrect:!1}],explanation:"To maintain the same number of instances, Amazon EC2 Auto Scaling performs a periodic health check on running instances within an Auto Scaling group. When it finds that an instance is unhealthy, it terminates that instance and launches a new one.Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance. Read more: https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance/https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html#replace-unhealthy-instance"},{question:"What's the maximum data size that can be encrypted by KMS?",answers:[{text:"4KB",isCorrect:!0},{text:"1MB",isCorrect:!1},{text:"10MB",isCorrect:!1},{text:"16KB",isCorrect:!1}],explanation:"You can encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information. Read more: https://docs.aws.amazon.com/kms/latest/APIReference/API_Encrypt.html"},{question:"Which of the following services will not help you in understanding an authorization exception that happened when you ran a CLI command?",answers:[{text:"CloudWatch",isCorrect:!0},{text:"IAM",isCorrect:!1},{text:"CloudTrail",isCorrect:!1},{text:"STS",isCorrect:!1}],explanation:"CloudWatch helps only with metrics monitoring. The rest is relevant to help you debug authorization exceptions"},{question:"Which function should you use to reference a CloudFormation parameter?",answers:[{text:"!Ref",isCorrect:!0},{text:"!GetAtt",isCorrect:!1},{text:"!Param",isCorrect:!1},{text:"!Join",isCorrect:!1}],explanation:"The intrinsic function Ref returns the value of the specified parameter or resource.When you specify a parameter's logical name, it returns the value of the parameter.When you specify a resource's logical name, it returns a value that you can typically use to refer to that resource, such as a physical ID. Read more: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-ref.html"},{question:"You are looking for a pub sub solution where you send messages once and they are received by many different locations. Which technology helps you achieve that?",answers:[{text:"SNS",isCorrect:!0},{text:"SQS",isCorrect:!1},{text:"Kinesis",isCorrect:!1},{text:"Lambda",isCorrect:!1}],explanation:"Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.  Read more: https://aws.amazon.com/pub-sub-messaging/"},{question:"You have a micro service architecture and find it hard to debug which microservice is failing when requests are done in your AWS infrastructure. Which service can help remedy the situation?",answers:[{text:"X-Ray",isCorrect:!0},{text:"CloudTrail",isCorrect:!1},{text:"CloudWatch Logs",isCorrect:!1},{text:"Systems Manager",isCorrect:!1}],explanation:"AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application's underlying components. Read more: https://aws.amazon.com/xray/"},{question:"How can you easily encrypt your build artifacts coming out of CodeBuild?",answers:[{text:"Specify a KMS key to use",isCorrect:!0},{text:"Use an AWS Lambda Hook",isCorrect:!1},{text:"Use the AWS Encryption SDK",isCorrect:!1},{text:"Use In Flight encryption (SSL)",isCorrect:!1}],explanation:" Read more: CloudFormation doc shows you can encrypt output in S3: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-codebuild-project-artifacts.html"},{question:"You want to send customised email to your users. You should use",answers:[{text:"SES",isCorrect:!0},{text:"SNS",isCorrect:!1},{text:"SQS with Lambda",isCorrect:!1},{text:"Kinesis",isCorrect:!1}],explanation:"Amazon Simple Email Service (Amazon SES) is a cloud-based email sending service designed to help digital marketers and application developers send marketing, notification, and transactional emails.  Read more: https://aws.amazon.com/ses/"},{question:"You would like to use a service to create virtual machines in the cloud, and being able to fully control them in order to run your database software. Which service would you choose?",answers:[{text:"EC2",isCorrect:!0},{text:"RDS",isCorrect:!1},{text:"ElastiCache",isCorrect:!1},{text:"DynamoDB",isCorrect:!1}],explanation:"Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers. Read more: https://aws.amazon.com/ec2/"},{question:"You have performed changes in your API gateway and these changes are not active. What should you do?",answers:[{text:"Deploy to a stage",isCorrect:!0},{text:"Enable Custom Authorizer",isCorrect:!1},{text:"Use Stage Variables",isCorrect:!1},{text:"Check IAM permissions",isCorrect:!1}],explanation:"Each stage is a snapshot of the API and is made available for the client to call. Every time you update an API, which includes modification of methods, integrations, authorizers, and anything else other than stage settings, you must redeploy the API to an existing stage or to a new stage. As your API evolves, you can continue to deploy it to different stages as different versions of the API. Read more: https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html"},{question:'You have created the S3 bucket named "prod", but the bucket creation failed. You triple checked and you are certain no other bucket in your AWS account exist with the name "prod". What\'s the most likely reason for this issue?',answers:[{text:'Someone else in the world already created a bucket named "prod"',isCorrect:!0},{text:"You have the wrong IAM permissions",isCorrect:!1},{text:"The bucket name is too short",isCorrect:!1},{text:'"prod" is a reserved bucket name',isCorrect:!1}],explanation:"Bucket names must be unique across all existing bucket names in Amazon S3. Read more: https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html"}]},{id:"aws-developer-4540362",title:"Practice Test #3 (AWS Certified Developer Associate - DVA-C02)",description:"Practice questions for AWS Developer Associate (DVA-C02) exam — covering core AWS services like SQS, Lambda, DynamoDB, and more.",questions:[{question:"You would like to allow a production on-premise instance to run code using the AWS SDK. You already have an internal secure way of identifying production machines within your infrastructure. What's the most secure way of achieving this?",answers:[{text:"Enable Federated Identities integration with Cognito",isCorrect:!0},{text:"Put your IAM credentials onto the production instance",isCorrect:!1},{text:"IAM Roles for EC2",isCorrect:!1},{text:"Create an IAM user for your production instances, and run aws configure there",isCorrect:!1}],explanation:"Federation enables you to manage access to your AWS Cloud resources centrally. With federation, you can use single sign-on (SSO) to access your AWS accounts using credentials from your corporate directory. Read more: https://aws.amazon.com/identity/federation/"},{question:"You want to setup a highly-available application that consumes messages from SQS. Which Elastic Beanstalk fits your needs?",answers:[{text:"ASG Worker Nodes",isCorrect:!0},{text:"ASG and ELB",isCorrect:!1},{text:"Single Instance Worker node",isCorrect:!1},{text:"Single Instance with Elastic IP",isCorrect:!1}],explanation:"If your application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker environment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your application stays responsive under load. Read more: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html"},{question:"Your are deploying a mobile application that needs to access to API. Users will need to register and then access your API. Which authentication mechanism do you advise for your API Gateway layer?",answers:[{text:"Cognito User Pools",isCorrect:!0},{text:"IAM permissions with sigv4",isCorrect:!1},{text:"Lambda Authorizer",isCorrect:!1}],explanation:"A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Facebook or Amazon, and through SAML identity providers. Read more: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html"},{question:"Your API Gateway is started to feel pressured by the same GET REST API calls done over and over by a variety of clients. The results of these GET API calls is always the same. What can you do to improve performance and reduce costs?",answers:[{text:"Enable API Gateway Caching",isCorrect:!0},{text:"Use Mapping Templates",isCorrect:!1},{text:"Use Stage Variables",isCorrect:!1},{text:"Enable In Flight Encryption",isCorrect:!1}],explanation:"You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds.  Read more: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html"},{question:"Your application needs to auto scale based on the number of requests that are successfully processed by your algorithmic engine. The logic is complex. How should you achieve this goal?",answers:[{text:"Create a custom metric in CloudWatch and make your instances send data to it using PutMetricData. Create an alarm based on that metric",isCorrect:!0},{text:"Create a custom alarm for your ASG and make your instances trigger the alarm using PutAlarmData API",isCorrect:!1},{text:"Enable detailed monitoring for EC2 and ASG",isCorrect:!1},{text:"Migrate your application to AWS Lambda",isCorrect:!1}],explanation:"Amazon CloudWatch enables you to retrieve statistics as an ordered set of time-series data, known as metrics. You can use these metrics to verify that your system is performing as expected.A CloudWatch alarm is an object that monitors a single metric over a specific period. A metric is a variable that you want to monitor, such as average CPU usage of the EC2 instances, or incoming network traffic from many different EC2 instances. The alarm changes its state when the value of the metric breaches a defined range and maintains the change for a specified number of periods. Read more: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html#CloudWatchAlarmhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html"},{question:"You are looking to store all your code in a service that is private, secure, IAM managed and integrated with git command line. Which service would you choose?",answers:[{text:"CodeCommit",isCorrect:!0},{text:"S3",isCorrect:!1},{text:"GitHub",isCorrect:!1},{text:"CodeGit",isCorrect:!1}],explanation:"AWS CodeCommit is a fully-managed source control service that hosts secure Git-based repositiories. It makes it easy for teams to collaborate on code in a secure and highly scalable ecosystem. Read more: https://aws.amazon.com/codecommit/"},{question:"You are using the AWS SDK but haven't specified any region. You haven't run aws configure either and are not using the environment variable AWS_REGION. You are running the commands from your computer which is connected to a network in Europe. Which region will you API calls be made in?",answers:[{text:"us-east-1",isCorrect:!0},{text:"us-east-2",isCorrect:!1},{text:"us-west-1",isCorrect:!1},{text:"eu-west-1",isCorrect:!1}],explanation:"If you don't select a region, then us-east-1 will be used by default. Read more: https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html (setting the AWS region)"},{question:"An IAM user has two policies attached.The first policy states that the user has explicit Denied on all EC2 actions. The second policy states that the user has Allow on EC2:Describe permission. The user tries to describe an EC2 instance using the CLI. What will happen?",answers:[{text:"The user will get denied because the policy has an explicit denied",isCorrect:!0},{text:"The user will get allowed because it has an explicit allow",isCorrect:!1},{text:"This IAM user is invalid and the policy conflict must be resolved first",isCorrect:!1},{text:"The order of the policy matters. If the policy 1 is before 2, then it's denied, else if policy 2 is before 1, then it's allowed",isCorrect:!1}],explanation:"An explicit deny in any policy overrides any allows. Read more: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html"},{question:"You have an RDS database deployed within your VPC, in a private subnet. You would like your Lambda function to connect to it. How can you achieve this?",answers:[{text:"Deploy in a VPC and assign a Security Group",isCorrect:!0},{text:"Assign an IAM role",isCorrect:!1},{text:"Use Environment variables to pass in the RDS connection string",isCorrect:!1},{text:"Use aliases",isCorrect:!1}],explanation:" Read more: https://docs.aws.amazon.com/lambda/latest/dg/vpc-rds.html"},{question:"You would like to perform a point in time restore for your PostgreSQL database managed by RDS. What feature you should use?",answers:[{text:"Enable RDS backups and use the built-in feature",isCorrect:!0},{text:"Use AWS Lambda to track the database binlog and persist it in S3.",isCorrect:!1},{text:"Enable RDS Stream, persisted in DynamoDB and use the restore feature from DynamoDB when needed",isCorrect:!1},{text:"Enable RDS Multi AZ",isCorrect:!1}],explanation:"You can restore a DB instance to a specific point in time, creating a new DB instance. To determine the latest restorable time for a DB instance, use the AWS CLI describe-db-instances command and look at the value returned in the LatestRestorableTime field for the DB instance. In the AWS Management Console, this property is visible as the Latest restore time for the DB instance. You can restore to any point in time during your backup retention period. Read more: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIT.html"},{question:"Which of the following isn't a way to manage S3 security?",answers:[{text:"Security Groups",isCorrect:!0},{text:"IAM Policies",isCorrect:!1},{text:"ACLs",isCorrect:!1},{text:"Bucket Policies",isCorrect:!1}],explanation:"Access policies you attach to your resources (buckets and objects) are referred to as resource-based policies. For example, bucket policies and access control lists (ACLs) are resource-based policies. You can also attach access policies to users in your account. These are called user policies. You may choose to use resource-based policies, user policies, or some combination of these to manage permissions to your Amazon S3 resources. Read more: https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-access-control.html"},{question:"Your entire stack is integrated with AWS X-Ray but now you have too much data going into X-Ray and your costs are skyrocketting. Which feature can help you reduce costs?",answers:[{text:"Enable X-Ray sampling",isCorrect:!0},{text:"Filter the amount of data sent client side",isCorrect:!1},{text:"Custom configuration for the X-Ray agents",isCorrect:!1},{text:"Implement a network security rule",isCorrect:!1}],explanation:"To ensure efficient tracing and provide a representative sample of the requests that your application serves, the X-Ray SDK applies a sampling algorithm to determine which requests get traced. By default, the X-Ray SDK records the first request each second, and five percent of any additional requests. Read more: https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-sampling"},{question:"You are running a website with a load balancer and 10 EC2 instances. Your users are complaining about the fact that your website always asks them to re-authenticate when they switch pages. You are puzzled, because it's working just fine on your machine and in the dev environment with 1 server. What could be the reason?",answers:[{text:"The Load Balancer does not have stickiness enabled",isCorrect:!0},{text:"The application must have a bug",isCorrect:!1},{text:"The EC2 instances log out users because they don't see their true Ips",isCorrect:!1}],explanation:"By default, a Classic Load Balancer routes each request independently to the registered instance with the smallest load. However, you can use the sticky session feature (also known as session affinity), which enables the load balancer to bind a user's session to a specific instance. This ensures that all requests from the user during the session are sent to the same instance. Read more: https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html"},{question:"You would like to send messages of over 1MB to SQS but you know the limit is 256KB for messages. What should you do?",answers:[{text:"Use the SQS Extended Client",isCorrect:!0},{text:"Get a service limit increase from AWS",isCorrect:!1},{text:"Use gzip compression",isCorrect:!1},{text:"Use the MultiPart API",isCorrect:!1}],explanation:"To send messages larger than 256 KB, you can use the Amazon SQS Extended Client Library for Java. This library allows you to send an Amazon SQS message that contains a reference to a message payload in Amazon S3. The maximum payload size is 2 GB. Read more: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-limits.html"},{question:"You would like to audit which employees and applications are using your KMS key. Where should you look?",answers:[{text:"CloudTrail",isCorrect:!0},{text:"KMS Key Logs",isCorrect:!1},{text:"IAM",isCorrect:!1},{text:"VPC Flow Logs",isCorrect:!1}],explanation:"With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This event history simplifies security analysis, resource change tracking, and troubleshooting. Read more: https://aws.amazon.com/cloudtrail/"},{question:"Your users are complaining that they have to re-login into your website very often. You have deployed a traditional architecture of a 3-tier web application with a load balancer, an auto scaling group and an RDS database. The session information is stored in-memory for each application. What can you do without changing the code to improve the situation?",answers:[{text:"Add an ElastiCache Cluster",isCorrect:!0},{text:"Enable RDS read replicas",isCorrect:!1},{text:"Enable Load Balancer stickiness",isCorrect:!1},{text:"Use Elastic IP",isCorrect:!1}],explanation:"In order to address scalability and to provide a shared data storage for sessions that can be accessible from any individual web server, you can abstract the HTTP sessions from the web servers themselves. A common solution to for this is to leverage an ElastiCache service offerings which is an In-Memory Key/Value store such as Redis and Memcached. Read more: https://aws.amazon.com/caching/session-management/"},{question:"Which of the following AWS services rely on Cloudformation to provision resources? (select two)",answers:[{text:"Elastic Beanstalk",isCorrect:!0},{text:"Step Function",isCorrect:!0},{text:"Autoscaling",isCorrect:!1},{text:"CodeBuild",isCorrect:!1},{text:"Lambda",isCorrect:!1}],explanation:"You can use the Resources key in a configuration file to create and customize AWS resources in your environment. Resources defined in configuration files are added to the AWS CloudFormation template used to launch your environment. All AWS CloudFormation resources types are supported.You can now use CloudFormation templates to create and delete Step Functions state machines and Activities. Read more: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-resources.htmlhttps://aws.amazon.com/about-aws/whats-new/2017/02/aws-cloudformation-adds-support-for-aws-step-functions/"},{question:"Application Load Balancers handle all these protocols except",answers:[{text:"TCP",isCorrect:!0},{text:"HTTP",isCorrect:!1},{text:"HTTPS",isCorrect:!1},{text:"Websocket",isCorrect:!1}],explanation:"The Application Load Balancer supports following protocols: WebSocket and HTTP/2. Read more: https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/"},{question:"You would like to intercept state changes for CodePipeline (such as when a pipeline fails), which integration must you use?",answers:[{text:"CloudWatch Event Rules",isCorrect:!0},{text:"SNS",isCorrect:!1},{text:"SQS",isCorrect:!1},{text:"SES",isCorrect:!1}],explanation:"Amazon CloudWatch Events is a web service that monitors your AWS resources and the applications you run on AWS. You can use Amazon CloudWatch Events to detect and react to changes in the state of a pipeline, stage, or action. Then, based on rules you create, CloudWatch Events invokes one or more target actions when a pipeline, stage, or action enters the state you specify in a rule.  Read more: https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html"},{question:"You are looking to perform some analytics in your database. You should use",answers:[{text:"Redshift",isCorrect:!0},{text:"RDS",isCorrect:!1},{text:"DynamoDB",isCorrect:!1},{text:"ElastiCache",isCorrect:!1}],explanation:"Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of data and scale to a petabyte or more. This enables you to use your data to acquire new insights for your business and customers. Read more: https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html"},{question:"An RDS database is experiencing read heavy workload (99% of read versus 1% of writes) and the number of rows you have is quite small. You would like to scale the read performance to possibly high numbers as your website scales, but don't want to have to experience linear cost as the read load increases. You don't mind changing the application code logic. Which solution fits best this use case?",answers:[{text:"Setup ElastiCache in front of RDS",isCorrect:!0},{text:"Setup RDS Read Replicas",isCorrect:!1},{text:"Use Redshift",isCorrect:!1},{text:"Switch your application code to AWS Lambda",isCorrect:!1}],explanation:" Read more: https://aws.amazon.com/blogs/database/automating-sql-caching-for-amazon-elasticache-and-amazon-rds/"},{question:"What's absolutely necessary to run on your EC2 instances or on-premise servers in order for CodeDeploy to work?",answers:[{text:"CodeDeploy Agent",isCorrect:!0},{text:"AWS CloudWatch Log Agent",isCorrect:!1},{text:"Mount EBS volumes on the EC2 instances",isCorrect:!1},{text:"Have a load balancer in front of your instances",isCorrect:!1}],explanation:"The AWS CodeDeploy agent is a software package that, when installed and configured on an instance, enables that instance to be used in AWS CodeDeploy deployments. Read more: https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent-operations-install.html"},{question:"You want to send email notifications any time someone comments on a pul request in CodeCommit. How can you achieve that with no custom code?",answers:[{text:"CloudWatch Event Rules",isCorrect:!0},{text:"SNS + SES",isCorrect:!1},{text:"AWS Lambda",isCorrect:!1},{text:"SQS",isCorrect:!1}],explanation:"You can set up notifications for a repository so that repository users receive emails about the repository event types you specify. When you configure notifications, AWS CodeCommit creates an Amazon CloudWatch Events rule for your repository. This rule responds to the event types you select from the preconfigured options in the AWS CodeCommit console. Read more: https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-repository-email.html"},{question:"You have an auto scaling group with a minimum capacity of 1 and a maximum capacity of 5, configured to launch instances across 3 AZ. It is configured to scale based on a target CPU Utilisation of 35%. During a low utilisation period of your application, an entire AWS availability zone went down and your application experienced downtime. What should you do to ensure that your application is highly available?",answers:[{text:"Increase the minimum instances in the ASG to 2",isCorrect:!0},{text:"Change the target auto scaling policy for network bytes",isCorrect:!1},{text:"Configure ASG fast failover",isCorrect:!1},{text:"Enable RDS Multi AZ",isCorrect:!1}],explanation:" Read more: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html"},{question:"What is the maximum size of AWS Lambda environment variables?",answers:[{text:"4KB",isCorrect:!0},{text:"32KB",isCorrect:!1},{text:"1MB",isCorrect:!1},{text:"1KB",isCorrect:!1}],explanation:"There is no limit to the number of environment variables you can create as long as the total size of the set does not exceed 4 KB. Read more: https://docs.aws.amazon.com/lambda/latest/dg/env_variables.html"},{question:"You would like to trigger an external HTTP service (also called webhook) when someone pushes code to AWS CodeCommit. How can you implement that with no code changes?",answers:[{text:"SNS + HTTP Integration",isCorrect:!0},{text:"AWS Lambda",isCorrect:!1},{text:"AWS SES",isCorrect:!1},{text:"CloudWatch Event Rules",isCorrect:!1}],explanation:"You can create a trigger for an AWS CodeCommit repository so that events in that repository trigger notifications from an Amazon Simple Notification Service (Amazon SNS) topic. You might want to create a trigger to an Amazon SNS topic to enable users to subscribe to notifications about repository events, such as the deletion of branches. Read more: https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-sns.html"},{question:"You would like to audit all the denied API calls that have been made in your AWS infrastructure to ensure one of your employees isn't trying to go beyond his rights. Which technology allows you to do this?",answers:[{text:"CloudTrail",isCorrect:!0},{text:"VPC Flow Logs",isCorrect:!1},{text:"IAM",isCorrect:!1},{text:"CloudWatch Logs",isCorrect:!1}],explanation:"With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This event history simplifies security analysis, resource change tracking, and troubleshooting. Read more: https://aws.amazon.com/cloudtrail/"},{question:"What is the maximum resolution you can obtain to use custom metrics in CloudWatch?",answers:[{text:"1 second",isCorrect:!0},{text:"5 seconds",isCorrect:!1},{text:"10 seconds",isCorrect:!1},{text:"30 seconds",isCorrect:!1}],explanation:"Using the existing PutMetricData API, you can now publish Custom Metrics down to 1-second resolution. This gives you more immediate visibility and greater granularity into the state and performance of your custom applications, such as observing short-lived spikes and functions. Read more: https://aws.amazon.com/about-aws/whats-new/2017/07/amazon-cloudwatch-introduces-high-resolution-custom-metrics-and-alarms/"},{question:"Your company has a requirement to keep a monthly database backup for three years, for compliance and audit purposes. How can you implement that feature?",answers:[{text:"Create a cron event in CloudWatch, which triggers an AWS Lambda function that triggers the snapshotting feature",isCorrect:!0},{text:"Enable RDS periodic backups",isCorrect:!1},{text:"Enable RDS Read replicas",isCorrect:!1},{text:"Enable RDS Multi AZ",isCorrect:!1}],explanation:"You need to provide the automation layer as it doesn't exist. This is a very common use case of Lambda. Read more: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.htmlhttps://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html"},{question:"You would like to deploy a Lambda function which unzipped amounts to 300 MB of data. How can you do that?",answers:[{text:"You need to upload a smaller function and load extra files at runtime into the /tmp directory",isCorrect:!0},{text:"You need to place a service limit increase",isCorrect:!1},{text:"You need to zip your function with higher compression ratio",isCorrect:!1},{text:"The limit of unzipped functions is 512MB so you don't need to do any changes",isCorrect:!1}],explanation:" Read more: https://docs.aws.amazon.com/lambda/latest/dg/limits.html"},{question:"At which frequency do EC2 instances report their metrics under detailed monitoring configuration?",answers:[{text:"1 minute",isCorrect:!0},{text:"2 minutes",isCorrect:!1},{text:"30 seconds",isCorrect:!1},{text:"5 minutes",isCorrect:!1}],explanation:"Data is available in 1-minute periods for an additional cost. To get this level of data, you must specifically enable it for the instance. For the instances where you've enabled detailed monitoring, you can also get aggregated data across groups of similar instances. Read more: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html"},{question:"You have 20 partitions in your DynamoDB table and 200 WCU. You observe a usage of WCU of 30 WCU, and you are getting ProvisionedThroughputExceptions. What is the likely cause of this?",answers:[{text:"You have a hot partition",isCorrect:!0},{text:"Your IAM policy is wrong",isCorrect:!1},{text:"WCU are applied across to all your DynamoDB tables, not just one table",isCorrect:!1},{text:"CloudWatch monitoring is lagging",isCorrect:!1}],explanation:'It\'s not always possible to distribute read and write activity evenly all the time. When data access is imbalanced, a "hot" partition can receive such a higher volume of read and write traffic compared to other partitions. Read more: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html'},{question:"Your API Gateway is receiving requests from an authorized external domain name you don't control. As a result, your costs are sky-rocketting. What should you do to prevent illegitimate requests?",answers:[{text:"Restrict CORS",isCorrect:!0},{text:"Use Mapping Templates",isCorrect:!1},{text:"Assign a Security Groups to your API Gateway",isCorrect:!1},{text:"Enable Caching",isCorrect:!1}],explanation:"When your API's resources receive requests from a domain other than the API's own domain and you want to restrict servicing these requests, you must disable cross-origin resource sharing (CORS) for selected methods on the resource.  Read more: https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html"},{question:"Which of the following operations do have eventual consistency in S3? (select two)",answers:[{text:"Updating an existing object",isCorrect:!0},{text:"Deleting an existing object",isCorrect:!0},{text:"Creating a new object",isCorrect:!1}],explanation:"Amazon S3 offers eventual consistency for overwrite PUTS and DELETES in all regions. Read more: https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html"},{question:"You would like your users to be able to access your application using a stable URL. The architecture of your application may evolve but the users should not be aware of it. How can you do that?",answers:[{text:"Expose a domain name created with Route53",isCorrect:!0},{text:"Use the domain name provided by your ELB",isCorrect:!1},{text:"Use the domain name provided by your API gateway",isCorrect:!1},{text:"Use the domain name provided by CloudFront",isCorrect:!1}],explanation:"Route53 allows you to change the record type of your domain and lets you be completely free for what architecture you want to implement or change in the future Read more: https://aws.amazon.com/route53/"},{question:"Your lambda function acts as a proxy for applications to upload data to S3. You notice that each time your function is called, a new S3 client is created and that heavily impacts your function performance. What should you do to improve the performance?",answers:[{text:"Move the S3 client initialisation out of your function handler",isCorrect:!0},{text:"Assign more RAM to the function",isCorrect:!1},{text:"Change the IAM role",isCorrect:!1}],explanation:"Limit the re-initialization of variables/objects on every invocation. Instead use static initialization/constructor, global/static variables and singletons. Keep alive and reuse connections (HTTP, database, etc.) that were established during a previous invocation. Read more: https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html"},{question:"Your application is running behind an application load balancer and you would like to be able to track client IP. How do you get that information in your application?",answers:[{text:"We should look at the header X-Forwarded-For",isCorrect:!0},{text:"We should change the client application to send their IP as part of the payloads",isCorrect:!1},{text:"The IP incoming to our EC2 server is the one of the clients, therefore we do not have to do anything",isCorrect:!1},{text:"Change the security groups rules",isCorrect:!1}],explanation:"The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header. Read more: https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html"},{question:"You would like to allow a production on-premise instance to run code using the AWS SDK. You already have an internal secure way of identifying production machines within your infrastructure. What's the most secure way of achieving this?",answers:[{text:"Create one CodePipeline for your entire flow, and add a manual approval step",isCorrect:!0},{text:"Create multiple CodePipelines for each environment and link them using AWS Lambda",isCorrect:!1},{text:"Create deeply integrated AWS CodePipelines for each environment",isCorrect:!1},{text:"Use CodeDeploy",isCorrect:!1}],explanation:"It is fine to chain up the environments and have complicated CodePipeline. A manual step before deploying to production is also very common Read more:  https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html"},{question:"You are looking to provide better latency for retrieving files from S3 for your users around the globe. Which technology can help you?",answers:[{text:"CloudFront",isCorrect:!0},{text:"ElastiCache",isCorrect:!1},{text:"S3 Caching",isCorrect:!1},{text:"EFS",isCorrect:!1}],explanation:"Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. Read more: https://aws.amazon.com/cloudfront/"},{question:"Your manager has a requirement that anyone who pushes code in CodeCommit must have signed an CLA first. These requirements must be checked in real-time. He has provided you with the code to check whether a committer has signed a CLA. How can you implement this solution easily?",answers:[{text:"AWS Lambda",isCorrect:!0},{text:"SNS",isCorrect:!1},{text:"SES",isCorrect:!1},{text:"Cron Jobs",isCorrect:!1},{text:"Kinesis",isCorrect:!1}],explanation:"Lambda can be used to retrieve commits, analyse code and committers and perform creative tasks such as checking a CLA. Read more:  https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-lambda.html"},{question:"To get the instance id of my EC2 machine from the EC2 machine, the best thing is to  ",answers:[{text:"Query the meta data at http://169.254.169.254/latest/meta-data",isCorrect:!0},{text:"Query the user data at http://169.254.169.254/latest/user-data",isCorrect:!1},{text:'Create an IAM role and attach it to my EC2 instance so I can perform a "describe" API call',isCorrect:!1},{text:"Query the user data at http://254.169.254.169/latest/meta-data",isCorrect:!1}],explanation:"Because your instance metadata is available from your running instance, you do not need to use the Amazon EC2 console or the AWS CLI. This can be helpful when you're writing scripts to run from your instance.To view all categories of instance metadata from within a running instance, use the following URI:http://169.254.169.254/latest/meta-data/ Read more: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html"},{question:"You need a service to send and receive emails, controlled by IAM. What do you suggest?",answers:[{text:"SES",isCorrect:!0},{text:"SNS",isCorrect:!1},{text:"SQS",isCorrect:!1},{text:"Kinesis",isCorrect:!1}],explanation:"Amazon Simple Email Service (Amazon SES) is a cloud-based email sending service designed to help digital marketers and application developers send marketing, notification, and transactional emails. Read more: https://aws.amazon.com/ses/"},{question:"Which API should you call to delete all the messages an SQS queue efficiently while guaranteing uptime?",answers:[{text:"PurgeQueue",isCorrect:!0},{text:"DeleteQueue then CreateQueue",isCorrect:!1},{text:"BatchDelete",isCorrect:!1},{text:"ChangeVisibilityTimeout",isCorrect:!1}],explanation:"PurgeQueue deletes the messages in a queue specified by the QueueURL parameter. Read more: https://docs.aws.amazon.com/cli/latest/reference/sqs/purge-queue.html"},{question:"You are preparing for the biggest day of sale of the year, where your traffic will increase by 100x. You have already setup SQS standard queue. What should you do?",answers:[{text:"Do nothing, SQS scales automatically",isCorrect:!0},{text:"Increase the capacity of the SQS queue",isCorrect:!1},{text:"Enable auto scaling in the SQS queue",isCorrect:!1},{text:"Open a support ticket to pre-warm the SQS queue",isCorrect:!1}],explanation:"Amazon SQS leverages the AWS cloud to dynamically scale based on demand. SQS scales elastically with your application so you don't have to worry about capacity planning and pre-provisioning. There is no limit to the number of messages per queue, and standard queues provide nearly unlimited throughput. Read more: https://aws.amazon.com/sqs/"},{question:"What are the only resolutions you can choose for CloudWatch Alarms for High Resolution custom metrics? (select two)",answers:[{text:"10 seconds",isCorrect:!0},{text:"30 seconds",isCorrect:!0},{text:"1 second",isCorrect:!1},{text:"1 minute",isCorrect:!1}],explanation:"If you set an alarm on a high-resolution metric, you can specify a high-resolution alarm with a period of 10 seconds or 30 seconds, or you can set a regular alarm with a period of any multiple of 60 seconds. Read more: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html#high-resolution-alarms"},{question:"You have enabled a SQS DLQ for your AWS Lambda function, but it seems that even after a failed retried asynchronous invocation, the message does not appear in SQS. What's the reason?",answers:[{text:"Fix the IAM Role",isCorrect:!0},{text:"Use synchronous invocations instead",isCorrect:!1},{text:"Use aliases instead",isCorrect:!1},{text:"You need to disable encryption",isCorrect:!1}],explanation:" Read more: https://docs.aws.amazon.com/lambda/latest/dg/dlq.html"},{question:"You are getting Throttle errors when you push data to CloudWatch using the PutMetric API. What should you do?",answers:[{text:"Use Exponential Backoff for retries",isCorrect:!0},{text:"Use Kinesis Stream",isCorrect:!1},{text:"Check IAM policy",isCorrect:!1},{text:"Enable high throughput metrics in CloudWatch",isCorrect:!1}],explanation:"each AWS SDK implements exponential backoff algorithm for better flow control. The idea behind exponential backoff is to use progressively longer waits between retries for consecutive error responses. You should implement a maximum delay interval, as well as a maximum number of retries. The maximum delay interval and maximum number of retries are not necessarily fixed values, and should be set based on the operation being performed, as well as other local factors, such as network latency. Read more: https://docs.aws.amazon.com/general/latest/gr/api-retries.html"},{question:"How can you do traffic shifting deploymens and tests between two versions for your AWS Lambda?",answers:[{text:"Use AWS Lambda aliases",isCorrect:!0},{text:"Use environment variables",isCorrect:!1},{text:"Use Route53",isCorrect:!1},{text:"Deploy your Lambda in a VPC",isCorrect:!1}],explanation:"By default, an alias points to a single Lambda function version. When the alias is updated to point to a different function version, incoming request traffic in turn instantly points to the updated version. This exposes that alias to any potential instabilities introduced by the new version. To minimize this impact, you can implement the routing-config parameter of the Lambda alias that allows you to point to two different versions of the Lambda function and dictate what percentage of incoming traffic is sent to each version. Read more: https://docs.aws.amazon.com/lambda/latest/dg/lambda-traffic-shifting-using-aliases.html"},{question:"You need to obtain temporary credentials from a role you have access to. Which service do you need to use?",answers:[{text:"STS",isCorrect:!0},{text:"KMS",isCorrect:!1},{text:"SSM",isCorrect:!1},{text:"SES",isCorrect:!1}],explanation:"You can use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources Read more: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html"},{question:"Your company plans on using SQS but has strong security requirements that force every message to be encrypted at rest. Which option can help you achieve that without changing code?",answers:[{text:"Enable SQS KMS encryption",isCorrect:!0},{text:"Use the SSL endpoint",isCorrect:!1},{text:"Use Client side encryption",isCorrect:!1},{text:"Change the IAM policy",isCorrect:!1}],explanation:"You can now choose to have SQS encrypt messages stored in both Standard and FIFO queues using an encryption key provided by AWS Key Management Service (KMS). Read more: https://aws.amazon.com/blogs/aws/new-server-side-encryption-for-amazon-simple-queue-service-sqs/"},{question:"Your manager would like messages to be stored in SQS for 12 days. What can you do to achieve that?",answers:[{text:"Change the setting for message retention",isCorrect:!0},{text:"The maximum retention of messages is 7 days, therefore you can't have 12 days retention",isCorrect:!1},{text:"Enable Long Polling",isCorrect:!1},{text:"Use a FIFO queue",isCorrect:!1}],explanation:"Amazon SQS automatically deletes messages that have been in a queue for more than maximum message retention period. The default message retention period is 4 days. However, you can set the message retention period to a value from 60 seconds to 1,209,600 seconds (14 days) using the SetQueueAttributes action. Read more: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html"},{question:"You want to have parameters to control your Lambda function connection string to your RDS database. Which feature helps you achieve this?",answers:[{text:"Environment variables",isCorrect:!0},{text:"IAM Roles",isCorrect:!1},{text:"Timeouts",isCorrect:!1},{text:"Aliases",isCorrect:!1}],explanation:"Environment variables for Lambda functions enable you to dynamically pass settings to your function code and libraries, without making changes to your code. Environment variables are key-value pairs that you create and modify as part of your function configuration, using either the AWS Lambda Console, the AWS Lambda CLI or the AWS Lambda SDK. Read more: https://docs.aws.amazon.com/lambda/latest/dg/env_variables.html"},{question:"When you upload a code update to your AWS Lambda function, it creates a new version. How do you handle versioning to ensure your production environment isn't impacted by code updates done by your developers?",answers:[{text:"Use AWS Lambda aliases",isCorrect:!0},{text:"Disable AWS Lambda versioning",isCorrect:!1},{text:"Use environment variables",isCorrect:!1},{text:"Enable X-Ray integration",isCorrect:!1}],explanation:"AWS Lambda also supports creating aliases for each of your Lambda function versions. Conceptually, an AWS Lambda alias is a pointer to a specific Lambda function version. It's also a resource similar to a Lambda function, and each alias has a unique ARN. Each alias maintains an ARN for the function version to which it points. Read more: https://docs.aws.amazon.com/lambda/latest/dg/versioning-aliases.html"},{question:"You have total ordering requirements for your messages in SQS. Which queue should you use?",answers:[{text:"Use SQS FIFO",isCorrect:!0},{text:"Use SQS DLQ",isCorrect:!1},{text:"Use SQS Standard",isCorrect:!1},{text:"Use Kinesis",isCorrect:!1}],explanation:"FIFO (First-In-First-Out) queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can't be tolerated Read more: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html"},{question:"You would like to scale CodeBuild to run parallel builds. How can you achieve it?",answers:[{text:"CodeBuild scales automatically",isCorrect:!0},{text:"Increase the instance types for your CodeBuild instances",isCorrect:!1},{text:"Run CodeBuild in an ASG",isCorrect:!1},{text:"Enable CodeBuild Auto Scaling",isCorrect:!1}],explanation:"CodeBuild runs build in parallel automatically and has a 20 concurrent build limit that you can increase. Read more:  https://docs.aws.amazon.com/codebuild/latest/userguide/limits.html"},{question:"Your manager would like to track CPU Utilization of your EC2 instance with a resolution of 10 seconds. What do you advise?",answers:[{text:"Create a high resolution custom metric and push the data using cron",isCorrect:!0},{text:"Enable EC2 detailed monitoring",isCorrect:!1},{text:"Simply get it from the CloudWatch Metrics",isCorrect:!1},{text:"Open a support ticket with AWS",isCorrect:!1}],explanation:"You can also alert sooner with High-Resolution Alarms, as frequently as 10-second periods. High-Resolution Alarms allow you to react and take actions faster, and support the same actions available today with standard 1-minute alarms. Read more: https://aws.amazon.com/about-aws/whats-new/2017/07/amazon-cloudwatch-introduces-high-resolution-custom-metrics-and-alarms/"},{question:"Your company wants the most secure setup for your EBS volumes with minimal effort. You tell them",answers:[{text:"EBS volumes support in flight SSL encryption and do support encryption at rest using KMS",isCorrect:!0},{text:"EBS volumes do not support in flight SSL encryption but do support encryption at rest using KMS",isCorrect:!1},{text:"EBS volumes support in flight SSL encryption but no encryption at rest",isCorrect:!1},{text:"EBS volumes aren't secure",isCorrect:!1}],explanation:"When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted:Data at rest inside the volumeAll data moving between the volume and the instanceAll snapshots created from the volumeAll volumes created from those snapshots Read more: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html"},{question:"What are the correct statements about EBS encryption?",answers:[{text:"A snapshot of an encrypted volume is always encrypted",isCorrect:!0},{text:"Restoring a volume from an encrypted snapshot must be an encrypted volume",isCorrect:!0},{text:"A snapshot of an encrypted volume can be encrypted or unencrypted",isCorrect:!1},{text:"Restoring a volume from an encrypted snapshot can be an un-encrypted volume",isCorrect:!1},{text:"EBS encryption impacts performance",isCorrect:!1}],explanation:"Amazon EBS encryption uses AWS Key Management Service (AWS KMS) customer master keys (CMKs) when creating encrypted volumes and any snapshots created from them.  Read more: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html"},{question:"Your CodePipeline now fails and it seems it has been heavily modified. Which service can you leverage to figure out the origin of the modifications?",answers:[{text:"CloudTrail",isCorrect:!0},{text:"SNS",isCorrect:!1},{text:"SQS",isCorrect:!1},{text:"CloudWatch",isCorrect:!1}],explanation:"CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This event history simplifies security analysis, resource change tracking, and troubleshooting. Read more: https://aws.amazon.com/cloudtrail/"},{question:"What two types of deployment are supported by CodeDeploy? (select two)",answers:[{text:"In-place deployment",isCorrect:!0},{text:"Blue/Green",isCorrect:!0},{text:"Rolling with additional batches",isCorrect:!1},{text:"Rolling",isCorrect:!1},{text:"Immutable",isCorrect:!1}],explanation:"In-place deployment: The application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated.Blue/green on an EC2/On-Premises compute platform: The instances in a deployment group (the original environment) are replaced by a different set of instances (the replacement environment)Blue/green on an AWS Lambda compute platform: Traffic is shifted from your current serverless environment to one with your updated Lambda function versions. Read more: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html"},{question:"You want to be able to quickly rollback any failed deployment in Elastic Beanstalk. Costing is not a problem. Which deployment mode fits your needs?",answers:[{text:"immutable",isCorrect:!0},{text:"all at once",isCorrect:!1},{text:"rolling with additional batches",isCorrect:!1},{text:"rolling",isCorrect:!1},{text:"blue/green",isCorrect:!1}],explanation:"Immutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments. If the new instances don't pass health checks, Elastic Beanstalk terminates them, leaving the original instances untouched. Read more: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html"},{question:"You would like to troubleshoot which service is failing in your microservice architecture when your users get the error code 501. Which service should you use?",answers:[{text:"X-Ray",isCorrect:!0},{text:"API Gateway",isCorrect:!1},{text:"CloudWatch",isCorrect:!1},{text:"CloudTrail",isCorrect:!1}],explanation:"AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. Read more: https://aws.amazon.com/xray/"},{question:"You would like to perform envelope encryption. Which API call do you need to call on KMS?",answers:[{text:"GenerateDataKey",isCorrect:!0},{text:"Encrypt",isCorrect:!1},{text:"RetrieveCMK",isCorrect:!1},{text:"Decrypt",isCorrect:!1}],explanation:"GenerateDataKey returns a data encryption key that you can use in your application to encrypt data locally. Read more: https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html"},{question:"You would like to perform SQL analytics on data streams. Which solution is the easiest to deploy on AWS ?",answers:[{text:"Kinesis Stream + Analytics",isCorrect:!0},{text:"SQS + Lambda",isCorrect:!1},{text:"EC2 + EMR",isCorrect:!1},{text:"SNS + HTTP endpoint",isCorrect:!1}],explanation:"Amazon Kinesis Data Analytics is the easiest way to process streaming data in real time with standard SQL without having to learn new programming languages or processing frameworks. Amazon Kinesis Data Analytics enables you to query streaming data or build entire streaming applications using SQL, so that you can gain actionable insights and respond to your business and customer needs promptly. Read more: https://aws.amazon.com/kinesis/data-analytics/"},{question:"You have enabled a DLQ for AWS Lambda. Which of the following will put a message into a DLQ after being processed by AWS Lambda? (select two)",answers:[{text:"The invocation was asynchronous",isCorrect:!0},{text:"The invocation failed",isCorrect:!0},{text:"The invocation was synchronous",isCorrect:!1},{text:"The invocation succeeded",isCorrect:!1},{text:"The invocation failed only once but succeeded afterwards",isCorrect:!1}],explanation:"Any Lambda function invoked asynchronously is retried twice before the event is discarded. If the retries fail and you're unsure why, use Dead Letter Queues (DLQ) to direct unprocessed events to an Amazon SQS queue. Read more: https://docs.aws.amazon.com/lambda/latest/dg/dlq.html"}]},{id:"aws-developer-4540364",title:"Practice Test #4 (AWS Certified Developer Associate - DVA-C02)",description:"Practice questions for AWS Developer Associate (DVA-C02) exam — covering core AWS services like SQS, Lambda, DynamoDB, and more.",questions:[{question:"You would like to use SSE-S3 encryption mechanism. What header must you set in your request?",answers:[{text:'"x-amz-server-side-encryption": "AES256"',isCorrect:!0},{text:'"x-amz-server-side-encryption": "SSE-S3"',isCorrect:!1},{text:'"x-amz-server-side-encryption": "SSE-KMS"',isCorrect:!1},{text:'"x-amz-server-side-encryption": "aws:kms"',isCorrect:!1}],explanation:"To request server-side encryption using the object creation REST APIs, provide the , x-amz-server-side-encryption request header. Read more: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html"},{question:"You want to be able to stage your deployment to one third of your fleet, then another third and finally the last third. How can you best achieve this using CodeDeploy?",answers:[{text:"CodeDeploy deployment groups",isCorrect:!0},{text:"Tags",isCorrect:!1},{text:"CodeDeploy Hooks",isCorrect:!1},{text:"Use multiple CodeDeploy",isCorrect:!1}],explanation:"In an EC2/On-Premises deployment, a deployment group is a set of individual instances targeted for a deployment. A deployment group contains individually tagged instances, Amazon EC2 instances in Amazon EC2 Auto Scaling groups, or both. Read more: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html"},{question:"You would like to query items based on the same partition key but an attribute that is not part of the same sort key. What should you do?",answers:[{text:"Create a LSI",isCorrect:!0},{text:"Call Scan",isCorrect:!1},{text:"Create a GSI",isCorrect:!1},{text:"Migrate away from DynamoDB",isCorrect:!1}],explanation:"Some applications only need to query data using the base table's primary key; however, there may be situations where an alternate sort key would be helpful. To give your application a choice of sort keys, you can create one or more local secondary indexes on a table and issue Query or Scan requests against these indexes. Read more: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html"},{question:"You are designing a high performance application that will require millions of connections to be handled, as well as low latency. The best Load Balancer for this is",answers:[{text:"Network Load Balancer",isCorrect:!0},{text:"Application Load Balancer",isCorrect:!1},{text:"Elastic Load Balancer",isCorrect:!1}],explanation:"A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration. Read more: https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html"},{question:"How can you ensure that your EC2 instances execute a customizable set of instructions when they first start?",answers:[{text:"With EC2 User Data",isCorrect:!0},{text:"Mount EFS network drives",isCorrect:!1},{text:"With EC2 Meta Data",isCorrect:!1},{text:"By building a custom AMI",isCorrect:!1}],explanation:"When you launch an instance in Amazon EC2, you have the option of passing user data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. Read more: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html"},{question:"At which frequency do EC2 instances report their metrics under normal configurations?",answers:[{text:"5 minutes",isCorrect:!0},{text:"10 minutes",isCorrect:!1},{text:"2 minutes",isCorrect:!1},{text:"1 minute",isCorrect:!1}],explanation:"By default, Amazon EC2 sends metric data to CloudWatch in 5-minute periods. Read more: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch.html"},{question:"You are storing your video files in a separate bucket from your main S3 website bucket. If using directly the video URL, we users are able to download the videos, but it is impossible for them to play the videos while visiting your main website. What's the problem?",answers:[{text:"Enable CORS",isCorrect:!0},{text:"Change the bucket policy",isCorrect:!1},{text:"Amend the IAM policy",isCorrect:!1},{text:"Disable Encryption",isCorrect:!1}],explanation:"Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources. Read more: https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html"},{question:"You would like to deploy a function programmed in Rust to AWS Lambda. Rust is not yet a supported runtime. What should you do?",answers:[{text:"You can't deploy functions that aren't supported runtime",isCorrect:!0},{text:"Package as a docker image and deploy to AWS Lambda",isCorrect:!1},{text:"Define a custom runtime for AWS Lambda",isCorrect:!1},{text:"Open a support ticket with AWS",isCorrect:!1}],explanation:" Read more: https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html"},{question:"(legacy question) You are looking to improve S3 performance. How should you name your files?",answers:[{text:"<my_bucket>/<random 4 characters>_my_folder/my_file1.txt",isCorrect:!0},{text:"<my_bucket>/MMDDYYYY_my_folder/my_file1.txt",isCorrect:!1},{text:"<my_bucket>/DDMMYYYY_my_folder/my_file1.txt",isCorrect:!1},{text:"<my_bucket>/YYMMDD_my_folder/my_file1.txt",isCorrect:!1}],explanation:"This is legacy, for now still asked in the exam. See new guidelines here https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/"},{question:"Your boss wants to scale your ASG based on the number of requests per minute your application makes to your database.",answers:[{text:"You create a CloudWatch custom metric and build an alarm on this to scale your ASG",isCorrect:!0},{text:"You politely tell him it's impossible",isCorrect:!1},{text:"You enable detailed monitoring and use that to scale your ASG",isCorrect:!1}],explanation:"Amazon CloudWatch is a web service that enables you to monitor, manage, and publish various metrics, as well as configure alarm actions based on data from metrics. You can define custom metrics for your own use, and Elastic Beanstalk will push those metrics to Amazon CloudWatch. Once Amazon CloudWatch contains your custom metrics, you can view those in the Amazon CloudWatch console. Read more: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-containers-cw.html"},{question:"Your ELB is sending traffic to all your EC2 instances. When one of your server crashes, you realise the ELB still sends traffic to it, which results in bad user experience. What can you do to minimise this problem?",answers:[{text:"Enable Health Checks",isCorrect:!0},{text:"Enable Stickiness",isCorrect:!1},{text:"Enable Multi AZ deployments",isCorrect:!1},{text:"Enable SSL",isCorrect:!1}],explanation:"To discover the availability of your EC2 instances, a load balancer periodically sends pings, attempts connections, or sends requests to test the EC2 instances. These tests are called health checks. The status of the instances that are healthy at the time of the health check is InService. The status of any instances that are unhealthy at the time of the health check is OutOfService.  Read more: https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html"},{question:"CodeDeploy is deploying to EC2 instances but they seem to have problem pulling the code from S3. What should you do?",answers:[{text:"Fix the IAM permissions for the EC2 instance role",isCorrect:!0},{text:"Fix the IAM permissions for the CodeDeploy service role",isCorrect:!1},{text:"Make the S3 bucket public",isCorrect:!1},{text:"Enable CodeDeploy Proxy",isCorrect:!1}],explanation:"Make sure the EC2 instance roles has proper permissions to pull from S3"},{question:"You are trying to collect metrics in near real time in order to track your application performance. Which service should you use?",answers:[{text:"CloudWatch Metrics",isCorrect:!0},{text:"X-Ray",isCorrect:!1},{text:"CloudWatch Alarms",isCorrect:!1},{text:"CloudTrail",isCorrect:!1}],explanation:"Amazon CloudWatch monitors your Amazon Web Services (AWS) resources and the applications you run on AWS in real time. You can use CloudWatch to collect and track metrics, which are variables you can measure for your resources and applications. CloudWatch alarms send notifications or automatically make changes to the resources you are monitoring based on rules that you define. Read more: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html"},{question:"Messages produced to SQS by bidders in your betting application must not be visible to other applications for 30 seconds. How can you achieve that easily?",answers:[{text:"Use DelaySeconds parameter",isCorrect:!0},{text:"implement application-side delay",isCorrect:!1},{text:"Enable ConsumerReadDelay",isCorrect:!1},{text:"Enable LongPolling",isCorrect:!1}],explanation:"Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. Read more: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html"},{question:"What are some key differences between a standard SQS and SWF? (select two)",answers:[{text:"SWF ensures the task is assigned only once while SQS may deliver the message multiple times",isCorrect:!0},{text:"SWF is task oriented API and SQS is message-oriented API",isCorrect:!0},{text:"SQS is task oriented API and SWF is message-oriented API",isCorrect:!1},{text:"SQS ensures the task is assigned only once while SWF may deliver the message multiple times",isCorrect:!1}],explanation:"Amazon SWF provides useful guarantees around task assignment. It ensures that a task is never duplicated and is assigned only once. Thus, even though you may have multiple workers for a particular activity type (or a number of instances of a decider), Amazon SWF will give a specific task to only one worker (or one decider instance). Read more: https://aws.amazon.com/swf/faqs/"},{question:"You would like to trigger a notification when a CodeBuild fails. Which AWS service helps you in achieving that?",answers:[{text:"AWS CloudWatch Alarms",isCorrect:!0},{text:"SNS",isCorrect:!1},{text:"Kinesis",isCorrect:!1},{text:"CloudTrail",isCorrect:!1}],explanation:"You can create a CloudWatch alarm for your builds. An alarm watches a single metric over a period of time that you specify and performs one or more actions based on the value of the metric relative to a specified threshold over a number of time periods. Using native CloudWatch alarm functionality, you can specify any of the actions supported by CloudWatch when a threshold is exceeded. Read more: https://docs.aws.amazon.com/codebuild/latest/userguide/monitoring-alarms.html"},{question:"Your application runs on Elastic Beanstalk. Its configuration values change often and the devops team does not want to re-deploy the application every time a configuration changes. They would rather manage configuration externally and securely and have it load dynamically into the application at runtime. What advice do you give?",answers:[{text:"Use SSM Parameter Store",isCorrect:!0},{text:"use Environment variables",isCorrect:!1},{text:"Use Stage Variables",isCorrect:!1},{text:"Use S3",isCorrect:!1}],explanation:"AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values.  Read more: https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html"},{question:"We need to perform 16 eventually consistent reads per seconds of 12KB each. How many RCU do we need?",answers:[{text:"24",isCorrect:!0},{text:"192",isCorrect:!1},{text:"12",isCorrect:!1},{text:"48",isCorrect:!1}],explanation:"One read capacity unit represents two eventually consistent reads per second, for an item up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units. The total number of read capacity units required depends on the item size. Read more: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html"},{question:"You are running a public DNS service on EC2 and a requirement is that it is accessible using the same IP. You wish to sometimes upgrade your DNS service but would like to do it without downtime. Which service will help you accomplish this?",answers:[{text:"Elastic IP",isCorrect:!0},{text:"Create a Load Balancer and an auto scaling group",isCorrect:!1},{text:"Provide a static private IP",isCorrect:!1},{text:"Use Route53",isCorrect:!1}],explanation:"DNS services are identified by a public IP, so you need to use Elastic IP. Read more: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html#using-instance-addressing-eips-associating-different"},{question:"You have enabled versioning on an S3 bucket. What isn't true about the following?",answers:[{text:"Versioning can be enabled only for a specific folder",isCorrect:!0},{text:"Overwriting a file increases its versions",isCorrect:!1},{text:"Deleting a file is a recoverable operation",isCorrect:!1},{text:'Any file that was unversioned before enabling versioning will have the "null" version',isCorrect:!1}],explanation:"The versioning state applies to all (never some) of the objects in that bucket. The first time you enable a bucket for versioning, objects in it are thereafter always versioned and given a unique version ID. Read more: https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html"},{question:"You want to send emails using the AWS SDK. You should use",answers:[{text:"SES",isCorrect:!0},{text:"SNS",isCorrect:!1},{text:"SQS",isCorrect:!1},{text:"Kinesis",isCorrect:!1}],explanation:"Amazon Simple Email Service (Amazon SES) is a cloud-based email sending service designed to help digital marketers and application developers send marketing, notification, and transactional emails. Read more: https://aws.amazon.com/ses/"},{question:"You would like to receive notifications when a parameter changes in Parameter Store, for security reasons. Which service will help you achieve that?",answers:[{text:"SNS",isCorrect:!0},{text:"SQS",isCorrect:!1},{text:"SES",isCorrect:!1},{text:"CloudTrail",isCorrect:!1}],explanation:"You can use Amazon CloudWatch Events and Amazon SNS to notify you about changes to Systems Manager Parameters. You can be notified when a parameter is created, updated, or deleted. Read more: https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-cwe.html"},{question:"Where is CloudFront SSL in flight encryption available?",answers:[{text:"Between clients and CloudFront and CloudFront and backend",isCorrect:!0},{text:"Between clients and CloudFront only",isCorrect:!1},{text:"Between CloudFront and backend only",isCorrect:!1},{text:"Nowhere",isCorrect:!1}],explanation:"For web distributions, you can configure CloudFront to require that viewers use HTTPS to request your objects, so connections are encrypted when CloudFront communicates with viewers. You also can configure CloudFront to use HTTPS to get objects from your origin, so connections are encrypted when CloudFront communicates with your origin. Read more: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/secure-connections-supported-viewer-protocols-ciphers.html#secure-connections-supported-ciphers-cloudfront-to-origin"},{question:"You would like to run command from the CLI on your EC2 instance. The instance should automatically obtain credentials to perform these commands. What feature will allow you to do this in the most secure way?",answers:[{text:"IAM Roles for EC2",isCorrect:!0},{text:"EC2 User Data",isCorrect:!1},{text:"Run `aws configure` on the EC2 instance",isCorrect:!1},{text:"CloudFormation",isCorrect:!1}],explanation:"IAM roles have been incorporated so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles. Read more: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html"},{question:"Which are the two deployment options provided by CodeDeploy? (select two)",answers:[{text:"In-place deployment",isCorrect:!0},{text:"Blue/green Deployment",isCorrect:!0},{text:"Cattle deployment",isCorrect:!1},{text:"Classic Deployment",isCorrect:!1}],explanation:"The application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete.With a blue/green deployment, you provision a new set of instances on which CodeDeploy installs the latest version of your application. CodeDeploy then reroutes load balancer traffic from an existing set of instances running the previous version of your application to the new set of instances running the latest version. After traffic is rerouted to the new instances, the existing instances can be terminated. Read more: https://aws.amazon.com/about-aws/whats-new/2017/01/aws-codedeploy-introduces-blue-green-deployments/"},{question:"Which function should you use to make cross stack references in CloudFormation?",answers:[{text:"!ImportValue",isCorrect:!0},{text:"!Ref",isCorrect:!1},{text:"!GetAtt",isCorrect:!1},{text:"!Sub",isCorrect:!1}],explanation:"The intrinsic function Fn::ImportValue returns the value of an output exported by another stack. You typically use this function to create cross-stack references.  Read more: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html"},{question:"You would like to run multiple versions of your application in Elastic Beanstalk to ensure you can perform regular development and load testing. What do you recommend?",answers:[{text:'Define a dev environment with a single instance and a "load test"  environment that has settings close to prod',isCorrect:!0},{text:"You can't have multiple development environment in Elastic Beanstalk, just one development and one production environment",isCorrect:!1},{text:"Use only one Beanstalk environment and perform configuration changes using an Ansible script",isCorrect:!1},{text:"Create an Application Load Balancer to route based on hostname so you can pass on parameters to the development Elastic Beanstalk environment. Create a file in .ebextensions/ in order to know how to handle the traffic coming from the ALB",isCorrect:!1}],explanation:"It is common practice to have many environments for the same application"},{question:"You get the following exception: An error occurred (InvalidParameterValueException) when calling the UpdateFunctionCode operation: Unzipped size must be smaller than 262144000 bytes",answers:[{text:"The uncompressed zip file exceeds AWS Lambda limits",isCorrect:!0},{text:"You have uploaded a zip file larger than 50 MB to AWS Lambda",isCorrect:!1},{text:"Your zip file is corrupt",isCorrect:!1},{text:"You have used the wrong API call",isCorrect:!1}],explanation:"The deployment package unzipped size canot exceed 256MB. Read more: https://docs.aws.amazon.com/lambda/latest/dg/limits.html"},{question:"The application load balancer can redirect to different target groups based on which of these variables? (select two)",answers:[{text:"hostname",isCorrect:!0},{text:"request path",isCorrect:!0},{text:"client IP",isCorrect:!1},{text:"web browser version",isCorrect:!1},{text:"cookie value",isCorrect:!1}],explanation:"You can create a listener with rules to forward requests based on the URL path. This is known as path-based routing. If you are running microservices, you can route traffic to multiple back-end services using path-based routing. For example, you can route general requests to one target group and requests to render images to another target group. Read more: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/tutorial-load-balancer-routing.html"},{question:"Your company does not want to have to manage S3 encryption keys and would like to have server side encryption. Which encryption mechanism suits best?",answers:[{text:"SSE-S3",isCorrect:!0},{text:"SSE-C",isCorrect:!1},{text:"Client Side Encryption",isCorrect:!1},{text:"SSE-KMS",isCorrect:!1}],explanation:"Use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)   : Each object is encrypted with a unique key employing strong multi-factor encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.  Read more: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html"},{question:"We need to perform 10 strongly consistent reads per second of 4KB each. How many RCU do we need?",answers:[{text:"10",isCorrect:!0},{text:"40",isCorrect:!1},{text:"20",isCorrect:!1},{text:"5",isCorrect:!1}],explanation:"One read capacity unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units. The total number of read capacity units required depends on the item size, and whether you want an eventually consistent or strongly consistent read. Read more: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html"},{question:"You are running at desired capacity of 3 and the maximum capacity of 3. You have alarms set at 60% CPU to scale out your application. Your application is now running at 80% capacity. What will happen?",answers:[{text:"Nothing",isCorrect:!0},{text:"The desired capacity will go up to 4 and the maximum will stay at 3",isCorrect:!1},{text:"The desired capacity will go up to 4 and the maximum will stay at 4",isCorrect:!1}],explanation:"You are already running at max capacity. Read more: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html"},{question:"The AWS CLI depends on which language?",answers:[{text:"Python",isCorrect:!0},{text:"C#",isCorrect:!1},{text:"Golang",isCorrect:!1},{text:"Java",isCorrect:!1}],explanation:"The primary distribution method for the AWS CLI on Linux, Windows, and macOS is pip, a package manager for Python that provides an easy way to install, upgrade, and remove Python packages and their dependencies. Read more: https://docs.aws.amazon.com/cli/latest/userguide/installing.html"},{question:"A producer application needs to deliver many messages to consumer applications. These consumer applications have various consumption patterns and can either consume the message instantaneously or up to 10 days later. The number of consumer applications can grow and shouldn't trigger a code change for the producing application. Which solution should you use?",answers:[{text:"SNS + SQS",isCorrect:!0},{text:"SNS + Kinesis",isCorrect:!1},{text:"SNS + Lambda",isCorrect:!1},{text:"SQS",isCorrect:!1}],explanation:" Read more: https://aws.amazon.com/getting-started/tutorials/send-fanout-event-notifications/"},{question:"You would like to access the amount of RAM as a metric on your EC2 instances. How can you get that information?",answers:[{text:"Use a cron job on the instances that push the EC2 RAM metric as a Custom metric",isCorrect:!0},{text:"Enable EC2 detailed monitoring",isCorrect:!1},{text:"Access as a standard CloudWatch metric",isCorrect:!1},{text:"Use X-Ray",isCorrect:!1}],explanation:"The Amazon CloudWatch Monitoring Scripts for Amazon Elastic Compute Cloud (Amazon EC2) Linux-based instances demonstrate how to produce and consume Amazon CloudWatch custom metrics. These Perl scripts comprise a fully functional example that reports memory, swap, and disk space utilization metrics for a Linux instance. Read more: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html"},{question:"You would like to store small encrypted secrets in SSM Parameter store. What is the right way of doing it?",answers:[{text:"Enable seamless encryption with KMS integration",isCorrect:!0},{text:"Encrypt first with KMS then store in Parameter store",isCorrect:!1},{text:"Encrypt with the Encryption SDK then store in Parameter Store",isCorrect:!1},{text:"Store in S3 with SSE-KMS encryption and put a reference to the secret in the parameter store",isCorrect:!1}],explanation:"With AWS Systems Manager Parameter Store, you can create Secure String parameters, which are parameters that have a plaintext parameter name and an encrypted parameter value. Parameter Store uses AWS KMS to encrypt and decrypt the parameter values of Secure String parameters.Also, if you are using customer managed CMKs, you can use IAM policies and key policies to manage encrypt and decrypt permissions.  Read more: https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html"},{question:"You want to orchestrate your Lambda functions as a state machine. You should use",answers:[{text:"AWS Step Functions",isCorrect:!0},{text:"CloudWatch Rules",isCorrect:!1},{text:"AWS SWF",isCorrect:!1},{text:"AWS ECS",isCorrect:!1}],explanation:"sing Step Functions, you can design and run workflows that stitch together services such as AWS Lambda and Amazon ECS into feature-rich applications. Workflows are made up of a series of steps, with the output of one step acting as input into the next. Read more: https://aws.amazon.com/step-functions/"},{question:"You are performing a DynamoDB scan but the performance is really low. You have a high number of partitions and would like to scan faster. How can you achieve that?",answers:[{text:"Use parallel scans",isCorrect:!0},{text:"Use a ProjectionExpression",isCorrect:!1},{text:"Use a FilterExpression",isCorrect:!1},{text:"Use a Query",isCorrect:!1}],explanation:"In order to give you the ability to retrieve data from your DynamoDB tables more rapidly, AWS introduced a new parallel scan model.  To make use of this feature, you will need to run multiple worker threads or processes in parallel. Each worker will be able to scan a separate segment of a table concurently with the other workers. Read more: https://aws.amazon.com/blogs/aws/amazon-dynamodb-parallel-scans-and-other-good-news/"},{question:"What are the two commands to run to upload Lambda functions and CloudFormation templates to AWS?",answers:[{text:"cloudformation package and cloudformation deploy",isCorrect:!0},{text:"cloudformation package and cloudformation upload",isCorrect:!1},{text:"cloudformation zip and cloudformation upload",isCorrect:!1},{text:"cloudformation zip and cloudformation deploy",isCorrect:!1}],explanation:" Read more: https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.htmlhttps://docs.aws.amazon.com/cli/latest/reference/cloudformation/deploy/index.html"},{question:"You would like to run an application continously for a year and can predict how much capacity you will need. You need the application instances to be stable and not terminated abruptly as you believe this will impact your users. What should you use?",answers:[{text:"EC2 Reserved Instances",isCorrect:!0},{text:"EC2 On Demand Instances",isCorrect:!1},{text:"EC2 Spot Instances",isCorrect:!1},{text:"Bring your own EC2 instance",isCorrect:!1}],explanation:"Reserved instances can provide a capacity reservation, offering additional confidence in your ability to launch the number of instances you have reserved when you need them. Read more: https://aws.amazon.com/ec2/pricing/reserved-instances/"},{question:"You want to send your CloudWatch logs to S3 for archival. How can you achieve it?",answers:[{text:"Use CloudWatch integration feature with S3",isCorrect:!0},{text:"Use CloudWatch integration feature with Kinesis",isCorrect:!1},{text:"Use CloudWatch integration feature with Lambda",isCorrect:!1},{text:"Use CloudWatch integration feature with Glue",isCorrect:!1}],explanation:"You can export log data from your log groups to an Amazon S3 bucket and use this data in custom processing and analysis, or to load onto other systems. Read more: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html"},{question:"You use the CLI and it fails with the following exception: You are not authorized to perform this operation. Encoded authorization failure message: 6h34GtpmGjJJUm946eDVBfzWQJk6z5GePbbGDs9Z2T8xZj9EZtEduSnTbmrR7pMqpJrVYJCew2m8YBZQf4HRWEtrpncANrZMsnzk   How do you decode the message?",answers:[{text:" AWS STS decode-authorization-message ",isCorrect:!0},{text:" AWS IAM decode-authorization-message ",isCorrect:!1},{text:" Use KMS decode-authorization-message ",isCorrect:!1},{text:"AWS Cognito Decoder",isCorrect:!1}],explanation:"Use decode-authorization-message to decode additional information about the authorization status of a request from an encoded message returned in response to an AWS request. Read more: https://docs.aws.amazon.com/cli/latest/reference/sts/decode-authorization-message.html"},{question:"You would like to audit which API calls are made to Amazon S3 within your environment, what do you use?",answers:[{text:"CloudTrail",isCorrect:!0},{text:"S3 Access Logs",isCorrect:!1},{text:"VPC Flow Logs",isCorrect:!1},{text:"IAM",isCorrect:!1}],explanation:"With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. Read more: https://aws.amazon.com/cloudtrail/"},{question:"You have a read heavy workload on your DynamoDB table and this causes a problem a hot partition problem, as one popular item in your catalog is always requested. What technology will allow you to reduce the read load on your DynamoDB table while minimising code changes?",answers:[{text:"DynamoDB DAX",isCorrect:!0},{text:"DynamoDB Streams",isCorrect:!1},{text:"ElastiCache",isCorrect:!1},{text:"More partitions",isCorrect:!1}],explanation:"Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement   : from milliseconds to microseconds   : even at millions of requests per second. Read more: https://aws.amazon.com/dynamodb/dax/"},{question:"You have an application producing messages from 20 KB to 200 KB. These messages go to SQS, is this possible?",answers:[{text:"Yes, the max message size is 256KB",isCorrect:!0},{text:"Yes, the max message size is 512KB",isCorrect:!1},{text:"No, the max message size is 128KB",isCorrect:!1},{text:"No, the max message size is 64KB",isCorrect:!1}],explanation:"The minimum message size is 1 byte (1 character). The maximum is 262,144 bytes (256 KB). Read more: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-limits.html"},{question:"You would like to use SSE-KMS encryption mechanism. What header must you set in your request?",answers:[{text:'"x-amz-server-side-encryption": "aws:kms"',isCorrect:!0},{text:'"x-amz-server-side-encryption": "SSE-S3"',isCorrect:!1},{text:'"x-amz-server-side-encryption": "SSE-KMS"',isCorrect:!1},{text:'"x-amz-server-side-encryption": "AES256"',isCorrect:!1}],explanation:"If the request does not include the x-amz-server-side-encryption header, then the request is denied. Read more: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html"},{question:"Which Beanstalk deployment modes allows you for the quickest deployment strategy?",answers:[{text:"all at once",isCorrect:!0},{text:"rolling",isCorrect:!1},{text:"rolling with additional batches",isCorrect:!1},{text:"immutable",isCorrect:!1},{text:"blue/green",isCorrect:!1}],explanation:"All at once   : Deploy the new version to all instances simultaneously and hence the shortest time. All instances in your environment are out of service for a short time while the deployment occurs. Read more: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html"},{question:"Your organisation has a requirement that all the code must be encrypted while in the cloud. You have proposed CodeCommit as a solution for this. How can you ensure the code is encrypted?",answers:[{text:"Repositories are automatically encrypted at rest",isCorrect:!0},{text:"Enable KMS encryption",isCorrect:!1},{text:"Use AWS Lamdba as a hook to encrypt the pushed code",isCorrect:!1},{text:"Use a git command line hook to encrypt the code client side",isCorrect:!1}],explanation:"Data in AWS CodeCommit repositories is encrypted in transit and at rest. When data is pushed into an AWS CodeCommit repository (for example, by calling git push), AWS CodeCommit encrypts the received data as it is stored in the repository. Read more: https://docs.aws.amazon.com/codecommit/latest/userguide/encryption.html"},{question:"AWS Route 53 can be used almost as an alternative of another AWS service.Which one ?",answers:[{text:"ELB",isCorrect:!0},{text:"CloudFront",isCorrect:!1},{text:"S3",isCorrect:!1}],explanation:" Read more: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html"},{question:"For which encryption mechanism is HTTPS (SSL) mandatory?",answers:[{text:"SSE-C",isCorrect:!0},{text:"SSE-KMS",isCorrect:!1},{text:"Client Side Encryption",isCorrect:!1},{text:"SSE-S3",isCorrect:!1}],explanation:"You must use https.Amazon S3 will reject any requests made over http when using SSE-C. For security considerations, we recommend you consider any key you send erroneously using http to be compromised. You should discard the key, and rotate as appropriate. Read more: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html"},{question:"What is the maximum number of messages one SQS consumer can receive at a time?",answers:[{text:"10",isCorrect:!0},{text:"5",isCorrect:!1},{text:"20",isCorrect:!1},{text:"100",isCorrect:!1}],explanation:"SQS Consumer retrieves one or more messages (up to 10), from the specified queue. Read more: https://docs.aws.amazon.com/cli/latest/reference/sqs/receive-message.html"},{question:"You are running an application leveraging the SDK on an EC2 instance. How do you pass credentials to the SDK?",answers:[{text:"Use an IAM Instance Role",isCorrect:!0},{text:"Use environment variables",isCorrect:!1},{text:"Hardcode the credentials in the application code",isCorrect:!1},{text:"Use the SSM parameter store",isCorrect:!1}],explanation:"An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. Read more: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html"},{question:"Your mobile application needs to directly perform API calls to DynamoDB. You do not want to store secrets onto the mobile devices and need all the calls to DynamoDB done with a different identity per mobile device. Which service allows you to achieve this?",answers:[{text:"Cognito Identity Pools",isCorrect:!0},{text:"Cognito User Pools",isCorrect:!1},{text:"Cognito Sync",isCorrect:!1},{text:"IAM",isCorrect:!1}],explanation:"Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token. An identity pool is a store of user identity data specific to your account. Read more: https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html"},{question:"Your company does want S3 server side encryption but wants to manage the keys through their AWS portal. Which encryption mechanism suits best?",answers:[{text:"SSE-KMS",isCorrect:!0},{text:"SSE-C",isCorrect:!1},{text:"Client Side Encryption",isCorrect:!1},{text:"SSE-S3",isCorrect:!1}],explanation:"You have the option to create and manage encryption keys yourself, or use a default key that is unique to you, the service you're using, and the region you're working in. Read more: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html"},{question:"Which is the correct order for CodeDeploy steps in appspec.yml?",answers:[{text:"DownloadBundle -> BeforeInstall -> ApplicationStart -> ValidateService",isCorrect:!0},{text:"BeforeInstall -> ApplicationStart -> DownloadBundle -> ValidateService",isCorrect:!1},{text:"ValidateService -> BeforeInstall ->DownloadBundle -> ApplicationStart",isCorrect:!1},{text:"BeforeInstall -> ValidateService ->DownloadBundle -> ApplicationStart",isCorrect:!1}],explanation:" Read more: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order-lambda"},{question:"You want to delete all the items in your DynamoDB table (over 10 million of them), how you should perform that the fastest while maintaining low costs?",answers:[{text:"Delete then re-create the table",isCorrect:!0},{text:"Scan and call DeleteItem",isCorrect:!1},{text:"Scan and call BatchDeleteItem",isCorrect:!1},{text:"Call PurgeTable",isCorrect:!1}],explanation:"The DeleteTable operation deletes a table and all of its items. Read more: https://docs.aws.amazon.com/cli/latest/reference/dynamodb/delete-table.html"},{question:"You would like to programmatically define your CodeBuild steps. What should you do?",answers:[{text:"define a .buildspec file in the root directory",isCorrect:!0},{text:"define a .appspec file in the root directory",isCorrect:!1},{text:"define a .buildspec file in the codebuild/ directory",isCorrect:!1},{text:"define a .appspec file in the codebuild/ directory",isCorrect:!1}],explanation:"A build spec is a collection of build commands and related settings, in YAML format, that AWS CodeBuild uses to run a build. You can include a build spec as part of the source code or you can define a build spec when you create a build project. Read more: https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html"},{question:"You are looking to deploy a website to Amazon S3. Which service should you use?",answers:[{text:"CodeBuild",isCorrect:!0},{text:"CodePipeline",isCorrect:!1},{text:"CodeDeploy",isCorrect:!1},{text:"AWS Lambda",isCorrect:!1}],explanation:" Read more: https://stelligent.com/2017/09/05/continuous-delivery-to-s3-via-codepipeline-and-codebuild/"},{question:"You have a Kinesis stream with 10 shards, and from the metrics you are well below the throughput utilisation (10MB/s) send. You currently send 3 MB/s of data. Yet, you are getting a ProvisionedThroughputException. What is the likely cause of it?",answers:[{text:"The partition key that you have selected isn't distributed enough",isCorrect:!0},{text:"Metrics are slow to update",isCorrect:!1},{text:"You have too many shards",isCorrect:!1},{text:"The data retention period is too long",isCorrect:!1}],explanation:"As the partition key is not distributed enough, all the data is getting skewed at few specific shards and not leveraging the entire cluster of shards. Read more: https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html"},{question:"Your application clients are resolving the public IP for your load balancers at boot time. The public IP is used for the remaining of the time. After a while, the application breaks. Why?",answers:[{text:"The load balancer is highly available and its public IP may change. The DNS is constant",isCorrect:!0},{text:"Your security groups are not stable",isCorrect:!1},{text:"You need to enable stickiness",isCorrect:!1},{text:"You need to disable multi AZ deployments",isCorrect:!1}],explanation:"Never resolve IP of load balancer, always use the DNS name"},{question:"Your application is issuing too many polls to your SQS queue, which all return empty because you don't have a lot of traffic on it. What should you do to decrease your cost while maintaining your processing latency?",answers:[{text:"Use LongPolling",isCorrect:!0},{text:"Increase the VisibiltyTimeout",isCorrect:!1},{text:"Use a FIFO queue",isCorrect:!1},{text:"Decrease DelaySeconds",isCorrect:!1}],explanation:"Eliminate empty responses by allowing Amazon SQS to wait until a message is available in a queue before sending a response. Unless the connection times out, the response to the ReceiveMessage request contains at least one of the available messages, up to the maximum number of messages specified in the ReceiveMessage action.Eliminate false empty responses by querying all rather than a subset of Amazon SQS servers. Read more: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html"},{question:"Every time your CodeBuild runs its build step, it has to resolve Java dependencies from an external Ivy repository. As your project as over 100 dependencies, this takes a long time. Your manager wants you to speed up the execution of the CodeBuild. How should you proceed with minimal changes?",answers:[{text:"Enable S3 caching",isCorrect:!0},{text:"Reduce the number of dependencies",isCorrect:!1},{text:"Ship all the dependencies as part of the source code",isCorrect:!1},{text:"Setup a Nexus repository in EC2 that will act as an internal dependency cache",isCorrect:!1}],explanation:"Downloading dependencies is a critical phase in the build process. These dependent files can range in size from a few KBs to multiple MBs. Because most of the dependent files do not change frequently between builds, you can noticeably reduce your build time by caching dependencies in S3. Read more: https://aws.amazon.com/blogs/devops/how-to-enable-caching-for-aws-codebuild/"},{question:"You need a database from which you can extract an event stream of every changes that happened. Which database technology will you choose?",answers:[{text:"DynamoDB",isCorrect:!0},{text:"RDS",isCorrect:!1},{text:"ElastiCache",isCorrect:!1},{text:"Kinesis",isCorrect:!1}],explanation:"DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table, and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real time. Read more: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html"},{question:"You would like to trigger AWS Lambda based on a specific service state change. That service does not have a direct integration with AWS Lambda. How can you best achieve it?",answers:[{text:"CloudWatch Event Rules with AWS Lambda",isCorrect:!0},{text:"AWS Lambda Custom Sources",isCorrect:!1},{text:"Open a support ticket with AWS",isCorrect:!1},{text:"Cron jobs to trigger AWS Lambda to check the state of your service",isCorrect:!1}],explanation:" Read more: https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.htmlhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html"},{question:"How can REST API clients invalidate the API Gateway Cache?",answers:[{text:"Using the Header Cache-Control: max-age=0",isCorrect:!0},{text:"Use the Request parameter: ?bypass_cache=1",isCorrect:!1},{text:"Using the Header Bypass-Cache=1",isCorrect:!1},{text:"Using the request parameter ?cache-control-max-age=0",isCorrect:!1}],explanation:"A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the Cache-Control: max-age=0 header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint. Read more: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching"}]},{id:"aws-developer-5249670",title:"Practice Test #6 (AWS Certified Developer Associate - DVA-C02)",description:"Practice questions for AWS Developer Associate (DVA-C02) exam — covering core AWS services like SQS, Lambda, DynamoDB, and more.",questions:[{question:"The development team at a retail organization wants to allow a Lambda function in its AWS Account A to access a DynamoDB table in another AWS Account B.\n\nAs a Developer Associate, which of the following solutions would you recommend for the given use-case?\n",answers:[{text:"Create a clone of the Lambda function in AWS Account B so that it can access the DynamoDB table in the same account",isCorrect:!1},{text:"Add a resource policy to the DynamoDB table in AWS Account B to give access to the Lambda function in Account A",isCorrect:!1},{text:"Create an IAM role in Account B with access to DynamoDB. Modify the trust policy of the role in Account B to allow the execution role of Lambda to assume this role. Update the Lambda function code to add the AssumeRole API call",isCorrect:!0},{text:"Create an IAM role in Account B with access to DynamoDB. Modify the trust policy of the execution role in Account A to allow the execution role of Lambda to assume the IAM role in Account B. Update the Lambda function code to add the AssumeRole API call",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Create an IAM role in account B with access to DynamoDB. Modify the trust policy of the role in Account B to allow the execution role of Lambda to assume this role. Update the Lambda function code to add the AssumeRole API call</strong>\n\nYou can give a Lambda function created in one account ("account A") permissions to assume a role from another account ("account B") to access resources such as DynamoDB or S3 bucket. You need to create an execution role in Account A that gives the Lambda function permission to do its work. Then you need to create a role in account B that the Lambda function in account A assumes to gain access to the cross-account DynamoDB table. Make sure that you modify the trust policy of the role in Account B to allow the execution role of Lambda to assume this role. Finally, update the Lambda function code to add the AssumeRole API call.\n\nSample use-case to configure a Lambda function to assume a role from another AWS account:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q1-i1.jpg">\nvia - <a href="https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/</a>\n\nIncorrect options:\n\n<strong>Create a clone of the Lambda function in AWS Account B so that it can access the DynamoDB table in the same account</strong> - Creating a clone of the Lambda function is a distractor as this does not solve the use-case outlined in the problem statement.\n\n<strong>Add a resource policy to the DynamoDB table in AWS Account B to give access to the Lambda function in Account A</strong> - You cannot attach a resource policy to a DynamoDB table, so this option is incorrect.\n\n<strong>Create an IAM role in Account B with access to DynamoDB. Modify the trust policy of the execution role in Account A to allow the execution role of Lambda to assume the IAM role in Account B. Update the Lambda function code to add the AssumeRole API call</strong> - As mentioned in the explanation above, you need to modify the trust policy of the IAM role in Account B so that it allows the execution role of Lambda function in account A to assume the IAM role in Account B.\n\nReference:\n<a href="https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/</a>\n'},{question:"The development team at an e-commerce company wants to run a serverless data store service on two docker containers using shared memory.\n\nWhich of the following ECS configurations can be used to facilitate this use-case?\n",answers:[{text:"Put the two containers into a single task definition using a Fargate Launch Type",isCorrect:!0},{text:"Put the two containers into two separate task definitions using a Fargate Launch Type",isCorrect:!1},{text:"Put the two containers into two separate task definitions using an EC2 Launch Type",isCorrect:!1},{text:"Put the two containers into a single task definition using an EC2 Launch Type",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Put the two containers into a single task definition using a Fargate Launch Type</strong>\n\nAmazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control over your infrastructure, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type.\n\n<img src="https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png">\nvia - <a href="https://aws.amazon.com/ecs/">https://aws.amazon.com/ecs/</a>\n\nAs the development team is looking for a serverless data store service, therefore the two containers should be launched into a single task definition using a Fargate Launch Type. Using a single task definition allows the two containers to share memory. Please see these use-cases for Fargate Launch type when you should put multiple containers into the same task definition:\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q2-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html</a>\n\nFor a deep-dive on understanding how Amazon ECS manages CPU and memory resources, please review this excellent blog-\n<a href="https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/">https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/</a>\n\nIncorrect options:\n\n<strong>Put the two containers into two separate task definitions using a Fargate Launch Type</strong> - This option contradicts the details provided in the explanation above, so this option is ruled out.\n\n<strong>Put the two containers into two separate task definitions using an EC2 Launch Type</strong>\n\n<strong>Put the two containers into a single task definition using an EC2 Launch Type</strong>\n\nAs the development team is looking for a serverless data store service, therefore EC2 Launch Type is ruled out. So both these options are incorrect.\n\nReferences:\n\n<a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html</a>\n\n<a href="https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/">https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/</a>\n'},{question:"The development team at an IT company uses CloudFormation to manage its AWS infrastructure. The team has created a network stack containing a VPC with subnets and a web application stack with EC2 instances and an RDS instance. The team wants to reference the VPC created in the network stack into its web application stack.\n\nAs a Developer Associate, which of the following solutions would you recommend for the given use-case?\n",answers:[{text:"Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack",isCorrect:!1},{text:"Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack",isCorrect:!1},{text:"Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack",isCorrect:!0},{text:"Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</strong>\n\nAWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion.\n\nHow CloudFormation Works:\n<img src="https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png">\nvia - <a href="https://aws.amazon.com/cloudformation/">https://aws.amazon.com/cloudformation/</a>\n\nYou can create a cross-stack reference to export resources from one AWS CloudFormation stack to another. For example, you might have a network stack with a VPC and subnets and a separate public web application stack. To use the security group and subnet from the network stack, you can create a cross-stack reference that allows the web application stack to reference resource outputs from the network stack. With a cross-stack reference, owners of the web application stacks don\'t need to create or maintain networking rules or assets.\n\nTo create a cross-stack reference, use the Export output field to flag the value of a resource output for export. Then, use the Fn::ImportValue intrinsic function to import the value.\n\nYou cannot use the Ref intrinsic function to import the value.\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q3-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html</a>\n\nIncorrect options:\n\n<strong>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Fn::ImportValue intrinsic function to import the value of VPC into the web application stack</strong>\n\n<strong>Create a cross-stack reference and use the Outputs output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</strong>\n\n<strong>Create a cross-stack reference and use the Export output field to flag the value of VPC from the network stack. Then use Ref intrinsic function to reference the value of VPC into the web application stack</strong>\n\nThese three options contradict the details provided in the explanation above, so these options are not correct.\n\nReferences:\n\n<a href="https://aws.amazon.com/cloudformation/">https://aws.amazon.com/cloudformation/</a>\n\n<a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html</a>\n'},{question:"A photo-sharing application manages its EC2 server fleet running behind an Application Load Balancer and the traffic is fronted by a CloudFront distribution. The development team wants to decouple the user authentication process for the application so that the application servers can just focus on the business logic.\n\nAs a Developer Associate, which of the following solutions would you recommend to address this use-case with minimal development effort?\n",answers:[{text:"Use Cognito Authentication via Cognito User Pools for your Application Load Balancer",isCorrect:!0},{text:"Use Cognito Authentication via Cognito Identity Pools for your Application Load Balancer",isCorrect:!1},{text:"Use Cognito Authentication via Cognito User Pools for your CloudFront distribution",isCorrect:!1},{text:"Use Cognito Authentication via Cognito Identity Pools for your CloudFront distribution",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Use Cognito Authentication via Cognito User Pools for your Application Load Balancer</strong>\n\nApplication Load Balancer can be used to securely authenticate users for accessing your applications. This enables you to offload the work of authenticating users to your load balancer so that your applications can focus on their business logic. You can use Cognito User Pools to authenticate users through well-known social IdPs, such as Amazon, Facebook, or Google, through the user pools supported by Amazon Cognito or through corporate identities, using SAML, LDAP, or Microsoft AD, through the user pools supported by Amazon Cognito. You configure user authentication by creating an authenticate action for one or more listener rules. The authenticate-cognito and authenticate-oidc action types are supported only with HTTPS listeners.\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html</a>\n\nPlease make sure that you adhere to the following configurations while using CloudFront distribution in front of your Application Load Balancer:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html</a>\n\nExam Alert:\n\nPlease review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i3.jpg">\nvia - <a href="https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a>\n\nIncorrect options:\n\n<strong>Use Cognito Authentication via Cognito Identity Pools for your Application Load Balancer</strong> - There is no such thing as using Cognito Authentication via Cognito Identity Pools for managing user authentication for the application. Application-specific user authentication can be provided via Cognito User Pools. Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token.\n\n<strong>Use Cognito Authentication via Cognito User Pools for your CloudFront distribution</strong> - You cannot directly integrate Cognito User Pools with CloudFront distribution as you have to create a separate Lambda@Edge function to accomplish the authentication via Cognito User Pools. This involves additional development effort, so this option is not the best fit for the given use-case.\n\n<strong>Use Cognito Authentication via Cognito Identity Pools for your CloudFront distribution</strong> - You cannot use Cognito Identity Pools for managing user authentication, so this option is not correct.\n\nReferences:\n\n<a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html</a>\n\n<a href="https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a>\n\n<a href="https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/">https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/</a>\n'},{question:"A video encoding application running on an EC2 instance takes about 20 seconds on average to process each raw footage file. The application picks the new job messages from an SQS queue. The development team needs to account for the use-case when the video encoding process takes longer than usual so that the same raw footage is not processed by multiple consumers.\n\nAs a Developer Associate, which of the following solutions would you recommend to address this use-case?\n",answers:[{text:"Use ChangeMessageVisibility action to extend a message's visibility timeout",isCorrect:!0},{text:"Use DelaySeconds action to delay a message's visibility timeout",isCorrect:!1},{text:"Use WaitTimeSeconds action to short poll and extend a message's visibility timeout",isCorrect:!1},{text:"Use WaitTimeSeconds action to long poll and extend a message's visibility timeout",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Use ChangeMessageVisibility action to extend a message\'s visibility timeout</strong>\n\nAmazon SQS uses a visibility timeout to prevent other consumers from receiving and processing the same message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours.\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q5-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a>\n\nFor example, you have a message with a visibility timeout of 5 minutes. After 3 minutes, you call ChangeMessageVisibility with a timeout of 10 minutes. You can continue to call ChangeMessageVisibility to extend the visibility timeout to the maximum allowed time. If you try to extend the visibility timeout beyond the maximum, your request is rejected. So, for the given use-case, the application can set the initial visibility timeout to 1 minute and then continue to update the ChangeMessageVisibility value if required.\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q5-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a>\n\nIncorrect options:\n\n<strong>Use DelaySeconds action to delay a message\'s visibility timeout</strong> - Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. To set delay seconds on individual messages, rather than on an entire queue, use message timers to allow Amazon SQS to use the message timer\'s DelaySeconds value instead of the delay queue\'s DelaySeconds value. You cannot use DelaySeconds to alter the visibility of a message which has been picked for processing.\n\n<strong>Use WaitTimeSeconds action to short poll and extend a message\'s visibility timeout</strong>\n\n<strong>Use WaitTimeSeconds action to long poll and extend a message\'s visibility timeout</strong>\n\nAmazon SQS provides short polling and long polling to receive messages from a queue. Both these options have been added as distractors as WaitTimeSeconds (via short polling or long polling) cannot be used to influence the message\'s visibility.\n\nReferences:\n\n<a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a>\n\n<a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html</a>\n'},{question:"The development team at an IT company has configured an Application Load Balancer (ALB) with a Lambda function A as the target but the Lambda function A is not able to process any request from the ALB. Upon investigation, the team finds that there is another Lambda function B in the AWS account that is exceeding the concurrency limits.\n\nHow can the development team address this issue?\n",answers:[{text:"Set up provisioned concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit",isCorrect:!1},{text:"Set up reserved concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit",isCorrect:!0},{text:"Use an API Gateway instead of an Application Load Balancer (ALB) for Lambda function A",isCorrect:!1},{text:"Use a Cloudfront Distribution instead of an Application Load Balancer (ALB) for Lambda function A",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Set up reserved concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit</strong>\n\nConcurrency is the number of requests that a Lambda function is serving at any given time. If a Lambda function is invoked again while a request is still being processed, another instance is allocated, which increases the function\'s concurrency.\n\nTo ensure that a function can always reach a certain level of concurrency, you can configure the function with reserved concurrency. When a function has reserved concurrency, no other function can use that concurrency. More importantly, reserved concurrency also limits the maximum concurrency for the function, and applies to the function as a whole, including versions and aliases.\n\nPlease review this note to understand how reserved concurrency works:\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q6-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved</a>\n\nTherefore using reserved concurrency for Lambda function B would limit its maximum concurrency and allow Lambda function A to execute without getting throttled.\n\nIncorrect options:\n\n<strong>Set up provisioned concurrency for the Lambda function B so that it throttles if it goes above a certain concurrency limit</strong> - You should use provisioned concurrency to enable your function to scale without fluctuations in latency. By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with very low latency. Provisioned concurrency is not used to limit the maximum concurrency for a given Lambda function, so this option is incorrect.\n\n<strong>Use an API Gateway instead of an Application Load Balancer (ALB) for Lambda function A</strong> - This has been added as a distractor as using an API Gateway for Lambda function A has no bearing on limiting the concurrency of Lambda function B, so this option is incorrect.\n\n<strong>Use a Cloudfront Distribution instead of an Application Load Balancer (ALB) for Lambda function A</strong> - When you associate a CloudFront distribution with a Lambda function (known as Lambda@Edge), CloudFront intercepts requests and responses at CloudFront edge locations and runs the function. Again, this has no bearing on limiting the concurrency of Lambda function B, so this option is incorrect.\n\nReferences:\n\n<a href="https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved</a>\n\n<a href="https://aws.amazon.com/blogs/networking-and-content-delivery/lambda-functions-as-targets-for-application-load-balancers/">https://aws.amazon.com/blogs/networking-and-content-delivery/lambda-functions-as-targets-for-application-load-balancers/</a>\n'},{question:"An e-commerce company has a fleet of EC2 based web servers running into very high CPU utilization issues. The development team has determined that serving secure traffic via HTTPS is a major contributor to the high CPU load.\n\nWhich of the following steps can take the high CPU load off the web servers? (Select two)\n",answers:[{text:"Create an HTTP listener on the Application Load Balancer with SSL termination",isCorrect:!1},{text:"Configure an SSL/TLS certificate on an Application Load Balancer via AWS Certificate Manager (ACM)",isCorrect:!0},{text:"Create an HTTPS listener on the Application Load Balancer with SSL termination",isCorrect:!0},{text:"Create an HTTPS listener on the Application Load Balancer with SSL pass-through",isCorrect:!1},{text:"Create an HTTP listener on the Application Load Balancer with SSL pass-through",isCorrect:!1}],explanation:'Correct option:\n\n"Configure an SSL/TLS certificate on an Application Load Balancer via AWS Certificate Manager (ACM)"\n\n"Create an HTTPS listener on the Application Load Balancer with SSL termination"\n\nAn Application load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. A listener checks for connection requests from clients, using the protocol and port that you configure. The rules that you define for a listener determine how the load balancer routes requests to its registered targets. Each rule consists of a priority, one or more actions, and one or more conditions.\n\n<img src="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/images/component_architecture.png">\nvia - <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a>\n\nTo use an HTTPS listener, you must deploy at least one SSL/TLS server certificate on your load balancer. You can create an HTTPS listener, which uses encrypted connections (also known as SSL offload). This feature enables traffic encryption between your load balancer and the clients that initiate SSL or TLS sessions. As the EC2 instances are under heavy CPU load, the load balancer will use the server certificate to terminate the front-end connection and then decrypt requests from clients before sending them to the EC2 instances.\n\nPlease review this resource to understand how to associate an ACM SSL/TLS certificate with an Application Load Balancer:\n<a href="https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/">https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/</a>\n\nIncorrect options:\n\n"Create an HTTPS listener on the Application Load Balancer with SSL pass-through" - If you use an HTTPS listener with SSL pass-through, then the EC2 instances would continue to be under heavy CPU load as they would still need to decrypt the secure traffic\nat the instance level. Hence this option is incorrect.\n\n"Create an HTTP listener on the Application Load Balancer with SSL termination"\n\n"Create an HTTP listener on the Application Load Balancer with SSL pass-through"\n\nYou cannot have an HTTP listener for an Application Load Balancer to support SSL termination or SSL pass-through, so both these options are incorrect.\n\nReferences:\n\n<a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html</a>\n\n<a href="https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/">https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/</a>\n'},{question:"An IT company has a HealthCare application with data security requirements such that the encryption key must be stored in a custom application running on-premises. The company wants to offload the data storage as well as the encryption process to Amazon S3 but continue to use the existing encryption keys.\n\nWhich of the following S3 encryption options allows the company to leverage Amazon S3 for storing data with given constraints?\n",answers:[{text:"Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)",isCorrect:!1},{text:"Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS)",isCorrect:!1},{text:"Server-Side Encryption with Customer-Provided Keys (SSE-C)",isCorrect:!0},{text:"Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Server-Side Encryption with Customer-Provided Keys (SSE-C)</strong>\n\nYou have the following options for protecting data at rest in Amazon S3:\n\nServer-Side Encryption – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.\n\nClient-Side Encryption – Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.\n\nFor the given use-case, the company wants to manage the encryption keys via its custom application and let S3 manage the encryption, therefore you must use Server-Side Encryption with Customer-Provided Keys (SSE-C).\n\nPlease review these three options for Server Side Encryption on S3:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q8-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a>\n\nIncorrect options:\n\n<strong>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. So this option is incorrect.\n\n<strong>Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS)</strong> - Server-Side Encryption with Customer Master Keys (CMKs) stored in AWS Key Management Service (SSE-KMS) is similar to SSE-S3. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Additionally, you can create and manage customer-managed CMKs or use AWS managed CMKs that are unique to you, your service, and your Region.\n\n<strong>Client-Side Encryption with data encryption is done on the client-side before sending it to Amazon S3</strong> - You can encrypt the data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a>\n'},{question:"A data analytics company wants to use clickstream data for Machine Learning tasks, develop algorithms, and create visualizations and dashboards to support the business stakeholders. Each of these business units works independently and would need real-time access to this clickstream data for their applications.\n\nAs a Developer Associate, which of the following AWS services would you recommend such that it provides a highly available and fault-tolerant solution to capture the clickstream events from the source and then provide a simultaneous feed of the data stream to the consumer applications?\n",answers:[{text:"AWS Kinesis Data Streams",isCorrect:!0},{text:"AWS Kinesis Data Firehose",isCorrect:!1},{text:"AWS Kinesis Data Analytics",isCorrect:!1},{text:"Amazon SQS",isCorrect:!1}],explanation:'Correct option:\n\n<strong>AWS Kinesis Data Streams</strong>\n\nAmazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.\n\nAmazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).\nAmazon Kinesis Data Streams is recommended when you need the ability for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another application that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently.\n\nKDS provides the ability for multiple applications to consume the same stream concurrently\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q9-i1.jpg">\nvia - <a href="https://aws.amazon.com/kinesis/data-streams/faqs/">https://aws.amazon.com/kinesis/data-streams/faqs/</a>\n\nIncorrect options:\n\n<strong>AWS Kinesis Data Firehose</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect.\n\n<strong>AWS Kinesis Data Analytics</strong> - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time.  You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect.\n\n<strong>Amazon SQS</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers at the same time, therefore this option is incorrect.\n\nExam alert:\n\nPlease remember that Kinesis Data Firehose is used to load streaming data into data stores (Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk) whereas Kinesis Data Streams provides support for real-time processing of streaming data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple downstream Amazon Kinesis Applications.\n\nReferences:\n\n<a href="https://aws.amazon.com/kinesis/data-streams/faqs/">https://aws.amazon.com/kinesis/data-streams/faqs/</a>\n\n<a href="https://aws.amazon.com/kinesis/data-firehose/faqs/">https://aws.amazon.com/kinesis/data-firehose/faqs/</a>\n\n<a href="https://aws.amazon.com/kinesis/data-analytics/faqs/">https://aws.amazon.com/kinesis/data-analytics/faqs/</a>\n'},{question:"A retail company manages its IT infrastructure on AWS Cloud via Elastic Beanstalk. The development team at the company is planning to deploy the next version with MINIMUM application downtime and the ability to rollback quickly in case deployment goes wrong.\n\nAs a Developer Associate, which of the following options would you recommend to the development team?\n",answers:[{text:"Deploy the new application version using 'All at once' deployment policy",isCorrect:!1},{text:"Deploy the new application version using 'Rolling' deployment policy",isCorrect:!1},{text:"Deploy the new version to a separate environment via Blue/Green Deployment, and then swap Route 53 records of the two environments to redirect traffic to the new version",isCorrect:!0},{text:"Deploy the new application version using 'Rolling with additional batch' deployment policy",isCorrect:!1}],explanation:"Correct option:\n\n<strong>Deploy the new version to a separate environment via Blue/Green Deployment, and then swap Route 53 records of the two environments to redirect traffic to the new version</strong>\n\nWith deployment policies such as 'All at once', AWS Elastic Beanstalk performs an in-place update when you update your application versions and your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs (via Route 53) of the two environments to redirect traffic to the new version instantly. In case of any deployment issues, the rollback process is very quick via swapping the URLs for the two environments.\n\nOverview of Elastic Beanstalk Deployment Policies:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt1-q10-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a>\n\nIncorrect options:\n\n<strong>Deploy the new application version using 'All at once' deployment policy</strong> - Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time. So this option is not correct.\n\n<strong>Deploy the new application version using 'Rolling' deployment policy</strong> - This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. However rollback process is via manual redeploy, so it's not as quick as the Blue/Green deployment.\n\n<strong>Deploy the new application version using 'Rolling with additional batch' deployment policy</strong> - This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. However rollback process is via manual redeploy, so it's not as quick as the Blue/Green deployment.\n\nReference:\n\n<a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a>\n"},{question:"You're a developer for 'Movie Gallery', a company that just migrated to the cloud. A database must be created using NoSQL technology to hold movies that are listed for public viewing. You are taking an important step in designing the database with DynamoDB and need to choose the appropriate partition key.\n\nWhich of the following unique attributes satisfies this requirement?\n",answers:[{text:"lead_actor_name",isCorrect:!1},{text:"producer_name",isCorrect:!1},{text:"movie_id",isCorrect:!0},{text:"movie_language",isCorrect:!1}],explanation:'Correct option:\n\nDynamoDB stores data as groups of attributes, known as items. Items are similar to rows or records in other database systems. DynamoDB stores and retrieves each item based on the primary key value, which must be unique. Items are distributed across 10-GB storage units, called partitions (physical storage internal to DynamoDB).\n\nDynamoDB uses the partition key’s value as an input to an internal hash function. The output from the hash function determines the partition in which the item is stored. Each item’s location is determined by the hash value of its partition key.\n\nPlease see these details for the DynamoDB Partition Keys:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q11-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/">https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/</a>\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q11-i2.jpg">\nvia - <a href="https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/">https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/</a>\n\n<strong>movie_id</strong>\n\nThe movie_id attribute has high-cardinality across the entire collection of the movie database, hence it is the most suitable candidate for the partition key in this use case.\n\nIncorrect options:\n\n<strong>producer_name</strong>  - Does not qualify because the attribute will have duplicate values (and therefore low cardinality)\n\n<strong>lead_actor_name</strong> - Does not qualify because the attribute will have duplicate values (and therefore low cardinality)\n\n<strong>movie_language</strong> - Does not qualify because the attribute will have duplicate values (and therefore low cardinality)\n\nReference:\n\n<a href="https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/">https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/</a>\n'},{question:"A company wants to automate the creation of ECS clusters using CloudFormation. The process has worked for a while, but after creating task definitions and assigning roles, the development team discovers that the tasks for containers are not using the permissions assigned to them.\n\nWhich ECS config must be set in /etc/ecs/ecs.config to allow ECS tasks to use IAM roles?\n",answers:[{text:"ECS_AVAILABLE_LOGGING_DRIVERS",isCorrect:!1},{text:"ECS_ENGINE_AUTH_DATA",isCorrect:!1},{text:"ECS_ENABLE_TASK_IAM_ROLE",isCorrect:!0},{text:"ECS_CLUSTER",isCorrect:!1}],explanation:'Correct option:\n\n<strong>ECS_ENABLE_TASK_IAM_ROLE</strong>\n\nThis configuration item is used to enable IAM roles for tasks for containers with the bridge and default network modes.\n\nIncorrect options:\n\n<strong>ECS_ENGINE_AUTH_DATA</strong> - This refers to the authentication data within a Docker configuration file, so this is not the correct option.\n\n<strong>ECS_AVAILABLE_LOGGING_DRIVERS</strong> - The Amazon ECS container agent running on a container instance must register the logging drivers available on that instance with this variable. This configuration item refers to the logging driver.\n\n<strong>ECS_CLUSTER</strong> - This refers to the ECS cluster that the ECS agent should check into. This is passed to the container instance at launch through Amazon EC2 user data.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html</a>\n'},{question:"As an AWS Certified Developer Associate, you are writing a CloudFormation template in YAML. The template consists of an EC2 instance creation and one RDS resource. Once your resources are created you would like to output the connection endpoint for the RDS database.\n\nWhich intrinsic function returns the value needed?\n",answers:[{text:"!Sub",isCorrect:!1},{text:"!Ref",isCorrect:!1},{text:"!GetAtt",isCorrect:!0},{text:"!FindInMap",isCorrect:!1}],explanation:'Correct option:\n\nAWS CloudFormation provides several built-in functions that help you manage your stacks. Intrinsic functions are used in templates to assign values to properties that are not available until runtime.\n\n<strong>!GetAtt</strong> - The Fn::GetAtt intrinsic function returns the value of an attribute from a resource in the template. This example snippet returns a string containing the DNS name of the load balancer with the logical name myELB -\nYML :   !GetAtt myELB.DNSName\nJSON :   "Fn::GetAtt" : [ "myELB" , "DNSName" ]\n\nIncorrect options:\n\n<strong>!Sub</strong> - The intrinsic function Fn::Sub substitutes variables in an input string with values that you specify. In your templates, you can use this function to construct commands or outputs that include values that aren\'t available until you create or update a stack.\n\n<strong>!Ref</strong> - The intrinsic function Ref returns the value of the specified parameter or resource.\n\n<strong>!FindInMap</strong> - The intrinsic function Fn::FindInMap returns the value corresponding to keys in a two-level map that is declared in the Mappings section. For example, you can use this in the Mappings section that contains a single map, RegionMap, that associates AMIs with AWS regions.\n\nReferences:\n\n<a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html</a>\n\n<a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html</a>\n'},{question:"You are a developer working at a cloud company that embraces serverless. You have performed your initial deployment and would like to work towards adding API Gateway stages and associate them with existing deployments. Your stages will include prod, test, and dev and will need to match a Lambda function variant that can be updated over time.\n\nWhich of the following features must you add to achieve this? (select two)\n",answers:[{text:"Lambda X-Ray integration",isCorrect:!1},{text:"Stage Variables",isCorrect:!0},{text:"Lambda Versions",isCorrect:!1},{text:"Lambda Aliases",isCorrect:!0},{text:"Mapping Templates",isCorrect:!1}],explanation:'Correct options:\n\n<strong>Stage Variables</strong>\n\nStage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of an API. They act like environment variables and can be used in your API setup and mapping templates. With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints.\n\nFor example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, http://example.com). In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage and calls a different web host (for example, beta.example.com).\n\n<strong>Lambda Aliases</strong>\n\nA Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN.\n\nLambda Aliases allow you to create a "mutable" Lambda version that points to whatever version you want in the backend. This allows you to have a "dev", "test", prod" Lambda alias that can remain stable over time.\n\nIncorrect options:\n\n<strong>Lambda Versions</strong> - Versions are immutable and cannot be updated over time. So this option is not correct.\n\n<strong>Lambda X-Ray integration</strong> - This is good for tracing and debugging requests so it can be looked at as a good option for troubleshooting issues in the future. This is not the right fit for the given use-case.\n\n<strong>Mapping Templates</strong> - Mapping template overrides provides you with the flexibility to perform many-to-one parameter mappings; override parameters after standard API Gateway mappings have been applied; conditionally map parameters based on body content or other parameter values; programmatically create new parameters on the fly, and override status codes returned by your integration endpoint. This is not the right fit for the given use-case.\n\nReferences:\n\n<a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html</a>\n\n<a href="https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html">https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html</a>\n\n<a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html</a>\n'},{question:"A company has AWS Lambda functions where each is invoked by other AWS services such as Amazon Kinesis Data Firehose, Amazon API Gateway, Amazon Simple Storage Service, or Amazon CloudWatch Events. What these Lambda functions have in common is that they process heavy workloads such as big data analysis, large file processing, and statistical computations.\n\nWhat should you do to improve the performance of your AWS Lambda functions without changing your code?\n",answers:[{text:"Change the instance type for your Lambda function",isCorrect:!1},{text:"Increase the RAM assigned to your Lambda function",isCorrect:!0},{text:"Change your Lambda function runtime to use Golang",isCorrect:!1},{text:"Increase the Lambda function timeout",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Increase the RAM assigned to your Lambda function</strong>\n\nAWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\n\nIn the AWS Lambda resource model, you choose the amount of memory you want for your function which allocates proportional CPU power and other resources. This means you will have access to more compute power when you choose one of the new larger settings. You can set your memory in 64MB increments from 128MB to 3008MB. You access these settings when you create a function or update its configuration. The settings are available using the AWS Management Console, AWS CLI, or SDKs.\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q15-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html">https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html</a>\n\nTherefore, by increasing the amount of memory available to the Lambda functions, you can run the compute-heavy workflows.\n\nIncorrect options:\n\n<strong>Change the instance type for your Lambda function</strong> - Instance types apply to the EC2 service and not to Lambda function as its a serverless service.\n\n<strong>Change your Lambda function runtime to use Golang</strong> - This changes programming language which requires code changes, so this option is not correct. Besides, changing the runtime may not even address the performance issues.\n\n<strong>Increase the Lambda function timeout</strong> - This option would increase the amount of time for which the Lambda function executes, which may help in case you have some heavy processing, but won\'t help with the actual performance of your Lambda function.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html">https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html</a>\n'},{question:"A cyber forensics application, running behind an ALB, wants to analyze patterns for the client IPs.\n\nWhich of the following headers can be used for this requirement?\n",answers:[{text:"X-Forwarded-Proto",isCorrect:!1},{text:"X-Forwarded-Port",isCorrect:!1},{text:"X-Forwarded-IP",isCorrect:!1},{text:"X-Forwarded-For",isCorrect:!0}],explanation:'Correct option:\n\nHTTP requests and HTTP responses use header fields to send information about the HTTP messages. Header fields are colon-separated name-value pairs that are separated by a carriage return (CR) and a line feed (LF).\n\n<strong>X-Forwarded-For</strong> - The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header.\n\nIncorrect options:\n\n<strong>X-Forwarded-Proto</strong> - The X-Forwarded-Proto request header helps you identify the protocol (HTTP or HTTPS) that a client used to connect to your load balancer. Your server access logs contain only the protocol used between the server and the load balancer; they contain no information about the protocol used between the client and the load balancer. To determine the protocol used between the client and the load balancer, use the X-Forwarded-Proto request header.\n\n<strong>X-Forwarded-Port</strong> - The X-Forwarded-Port request header helps you identify the destination port that the client used to connect to the load balancer.\n\n<strong>X-Forwarded-IP</strong> - This is a made-up option and has been added as a distractor.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html#x-forwarded-for">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html#x-forwarded-for</a>\n'},{question:"A new recruit is trying to understand the nuances of EC2 Auto Scaling. As an AWS Certified Developer Associate, you have been asked to mentor the new recruit.\n\nCan you identify and explain the correct statements about Auto Scaling to the new recruit? (Select two).\n",answers:[{text:"EC2 Auto Scaling groups are regional constructs. They span across Availability Zones and AWS regions",isCorrect:!1},{text:"Amazon EC2 Auto Scaling cannot add a volume to an existing instance if the existing volume is approaching capacity",isCorrect:!0},{text:"Every time you create an Auto Scaling group from an existing instance, it creates a new AMI (Amazon Machine Image)",isCorrect:!1},{text:"Amazon EC2 Auto Scaling works with both Application Load Balancers and Network Load Balancers",isCorrect:!0},{text:"You cannot use Amazon EC2 Auto Scaling for health checks (to replace unhealthy instances) if you are not using Elastic Load Balancing (ELB)",isCorrect:!1}],explanation:'Correct options:\n\nAmazon EC2 Auto Scaling is a fully managed service designed to launch or terminate Amazon EC2 instances automatically to help ensure you have the correct number of Amazon EC2 instances available to handle the load for your application.\n\n<strong>Amazon EC2 Auto Scaling cannot add a volume to an existing instance if the existing volume is approaching capacity</strong> - A volume is attached to a new instance when it is added. Amazon EC2 Auto Scaling doesn\'t automatically add a volume when the existing one is approaching capacity. You can use the EC2 API to add a volume to an existing instance.\n\n<strong>Amazon EC2 Auto Scaling works with both Application Load Balancers and Network Load Balancers</strong> - Amazon EC2 Auto Scaling works with Application Load Balancers and Network Load Balancers including their health check feature.\n\nIncorrect options:\n\n<strong>EC2 Auto Scaling groups are regional constructs. They span across Availability Zones and AWS regions</strong> - This is an incorrect statement. EC2 Auto Scaling groups are regional constructs. They can span Availability Zones, but not AWS regions.\n\n<strong>Every time you create an Auto Scaling group from an existing instance, it creates a new AMI (Amazon Machine Image)</strong> - This is an incorrect statement. When you create an Auto Scaling group from an existing instance, it does not create a new AMI.\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q17-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html</a>\n\n<strong>You cannot use Amazon EC2 Auto Scaling for health checks (to replace unhealthy instances) if you are not using Elastic Load Balancing (ELB)</strong> - This is an incorrect statement. You don\'t have to use ELB to use Auto Scaling. You can use the EC2 health check to identify and replace unhealthy instances.\n\nReferences:\n\n<a href="https://aws.amazon.com/ec2/autoscaling/faqs/">https://aws.amazon.com/ec2/autoscaling/faqs/</a>\n\n<a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html</a>\n'},{question:"A company that specializes in cloud communications platform as a service allows software developers to programmatically use their services to send and receive text messages. The initial platform did not have a scalable architecture as all components were hosted on one server and should be redesigned for high availability and scalability.\n\nWhich of the following options can be used to implement the new architecture? (select two)\n",answers:[{text:"ALB + ECS",isCorrect:!0},{text:"EBS + RDS",isCorrect:!1},{text:"SES + S3",isCorrect:!1},{text:"CloudWatch + CloudFront",isCorrect:!1},{text:"API Gateway + Lambda",isCorrect:!0}],explanation:'Correct options:\n\n<strong>ALB + ECS</strong>\n\nAmazon Elastic Container Service (ECS) is a highly scalable, high-performance container management service that supports Docker containers and allows you to easily run applications on a managed cluster of Amazon EC2 instances.\n\nHow ECS Works:\n<img src="https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png">\nvia - <a href="https://aws.amazon.com/ecs/">https://aws.amazon.com/ecs/</a>\n\nElastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones.\n\nWhen you use ECS with a load balancer such as ALB deployed across multiple Availability Zones, it helps provide a scalable and highly available REST API.\n\n<strong>API Gateway + Lambda</strong>\n\nAmazon API Gateway is a fully managed service that makes it easy for developers to publish, maintain, monitor, and secure APIs at any scale. Using API Gateway, you can create an API that acts as a “front door” for applications to access data, business logic, or functionality from your back-end services, such as EC2 or Lambda functions.\n\nHow API Gateway Works:\n<img src="https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png">\nvia - <a href="https://aws.amazon.com/api-gateway/">https://aws.amazon.com/api-gateway/</a>\n\nAWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\n\nHow Lambda function works:\n<img src="https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png">\nvia - <a href="https://aws.amazon.com/lambda/">https://aws.amazon.com/lambda/</a>\n\nAPI Gateway and Lambda help achieve the same purpose integrating some capabilities such as authentication in a serverless fashion, with fully scalable and highly available architectures.\n\nIncorrect options:\n\n<strong>SES + S3</strong> - The combination of these services only provide email and object storage services.\n\n<strong>CloudWatch + CloudFront</strong> - The combination of these services only provide monitoring and fast content delivery network (CDN) services.\n\n<strong>EBS + RDS</strong> - The combination of these services only provide elastic block storage and database services.\n\nReferences:\n\n<a href="https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-4/">https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-4/</a>\n\n<a href="https://aws.amazon.com/blogs/compute/microservice-delivery-with-amazon-ecs-and-application-load-balancers/">https://aws.amazon.com/blogs/compute/microservice-delivery-with-amazon-ecs-and-application-load-balancers/</a>\n'},{question:"As an AWS Certified Developer Associate, you have been hired to consult with a company that uses the NoSQL database for mobile applications. The developers are using DynamoDB to perform operations such as GetItem but are limited in knowledge. They would like to be more efficient with retrieving some attributes rather than all.\n\nWhich of the following recommendations would you provide?\n",answers:[{text:"Use a FilterExpression",isCorrect:!1},{text:"Specify a ProjectionExpression",isCorrect:!0},{text:"Use the --query parameter",isCorrect:!1},{text:"Use a Scan",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Specify a ProjectionExpression</strong>: A projection expression is a string that identifies the attributes you want. To retrieve a single attribute, specify its name. For multiple attributes, the names must be comma-separated.\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q19-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html</a>\n\nIncorrect options:\n\n<strong>Use a FilterExpression</strong> - If you need to further refine the Query results, you can optionally provide a filter expression. A filter expression determines which items within the Query results should be returned to you. All of the other results are discarded. A filter expression is applied after Query finishes, but before the results are returned. Therefore, a Query consumes the same amount of read capacity, regardless of whether a filter expression is present. A Query operation can retrieve a maximum of 1 MB of data. This limit applies before the filter expression is evaluated.\n\n<strong>Use the --query parameter</strong> - The Query operation in Amazon DynamoDB finds items based on primary key values. You must provide the name of the partition key attribute and a single value for that attribute. The Query returns all items with that partition key value. Optionally, you can provide a sort key attribute and use a comparison operator to refine the search results.\n\n<strong>Use a Scan</strong> - A Scan operation in Amazon DynamoDB reads every item in a table or a secondary index. By default, a Scan operation returns all of the data attributes for every item in the table or index. You can also use the ProjectionExpression parameter so that Scan only returns some of the attributes, rather than all of them.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html</a>\n'},{question:"A developer at a university is encrypting a large XML payload transferred over the network using AWS KMS and wants to test the application before going to production.\n\nWhat is the maximum data size supported by AWS KMS?\n",answers:[{text:"16KB",isCorrect:!1},{text:"1MB",isCorrect:!1},{text:"10MB",isCorrect:!1},{text:"4KB",isCorrect:!0}],explanation:'Correct option:\n\n<strong>4 KB</strong>\n\nYou can encrypt up to 4 kilobytes (4096 bytes) of arbitrary data such as an RSA key, a database password, or other sensitive information.\n\nWhile AWS KMS does support sending data up to 4 KB to be encrypted directly, envelope encryption can offer significant performance benefits. When you encrypt data directly with AWS KMS it must be transferred over the network. Envelope encryption reduces the network load since only the request and delivery of the much smaller data key go over the network. The data key is used locally in your application or encrypting AWS service, avoiding the need to send the entire block of data to AWS KMS and suffer network latency.\n\nIncorrect options:\n\n<strong>1MB</strong> - For anything over 4 KB, you may want to look at envelope encryption\n\n<strong>10MB</strong> - For anything over 4 KB, you may want to look at envelope encryption\n\n<strong>16KB</strong> - For anything over 4 KB, you may want to look at envelope encryption\n\nReference:\n\n<a href="https://aws.amazon.com/kms/faqs/">https://aws.amazon.com/kms/faqs/</a>\n'},{question:"As a Developer, you are working on a mobile application that utilizes Amazon Simple Queue Service (SQS) for sending messages to downstream systems for further processing. One of the requirements is that the messages should be stored in the queue for a period of 12 days.\n\nHow will you configure this requirement?\n",answers:[{text:"Enable Long Polling for the SQS queue",isCorrect:!1},{text:"The maximum retention period of SQS messages is 7 days, therefore retention period of 12 days is not possible",isCorrect:!1},{text:"Change the queue message retention setting",isCorrect:!0},{text:"Use a FIFO SQS queue",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Change the queue message retention setting</strong> - Amazon SQS automatically deletes messages that have been in a queue for more than the maximum message retention period. The default message retention period is 4 days. However, you can set the message retention period to a value from 60 seconds to 1,209,600 seconds (14 days) using the SetQueueAttributes action.\n\nMore info here:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q21-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html</a>\n\nIncorrect options:\n\n<strong>Enable Long Polling for the SQS queue</strong> - Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren\'t included in a response). This feature is not useful for the current use case.\n\n<strong>The maximum retention period of SQS messages is 7 days, therefore retention period of 12 days is not possible</strong> - This is an incorrect statement. Retention period of up to 14 days is possible.\n\n<strong>Use a FIFO SQS queue</strong> - FIFO (First-In-First-Out) queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can\'t be tolerated. This is not useful for the current scenario.\n\nReferences:\n\n<a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html</a>\n\n<a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html</a>\n\n<a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</a>\n'},{question:"A company uses microservices-based infrastructure to process the API calls from clients, perform request filtering and cache requests using the AWS API Gateway. Users report receiving 501 error code and you have been contacted to find out what is failing.\n\nWhich service will you choose to help you troubleshoot?\n",answers:[{text:"Use CloudTrail service",isCorrect:!1},{text:"Use API Gateway service",isCorrect:!1},{text:"Use CloudWatch service",isCorrect:!1},{text:"Use X-Ray service",isCorrect:!0}],explanation:'Correct option:\n\n<strong>Use X-Ray service</strong> - AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components. You can use X-Ray to analyze both applications in development and in production, from simple three-tier applications to complex microservices applications consisting of thousands of services.\n\nX-Ray Overview:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i1.jpg">\nvia - <a href="https://aws.amazon.com/xray/">https://aws.amazon.com/xray/</a>\n\nAWS X-Ray creates a map of services used by your application with trace data that you can use to drill into specific services or issues. This provides a view of connections between services in your application and aggregated data for each service, including average latency and failure rates. You can create dependency trees, perform cross-availability zone or region call detections, and more.\n\nX-Ray Service maps:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i2.jpg">\nvia - <a href="https://aws.amazon.com/xray/features/">https://aws.amazon.com/xray/features/</a>\n\nX-Ray Traces:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q22-i3.jpg">\nvia - <a href="https://aws.amazon.com/xray/features/">https://aws.amazon.com/xray/features/</a>\n\nIncorrect options:\n\n<strong>Use CloudTrail service</strong> - With CloudTrail, you can get a history of AWS API calls for your account - including API calls made via the AWS Management Console, AWS SDKs, command-line tools, and higher-level AWS services (such as AWS CloudFormation). This is a very useful service for general monitoring and tracking. But, it will not give a detailed analysis of the outcome of microservices or drill into specific issues. For the current use case, X-Ray offers a better solution.\n\n<strong>Use API Gateway service</strong> - Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API Gateway will not be able to drill into the flow between different microservices or their issues.\n\n<strong>Use CloudWatch service</strong> - Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, and on-premises applications and infrastructure resources. CloudWatch can collect numbers and respond to AWS service-related events, but it can\'t help you debug microservices specific issues on AWS.\n\nReferences:\n\n<a href="https://aws.amazon.com/cloudtrail/">https://aws.amazon.com/cloudtrail/</a>\n\n<a href="https://aws.amazon.com/api-gateway/">https://aws.amazon.com/api-gateway/</a>\n\n<a href="https://aws.amazon.com/cloudwatch/features/">https://aws.amazon.com/cloudwatch/features/</a>\n'},{question:"A Company uses a large set of EBS volumes for their fleet of Amazon EC2 instances. As an AWS Certified Developer Associate, your help has been requested to understand the security features of the EBS volumes. The company does not want to build or maintain their own encryption key management infrastructure.\n\nCan you help them understand what works for Amazon EBS encryption? (Select two)\n",answers:[{text:"Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region",isCorrect:!0},{text:"You can encrypt an existing unencrypted volume or snapshot by using AWS Key Management Service (KMS) AWS SDKs",isCorrect:!1},{text:"A snapshot of an encrypted volume can be encrypted or unencrypted",isCorrect:!1},{text:"A volume restored from an encrypted snapshot, or a copy of an encrypted snapshot is always encrypted",isCorrect:!0},{text:"Encryption by default is an AZ specific setting. If you enable it for an AZ, you cannot disable it for individual volumes or snapshots in that AZ",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region</strong> - You can configure your AWS account to enforce the encryption of the new EBS volumes and snapshot copies that you create. Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region.\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q23-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a>\n\n<strong>A volume restored from an encrypted snapshot, or a copy of an encrypted snapshot, is always encrypted</strong> - By default, the CMK that you selected when creating a volume encrypts the snapshots that you make from the volume and the volumes that you restore from those encrypted snapshots. You cannot remove encryption from an encrypted volume or snapshot, which means that a volume restored from an encrypted snapshot, or a copy of an encrypted snapshot is always encrypted.\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q23-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a>\n\nIncorrect options:\n\n<strong>You can encrypt an existing unencrypted volume or snapshot by using AWS Key Management Service (KMS) AWS SDKs</strong> - This is an incorrect statement. There is no direct way to encrypt an existing unencrypted volume or snapshot. You can encrypt an unencrypted snapshot by copying and enabling encryption while copying the snapshot. To encrypt an EBS volume, you need to create a snapshot and then encrypt the snapshot as described earlier. From this new encrypted snapshot, you can then create an encrypted volume.\n\n<strong>A snapshot of an encrypted volume can be encrypted or unencrypted</strong> - This is an incorrect statement. You cannot remove encryption from an encrypted volume or snapshot, which means that a volume restored from an encrypted snapshot, or a copy of an encrypted snapshot is always encrypted.\n\n<strong>Encryption by default is an AZ specific setting. If you enable it for an AZ, you cannot disable it for individual volumes or snapshots in that AZ</strong> - This is an incorrect statement. Encryption by default is a Region-specific setting. If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region.\n\nReferences:\n\n<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a>\n\n<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#encrypt-unencrypted">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#encrypt-unencrypted</a>\n'},{question:"A multi-national company runs its technology operations on AWS Cloud. As part of their storage solution, they use a large number of EBS volumes, with AWS Config and CloudTrail activated. A manager has tried to find the user name that created an EBS volume by searching CloudTrail events logs but wasn't successful.\n\nAs a Developer Associate, which of the following would you recommend as the correct solution?\n",answers:[{text:"AWS CloudTrail event logs for 'ManageVolume' aren't available for EBS volumes created during an Amazon EC2 launch",isCorrect:!1},{text:"Amazon EBS CloudWatch metrics are disabled",isCorrect:!1},{text:"AWS CloudTrail event logs for 'CreateVolume' aren't available for EBS volumes created during an Amazon EC2 launch",isCorrect:!0},{text:"EBS volume status checks are disabled",isCorrect:!1}],explanation:"Correct option:\n\n<strong>AWS CloudTrail event logs for 'CreateVolume' aren't available for EBS volumes created during an Amazon EC2 launch</strong> - AWS CloudTrail event logs for 'CreateVolume' aren't available for EBS volumes created during an Amazon Elastic Compute Cloud (Amazon EC2) launch.\n\nIncorrect options:\n\n<strong>AWS CloudTrail event logs for 'ManageVolume' aren't available for EBS volumes created during an Amazon EC2 launch</strong> - Event 'ManageVolume' is a made-up option and has been added as a distractor.\n\n<strong>Amazon EBS CloudWatch metrics are disabled</strong> - Amazon Elastic Block Store (Amazon EBS) sends data points to CloudWatch for several metrics. Data is only reported to CloudWatch when the volume is attached to an instance. CloudWatch metrics are useful in tracking the status or life cycle changes of an EBS volume, they are not useful in knowing about the metadata of EBS volumes.\n\n<strong>EBS volume status checks are disabled</strong> - Volume status checks enable you to better understand, track and manage potential inconsistencies in the data on an Amazon EBS volume. They are designed to provide you with the information that you need to determine whether your Amazon EBS volumes are impaired, and to help you control how a potentially inconsistent volume is handled. Our current use case requires us to pull data about EBS volume metadata, which is not possible with this feature.\n\nReferences:\n\n<a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/find-ebs-user-config-cloudtrail/\">https://aws.amazon.com/premiumsupport/knowledge-center/find-ebs-user-config-cloudtrail/</a>\n\n<a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using_cloudwatch_ebs.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using_cloudwatch_ebs.html</a>\n\n<a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html</a>\n"},{question:"An AWS CodePipeline was configured to be triggered by Amazon CloudWatch Events. Recently the pipeline failed and upon investigation, the Team Lead noticed that the source was changed from AWS CodeCommit to Amazon Simple Storage Service (S3). The Team Lead has requested you to find the user who had made the changes.\n\nWhich service will help you solve this?\n",answers:[{text:"Amazon CloudWatch",isCorrect:!1},{text:"Amazon Inspector",isCorrect:!1},{text:"AWS X-Ray",isCorrect:!1},{text:"AWS CloudTrail",isCorrect:!0}],explanation:'Correct option:\n\n<strong>AWS CloudTrail</strong>\n\nAWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides an event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services.\n\nAWS CloudTrail increases visibility into your user and resource activity by recording AWS Management Console actions and API calls. You can identify which users and accounts called AWS, the source IP address from which the calls were made, and when the calls occurred.\n\nHow CloudTrail works:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q25-i1.jpg">\nvia - <a href="https://aws.amazon.com/cloudtrail/">https://aws.amazon.com/cloudtrail/</a>\n\nIncorrect options:\n\n<strong>Amazon CloudWatch</strong> - Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, and on-premises applications and infrastructure resources. CloudWatch can collect numbers and respond to AWS service-related events, but it does not help in user activity logging.\n\n<strong>AWS X-Ray</strong> - AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray is a very important tool in troubleshooting but is not useful in logging user activity.\n\n<strong>Amazon Inspector</strong> - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. This does not log User activity at the account level.\n\nReferences:\n\n<a href="https://aws.amazon.com/xray/">https://aws.amazon.com/xray/</a>\n\n<a href="https://aws.amazon.com/inspector/">https://aws.amazon.com/inspector/</a>\n\n<a href="https://aws.amazon.com/cloudwatch/">https://aws.amazon.com/cloudwatch/</a>\n'},{question:"You are a system administrator whose company recently moved its production application to AWS and migrated data from MySQL to AWS DynamoDB. You are adding new tables to AWS DynamoDB and need to allow your application to query your data by the primary key and an alternate key. This option must be added when first creating tables otherwise changes cannot be made afterward.\n\nWhich of the following actions should you take?\n",answers:[{text:"Create a GSI",isCorrect:!1},{text:"Call Scan",isCorrect:!1},{text:"Create a LSI",isCorrect:!0},{text:"Migrate away from DynamoDB",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Create an LSI</strong>\n\nLSI stands for Local Secondary Index. Some applications only need to query data using the base table\'s primary key; however, there may be situations where an alternate sort key would be helpful. To give your application a choice of sort keys, you can create one or more local secondary indexes on a table and issue Query or Scan requests against these indexes.\n\nDifferences between GSI and LSI:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q26-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html</a>\n\nIncorrect options:\n\n<strong>Call Scan</strong> - Scan is an operation on the data. Once you create your local secondary indexes on a table you can then issue a Scan requests again.\n\n<strong>Create a GSI</strong> - GSI (Global Secondary Index) is an index with a partition key and a sort key that can be different from those on the base table.\n\n<strong>Migrate away from DynamoDB</strong> - Migrating to another database that is not NoSQL may cause you to make changes that require substantial code changes.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html</a>\n'},{question:"An organization with high data volume workloads have successfully moved to DynamoDB after having many issues with traditional database systems. However, a few months into production, DynamoDB tables are consistently recording high latency.\n\nAs a Developer Associate, which of the following would you suggest to reduce the latency? (Select two)\n",answers:[{text:"Increase the request timeout settings, so the client gets enough time to complete the requests, thereby reducing retries on the system",isCorrect:!1},{text:"Consider using Global tables if your application is accessed by globally distributed users",isCorrect:!0},{text:"Reduce connection pooling, which keeps the connections alive even when user requests are not present, thereby, blocking the services",isCorrect:!1},{text:"Use eventually consistent reads in place of strongly consistent reads whenever possible",isCorrect:!0},{text:"Use DynamoDB Accelerator (DAX) for businesses with heavy write-only workloads",isCorrect:!1}],explanation:'Correct option:\n\nAmazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It\'s a fully managed, multi-Region, multi-master, durable database with built-in security, backup, and restore and in-memory caching for internet-scale applications.\n\n<strong>Consider using Global tables if your application is accessed by globally distributed users</strong> - If you have globally dispersed users, consider using global tables. With global tables, you can specify the AWS Regions where you want the table to be available. This can significantly reduce latency for your users. So, reducing the distance between the client and the DynamoDB endpoint is an important performance fix to be considered.\n\n<strong>Use eventually consistent reads in place of strongly consistent reads whenever possible</strong> - If your application doesn\'t require strongly consistent reads, consider using eventually consistent reads. Eventually consistent reads are cheaper and are less likely to experience high latency.\n\nIncorrect options:\n\n<strong>Increase the request timeout settings, so the client gets enough time to complete the requests, thereby reducing retries on the system</strong> - This statement is incorrect. The right way is to reduce the request timeout settings. This causes the client to abandon high latency requests after the specified time period and then send a second request that usually completes much faster than the first.\n\n<strong>Reduce connection pooling, which keeps the connections alive even when user requests are not present, thereby, blocking the services</strong> - This is not correct. When you\'re not making requests, consider having the client send dummy traffic to a DynamoDB table. Alternatively, you can reuse client connections or use connection pooling. All of these techniques keep internal caches warm, which helps keep latency low.\n\n<strong>Use DynamoDB Accelerator (DAX) for businesses with heavy write-only workloads</strong> - This is not correct. If your traffic is read-heavy, consider using a caching service such as DynamoDB Accelerator (DAX). DAX is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement—from milliseconds to microseconds—even at millions of requests per second.\n\nReferences:\n\n<a href="https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-high-latency/">https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-high-latency/</a>\n\n<a href="https://aws.amazon.com/dynamodb/">https://aws.amazon.com/dynamodb/</a>\n'},{question:"A multi-national company maintains separate AWS accounts for different verticals in their organization. The project manager of a team wants to migrate the Elastic Beanstalk environment from Team A's AWS account into Team B's AWS account. As a Developer, you have been roped in to help him in this process.\n\nWhich of the following will you suggest?\n",answers:[{text:"Create a saved configuration in Team A's account and configure it to Export. Now, log into Team B's account and choose the Import option. Here, you need to specify the name of the saved configuration and allow the system to create the new application. This takes a little time based on the Regions the two accounts belong to",isCorrect:!1},{text:"It is not possible to migrate Elastic Beanstalk environment from one AWS account to the other",isCorrect:!1},{text:"Create an export configuration from the Elastic Beanstalk console from Team A's account. This configuration has to be shared with the IAM Role of Team B's account. The import option of Team B's account will show the saved configuration, that can be used to create a new Beanstalk application",isCorrect:!1},{text:"Create a saved configuration in Team A's account and download it to your local machine. Make the account-specific parameter changes and upload to the S3 bucket in Team B's account. From Elastic Beanstalk console, create an application from 'Saved Configurations'",isCorrect:!0}],explanation:"Correct option:\n\n<strong>Create a saved configuration in Team A's account and download it to your local machine. Make the account-specific parameter changes and upload to the S3 bucket in Team B's account. From Elastic Beanstalk console, create an application from 'Saved Configurations</strong> - You must use saved configurations to migrate an Elastic Beanstalk environment between AWS accounts.\nYou can save your environment's configuration as an object in Amazon Simple Storage Service (Amazon S3) that can be applied to other environments during environment creation, or applied to a running environment. Saved configurations are YAML formatted templates that define an environment's platform version, tier, configuration option settings, and tags.\n\nDownload the saved configuration to your local machine. Change your account-specific parameters in the downloaded configuration file, and then save the changes. For example, change the key pair name, subnet ID, or application name (such as application-b-name). Upload the saved configuration from your local machine to an S3 bucket in Team B's account. From this account, create a new Beanstalk application by choosing 'Saved Configurations' from the navigation panel.\n\nIncorrect options:\n\n<strong>Create a saved configuration in Team A's account and configure it to Export. Now, log into Team B's account and choose the Import option. Here, you need to specify the name of the saved configuration and allow the system to create the new application. This takes a little time based on the Regions the two accounts belong to</strong> - There is no direct Export and Import\noption for migrating Elastic Beanstalk configurations.\n\n<strong>It is not possible to migrate Elastic Beanstalk environment from one AWS account to the other</strong> - This is an incorrect statement.\n\n<strong>Create an export configuration from the Elastic Beanstalk console from Team A's account. This configuration has to be shared with the IAM Role of Team B's account. The import option of the Team B's account will show the saved configuration, that can be used to create a new Beanstalk application</strong> - This contradicts the explanation provided earlier.\n\nReferences:\n\n<a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-migration-accounts/\">https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-migration-accounts/</a>\n\n<a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-savedconfig.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-savedconfig.html</a>\n"},{question:"Your team-mate has configured an Amazon S3 event notification for an S3 bucket that holds sensitive audit data of a firm. As the Team Lead, you are receiving the SNS notifications for every event in this bucket. After validating the event data, you realized that few events are missing.\n\nWhat could be the reason for this behavior and how to avoid this in the future?\n",answers:[{text:"If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent",isCorrect:!0},{text:"Someone could have created a new notification configuration and that has overridden your existing configuration",isCorrect:!1},{text:"Versioning is enabled on the S3 bucket and event notifications are getting fired for only one version",isCorrect:!1},{text:"Your notification action is writing to the same bucket that triggers the notification",isCorrect:!1}],explanation:'Correct option:\n\n<strong>If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent</strong> - Amazon S3 event notifications are designed to be delivered at least once. Typically, event notifications are delivered in seconds but can sometimes take a minute or longer.\n\nIf two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent. If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket. With versioning, every successful write will create a new version of your object and will also send event notification.\n\nIncorrect options:\n\n<strong>Someone could have created a new notification configuration and that has overridden your existing configuration</strong> - It is possible that the configuration can be overridden. But, in the current scenario, the team lead is receiving notifications for most of the events, which nullifies the claim that the configuration is overridden.\n\n<strong>Versioning is enabled on the S3 bucket and event notifications are getting fired for only one version</strong> - This is an incorrect statement. If you want to ensure that an event notification is sent for every successful write, you should enable versioning on your bucket. With versioning, every successful write will create a new version of your object and will also send event notification.\n\n<strong>Your notification action is writing to the same bucket that triggers the notification</strong> - If your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop. But it will not result in missing events.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a>\n'},{question:"A startup manages its Cloud resources with Elastic Beanstalk. The environment consists of few Amazon EC2 instances, an Auto Scaling Group (ASG), and an Elastic Load Balancer. Even after the Load Balancer marked an EC2 instance as unhealthy, the ASG has not replaced it with a healthy instance.\n\nAs a Developer, suggest the necessary configurations to automate the replacement of unhealthy instance.\n",answers:[{text:"Health check parameters were configured for checking the instance health alone. The instance failed because of application failure which was not configured as a parameter for health check status",isCorrect:!1},{text:"The health check type of your instance's Auto Scaling group, must be changed from EC2 to ELB by using a configuration file",isCorrect:!0},{text:"Auto Scaling group doesn't automatically replace the unhealthy instances marked by the load balancer. They have to be manually replaced from AWS Console",isCorrect:!1},{text:"The ping path field of the Load Balancer is configured incorrectly",isCorrect:!1}],explanation:"Correct option:\n\n<strong>The health check type of your instance's Auto Scaling group, must be changed from EC2 to ELB by using a configuration file</strong> - By default, the health check configuration of your Auto Scaling group is set as an EC2 type that performs a status check of EC2 instances. To automate the replacement of unhealthy EC2 instances, you must change the health check type of your instance's Auto Scaling group from EC2 to ELB by using a configuration file.\n\nIncorrect options:\n\n<strong>Health check parameters were configured for checking the instance health alone. The instance failed because of application failure which was not configured as a parameter for health check status</strong> - This is an incorrect statement. Status checks, by definition, cover only an EC2 instance's health, and not the health of your application, server, or any Docker containers running on the instance.\n\n<strong>Auto Scaling group doesn't automatically replace the unhealthy instances marked by the load balancer. They have to be manually replaced from AWS Console</strong> - Incorrect statement. As discussed above, if the health check type of ASG is changed from EC2 to ELB, Auto Scaling will be able to replace the unhealthy instance.\n\n<strong>The ping path field of the Load Balancer is configured incorrectly</strong> - Ping path is a health check configuration field of Elastic Load Balancer. If the ping path is configured wrong, ELB will not be able to reach the instance and hence will consider the instance unhealthy. However, this would then apply to all instances, not just once instance. So it does not address the issue given in the use-case.\n\nReferences:\n\n<a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-instance-automation/\">https://aws.amazon.com/premiumsupport/knowledge-center/elastic-beanstalk-instance-automation/</a>\n\n<a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html</a>\n"},{question:"An e-commerce company uses Amazon SQS queues to decouple their application architecture. The development team has observed message processing failures for an edge case scenario when a user places an order for a particular product ID, but the product ID is deleted, thereby causing the application code to fail.\n\nAs a Developer Associate, which of the following solutions would you recommend to address such message failures?\n",answers:[{text:"Use a temporary queue to handle message processing failures",isCorrect:!1},{text:"Use a dead-letter queue to handle message processing failures",isCorrect:!0},{text:"Use short polling to handle message processing failures",isCorrect:!1},{text:"Use long polling to handle message processing failures",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Use a dead-letter queue to handle message processing failures</strong>\n\nDead-letter queues can be used by other queues (source queues) as a target for messages that can\'t be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn\'t succeed.\n\nSometimes, messages can’t be processed because of a variety of possible issues, such as when a user comments on a story but it remains unprocessed because the original story itself is deleted by the author while the comments were being posted. In such a case, the dead-letter queue can be used to handle message processing failures.\n\nHow do dead-letter queues work?\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q31-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a>\n\nUse-cases for dead-letter queues:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q31-i2.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a>\n\nIncorrect options:\n\n<strong>Use a temporary queue to handle message processing failures</strong> - The most common use case for temporary queues is the request-response messaging pattern (for example, processing a login request), where a requester creates a temporary queue for receiving each response message. To avoid creating an Amazon SQS queue for each response message, the Temporary Queue Client lets you create and delete multiple temporary queues without making any Amazon SQS API calls. Temporary queues cannot be used to handle message processing failures.\n\n<strong>Use short polling to handle message processing failures</strong>\n\n<strong>Use long polling to handle message processing failures</strong>\n\nAmazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.\nNeither short polling nor long polling can be used to handle message processing failures.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</a>\n'},{question:"An analytics company is using Kinesis Data Streams (KDS) to process automobile health-status data from the taxis managed by a taxi ride-hailing service. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams.\n\nAs a Developer Associate, which of the following options would you suggest for improving the performance for the given use-case?\n",answers:[{text:"Swap out Kinesis Data Streams with SQS Standard queues",isCorrect:!1},{text:"Swap out Kinesis Data Streams with SQS FIFO queues",isCorrect:!1},{text:"Use Enhanced Fanout feature of Kinesis Data Streams",isCorrect:!0},{text:"Swap out Kinesis Data Streams with Kinesis Data Firehose",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Use Enhanced Fanout feature of Kinesis Data Streams</strong>\n\nAmazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\n\nBy default, the 2MB/second/shard output is shared between all of the applications consuming data from the stream. You should use enhanced fan-out if you have multiple consumers retrieving data from a stream in parallel. With enhanced fan-out developers can register stream consumers to use enhanced fan-out and receive their own 2MB/second pipe of read throughput per shard, and this throughput automatically scales with the number of shards in a stream.\n\nKinesis Data Streams Fanout\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q33-i1.jpg">\nvia - <a href="https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a>\n\nIncorrect options:\n\n<strong>Swap out Kinesis Data Streams with Kinesis Data Firehose</strong> -  Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. Kinesis Data Firehose can only write to S3, Redshift, Elasticsearch or Splunk. You can\'t have applications consuming data streams from Kinesis Data Firehose, that\'s the job of Kinesis Data Streams. Therefore this option is not correct.\n\n<strong>Swap out Kinesis Data Streams with SQS Standard queues</strong>\n\n<strong>Swap out Kinesis Data Streams with SQS FIFO queues</strong>\n\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. As multiple applications are consuming the same stream concurrently, both SQS Standard and SQS FIFO are not the right fit for the given use-case.\n\nExam Alert:\n\nPlease understand the differences between the capabilities of Kinesis Data Streams vs SQS, as you may be asked scenario-based questions on this topic in the exam.\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q33-i2.jpg">\nvia - <a href="https://aws.amazon.com/kinesis/data-streams/faqs/">https://aws.amazon.com/kinesis/data-streams/faqs/</a>\n\nReferences:\n\n<a href="https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/">https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</a>\n\n<a href="https://aws.amazon.com/kinesis/data-streams/faqs/">https://aws.amazon.com/kinesis/data-streams/faqs/</a>\n'},{question:"You manage a group of developers that are experienced with the AWS SDK for Java. You have given them a requirement to build a state machine workflow where each state executes an AWS Lambda function written in Java. Data payloads of 1KB in size will be passed between states and should allow for two retry attempts if the state fails.\n\nWhich of the following options will assist your developers with this requirement?\n",answers:[{text:"AWS Step Functions",isCorrect:!0},{text:"CloudWatch Rules",isCorrect:!1},{text:"Amazon Simple Workflow Service",isCorrect:!1},{text:"AWS ECS",isCorrect:!1}],explanation:'Correct option:\n\n<strong>AWS Step Functions</strong>\n\nAWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or task, allowing you to scale and change applications quickly.\n\nUsing Step Functions, you can design and run workflows that stitch together services such as AWS Lambda and Amazon ECS into feature-rich applications. Workflows are made up of a series of steps, with the output of one step acting as input into the next.\n\nHow Step Functions Work:\n<img src="https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png">\nvia - <a href="https://aws.amazon.com/step-functions/">https://aws.amazon.com/step-functions/</a>\n\nIncorrect options:\n\n<strong>CloudWatch Rules</strong> - CloudWatch rules can integrate with an AWS Lambda function on a schedule. You can send the matching events to an AWS Step Functions state machine to start a workflow responding to the event of interest but CloudWatch rules alone do not create a state machine.\n\n<strong>Amazon Simple Workflow Service</strong> - Amazon Simple Workflow Service is similar to AWS Step functions. With Amazon Simple Workflow Service you can write a program that separates activity steps, allows for more control but increases the complexity of the application. With Amazon Simple Workflow Service you create decider programs and with Step Functions, you define state machines.\n\n<strong>AWS ECS</strong> - The underlying work in your state machine is done by tasks. A task performs work by using an activity, a Lambda function, or by sending parameters to the API actions of other services. You can use AWS ECS to host an activity.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html">https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html</a>\n'},{question:"A company has configured an Auto Scaling group with health checks. The configuration is set to the desired capacity value of 3 and maximum capacity value of 3. The EC2 instances of your Auto Scaling group are configured to scale when CPU utilization is at 60 percent and is now running at 80 percent utilization.\n\nWhich of the following will take place?\n",answers:[{text:"System will trigger CloudWatch alarms to AWS support",isCorrect:!1},{text:"The desired capacity will go up to 4 and the maximum capacity will stay at 3",isCorrect:!1},{text:"The desired capacity will go up to 4 and the maximum capacity will also go up to 4",isCorrect:!1},{text:"System will keep running as is",isCorrect:!0}],explanation:'Correct option:\n\n<strong>System will keep running as is</strong>\n\nYou are already running at max capacity. After you have created your Auto Scaling group, the Auto Scaling group starts by launching enough EC2 instances to meet its minimum capacity (or its desired capacity, if specified). If there are no other scaling conditions attached to the Auto Scaling group, the Auto Scaling group maintains this number of running instances even if an instance becomes unhealthy.\n\nSetting Capacity Limits for Your Auto Scaling Group:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q34-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html</a>\n\nIncorrect options:\n\n<strong>The desired capacity will go up to 4 and the maximum capacity will stay at 3</strong> - The desired capacity cannot go over the maximum capacity.\n\n<strong>The desired capacity will go up to 4 and the maximum capacity will also go up to 4</strong> - The maximum capacity cannot change on its own just because the desired capacity has been set to a higher value. You will have to make those changes to the maximum capacity manually.\n\n<strong>System will trigger CloudWatch alarms to AWS support</strong> - This option has been added as a distractor. You already have alarms configured based on rules but AWS support will not intervene for you.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html</a>\n'},{question:"Your application sends messages to an Amazon Simple Queue Service (SQS) queue frequently, which are then polled by another application that specifies which message to retrieve.\n\nWhich of the following options describe the maximum number of messages that can be retrieved at one time?\n",answers:[{text:"100",isCorrect:!1},{text:"5",isCorrect:!1},{text:"20",isCorrect:!1},{text:"10",isCorrect:!0}],explanation:'Correct option:\n\n<strong>10</strong>\n\nAfter you send messages to a queue, you can receive and delete them. When you request messages from a queue, you can\'t specify which messages to retrieve. Instead, you specify the maximum number of messages (up to 10) that you want to retrieve.\n\nIncorrect options:\n\n<strong>5</strong>\n\n<strong>20</strong>\n\n<strong>100</strong>\n\nReference:\n\n<a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-receive-delete-message.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-using-receive-delete-message.html</a>\n'},{question:"You're a developer maintaining a web application written in .NET. The application makes references to public objects in a public S3 accessible bucket using a public URL. While doing a code review your colleague advises that the approach is not a best practice because some of the objects contain private data. After the administrator makes the S3 bucket private you can no longer access the S3 objects but you would like to create an application that will enable people to access some objects as needed with a time policy constraint.\n\nWhich of the following options will give access to the objects?\n",answers:[{text:"Using bucket policy",isCorrect:!1},{text:"Using pre-signed URL",isCorrect:!0},{text:"Using Routing Policy",isCorrect:!1},{text:"Using IAM policy",isCorrect:!1}],explanation:'Correct option:\n\n"Using pre-signed URL"\n\nAll objects by default are private, with object owner having permission to access the objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects. When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The pre-signed URLs are valid only for the specified duration.\n\nPlease see this note for more details:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q36-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a>\n\nIncorrect:\n\n"Using bucket policy" - You can use this policy to limit users from a source IP address however for time-based constraints you are better off using a pre-signed URL.\n\n"Using Routing Policy" - This concept applies to DNS in Route 53, so this option is ruled out.\n\n"Using IAM policy" - You can use IAM policy to grant access toa specific bucket however for time-based constraints you are better off using a pre-signed URL.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html</a>\n'},{question:"As a site reliability engineer, you work on building and running large-scale, distributed, fault-tolerant systems in the cloud using automation. You have just replaced the company's Jenkins based CI/CD platform with AWS CodeBuild and would like to programmatically define your build steps.\n\nWhich of the following options should you choose?\n",answers:[{text:"Define an appspec.yml file in the root directory",isCorrect:!1},{text:"Define a buildspec.yml file in the root directory",isCorrect:!0},{text:"Define a buildspec.yml file in the codebuild/ directory",isCorrect:!1},{text:"Define an appspec.yml file in the codebuild/ directory",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Define a buildspec.yml file in the root directory</strong>\n\nAWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications.\n\nA build spec is a collection of build commands and related settings, in YAML format, that AWS CodeBuild uses to run a build. You can include a build spec as part of the source code or you can define a build spec when you create a build project.\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q37-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a>\n\nIncorrect options:\n\n<strong>Define an appspec.yml file in the root directory</strong> - The AppSpec file is used for deployment in the CodeDeploy service.\n\n<strong>Define a buildspec.yml file in the codebuild/ directory</strong> - The file is correct but must be in the root directory.\n\n<strong>Define an appspec.yml file in the codebuild/ directory</strong> - The AppSpec file is used for deployment in the CodeDeploy service.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html">https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html</a>\n'},{question:"You are a software engineer working for an IT company and are asked to contribute to a growing internal application that includes dashboards for data visualization. You are provisioning your AWS DynamoDB table and need to perform 10 strongly consistent reads per second of 4 KB in size each.\n\nHow many Read Capacity Units (RCUs) are needed?\n",answers:[{text:"10",isCorrect:!0},{text:"40",isCorrect:!1},{text:"20",isCorrect:!1},{text:"5",isCorrect:!1}],explanation:'Correct option:\n\nBefore proceeding with the calculations, please review the following:\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q38-i1.jpg">\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q38-i2.jpg">\n\nvia - <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a>\n\n<strong>10</strong>\n\nOne Read Capacity Unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. If you need to read an item that is larger than 4 KB, DynamoDB will need to consume additional read capacity units. The total number of Read Capacity Units required depends on the item size, and whether you want an eventually consistent or strongly consistent read.\n\n1) Item Size / 4KB, rounding to the nearest whole number.\n\nSo, in the above case, 4KB / 4 KB = 1 read capacity unit.\n\n2) 1 read capacity unit per item (since strongly consistent read) \xd7 No of reads per second\n\nSo, in the above case, 1 x 10 = 10 read capacity units.\n\nIncorrect options:\n\n<strong>40</strong>\n\n<strong>20</strong>\n\n<strong>5</strong>\n\nThese three options contradict the details provided in the explanation above, so these are incorrect.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a>\n'},{question:"A development team has inherited a web application running in the us-east-1 region with three availability zones (us-east-1a, us-east1-b, and us-east-1c) whose incoming web traffic is routed by a load balancer. When one of the EC2 instances hosting the web application crashes, the team realizes that the load balancer continues to route traffic to that instance causing intermittent issues.\n\nWhich of the following should the development team do to minimize this problem?\n",answers:[{text:"Enable Health Checks",isCorrect:!0},{text:"Enable Stickiness",isCorrect:!1},{text:"Enable Multi AZ deployments",isCorrect:!1},{text:"Enable SSL",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Enable Health Checks</strong>\n\nTo discover the availability of your EC2 instances, a load balancer periodically sends pings, attempts connections, or sends requests to test the EC2 instances. These tests are called health checks. The status of the instances that are healthy at the time of the health check is InService. The status of any instances that are unhealthy at the time of the health check is OutOfService.\n\nLoad Balancer Health Checks:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q39-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a>\n\nIncorrect options:\n\n<strong>Enable Stickiness</strong> - Stickiness enables the load balancer to bind a user\'s session to a specific instance, it cannot be used for gauging the health of an instance.\n\n<strong>Enable Multi-AZ deployments</strong> - It\'s a good practice to provision instances in more than one availability zone however you still need a way to check the health status of the instances, so this option is incorrect.\n\n<strong>Enable SSL</strong> - This option has been added as a distractor. SSL encrypts the transmission of data between a web server and a browser.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a>\n'},{question:"You are working for a technology startup building web and mobile applications. You would like to pull Docker images from the ECR repository called demo so you can start running local tests against the latest application version.\n\nWhich of the following commands must you run to pull existing Docker images from ECR? (Select two)\n",answers:[{text:"docker pull 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest",isCorrect:!0},{text:"$(aws ecr get-login --no-include-email)",isCorrect:!0},{text:"docker login -u $AWS_ACCESS_KEY_ID -p $AWS_SECRET_ACCESS_KEY",isCorrect:!1},{text:"aws docker push 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest",isCorrect:!1},{text:"docker build -t 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest",isCorrect:!1}],explanation:'Correct options:\n\n<strong>$(aws ecr get-login --no-include-email)</strong>\n\n<strong>docker pull 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</strong>\n\nThe get-login command retrieves a token that is valid for a specified registry for 12 hours, and then it prints a docker login command with that authorization token. You can execute the printed command to log in to your registry with Docker, or just run it automatically using the $() command wrapper. After you have logged in to an Amazon ECR registry with this command, you can use the Docker CLI to push and pull images from that registry until the token expires. The docker pull command is used to pull an image from the ECR registry.\n\nIncorrect options:\n\n<strong>docker login -u $AWS_ACCESS_KEY_ID -p $AWS_SECRET_ACCESS_KEY</strong> - You cannot login to AWS ECR this way. AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are only used by the CLI and not by docker.\n\n<strong>aws docker push 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</strong> - docker push here is the wrong answer, you need to use docker pull.\n\n<strong>docker build -t 1234567890.dkr.ecr.eu-west-1.amazonaws.com/demo:latest</strong> - This is a docker command that is used to build Docker images from a Dockerfile.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/cli/latest/reference/ecr/get-login.html">https://docs.aws.amazon.com/cli/latest/reference/ecr/get-login.html</a>\n'},{question:"A video streaming application uses Amazon CloudFront for its data distribution. The development team has decided to use CloudFront with origin failover for high availability.\n\nWhich of the following options are correct while configuring CloudFront with Origin Groups? (Select two)\n",answers:[{text:"CloudFront routes all incoming requests to the primary origin, even when a previous request failed over to the secondary origin",isCorrect:!0},{text:"When there’s a cache hit, CloudFront routes the request to the primary origin in the origin group",isCorrect:!1},{text:"To set up origin failover, you must have a distribution with at least three origins",isCorrect:!1},{text:"In the Origin Group of your distribution, all the origins are defined as primary for automatic failover in case an origin fails",isCorrect:!1},{text:"CloudFront fails over to the secondary origin only when the HTTP method of the viewer request is GET, POST or HEAD",isCorrect:!0}],explanation:'Correct options:\n\n<strong>CloudFront routes all incoming requests to the primary origin, even when a previous request failed over to the secondary origin</strong>\n\nCloudFront routes all incoming requests to the primary origin, even when a previous request failed over to the secondary origin. CloudFront only sends requests to the secondary origin after a request to the primary origin fails.\n\n<strong>CloudFront fails over to the secondary origin only when the HTTP method of the viewer request is GET, POST or HEAD</strong>\n\nCloudFront fails over to the secondary origin only when the HTTP method of the viewer request is GET, HEAD, or OPTIONS. CloudFront does not failover when the viewer sends a different HTTP method (for example POST, PUT, and so on).\n\nHow origin failover works:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q41-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a>\n\nIncorrect options:\n\n<strong>When there’s a cache hit, CloudFront routes the request to the primary origin in the origin group</strong> - When there’s a cache miss, CloudFront routes the request to the primary origin in the origin group. When there’s a cache hit, CloudFront returns the requested file.\n\n<strong>To set up origin failover, you must have a distribution with at least three origins</strong> - Two origins are enough to set up an origin failover.\n\n<strong>In the Origin Group of your distribution, all the origins are defined as primary for automatic failover in case an origin fails</strong> - To set up origin failover, you must have a distribution with at least two origins. Next, you create an origin group for your distribution that includes two origins, setting one as the primary. Only one origin can be set as primary.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a>\n'},{question:"An investment firm wants to continuously generate time-series analytics of the stocks being purchased by its customers. The firm wants to build a live leaderboard with real-time analytics for these in-demand stocks.\n\nWhich of the following represents a fully managed solution to address this use-case?\n",answers:[{text:"Use Kinesis Data Streams to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics",isCorrect:!1},{text:"Use Kinesis Data Streams to ingest data and Amazon Kinesis Client Library to the application logic to generate leaderboard scores and time-series analytics",isCorrect:!1},{text:"Use Kinesis Firehose to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics",isCorrect:!0},{text:"Use Kinesis Firehose to ingest data and Amazon Athena to generate leaderboard scores and time-series analytics",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Use Kinesis Firehose to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics</strong>\n\nAmazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and deliver streaming data to Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, generic HTTP endpoints, and service providers like Datadog, New Relic, MongoDB, and Splunk. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt your data streams before loading, minimizing the amount of storage used and increasing security.\n\nAmazon Kinesis Data Analytics is the easiest way to transform and analyze streaming data in real-time with Apache Flink. Apache Flink is an open source framework and engine for processing data streams. Amazon Kinesis Data Analytics reduces the complexity of building, managing, and integrating Apache Flink applications with other AWS services.\n\nAmazon Kinesis Data Analytics provides built-in functions to filter, aggregate, and transform streaming data for advanced analytics. It processes streaming data with sub-second latencies, enabling you to analyze and respond to incoming data and events in real-time.\n\nAmazon Kinesis Data Analytics is serverless; there are no servers to manage. It runs your streaming applications without requiring you to provision or manage any infrastructure. Amazon Kinesis Data Analytics automatically scales the infrastructure up and down as required to process incoming data.\n\nIncorrect options:\n\n<strong>Use Kinesis Data Streams to ingest data and Kinesis Data Analytics to generate leaderboard scores and time-series analytics</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.\n\nKinesis Data Streams needs manual provisioning of shards and a planning of shard requirements. The use-case clearly states that the company wants a fully managed solution as soon as possible with minimal effort. Kinesis Firehose is fully managed and requires no user input in provisioning of resources.\n\n<strong>Use Kinesis Data Streams to ingest data and Amazon Kinesis Client Library to the application logic to generate leaderboard scores and time-series analytics</strong> - The Amazon Kinesis Client Library (KCL) is a pre-built library that helps you build consumer applications for reading and processing data from an Amazon Kinesis data stream. The KCL handles complex issues such as adapting to changes in data stream volume, load balancing streaming data, coordinating distributed services, and processing data with fault-tolerance. The KCL enables you to focus on business logic while building applications.\n\nIf you want a fully managed solution and you want to use SQL to process the data from your data stream, you should use Kinesis Data Analytics. Use KCL if you need to build a custom processing solution whose requirements are not met by Kinesis Data Analytics, and you can manage the resulting consumer application.\n\n<strong>Use Kinesis Firehose to ingest data and Amazon Athena to generate leaderboard scores and time-series analytics</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Athena is used for running analytics on S3 based data. For running analytics on real-time streaming data, Kinesis Data Analytics is the right fit.\n\nReferences:\n\n<a href="https://docs.aws.amazon.com/firehose/latest/dev/data-analysis.html">https://docs.aws.amazon.com/firehose/latest/dev/data-analysis.html</a>\n\n<a href="https://aws.amazon.com/kinesis/data-analytics/faqs/">https://aws.amazon.com/kinesis/data-analytics/faqs/</a>\n'},{question:"A development team is configuring Kinesis Data Streams for ingesting real-time data from various appliances. The team has declared a shard capacity of one to test the configuration.\n\nWhat happens if the capacity limits of an Amazon Kinesis data stream are exceeded while the data producer adds data to the data stream?\n",answers:[{text:"The put data calls will be rejected with a AccessDeniedException exception once the limit is reached",isCorrect:!1},{text:"Data is lost unless the partition key of the data records is changed in order to write data to a different shard in the stream",isCorrect:!1},{text:"Contact AWS support to request an increase in the number of shards",isCorrect:!1},{text:"The put data calls will be rejected with a ProvisionedThroughputExceeded exception",isCorrect:!0}],explanation:'Correct option:\n\n<strong>The put data calls will be rejected with a ProvisionedThroughputExceeded exception</strong>\n\nThe capacity limits of an Amazon Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a ProvisionedThroughputExceeded exception. If this is due to a temporary rise of the data stream’s input data rate, retry by the data producer will eventually lead to completion of the requests. If this is due to a sustained rise of the data stream’s input data rate, you should increase the number of shards within your data stream to provide enough capacity for the put data calls to consistently succeed.\n\nIncorrect options:\n\n<strong>The put data calls will be rejected with a AccessDeniedException exception once the limit is reached</strong> - Access Denied error is thrown when the accessing system does not have enough permissions. Since data was getting ingested into Data Streams before reaching the capacity, this error is not possible.\n\n<strong>Data is lost unless the partition key of the data records is changed in order to write data to a different shard in the stream</strong> - Partition key is used to segregate and route records to different shards of a data stream. A partition key is specified by your data producer while adding data to an Amazon Kinesis data stream. The use case talks about provisioning only one shard. It is not possible to set up more shards by simply changing the partition key. Hence, this choice is incorrect.\n\n<strong>Contact AWS support to request an increase in the number of shards</strong> - This is a made-up option that acts as a distractor.\n\nReference:\n\n<a href="https://aws.amazon.com/kinesis/data-streams/faqs/">https://aws.amazon.com/kinesis/data-streams/faqs/</a>\n'},{question:"A development team has been using Amazon S3 service as an object store. With Amazon S3 turning strongly consistent, the team wants to understand the impact of this change on its data storage practices.\n\nAs a developer associate, can you identify the key characteristics of the strongly consistent data model followed by S3? (Select two)\n",answers:[{text:"If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list",isCorrect:!0},{text:"A process replaces an existing object and immediately tries to read it. Amazon S3 might return the old data",isCorrect:!1},{text:"A process deletes an existing object and immediately tries to read it. Amazon S3 can return data as the object deletion has not yet propagated completely",isCorrect:!1},{text:"A process deletes an existing object and immediately lists keys within its bucket. The object could still be visible for few more minutes till the change propagates",isCorrect:!1},{text:"A process deletes an existing object and immediately tries to read it. Amazon S3 will not return any data as the object has been deleted",isCorrect:!0}],explanation:'Correct options:\n\n<strong>If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list</strong> - Bucket configurations have an eventual consistency model. If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list.\n\n<strong>A process deletes an existing object and immediately tries to read it. Amazon S3 will not return any data as the object has been deleted</strong> - Amazon S3 provides strong read-after-write consistency for PUTs and DELETEs of objects in your Amazon S3 bucket in all AWS Regions. This applies to both writes to new objects as well as PUTs that overwrite existing objects and DELETEs.\n\nAmazon S3 data consistency model:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q44-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel</a>\n\nIncorrect options:\n\n<strong>A process deletes an existing object and immediately lists keys within its bucket. The object could still be visible for few more minutes till the change propagates</strong>\n\n<strong>A process deletes an existing object and immediately tries to read it. Amazon S3 can return data as the object deletion has not yet propagated completely</strong> -\n\nThese two options highlight an eventually consistent behavior. Amazon S3 is now strongly consistent and will not return any data as the object has been deleted. So both these options are incorrect.\n\n<strong>A process replaces an existing object and immediately tries to read it. Amazon S3 might return the old data</strong> - Amazon S3 will return the new data.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel</a>\n'},{question:"A developer is configuring the redirect actions for an Application Load Balancer. The developer stumbled upon the following snippet of code.\n\nWhich of the following is an example of a query string condition that the developer can use on AWS CLI?\n",answers:[{text:'[\n  {\n      "Field": "query-string",\n      "PathPatternConfig": {\n          "Values": ["/img/*"]\n      }\n  }\n]\n',isCorrect:!1},{text:'[\n  {\n      "Field": "query-string",\n      "StringHeaderConfig": {\n          "Values": ["*.example.com"]\n      }\n  }\n]\n',isCorrect:!1},{text:'[\n  {\n      "Field": "query-string",\n      "QueryStringConfig": {\n          "Values": [\n            {\n                "Key": "version",\n                "Value": "v1"\n            },\n            {\n                "Value": "*example*"\n            }\n          ]\n      }\n  }\n]\n\n',isCorrect:!0},{text:'[\n  {\n      "Type": "redirect",\n      "RedirectConfig": {\n          "Protocol": "HTTPS",\n          "Port": "443",\n          "Host": "#{host}",\n          "Path": "/#{path}",\n          "Query": "#{query}",\n          "StatusCode": "HTTP_301"\n      }\n  }\n]\n\n',isCorrect:!1}],explanation:'Correct option:\n\n**\n\n[\n  {\n      "Field": "query-string",\n      "QueryStringConfig": {\n          "Values": [\n            {\n                "Key": "version",\n                "Value": "v1"\n            },\n            {\n                "Value": "*example*"\n            }\n          ]\n      }\n  }\n]\n\n\n**\n\nYou can use query string conditions to configure rules that route requests based on key/value pairs or values in the query string. The match evaluation is not case-sensitive. The following wildcard characters are supported: * (matches 0 or more characters) and ? (matches exactly 1 character). You can specify conditions when you create or modify a rule.\n\nQuery parameters are often used along with the path component of the URL for applying a special logic to the resource being fetched.\n\nThe query string component starts after the first "?" in a URI. Typically query strings contain key-value pairs separated by a delimiter "&". Example: http://example.com/path/to/page?version=A&gender=female\n\nThe example condition given in the question is satisfied by requests with a query string that includes either a key/value pair of "version=v1" or any key set to "example".\n\nIncorrect options:\n\n**\n\n[\n  {\n      "Field": "query-string",\n      "PathPatternConfig": {\n          "Values": ["/img/*"]\n      }\n  }\n]\n\n\n**\n\n**\n\n[\n  {\n      "Field": "query-string",\n      "StringHeaderConfig": {\n          "Values": ["*.example.com"]\n      }\n  }\n]\n\n\n**\n\nThese two options are malformed and are incorrect.\n\n**\n\n[\n  {\n      "Type": "redirect",\n      "RedirectConfig": {\n          "Protocol": "HTTPS",\n          "Port": "443",\n          "Host": "#{host}",\n          "Path": "/#{path}",\n          "Query": "#{query}",\n          "StatusCode": "HTTP_301"\n      }\n  }ERROR!\n]\n\n\n** - This action redirects an HTTP request to an HTTPS request on port 443, with the same hostname, path, and query string as the HTTP request.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#query-string-conditions">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#query-string-conditions</a>\n'},{question:"A developer has just completed configuring the Application Load Balancer for the EC2 instances. Just as he started testing his configuration, he realized that he has missed assigning target groups to his ALB.\n\nWhich error code should he expect in his debug logs?\n",answers:[{text:"HTTP 500",isCorrect:!1},{text:"HTTP 503",isCorrect:!0},{text:"HTTP 504",isCorrect:!1},{text:"HTTP 403",isCorrect:!1}],explanation:"Correct option:\n\n<strong>HTTP 503</strong> - HTTP 503 indicates 'Service unavailable' error. This error in ALB is an indicator of the target groups for the load balancer having no registered targets.\n\nIncorrect options:\n\n<strong>HTTP 500</strong> - HTTP 500 indicates 'Internal server' error. There are several reasons for their error: A client submitted a request without an HTTP protocol, and the load balancer was unable to generate a redirect URL, there was an error executing the web ACL rules.\n\n<strong>HTTP 504</strong> - HTTP 504 is 'Gateway timeout' error. Several reasons for this error, to quote a few: The load balancer failed to establish a connection to the target before the connection timeout expired, The load balancer established a connection to the target but the target did not respond before the idle timeout period elapsed.\n\n<strong>HTTP 403</strong> - HTTP 403 is 'Forbidden' error. You configured an AWS WAF web access control list (web ACL) to monitor requests to your Application Load Balancer and it blocked a request.\n\nReference:\n\n<a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html</a>\n"},{question:"A developer in your company has configured a build using AWS CodeBuild. The build fails and the developer needs to quickly troubleshoot the issue to see which commands or settings located in the BuildSpec file are causing an issue.\n\nWhich approach will help them accomplish this?\n",answers:[{text:"Freeze the CodeBuild during its next execution",isCorrect:!1},{text:"SSH into the CodeBuild Docker container",isCorrect:!1},{text:"Run AWS CodeBuild locally using CodeBuild Agent",isCorrect:!0},{text:"Enable detailed monitoring",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Run AWS CodeBuild locally using CodeBuild Agent</strong>\n\nAWS CodeBuild is a fully managed build service. There are no servers to provision and scale, or software to install, configure, and operate.\n\nWith the Local Build support for AWS CodeBuild, you just specify the location of your source code, choose your build settings, and CodeBuild runs build scripts for compiling, testing, and packaging your code. You can use the AWS CodeBuild agent to test and debug builds on a local machine.\n\nBy building an application on a local machine you can:\n\nTest the integrity and contents of a buildspec file locally.\n\nTest and build an application locally before committing.\n\nIdentify and fix errors quickly from your local development environment.\n\nIncorrect options:\n\n<strong>SSH into the CodeBuild Docker container</strong> - It is not possible to SSH into the CodeBuild Docker container, that\'s why you should test and fix errors locally.\n\n<strong>Freeze the CodeBuild during its next execution</strong> - You cannot freeze the CodeBuild process but you can stop it. Please see more details on - <a href="https://docs.aws.amazon.com/codebuild/latest/userguide/stop-build.html">https://docs.aws.amazon.com/codebuild/latest/userguide/stop-build.html</a>\n\n<strong>Enable detailed monitoring</strong> - Detailed monitoring is available for EC2 instances. You do not enable detailed monitoring but you can specify output logs to be captured via CloudTrail.\n\nAWS CodeBuild is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in CodeBuild. CloudTrail captures all API calls for CodeBuild as events, including calls from the CodeBuild console and from code calls to the CodeBuild APIs. If you create a trail, you can enable continuous delivery of CloudTrail events to an S3 bucket, including events for CodeBuild.\n\nReferences:\n\n<a href="https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html">https://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html</a>\n\n<a href="https://aws.amazon.com/blogs/devops/announcing-local-build-support-for-aws-codebuild/">https://aws.amazon.com/blogs/devops/announcing-local-build-support-for-aws-codebuild/</a>\n\n<a href="https://docs.aws.amazon.com/codebuild/latest/userguide/use-codebuild-agent.html">https://docs.aws.amazon.com/codebuild/latest/userguide/use-codebuild-agent.html</a>\n'},{question:"A developer while working on Amazon EC2 instances, realized that an instance was not needed and had shut it down. But another instance of the same type automatically got launched in the account.\n\nWhich of the following options can attribute the given sequence of actions?\n",answers:[{text:"The user did not have the right permissions to shutdown the instance. User needs root permissions to terminate an instance",isCorrect:!1},{text:"Instance might be part of Auto Scaling Group and hence re-launched similar instance",isCorrect:!0},{text:"The instance could have been a part of Application Load Balancer and hence was automatically started",isCorrect:!1},{text:"The instance could have been a part of Network Load Balancer and hence was automatically started",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Instance might be part of Auto Scaling Group and hence re-launched similar instance</strong> - Auto Scaling groups can be configured to launch an instance to replace an instance that is undergoing maintenance. This could have been the reason why an instance of the same type got launched automatically. The size of an Auto Scaling group depends on the number of instances that you set as the desired capacity. If you wish to terminate an instance that is part of Auto Scaling Group, the configuration of the group should be changed to a reduced number of instances, so the automatic launch of instances does not happen when an unwanted instance is terminated.\n\nIncorrect options:\n\n<strong>The user did not have the right permissions to shutdown the instance. User needs root permissions to terminate an instance</strong> - This is an incorrect statement. If the user does not have enough permissions, then the action itself is unavailable for him. A user does not need root permissions to terminate an EC2 instance.\n\n<strong>The instance could have been a part of the Application Load Balancer and hence was automatically started</strong> - Application Load Balancer is used to balance the incoming traffic requests equally among the available EC2 instances so keep the performance and availability at its best. ALBs are configured with Auto Scaling Groups, but this is not specified in the use-case. In the absence of Auto Scaling Group, ALB cannot launch instances by itself.\n\n<strong>The instance could have been a part of Network Load Balancer and hence was automatically started</strong> - As explained above for ALB, a Network Load Balancer is not capable of launching instances by itself if it\'s not configured with an Auto Scaling Group.\n\nReferences:\n\n<a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html</a>\n\n<a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/detach-instance-asg.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/detach-instance-asg.html</a>\n'},{question:"Your organization has developers that merge code changes regularly to an AWS CodeCommit repository. Your pipeline has AWS CodeCommit as the source and you would like to configure a rule that reacts to changes in CodeCommit.\n\nWhich of the following options do you choose for this type of integration?\n",answers:[{text:"Use CloudTrail Event rules with Amazon Simple Email Service (SES)",isCorrect:!1},{text:"Use Lambda function with Amazon Simple Notification Service (SNS)",isCorrect:!1},{text:"Use Lambda Event Rules",isCorrect:!1},{text:"Use CloudWatch Event Rules",isCorrect:!0}],explanation:'Correct option:\n\n<strong>Use CloudWatch Event Rules</strong>\n\nAmazon CloudWatch Events is a web service that monitors your AWS resources and the applications you run on AWS. You can use Amazon CloudWatch Events to detect and react to changes in the state of a pipeline, stage, or action. Then, based on rules you create, CloudWatch Events invokes one or more target actions when a pipeline, stage, or action enters the state you specify in a rule.\nExamples of Amazon CloudWatch Events rules and targets:\n\n<ol>\n<li>A rule that sends a notification when the instance state changes, where an EC2 instance is the event source, and Amazon SNS is the event target.</li>\n<li>A rule that sends a notification when the build phase changes, where a CodeBuild configuration is the event source, and Amazon SNS is the event target.</li>\n<li>A rule that detects pipeline changes and invokes an AWS Lambda function.</li>\n</ol>\n\nIncorrect options:\n\n<strong>Use CloudTrail Event rules with Amazon Simple Email Service (SES)</strong> - This is an incorrect statement. There is no such thing as CloudTrail Event Rule.\n\n<strong>Use Lambda function with Amazon Simple Notification Service (SNS)</strong> - Lambda functions can be triggered by the use of CloudWatch Event Rules as discussed above. AWS CodePipeline does not trigger Lambda functions directly.\n\n<strong>Use Lambda Event Rules</strong> - This is an incorrect statement. There is no such thing as Lambda Event Rule.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html">https://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html</a>\n'},{question:"As a Developer Associate, you are responsible for the data management of the AWS Kinesis streams at your company. The security team has mandated stricter security requirements by leveraging mechanisms available with the Kinesis Data Streams service that won't require code changes on your end.\n\nWhich of the following features meet the given requirements? (Select two)\n",answers:[{text:"Envelope Encryption",isCorrect:!1},{text:"SSE-C encryption",isCorrect:!1},{text:"Encryption in flight with HTTPS endpoint",isCorrect:!0},{text:"Client-Side Encryption",isCorrect:!1},{text:"KMS encryption for data at rest",isCorrect:!0}],explanation:"Correct options:\n\n<strong>KMS encryption for data at rest</strong>\n\n<strong>Encryption in flight with HTTPS endpoint</strong>\n\nServer-side encryption is a feature in Amazon Kinesis Data Streams that automatically encrypts data before it's at rest by using an AWS KMS customer master key (CMK) you specify. Data is encrypted before it's written to the Kinesis stream storage layer and decrypted after it's retrieved from storage. As a result, your data is encrypted at rest within the Kinesis Data Streams service. Also, the HTTPS protocol ensures that data inflight is encrypted as well.\n\nIncorrect options:\n\n<strong>SSE-C encryption</strong> - SSE-C is functionality in Amazon S3 where S3 encrypts your data, on your behalf, using keys that you provide. This does not apply for the given use-case.\n\n<strong>Client-Side Encryption</strong> - This involves code changes, so the option is incorrect.\n\n<strong>Envelope Encryption</strong> - This involves code changes, so the option is incorrect.\n\nReference:\n\n<a href=\"https://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html\">https://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html</a>\n"},{question:"An e-commerce application writes log files into Amazon S3. The application also reads these log files in parallel on a near real-time basis. The development team wants to address any data discrepancies that might arise when the application overwrites an existing log file and then tries to read that specific log file.\n\nWhich of the following options BEST describes the capabilities of Amazon S3 relevant to this scenario?\n",answers:[{text:"A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data",isCorrect:!1},{text:"A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data",isCorrect:!1},{text:"A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data",isCorrect:!1},{text:"A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object",isCorrect:!0}],explanation:'Correct option:\n\n<strong>A process replaces an existing object and immediately tries to read it. Amazon S3 always returns the latest version of the object</strong>\n\nAmazon S3 delivers strong read-after-write consistency automatically, without changes to performance or availability, without sacrificing regional isolation for applications, and at no additional cost.\n\nAfter a successful write of a new object or an overwrite of an existing object, any subsequent read request immediately receives the latest version of the object. S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected.\n\nStrong read-after-write consistency helps when you need to immediately read an object after a write. For example, strong read-after-write consistency when you often read and list immediately after writing objects.\n\nTo summarize, all S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are strongly consistent. What you write is what you will read, and the results of a LIST will be an accurate reflection of what’s in the bucket.\n\nIncorrect options:\n\n<strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data</strong>\n\n<strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 does not return any data</strong>\n\n<strong>A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the new data</strong>\n\nThese three options contradict the earlier details provided in the explanation.\n\nReferences:\n\n<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel">https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel</a>\n\n<a href="https://aws.amazon.com/s3/faqs/">https://aws.amazon.com/s3/faqs/</a>\n'},{question:"As a site reliability engineer, you are responsible for improving the company’s deployment by scaling and automating applications. As new application versions are ready for production you ensure that the application gets deployed to different sets of EC2 instances at different times allowing for a smooth transition.\n\nUsing AWS CodeDeploy, which of the following options will allow you to do this?\n",answers:[{text:"CodeDeploy Hooks",isCorrect:!1},{text:"CodeDeploy Agent",isCorrect:!1},{text:"CodeDeploy Deployment Groups",isCorrect:!0},{text:"Define multiple CodeDeploy Applications",isCorrect:!1}],explanation:'Correct option:\n\n<strong>CodeDeploy Deployment Groups</strong>\n\nYou can specify one or more deployment groups for a CodeDeploy application. The deployment group contains settings and configurations used during the deployment. Most deployment group settings depend on the compute platform used by your application. Some settings, such as rollbacks, triggers, and alarms can be configured for deployment groups for any compute platform.\n\nIn an EC2/On-Premises deployment, a deployment group is a set of individual instances targeted for deployment. A deployment group contains individually tagged instances, Amazon EC2 instances in Amazon EC2 Auto Scaling groups, or both.\n\nIncorrect options:\n\n<strong>CodeDeploy Agent</strong> - The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The agent connects the EC2 instances to the CodeDeploy service.\n\n<strong>CodeDeploy Hooks</strong> - Hooks are found in the AppSec file used by AWS CodeDeploy to manage deployment. Hooks correspond to lifecycle events such as ApplicationStart, ApplicationStop, etc. to which you can assign a script.\n\n<strong>Define multiple CodeDeploy Applications</strong> - This option has been added as a distractor. Instead, you want to use deployment groups to use the same deployment and maybe separate the times when a group of instances receives the software updates.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html</a>\n'},{question:"Your company has a load balancer in a VPC configured to be internet facing. The public DNS name assigned to the load balancer is myDns-1234567890.us-east-1.elb.amazonaws.com. When your client applications first load they capture the load balancer DNS name and then resolve the IP address for the load balancer so that they can directly reference the underlying IP.\n\nIt is observed that the client applications work well but unexpectedly stop working after a while. What is the reason for this?\n",answers:[{text:"The load balancer is highly available and its public IP may change. The DNS name is constant",isCorrect:!0},{text:"Your security groups are not stable",isCorrect:!1},{text:"You need to enable stickiness",isCorrect:!1},{text:"You need to disable multi-AZ deployments",isCorrect:!1}],explanation:'Correct option:\n\n<strong>The load balancer is highly available and its public IP may change. The DNS name is constant</strong>\n\nWhen your load balancer is created, it receives a public DNS name that clients can use to send requests. The DNS servers resolve the DNS name of your load balancer to the public IP addresses of the load balancer nodes for your load balancer. Never resolve the IP of a load balancer as it can change with time. You should always use the DNS name.\n\nIncorrect options:\n\n<strong>Your security groups are not stable</strong> - You security groups to allow your load balancer to work with registered instances. It is stable if set correctly. If your application is working and stops after a while, the issue is not with the security groups.\n\n<strong>You need to enable stickiness</strong> - This enables the load balancer to bind a user\'s session to a specific instance, so this has no impact on the issue described in the given use-case.\n\n<strong>You need to disable multi-AZ deployments</strong> - This has been added as a distractor and this has no bearing on the use-case. The change is happening with the IP of the load balancer.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html</a>\n'},{question:"A developer is creating a RESTful API service using an Amazon API Gateway with AWS Lambda integration. The service must support different API versions for testing purposes.\n\nAs a Developer Associate, which of the following would you suggest as the best way to accomplish this?\n",answers:[{text:"Use an X-Version header to identify which version is being called and pass that header to the Lambda function",isCorrect:!1},{text:"Use an API Gateway Lambda authorizer to route API clients to the correct API version",isCorrect:!1},{text:"Set up an API Gateway resource policy to identify the API versions and provide context to the Lambda function",isCorrect:!1},{text:"Deploy the API versions as unique stages with unique endpoints and use stage variables to provide the context to identify the API versions",isCorrect:!0}],explanation:'Correct option:\n\n<strong>Deploy the API versions as unique stages with unique endpoints and use stage variables to provide the context to identify the API versions</strong> - A stage is a named reference to a deployment, which is a snapshot of the API. You use a Stage to manage and optimize a particular deployment. For example, you can configure stage settings to enable caching, customize request throttling, configure logging, define stage variables, or attach a canary release for testing.\n\nStage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. They act like environment variables and can be used in your API setup and mapping templates.\n\nWith deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints.\n\nFor example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, http://example.com). In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage, and calls a different web host (for example, beta.example.com). Similarly, stage variables can be used to specify a different AWS Lambda function name for each stage in your API.\n\nIncorrect options:\n\n<strong>Use an X-Version header to identify which version is being called and pass that header to the Lambda function</strong> - This is an invalid option, given only as a distractor.\n\n<strong>Use an API Gateway Lambda authorizer to route API clients to the correct API version</strong> - A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller\'s identity.\n\n<strong>Set up an API Gateway resource policy to identify the API versions and provide context to the Lambda function</strong> - Amazon API Gateway resource policies are JSON policy documents that you attach to an API to control whether a specified principal (typically an IAM user or role) can invoke the API. You can use API Gateway resource policies to allow your API to be securely invoked requestors. They are not meant for choosing the version of APIs.\n\nReferences:\n\n<a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html</a>\n\n<a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html</a>\n\n<a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a>\n'},{question:"A company wants to implement authentication for its new RESTful API service that uses Amazon API Gateway. To authenticate the calls, each request must include HTTP headers with a client ID and user ID. These credentials must be compared to the authentication data in a DynamoDB table.\n\nAs an AWS Certified Developer Associate, which of the following would you recommend for implementing this authentication in API Gateway? \n",answers:[{text:"Develop an AWS Lambda authorizer that references the authentication data in the DynamoDB table",isCorrect:!0},{text:"Set up an API Gateway Model that requires the credentials, then grant API Gateway access to the authentication table in DynamoDB",isCorrect:!1},{text:"Update the API Gateway integration requests to require the credentials, then grant API Gateway access to the authentication table in DynamoDB",isCorrect:!1},{text:"Authorize using Amazon Cognito that will reference the authentication table of DynamoDB",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Develop an AWS Lambda authorizer that references the DynamoDB authentication table</strong> - A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API.\n\nA Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller\'s identity.\n\nWhen a client makes a request to one of your API\'s methods, API Gateway calls your Lambda authorizer, which takes the caller\'s identity as input and returns an IAM policy as output.\n\nThere are two types of Lambda authorizers:\n\n<ol>\n<li>A token-based Lambda authorizer (also called a TOKEN authorizer) receives the caller\'s identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token.</li>\n<li>A request parameter-based Lambda authorizer (also called a REQUEST authorizer) receives the caller\'s identity in a combination of headers, query string parameters, state variables, and $context variables.</li>\n</ol>\n\nAPI Gateway Lambda authorization workflow:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q55-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a>\n\nIncorrect options:\n\n<strong>Set up an API Gateway Model that requires the credentials, then grant API Gateway access to the authentication table in DynamoDB</strong> - In API Gateway, a model defines the data structure of a payload. In API Gateway models are defined using the JSON schema draft 4. Models are not mandatory.\n\n<strong>Update the API Gateway integration requests to require the credentials, then grant API Gateway access to the authentication table in DynamoDB</strong> - After setting up an API method, you must integrate it with an endpoint in the backend. A backend endpoint is also referred to as an integration endpoint and can be a Lambda function, an HTTP webpage, or an AWS service action.\n\nAn integration request is an HTTP request that API Gateway submits to the backend, passing along the client-submitted request data, and transforming the data, if necessary. The HTTP method (or verb) and URI of the integration request are dictated by the backend (that is, the integration endpoint). They can be the same as or different from the method request\'s HTTP method and URI, respectively.\n\n<strong>Authorize using Amazon Cognito that will reference the authentication table of DynamoDB</strong> - As an alternative to using IAM roles and policies or Lambda authorizers (formerly known as custom authorizers), you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway.\n\nTo use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user in to the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request\'s Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn\'t authorized to make the call because the client did not have credentials that could be authorized.\n\nReferences:\n\n<a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html</a>\n\n<a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html</a>\n\n<a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-integration-settings.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-integration-settings.html</a>\n\n<a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/models-mappings.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/models-mappings.html</a>\n'},{question:"A development team has created AWS CloudFormation templates that are reusable by taking advantage of input parameters to name resources based on client names.\n\nYou would like to save your templates on the cloud, which storage option should you choose?\n",answers:[{text:"EFS",isCorrect:!1},{text:"EBS",isCorrect:!1},{text:"S3",isCorrect:!0},{text:"ECR",isCorrect:!1}],explanation:'Correct option:\n\n<strong>S3</strong>\n\nIf you upload a local template file, AWS CloudFormation uploads it to an Amazon Simple Storage Service (Amazon S3) bucket in your AWS account. If you don\'t already have an S3 bucket that was created by AWS CloudFormation, it creates a unique bucket for each region in which you upload a template file. If you already have an S3 bucket that was created by AWS CloudFormation in your AWS account, AWS CloudFormation adds the template to that bucket.\n\nSelecting a stack template for CloudFormation:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q56-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html</a>\n\nIncorrect options:\n\n<strong>EBS</strong> - An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible. Amazon EBS is a recommended storage option when data must be quickly accessible and requires long-term persistence. EBS cannot be used for selecting a stack template for CloudFormation.\n\n<strong>EFS</strong> - EFS is a file storage service where you mount the file system on an Amazon EC2 Linux-based instance which is not an option for CloudFormation.\n\n<strong>ECR</strong> - Amazon ECR eliminates the need to operate your container repositories or worry about scaling the underlying infrastructure which does not apply to CloudFormation.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-template.html</a>\n'},{question:"Your e-commerce company needs to improve its software delivery process and is moving away from the waterfall methodology. You decided that every application should be built using the best CI/CD practices and every application should be packaged and deployed as a Docker container. The Docker images should be stored in ECR and pushed with AWS CodePipeline and AWS CodeBuild.\n\nWhen you attempt to do this, the last step fails with an authorization issue. What is the most likely issue?\n",answers:[{text:"CodeBuild cannot talk to ECR because of security group issues",isCorrect:!1},{text:"The ECR repository is stale, you must delete and re-create it",isCorrect:!1},{text:"The IAM permissions are wrong for the CodeBuild service",isCorrect:!0},{text:"The ECS instances are misconfigured and must contain additional data in /etc/ecs/ecs.config",isCorrect:!1}],explanation:'Correct option:\n\n<strong>The IAM permissions are wrong for the CodeBuild service</strong>\n\nYou can push your Docker or Open Container Initiative (OCI) images to an Amazon ECR repository with the docker push command.\n\nAmazon ECR users require permission to call ecr:GetAuthorizationToken before they can authenticate to a registry and push or pull any images from any Amazon ECR repository. Amazon ECR provides several managed policies to control user access at varying levels\n\nIncorrect options:\n\n<strong>The ECR repository is stale, you must delete and re-create it</strong> - You can delete a repository when you are done using it, stale is not a concept within ECR. This option has been added as a distractor.\n\n<strong>CodeBuild cannot talk to ECR because of security group issues</strong> - A security group acts as a virtual firewall at the instance level and it is not related to pushing Docker images, so this option does not fit the given use-case.\n\n<strong>The ECS instances are misconfigured and must contain additional data in /etc/ecs/ecs.config</strong> - The error Authorization is an indication that there is an access issue, therefore you should not look at your configuration first but rather permissions.\n\nReferences:\n\n<a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html">https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html</a>\n\n<a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html">https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html</a>\n'},{question:"You are a DynamoDB developer for an aerospace company that requires you to write 6 objects per second of 4.5KB in size each.\n\nWhat write capacity unit is needed for your project?\n",answers:[{text:"46",isCorrect:!1},{text:"24",isCorrect:!1},{text:"15",isCorrect:!1},{text:"30",isCorrect:!0}],explanation:'Correct option:\n\nBefore proceeding with the calculations, please review the following:\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q58-i1.jpg">\n\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q58-i2.jpg">\n\nvia - <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a>\n\n<strong>30</strong>\n\nA write capacity unit represents one write per second, for an item up to 1 KB in size.\n\nItem sizes for writes are rounded up to the next 1 KB multiple. For example, writing a 500-byte item consumes the same throughput as writing a 1 KB item. So, for the given use-case, each object is of size 4.5 KB, which will be rounded up to 5KB.\n\nTherefore, for 6 objects, you need 6x5 = 30 WCUs.\n\nIncorrect options:\n\n<strong>24</strong>\n\n<strong>15</strong>\n\n<strong>46</strong>\n\nThese three options contradict the details provided in the explanation above, so these are incorrect.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html</a>\n'},{question:"Your web application front end consists of 5 EC2 instances behind an Application Load Balancer. You have configured your web application to capture the IP address of the client making requests. When viewing the data captured you notice that every IP address being captured is the same, which also happens to be the IP address of the Application Load Balancer.\n\nWhat should you do to identify the true IP address of the client?\n",answers:[{text:"Look into the X-Forwarded-Proto header in the backend",isCorrect:!1},{text:"Modify the front-end of the website so that the users send their IP in the requests",isCorrect:!1},{text:"Look into the X-Forwarded-For header in the backend",isCorrect:!0},{text:"Look into the client's cookie",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Look into the X-Forwarded-For header in the backend</strong>\n\nThe X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header. Elastic Load Balancing stores the IP address of the client in the X-Forwarded-For request header and passes the header to your server.\n\nIncorrect options:\n\n<strong>Modify the front-end of the website so that the users send their IP in the requests</strong> - When a user makes a request the IP address is sent with the request to the server and the load balancer intercepts it. There is no need to modify the application.\n\n<strong>Look into the X-Forwarded-Proto header in the backend</strong> - The X-Forwarded-Proto request header helps you identify the protocol (HTTP or HTTPS) that a client used to connect to your load balancer.\n\n<strong>Look into the client\'s cookie</strong> - For this, we would need to modify the client-side logic and server-side logic, which would not be efficient.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html</a>\n'},{question:"Your team has just signed up an year-long contract with a client maintaining a three-tier web application, that needs to be moved to AWS Cloud. The application has steady traffic throughout the day and needs to be on a reliable system with no down-time or access issues. The solution needs to be cost-optimal for this startup.\n\nWhich of the following options should you choose?\n",answers:[{text:"Amazon EC2 Spot Instances",isCorrect:!1},{text:"Amazon EC2 On Demand Instances",isCorrect:!1},{text:"Amazon EC2 Reserved Instances",isCorrect:!0},{text:"On-premise EC2 instance",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Amazon EC2 Reserved Instances</strong> - Reserved instances can provide a capacity reservation, offering additional confidence in your ability to launch the number of instances you have reserved when you need them. You save money going with Reserved instances vs on-demand especially in a year\'s worth of time.\n\nReserved Instances are not physical instances, but rather a billing discount applied to the use of On-Demand Instances in your account. These On-Demand Instances must match certain attributes, such as instance type and Region, to benefit from the billing discount. So, there is no performance difference between an On-Demand instance or a Reserved instance.\n\nHow RIs work:\n<img src="https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt5-q60-i1.jpg">\nvia - <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html</a>\n\nIncorrect options:\n\n<strong>Amazon EC2 Spot Instances</strong> - A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. Spot instances are useful if your applications can be interrupted, like data analysis, batch jobs, background processing, and optional tasks. Spot instances can be pulled down anytime without prior notice. Hence, not the right choice for the current scenario.\n\n<strong>Amazon EC2 On-Demand Instances</strong> - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. But, On-Demand instances cost a lot more than Reserved instances. Here, in our use case, we already know that the systems are required for a complete year, so making use of Reserved Instances discount makes a lot more sense.\n\n<strong>On-premise EC2 instance</strong> - On-premise implies the client has to maintain the physical machines, their capacity provisioning and maintenance. Not an option when the client is planning to move to AWS Cloud.\n\nReferences:\n\n<a href="https://aws.amazon.com/ec2/pricing/reserved-instances/">https://aws.amazon.com/ec2/pricing/reserved-instances/</a>\n\n<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html</a>\n\n<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html</a>\n\n<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html</a>\n'},{question:"A development team has noticed that one of the EC2 instances has been wrongly configured with the 'DeleteOnTermination' attribute set to True for its root EBS volume.\n\nAs a developer associate, can you suggest a way to disable this flag while the instance is still running?\n",answers:[{text:"Set the DeleteOnTermination attribute to False using the command line",isCorrect:!0},{text:"Update the attribute using AWS management console. Select the EC2 instance and then uncheck the Delete On Termination check box for the root EBS volume",isCorrect:!1},{text:"The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag",isCorrect:!1},{text:"Set the DisableApiTermination attribute of the instance using the API",isCorrect:!1}],explanation:'Correct option:\n\nWhen an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete the volume. By default, the DeleteOnTermination attribute is set to True for the root volume and is set to False for all other volume types.\n\n<strong>Set the DeleteOnTermination attribute to False using the command line</strong> - If the instance is already running, you can set DeleteOnTermination to False using the command line.\n\nIncorrect options:\n\n<strong>Update the attribute using AWS management console. Select the EC2 instance and then uncheck the Delete On Termination check box for the root EBS volume</strong> - You can set the DeleteOnTermination attribute to False when you launch a new instance. It is not possible to update this attribute of a running instance from the AWS console.\n\n<strong>Set the DisableApiTermination attribute of the instance using the API</strong> - By default, you can terminate your instance using the Amazon EC2 console, command-line interface, or API. To prevent your instance from being accidentally terminated using Amazon EC2, you can enable termination protection for the instance. The DisableApiTermination attribute controls whether the instance can be terminated using the console, CLI, or API. This option cannot be used to control the delete status for the EBS volume when the instance terminates.\n\n<strong>The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag</strong> - This statement is wrong and given only as a distractor.\n\nReferences:\n\n<a href="https://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/">https://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/</a>\n\n<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance</a>\n'},{question:"The development team at a health-care company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon RDS as the database tier for its flagship application.\n\nWhich of the following would you identify as correct for RDS Multi-AZ? (Select two)\n",answers:[{text:"To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests",isCorrect:!1},{text:"RDS applies OS updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby",isCorrect:!0},{text:"Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason",isCorrect:!0},{text:"For automated backups, I/O activity is suspended on your primary DB since backups are not taken from standby DB",isCorrect:!1},{text:"Updates to your DB Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync",isCorrect:!1}],explanation:'Correct options:\n\n<strong>RDS applies OS updates by performing maintenance on the standby, then promoting the standby to primary, and finally performing maintenance on the old primary, which becomes the new standby</strong>\n\nRunning a DB instance as a Multi-AZ deployment can further reduce the impact of a maintenance event because Amazon RDS applies operating system updates by following these steps:\n\nPerform maintenance on the standby.\n\nPromote the standby to primary.\n\nPerform maintenance on the old primary, which becomes the new standby.\n\nWhen you modify the database engine for your DB instance in a Multi-AZ deployment, then Amazon RDS upgrades both the primary and secondary DB instances at the same time. In this case, the database engine for the entire Multi-AZ deployment is shut down during the upgrade.\n\n<strong>Amazon RDS automatically initiates a failover to the standby, in case the primary database fails for any reason</strong> - You also benefit from enhanced database availability when running your DB instance as a Multi-AZ deployment. If an Availability Zone failure or DB instance failure occurs, your availability impact is limited to the time automatic failover takes to complete.\n\nAnother implied benefit of running your DB instance as a Multi-AZ deployment is that DB instance failover is automatic and requires no administration. In an Amazon RDS context, this means you are not required to monitor DB instance events and initiate manual DB instance recovery in the event of an Availability Zone failure or DB instance failure.\n\nIncorrect options:\n\n<strong>For automated backups, I/O activity is suspended on your primary DB since backups are not taken from standby DB</strong> - The availability benefits of Multi-AZ also extend to planned maintenance. For example, with automated backups, I/O activity is no longer suspended on your primary during your preferred backup window, since backups are taken from the standby.\n\n<strong>To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests</strong> - A Multi-AZ standby cannot serve read requests. Multi-AZ deployments are designed to provide enhanced database availability and durability, rather than read scaling benefits. As such, the feature uses synchronous replication between primary and standby. AWS implementation makes sure the primary and the standby are constantly in sync, but precludes using the standby for read or write operations.\n\n<strong>Updates to your DB Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync</strong> - When you create your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous “standby” replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across the Availability Zone to the standby in order to keep both in sync and protect your latest database updates against DB instance failure.\n\nReference:\n\n<a href="https://aws.amazon.com/rds/faqs/">https://aws.amazon.com/rds/faqs/</a>\n'},{question:"A company stores confidential data on an Amazon Simple Storage Service (S3) bucket. New regulatory guidelines require that files be stored with server-side encryption. The encryption used must be Advanced Encryption Standard (AES-256) and the company does not want to manage S3 encryption keys.\n\nWhich of the following options should you use?\n",answers:[{text:"SSE-S3",isCorrect:!0},{text:"SSE-C",isCorrect:!1},{text:"Client Side Encryption",isCorrect:!1},{text:"SSE-KMS",isCorrect:!1}],explanation:'Correct option:\n\n<strong>SSE-S3</strong>\n\nUsing Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key employing strong multi-factor encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.\n\nIncorrect options:\n\n<strong>SSE-C</strong> - You manage the encryption keys and Amazon S3 manages the encryption as it writes to disks and decryption when you access your objects.\n\n<strong>Client-Side Encryption</strong> - You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.\n\n<strong>SSE-KMS</strong> - Similar to SSE-S3 and also provides you with an audit trail of when your key was used and by whom. Additionally, you have the option to create and manage encryption keys yourself.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a>\n'},{question:"A company is looking at storing their less frequently accessed files on AWS that can be concurrently accessed by hundreds of EC2 instances. The company needs the most cost-effective file storage service that provides immediate access to data whenever needed.\n\nWhich of the following options represents the best solution for the given requirements?\n",answers:[{text:"Amazon Elastic File System (EFS) Standard–IA storage class",isCorrect:!0},{text:"Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class",isCorrect:!1},{text:"Amazon Elastic File System (EFS) Standard storage class",isCorrect:!1},{text:"Amazon Elastic Block Store (EBS)",isCorrect:!1}],explanation:'Correct option:\n\n<strong>Amazon Elastic File System (EFS) Standard–IA storage class</strong> - Amazon EFS is a file storage service for use with Amazon compute (EC2, containers, serverless) and on-premises servers. Amazon EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), and concurrently accessible storage for up to thousands of Amazon EC2 instances.\n\nThe Standard–IA storage class reduces storage costs for files that are not accessed every day. It does this without sacrificing the high availability, high durability, elasticity, and POSIX file system access that Amazon EFS provides. AWS recommends Standard-IA storage if you need your full dataset to be readily accessible and want to automatically save on storage costs for files that are less frequently accessed.\n\nIncorrect options:\n\n<strong>Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class</strong> - Amazon S3 is an object storage service. Amazon S3 makes data available through an Internet API that can be accessed anywhere. It is not a file storage service, as is needed in the use case.\n\n<strong>Amazon Elastic File System (EFS) Standard storage class</strong> - Amazon EFS Standard storage classes are ideal for workloads that require the highest levels of durability and availability. The EFS Standard storage class is used for frequently accessed files. It is the storage class to which customer data is initially written for Standard storage classes. The company is also looking at cutting costs by optimally storing the infrequently accessed data. Hence, EFS standard storage class is not the right solution for the given use case.\n\n<strong>Amazon Elastic Block Store (EBS)</strong> - Amazon EBS is a block-level storage service for use with Amazon EC2. Amazon EBS can deliver performance for workloads that require the lowest latency access to data from a single EC2 instance. EBS volume cannot be accessed by hundreds of EC2 instances concurrently. It is not a file storage service, as is needed in the use case.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html">https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html</a>\n'},{question:"A development team has configured an Elastic Load Balancer for host-based routing. The idea is to support multiple subdomains and different top-level domains.\n\nThe rule *.sample.com matches which of the following?\n",answers:[{text:"sample.com",isCorrect:!1},{text:"sample.test.com",isCorrect:!1},{text:"SAMPLE.COM",isCorrect:!1},{text:"test.sample.com",isCorrect:!0}],explanation:'Correct option:\n\n<strong>test.sample.com</strong> - You can use host conditions to define rules that route requests based on the hostname in the host header (also known as host-based routing). This enables you to support multiple subdomains and different top-level domains using a single load balancer.\n\nA hostname is not case-sensitive, can be up to 128 characters in length, and can contain any of the following characters:\n1. A–Z, a–z, 0–9\n2. - .\n3. * (matches 0 or more characters)\n4. ? (matches exactly 1 character)\n\nYou must include at least one "." character. You can include only alphabetical characters after the final "." character.\n\nThe rule *.sample.com matches test.sample.com but doesn\'t match sample.com.\n\nIncorrect options:\n\n<strong>sample.com</strong>\n\n<strong>sample.test.com</strong>\n\n<strong>SAMPLE.COM</strong>\n\nThese three options contradict the explanation provided above, so these options are incorrect.\n\nReference:\n\n<a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a>\n'}]}]},3999:(e,t,a)=>{a.d(t,{cn:()=>s});var o=a(2596),n=a(9688);function s(){for(var e=arguments.length,t=Array(e),a=0;a<e;a++)t[a]=arguments[a];return(0,n.QP)((0,o.$)(t))}},6315:(e,t,a)=>{a.d(t,{A9:()=>s,C6:()=>c,Fc:()=>d,LZ:()=>m,O0:()=>l,Zk:()=>r,eZ:()=>i,kx:()=>u,zc:()=>h});let o="quiz_results",n="quiz_progress";function s(e){try{let t=l();t.push(e),localStorage.setItem(o,JSON.stringify(t))}catch(e){console.error("Failed to save quiz result:",e)}}function i(e){try{localStorage.setItem(n,JSON.stringify(e))}catch(e){console.error("Failed to save quiz progress:",e)}}function r(e){try{let t=localStorage.getItem(n);if(!t)return null;let a=JSON.parse(t);return a.quizId===e?a:null}catch(e){return console.error("Failed to load quiz progress:",e),null}}function c(){try{localStorage.removeItem(n)}catch(e){console.error("Failed to clear quiz progress:",e)}}function l(){try{let e=localStorage.getItem(o);return e?JSON.parse(e):[]}catch(e){return console.error("Failed to load quiz results:",e),[]}}function h(){try{localStorage.removeItem(o)}catch(e){console.error("Failed to clear quiz results:",e)}}function d(e){let t=l().filter(t=>t.quizId===e);if(0===t.length)return null;let a=Math.max(...t.map(e=>e.score.percentage)),o=t.reduce((e,t)=>e+t.score.percentage,0)/t.length;return{bestScore:a,averageScore:o,attempts:t.length,lastAttempt:t[t.length-1]}}function u(){return JSON.stringify(l(),null,2)}function m(e){try{let t=JSON.parse(e);if(!Array.isArray(t))throw Error("Invalid data format");let a=[...l(),...t];return localStorage.setItem(o,JSON.stringify(a)),!0}catch(e){return console.error("Failed to import quiz results:",e),!1}}},7168:(e,t,a)=>{a.d(t,{$:()=>c});var o=a(5155);a(2115);var n=a(4624),s=a(2085),i=a(3999);let r=(0,s.F)("inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive",{variants:{variant:{default:"bg-primary text-primary-foreground hover:bg-primary/90",destructive:"bg-destructive text-white hover:bg-destructive/90 focus-visible:ring-destructive/20 dark:focus-visible:ring-destructive/40 dark:bg-destructive/60",outline:"border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50",secondary:"bg-secondary text-secondary-foreground hover:bg-secondary/80",ghost:"hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50",link:"text-primary underline-offset-4 hover:underline"},size:{default:"h-9 px-4 py-2 has-[>svg]:px-3",sm:"h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5",lg:"h-10 rounded-md px-6 has-[>svg]:px-4",icon:"size-9","icon-sm":"size-8","icon-lg":"size-10"}},defaultVariants:{variant:"default",size:"default"}});function c(e){let{className:t,variant:a,size:s,asChild:c=!1,...l}=e,h=c?n.DX:"button";return(0,o.jsx)(h,{"data-slot":"button",className:(0,i.cn)(r({variant:a,size:s,className:t})),...l})}},7434:(e,t,a)=>{a.d(t,{A:()=>o});let o=(0,a(9946).A)("FileText",[["path",{d:"M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z",key:"1rqfz7"}],["path",{d:"M14 2v4a2 2 0 0 0 2 2h4",key:"tnqrlb"}],["path",{d:"M10 9H8",key:"b1mrlr"}],["path",{d:"M16 13H8",key:"t4e002"}],["path",{d:"M16 17H8",key:"z1uh3a"}]])},8186:(e,t,a)=>{a.d(t,{A:()=>o});let o=(0,a(9946).A)("Trophy",[["path",{d:"M6 9H4.5a2.5 2.5 0 0 1 0-5H6",key:"17hqa7"}],["path",{d:"M18 9h1.5a2.5 2.5 0 0 0 0-5H18",key:"lmptdp"}],["path",{d:"M4 22h16",key:"57wxv0"}],["path",{d:"M10 14.66V17c0 .55-.47.98-.97 1.21C7.85 18.75 7 20.24 7 22",key:"1nw9bq"}],["path",{d:"M14 14.66V17c0 .55.47.98.97 1.21C16.15 18.75 17 20.24 17 22",key:"1np0yb"}],["path",{d:"M18 2H6v7a6 6 0 0 0 12 0V2Z",key:"u46fv3"}]])},8482:(e,t,a)=>{a.d(t,{BT:()=>c,Wu:()=>l,ZB:()=>r,Zp:()=>s,aR:()=>i});var o=a(5155);a(2115);var n=a(3999);function s(e){let{className:t,...a}=e;return(0,o.jsx)("div",{"data-slot":"card",className:(0,n.cn)("bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm",t),...a})}function i(e){let{className:t,...a}=e;return(0,o.jsx)("div",{"data-slot":"card-header",className:(0,n.cn)("@container/card-header grid auto-rows-min grid-rows-[auto_auto] items-start gap-2 px-6 has-data-[slot=card-action]:grid-cols-[1fr_auto] [.border-b]:pb-6",t),...a})}function r(e){let{className:t,...a}=e;return(0,o.jsx)("div",{"data-slot":"card-title",className:(0,n.cn)("leading-none font-semibold",t),...a})}function c(e){let{className:t,...a}=e;return(0,o.jsx)("div",{"data-slot":"card-description",className:(0,n.cn)("text-muted-foreground text-sm",t),...a})}function l(e){let{className:t,...a}=e;return(0,o.jsx)("div",{"data-slot":"card-content",className:(0,n.cn)("px-6",t),...a})}}}]);